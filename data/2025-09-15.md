<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 64]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出使用文档级知识图谱(KG)来表示临床文档，用于自动化ICD-9编码任务。该方法能保留90%信息的同时减少23%文本量，在PLM-ICD架构上实现Macro-F1提升3.20%，并提高训练效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床文档标准化编码对临床研究、医院管理和患者护理至关重要，但人工编码耗时费力。自动化编码面临高维长尾目标空间的挑战，现有方法在利用外部知识表示输入文档方面研究不足。

Method: 构建文档级知识图谱来结构化表示患者为中心的输入文档，将KG集成到最先进的PLM-ICD编码架构中，利用KG中的实体和关系信息增强文档表示。

Result: KG表示仅需原文本23%的篇幅即可保留90%信息，在ICD-9自动化编码任务中Macro-F1得分提升最高达3.20%，同时提高了训练效率。

Conclusion: 基于知识图谱的结构化文档表示方法能有效提升临床编码任务的性能，不仅提高了准确率，还增强了模型的可解释性，为临床文档自动化处理提供了新思路。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出Cross-Layer Attention Probing (CLAP)方法，通过处理LLM整个残差流中的激活来检测幻觉，在多个模型和任务上优于基线方法，支持细粒度检测和检测后缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域的广泛应用，其生成不准确文本（幻觉）的可靠性问题日益突出，需要有效的检测方法来提高模型可靠性。

Method: 提出跨层注意力探测(CLAP)技术，将LLM整个残差流中的激活作为联合序列进行处理，用于幻觉检测。

Result: 在5个LLM和3个任务上的实验表明，CLAP在贪婪解码和高温采样响应中都优于基线方法，能够实现细粒度检测，并在分布外场景下保持高可靠性。

Conclusion: CLAP提供了一种有效的幻觉检测方法，支持检测后缓解策略，相比直接缓解方法能更好地减少幻觉并提高LLM可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文从正则化角度研究端到端语音翻译中的多任务学习，探索跨模态和同模态的正则化方法，提出正则化地平线概念，在MuST-C数据集上达到接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译面临配对语音-文本数据稀缺的问题，需要利用机器翻译的双语文本数据进行多任务学习来克服这一缺陷。

Method: 从正则化视角构建多任务学习框架，研究一致性正则化（跨模态）和R-drop（同模态）的作用，分析机器翻译损失系数作为另一种正则化源，提出正则化地平线概念来优化超参数调优。

Result: 实验表明，在正则化地平线内调优超参数能够在MuST-C数据集上实现接近最先进的性能。

Conclusion: 通过整合三种正则化源（一致性正则化、R-drop和MT损失系数）并基于正则化地平线进行超参数优化，可以有效提升端到端语音翻译的性能，解决数据稀缺问题。

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: Creativity Benchmark是一个评估大语言模型在营销创意领域表现的框架，通过人类创意专家对11,012次匿名比较的偏好分析，显示模型表现紧密聚集，没有单一模型在所有品牌或提示类型中占主导地位。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门针对营销创意领域的评估框架，以系统评估大语言模型在品牌约束创意任务中的表现，并比较自动化评估与人类专家评估的差异。

Method: 覆盖100个品牌（12个类别）和三种提示类型（洞察、想法、疯狂想法），收集678名创意从业者的人类成对偏好数据，使用Bradley-Terry模型分析，并计算余弦距离来评估模型多样性。

Result: 模型表现紧密聚集（Δθ≈0.45），头对头获胜概率为61%；自动化评估与人类排名相关性弱且不一致；传统创造力测试仅部分适用于品牌约束任务。

Conclusion: 需要专家人类评估和多样性感知的工作流程，自动化评估不能替代人类评估。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: CTCC是一种新颖的基于规则的语言模型指纹框架，通过多轮对话上下文相关性编码来实现所有权验证，解决了现有方法在隐蔽性、鲁棒性和泛化性方面的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛部署引发了知识产权保护担忧，现有指纹方法存在可检测性、易受对抗攻击和泛化能力不足等问题，需要更可靠的模型所有权验证方案。

Method: 提出CTCC框架，采用规则驱动方法在多轮对话中编码上下文相关性（如反事实关系），而非依赖词级或单轮触发，支持黑盒访问下的指纹验证。

Result: 在多LLM架构上的广泛实验表明，CTCC相比现有方法 consistently 实现了更强的隐蔽性和鲁棒性，有效减少误报和指纹泄露风险。

Conclusion: CTCC为实际LLM部署场景提供了可靠实用的所有权验证解决方案，支持在部分触发暴露情况下基于共享语义规则的持续构建。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 研究语言模型在跨期选择中表现出的未来vs现在导向偏好，以及这些偏好是否可被系统性操纵。通过人类实验协议评估多模型，提出可操纵性时间导向指标(MTO)，发现推理型模型在面向未来的提示下更倾向选择延迟选项。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在时间偏好决策中的行为特征，了解其是否具有可预测的时间导向偏好，以及这些偏好是否可以被提示词操纵，为AI助手与人类长期目标对齐提供设计依据。

Method: 采用改编的人类实验协议，在时间权衡任务中评估多个语言模型，并与人类决策者进行基准比较。引入MTO指标来衡量模型在面向未来和现在提示下的偏好变化。

Result: 推理型模型(如DeepSeek-Reasoner和grok-3-mini)在面向未来的提示下更倾向于选择延迟选项，但在跨身份或地理位置的个性化决策方面表现有限。能够正确推理时间导向的模型会内化作为AI决策者的未来导向。

Conclusion: 研究为设计应与异质长期目标对齐的AI助手提供了启示，并提出了个性化情境校准和社会意识部署的研究议程，强调需要开发能够理解和适应不同用户时间偏好的AI系统。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 本研究探讨了小型LLM（2B-8B参数）在重复回答相同问题时的稳定性，分析了不同温度设置、模型大小、微调状态等因素对答案一致性的影响，并提出了新的分析工具。


<details>
  <summary>Details</summary>
Motivation: 评估小型LLM在重复回答相同问题时的答案一致性，了解不同因素（如推理温度、模型大小、微调状态）对一致性的影响，以及一致性与准确性之间的权衡关系。

Method: 使用开源LLM对MMLU-Redux和MedQA多选基准测试中的问题进行10次重复回答，分析不同推理温度（0.1-1.0）、小型vs中型模型（50B-80B）、微调vs基础模型等参数的影响，并开发新的分析和可视化工具。

Result: 小型模型在低推理温度下，能够一致回答的问题比例通常在50%-80%之间；一致答案的准确性与总体准确性存在合理相关性；中型模型显示出更高的答案一致性水平。

Conclusion: LLM的答案一致性因模型而异，小型模型在低温设置下表现相对稳定，一致性与准确性存在正相关关系，中型模型在一致性方面表现更优，需要权衡一致性和准确性来选择最佳模型。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 该论文通过稀疏自编码器分析指令调优大语言模型的拒绝行为机制，开发了一个三阶段搜索流程来识别导致有害提示从拒绝转向合规的关键特征集，揭示了安全行为的可解释潜在空间。


<details>
  <summary>Details</summary>
Motivation: 理解指令调优大语言模型中拒绝有害提示的安全行为内部机制，目前这方面的因果理解还很缺乏。

Method: 使用稀疏自编码器分析残差流激活，通过三阶段搜索流程：(1)拒绝方向识别，(2)贪婪过滤最小特征集，(3)交互发现使用因子分解机捕捉非线性交互。

Result: 成功识别出导致jailbreak的关键特征集，发现了冗余特征的存在，这些特征在早期特征被抑制时才会激活。

Conclusion: 研究展示了通过操纵可解释潜在空间进行细粒度审计和针对性干预安全行为的潜力，为理解模型拒绝机制提供了新见解。

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 该研究提出基于内容质量和参考文献有效性的量化评估指标，以及基于这些指标的迭代提示方法，显著提升了ChatGPT在学术写作中的表现并减少了参考文献错误和伪造问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在学术写作中日益普及，但存在参考文献错误或伪造等伦理问题，且当前的内容质量评估主要依赖主观人工判断，缺乏客观性和一致性。

Method: 提出两个关键评估指标（内容质量和参考文献有效性），并基于这些指标的得分设计迭代提示方法。通过大量实验验证方法的有效性。

Result: 提出的指标为评估ChatGPT写作性能提供了客观的量化框架，迭代提示方法显著提升了内容质量，同时减少了参考文献的不准确性和伪造问题。

Conclusion: 该方法有效解决了学术环境中ChatGPT使用中的关键伦理挑战，为LLM在学术写作中的量化评估和质量提升提供了可行方案。

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出使用大型语言模型生成个体出行日记的新方法，通过开源数据生成虚拟人物并合成出行记录，验证显示LLM在出行目的识别方面表现优异，整体真实性与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统出行模型依赖大量专有家庭出行调查数据，成本高且获取困难。本研究旨在开发基于开源数据的LLM方案，实现零样本生成具有统计代表性的个体出行日记。

Method: 使用美国社区调查和智能位置数据库开源数据随机生成虚拟人物，通过直接提示词生成出行日记。提出四指标综合真实性评分（出行次数、时间间隔、目的、方式），采用Jensen-Shannon散度验证生成日记与真实日记的分布相似性。

Result: LLM生成日记整体真实性评分0.485，与传统方法0.455相当。LLM在出行目的识别方面表现优异，且一致性更高（评分分布更窄），传统方法在出行次数和活动时长数值估计方面更优。聚合验证显示LLM统计代表性更好（0.612 vs 0.435）。

Conclusion: LLM方法在零样本条件下可行，建立了可量化的日记真实性评估指标，为未来合成出行日记评估系统提供了新范式。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: PsychiatryBench是一个基于权威精神病学教科书构建的基准测试，包含5300多个专家标注项目，评估显示前沿LLMs在临床一致性和安全性方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源主要依赖小型临床访谈语料库、社交媒体帖子或合成对话，限制了临床有效性，无法捕捉精神病学推理的复杂性。

Method: 基于权威专家验证的精神病学教科书和案例集，构建11个不同的问答任务，包括诊断推理、治疗计划等，使用传统指标和LLM-as-judge相似性评分框架评估多种LLMs。

Result: 结果显示在临床一致性和安全性方面存在显著差距，特别是在多轮随访和管理任务中。

Conclusion: 需要专门的模型调优和更强大的评估范式，PsychiatryBench为高风险心理健康应用中的LLM性能基准测试和改进提供了模块化、可扩展的平台。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 本研究比较了SFT和ORPO两种训练方法对小型LLM进行ACT治疗的能力影响，发现ORPO方法在治疗忠诚度和共情方面显著优于SFT和基础模型，而思维链推理仅对SFT模型有显著帮助。


<details>
  <summary>Details</summary>
Motivation: 探索不同后训练方法和显式推理对小型开源大语言模型提供接纳承诺疗法(ACT)能力的影响，以提升AI在心理治疗领域的应用效果。

Method: 使用Mistral-Large生成的50组合成ACT转录本，对Llama-3.2-3b-Instruct进行两种训练：监督微调(SFT)和几率比策略优化(ORPO)，每种方法都包含有/无思维链推理步骤。通过模拟治疗会话评估模型性能。

Result: ORPO训练模型在ACT忠诚度(χ²=185.15, p<.001)和治疗共情(χ²=140.37, p<.001)方面显著优于SFT和基础模型。思维链推理仅对SFT模型有显著益处(平均提升2.68分，p<.001)，对ORPO模型无显著优势。

Conclusion: 偏好对齐策略优化能有效培养小型LLM的ACT能力，显式推理的效用高度依赖于底层训练范式，ORPO通过学习治疗过程而非模仿内容展现出优势。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: HANRAG是一个基于启发式的RAG框架，通过查询路由、子查询分解和噪声过滤，有效解决多跳查询中的噪声积累和检索效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在处理多跳查询时存在过度依赖迭代检索、无法捕获子查询相关内容导致噪声积累等问题，需要更高效的解决方案。

Method: 提出HANRAG框架，使用强大的revealator进行查询路由、查询分解为子查询，并对检索文档进行噪声过滤。

Result: 在多个基准测试中，HANRAG在单跳和多跳问答任务上都取得了优于其他行业领先方法的性能表现。

Conclusion: HANRAG框架通过创新的启发式方法显著提升了RAG系统处理多样化查询的适应性和抗噪声能力。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本研究系统评估了18种语义相似度测量方法，发现常用指标存在严重问题，某些嵌入方法将语义相反的文本误判为相似度高达99.9%，而LLM方法在区分语义差异方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码搜索、API推荐等软件工程应用中广泛用于语义相似度评估，需要验证这些方法是否真正理解语义关系还是仅识别表面模式。

Method: 研究测试了18种不同方法（基于词、嵌入、LLM和结构感知算法），建立了系统测试框架，通过受控文本和代码变化来评估各方法处理不同类型语义关系的能力。

Result: 嵌入方法存在严重问题，某些情况下将语义相反内容误判为99.9%相似；从欧几里得距离切换到余弦相似度可提升性能24-66%；LLM方法表现更好，对真正不同的含义给出低相似度分数（0.00-0.29）。

Conclusion: 当前常用的语义相似度测量方法存在显著缺陷，需要更可靠的评估方法；距离计算方式的选择对嵌入方法性能影响很大；LLM在语义区分方面表现优于传统嵌入方法。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [15] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 本研究识别并表征了导致大语言模型产生幻觉的关键符号属性，发现模型规模增大能降低幻觉率但无法完全消除，修饰词和命名实体是主要的幻觉来源。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的幻觉问题已被广泛研究，但导致模型内在易产生幻觉的属性尚未被识别和研究。本研究旨在找出这些关键属性并分析模型内部机制的脆弱性。

Method: 使用HaluEval和TruthfulQA两个数据集，将原有的问答格式转换为多种其他格式，以确定导致幻觉的属性。测试了Gemma-2系列不同规模的模型(2B, 9B, 27B)。

Result: Gemma-2-2B的平均幻觉率为79.0%，随着模型规模增大，Gemma-2-9B降至73.6%，Gemma-2-27B降至63.9%。修饰词(84.76%-94.98%)和命名实体(83.87%-93.96%)在所有模型和数据集上都是主要的幻觉来源。

Conclusion: 符号元素持续使模型混淆，这表明大语言模型在处理此类输入时存在根本性弱点，且这种弱点不随模型规模增大而完全消除。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [16] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS是一个基于大型语言模型的系统，用于生成包含55万+指标的综合nomological网络，解决心理测量中构建理论网络的长期挑战。


<details>
  <summary>Details</summary>
Motivation: 自Cronbach和Meehl提出nomological网络70年来，构建理论网络一直是测量验证的核心挑战，这导致临床试验可能无法检测治疗效果，公共政策可能针对错误结果。

Method: 使用经过验证的问卷测量训练的大型语言模型系统ALIGNS，提供三个综合nomological网络，涵盖心理学、医学、社会政策等领域。

Result: 系统在三个评估中表现良好：发现NIH PROMIS焦虑和抑郁工具收敛为单一情绪困扰维度；识别儿童气质测量的四个新维度并质疑现有维度；专家评估显示系统具有重要性、可访问性和适用性。

Conclusion: ALIGNS是首个应用大型语言模型解决测量验证基础问题的系统，免费提供于nomologicalnetwork.org，为传统验证方法提供大规模nomological分析补充。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [17] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 基于技术间时序关系识别新兴技术机会的框架，利用专利文本分析和大型语言模型提取技术主题并追踪其演变


<details>
  <summary>Details</summary>
Motivation: 技术机会是推动技术、产业和创新进步的关键信息，需要系统化方法来识别新兴技术趋势和发展机会

Method: 从专利数据集中提取文本，将基于文本的主题映射以发现技术间关系，通过追踪主题随时间变化来识别技术机会，利用大型语言模型提取主题和聊天模型提示支持机会发现

Result: 使用美国专利商标局提供的人工智能专利数据集进行评估，实验结果表明人工智能技术正在向便于日常访问的形式演变

Conclusion: 该框架展示了识别未来技术机会的潜力，为技术发展和创新提供了有价值的分析工具

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [18] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 提出了一个轻量级双阶段生物医学实体链接系统BIBERT-Pipe，专门处理多语言嵌套命名实体链接问题，在BioNNE 2025评测中排名第三


<details>
  <summary>Details</summary>
Motivation: 现有生物医学实体链接研究主要针对英语单语和平坦实体，缺乏对多语言和嵌套实体的系统研究

Method: 采用双阶段检索-排序架构：检索阶段使用原始预训练模型，排序阶段进行领域特定微调；引入可学习的边界标记[Ms]/[Me]处理嵌套实体；通过三种数据源自动扩充训练语料

Result: 在BioNNE 2025多语言赛道排名第三，证明了这些最小但原则性修改的有效性和竞争力

Conclusion: 该方法通过轻量级修改实现了对多语言嵌套生物医学实体链接的有效处理，为这一更具挑战性的场景提供了实用解决方案

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [19] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 提出了一种利用LLM的非形式化和摘要能力，将机器可验证的形式化证明翻译为自然语言的方法


<details>
  <summary>Details</summary>
Motivation: 为了解决形式化证明难以被人类理解的问题，通过将机器验证的形式化证明转换为自然语言，提高证明的可读性和可理解性

Method: 利用大型语言模型(LLMs)的非形式化(形式语言证明步骤的言语化)和摘要能力，将形式化证明翻译为自然语言

Result: 在本科教科书自然语言证明数据上评估显示，该方法能生成高质量的自然语言证明；在Lean证明助手的现有形式证明库上应用，证明能输出高可读性和准确性的自然语言证明

Conclusion: 该方法能有效将机器可验证的形式化证明转换为高质量的自然语言证明，提高了证明的可读性和实用性

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [20] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 提出了一个多智能体框架，通过角色提示和检索增强生成来提升金融问答的准确性，相比零样本思维链基线提高了6.6-8.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融教育问答中难以捕捉专业推理需求，金融领域需要多步定量推理、专业术语理解和现实场景理解。

Method: 使用包含基础生成器、证据检索器和专家评审器的多智能体框架，结合RAG从6本金融教科书获取上下文证据，采用单次迭代生成精炼答案。

Result: 在Study.com的3,532个金融教育问题上测试，基于批判的细化方法比零样本思维链基线准确率提高6.6-8.3%，Gemini-2.0-Flash表现最佳，GPT-4o-mini达到与金融调优模型相当的性能。

Conclusion: 该方法为增强金融问答提供了一种经济有效的途径，并为多智能体金融LLM系统的进一步研究提供了见解。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [21] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 本文通过荟萃分析评估机器学习在Twitter情感分析中的性能，发现平均准确率为0.80，强调需要标准化性能报告和考虑类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习在Twitter情感分析中的整体性能表现，分析研究间的异质性，并探讨研究特征如何影响模型性能。

Method: 采用PRISMA指南进行文献检索，从20项研究中筛选195个试验，使用双反正弦变换和三层次随机效应模型分析总体准确率。

Result: AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]，发现总体准确率因对类别不平衡和情感类别数量的敏感性而容易产生误导。

Conclusion: 需要标准化模型性能报告（包括独立测试集的混淆矩阵），并对准确率进行归一化处理，以实现跨研究的可靠比较。

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [22] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: MultimodalHugs是一个基于Hugging Face构建的框架，旨在解决手语处理研究中存在的代码复杂、可复现性低和比较不公平等问题，通过支持更多数据模态和任务来提升实验灵活性。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究相比口语语言研究面临更多挑战，包括复杂的临时代码、低可复现性和不公平比较。现有工具如Hugging Face在手语实验集成方面不够灵活，这通过对SLP研究人员的调查得到证实。

Method: 在Hugging Face基础上构建MultimodalHugs框架，增加抽象层以支持更多数据模态和任务，同时继承Hugging Face生态系统的优势。

Result: MultimodalHugs能够适应多种模态数据，如手语的姿态估计数据和文本字符的像素数据，通过定量实验证明了其适用性。

Conclusion: MultimodalHugs不仅专注于手语处理，其抽象层设计使其更广泛适用于其他不符合Hugging Face标准模板的用例，提升了多模态实验的灵活性和可复现性。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [23] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: AncientDoc是首个针对中文古籍文档的基准测试，包含5个任务（页面级OCR、白话翻译、推理问答、知识问答、语言变体问答），涵盖14种文档类型、100多本书籍和约3000页内容，用于评估视觉语言模型在古籍处理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 中文古籍作为千年历史文化的宝贵载体，包含丰富知识但面临数字化和理解挑战。传统方法仅扫描图像，现有视觉语言模型难以处理其视觉和语言复杂性，且现有文档基准主要针对英文印刷文本或简体中文，缺乏对中文古籍的评估标准。

Method: 构建AncientDoc基准测试，包含五个核心任务：页面级OCR识别、文言文到白话文翻译、基于推理的问答、基于知识的问答、语言变体问答。数据集涵盖14种文档类型、超过100本书籍和约3000页内容。使用多种指标评估主流视觉语言模型，并辅以人工对齐的大语言模型进行评分。

Result: 创建了首个专门针对中文古籍文档的综合基准测试AncientDoc，为评估视觉语言模型在古籍处理领域的性能提供了标准化测试平台。

Conclusion: AncientDoc填补了中文古籍文档评估的空白，为推进视觉语言模型在古籍数字化和理解方面的研究提供了重要基础，有助于保护和传承中华文化遗产。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [24] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门针对MCP协议的综合基准测试，包含33个服务器、188个工具和600个查询，用于评估语言代理在工具交互中的真实性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估MCP协议下AI代理的真实性能，导致对其操作价值的误解和能力差异的无法可靠区分。

Method: 建立了包含33个操作服务器和188个不同工具的MCP测试床，开发了600个系统设计的查询，分布在6个不同复杂度的交互类别中，并引入了MCP-Eval结果导向的评估方法。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench为研究社区提供了标准化和可靠的框架，用于构建、验证和推进能够充分利用MCP变革性优势的AI代理，加速实现真正有能力且可互操作的AI系统。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [25] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在背景、性别和年龄方面的偏见，发现GPT-3.5和GPT-4o在决策任务中存在显著偏见，偏好女性、年轻年龄和特定背景（如非裔美国人），而摘要任务偏见较小。跨语言分析显示英语和荷兰语的偏见模式相似，但存在差异。提出的缓解指令能减少27%的偏见差距，GPT-4o表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在各个领域的快速集成，引发了对其可能加剧社会不平等和信息偏见的担忧。研究旨在系统分析LLMs在人口统计学特征上的偏见，特别是在决策和摘要任务中的表现，以及跨语言偏见传播和缓解策略的有效性。

Method: 使用Tamkin等人(2023)数据集的荷兰语翻译版本，创建了151,200个决策任务提示和176,400个摘要任务提示。测试了不同人口统计变量、指令、显著度水平和语言，在GPT-3.5和GPT-4o模型上进行实验。

Result: 两个模型在决策任务中都存在显著偏见：偏好女性性别、年轻年龄和特定背景（如非裔美国人）。摘要任务偏见较小，但GPT-3.5在英语中显示出显著的年龄相关差异。跨语言分析显示英语和荷兰语偏见模式相似但有差异。缓解指令能平均减少27%的最有利和最不利人口统计群体之间的差距。

Conclusion: 研究强调了谨慎采用LLMs和针对具体情境进行偏见测试的重要性，需要持续开发有效的缓解策略以确保AI的负责任部署。GPT-4o相比GPT-3.5在所有英语提示中都显示出减少的偏见，表明在新模型中基于提示的缓解具有特定潜力。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [26] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: HEFT是一种分层高效微调策略，结合权重空间（LoRA）和表示空间（ReFT）两种PEFT方法，在BoolQ推理任务上以更少的训练周期获得更好的性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在专业推理任务中计算资源受限的问题，探索不同参数高效微调方法的协同效应

Method: 提出HEFT分层适应策略：先在权重空间使用LoRA进行基础适配，然后在表示空间使用ReFT进行精确细化

Result: 在Llama-2-7B模型上，仅训练3个周期的HEFT达到85.17%准确率，优于训练20周期的LoRA-only（85.05%）和ReFT-only（83.36%）方法

Conclusion: PEFT方法的精心组合是强大的算法创新，为提升语言模型推理能力提供了更高效有效的路径

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [27] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 该论文提出了一个通过分析语用框架如何被概念化和唤起，来建模多模态对话轮次组织的框架，重点关注语言与交互手势之间的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管对话轮次组织已被多个领域研究，但特别是用于轮次组织的手势策略尚未被编码到可用于机器学习的数据集中。本研究旨在填补这一空白。

Method: 开发了一种注释方法，在Frame2多模态数据集（已标注语义框架）的基础上，增加了用于建模对话轮次组织的语用框架和手势标注。该数据集包含巴西电视剧的10集内容。

Result: 研究证实面对面交谈中人们使用手势作为传递、获取和保持对话轮次的工具，并发现了一些先前未记录的手势变体。数据表明语用框架标注有助于更深入理解人类认知和语言。

Conclusion: 手势的使用源于语用框架的概念化，涉及心理空间、概念整合和概念隐喻。语用框架的标注为理解人类多模态交流提供了新的视角。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [28] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出了一种基于主题引导强化学习的方法来改进多文档摘要中的内容选择，通过在GRPO框架中引入主题奖励来提升摘要与源文档的主题对齐度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单文档摘要中表现优异，但在多文档摘要中仍有改进空间，需要更好地整合多个来源的信息同时保持连贯性和主题相关性

Method: 首先通过显式提示模型使用主题标签来增强摘要信息量，然后在Group Relative Policy Optimization框架中提出新颖的主题奖励机制来衡量生成摘要与源文档的主题对齐度

Result: 在Multi-News和Multi-XScience数据集上的实验结果表明，该方法 consistently outperforms strong baselines

Conclusion: 利用主题线索在多文档摘要中是有效的，主题引导的强化学习方法能够显著提升多文档摘要的质量

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [29] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 评估大语言模型生成合成调查回复的可靠性，与智利概率抽样调查的真实人类回复对比，发现LLM在信任项目上表现优异，但存在项目级异质性和人口统计偏差。


<details>
  <summary>Details</summary>
Motivation: LLMs在调查研究中具有创新潜力，但需要验证其生成的合成回复是否能准确反映人类回答分布，避免复制训练数据中的社会刻板印象和偏见。

Method: 使用128个提示-模型-问题三元组生成189,696个合成配置文件，与智利概率抽样调查的真实回复对比，通过准确率、精确率、召回率和F1分数等指标进行元分析，测试关键社会人口统计维度上的偏差。

Result: 1) 合成回复在信任项目上表现优异(F1分数和准确率>0.90)；2) GPT-4o、GPT-4o-mini和Llama 4 Maverick表现相当；3) 45-59岁受访者的合成-人类对齐度最高；总体近似概率样本回复但存在项目级异质性。

Conclusion: LLM合成样本能近似概率样本回复，但要完全捕捉公众意见的细微差别仍需谨慎校准和额外分布测试，以确保算法保真度和减少误差。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [30] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文对基于大语言模型（LLM）的法律人工智能方法进行了全面综述，涵盖了16个法律LLM系列、47个LLM框架、15个基准测试和29个数据集，并分析了该领域面临的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在近年来显著推动法律人工智能的发展，提高了法律任务的效率和准确性，需要系统梳理和总结LLM在法律领域的应用现状，为初学者提供系统介绍并促进未来研究。

Method: 通过全面综述16个法律LLM系列和47个基于LLM的法律任务框架，收集15个基准测试和29个数据集来评估不同的法律能力，并对该领域的挑战和未来方向进行分析讨论。

Result: 提供了法律LLM领域的系统性资源汇总，包括模型、框架、基准测试和数据集，为研究者和实践者提供了全面的参考资源。

Conclusion: 本文为LLM在法律领域的应用提供了系统性介绍，识别了当前挑战并讨论了未来发展方向，旨在促进该领域的进一步研究和应用。相关资源已在GitHub上开源。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [31] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 提出了一个针对中国少数民族语言（藏语、维吾尔语、蒙古语）的新闻标题生成数据集CMHG，包含20万条数据，并提供了由母语者标注的高质量测试集作为基准。


<details>
  <summary>Details</summary>
Motivation: 中国少数民族语言由于书写系统与国际标准不同，导致相关语料库严重缺乏，特别是在监督任务如标题生成方面存在明显空白。

Method: 构建了CMHG数据集，包含10万条藏语和5万条维吾尔语、5万条蒙古语数据，专门用于标题生成任务，并提供了由母语者标注的高质量测试集。

Result: 创建了一个包含20万条数据的少数民族语言标题生成数据集，为相关研究提供了宝贵的资源。

Conclusion: 该数据集将成为推动中国少数民族语言标题生成研究的重要资源，并有助于相关基准的发展。

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [32] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS是一个无监督幻觉检测框架，利用LLM内部表示来识别生成内容的事实正确性，无需标注数据即可实现高效检测


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖与事实正确性无关的代理信号，导致检测偏向表面特征，限制了跨数据集和场景的泛化能力

Method: 通过提示LLM验证陈述真实性，获取上下文嵌入作为特征，并将响应不确定性作为真实性软伪标签进行训练

Result: IRIS在实验中一致优于现有无监督方法，计算成本低，即使训练数据少也能良好工作，适用于实时检测

Conclusion: IRIS框架通过利用LLM内部表示实现了高效的无监督幻觉检测，解决了现有方法依赖无关代理信号的问题

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [33] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文对开源大语言模型在多标签意图分类任务中的表现进行了全面分析，使用MultiWOZ 2.1数据集，在少样本设置下比较了LLama2-7B、Mistral-7B和Yi-6B的性能，并与BERT监督学习基线进行对比。


<details>
  <summary>Details</summary>
Motivation: 研究开源大语言模型在消费级硬件上处理多标签意图分类任务的有效性，为任务导向对话系统的自然语言理解提供实用框架。

Method: 使用MultiWOZ 2.1数据集，在少样本设置下（提示中包含20个示例）测试三个开源LLM（LLama2-7B、Mistral-7B、Yi-6B），并与BERT监督分类器进行对比，评估指标包括准确率、精确率、召回率和多种F1分数。

Result: Mistral-7B在14个意图类别中的11个上F分数表现最佳，加权平均F分数为0.50，具有较低的Hamming Loss和较高的Jaccard相似度。但BERT监督分类器的性能仍优于最佳少样本生成式LLM。

Conclusion: 虽然Mistral-7B在少样本设置中表现最佳，但监督学习的BERT模型在性能上仍具优势。研究为小型开源LLM在复杂多意图对话检测中的应用提供了实用框架。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [34] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 该研究通过社交媒体语言分析，开发了一种确定双相障碍诊断时间的方法，并分析了诊断前后长达24年的语言轨迹变化，发现双相障碍伴随广泛的语言改变和周期性情绪波动。


<details>
  <summary>Details</summary>
Motivation: 临床评估双相障碍的规模有限，而社交媒体语言具有高时间分辨率和纵向范围的优势，可以用于大规模心理健康监测。

Method: 引入确定用户诊断时间的方法，分析双相障碍患者从诊断前3年到诊断后21年的语言轨迹，并与单相抑郁症患者和健康对照组进行对比。

Result: 发现双相障碍诊断伴随反映情绪障碍、精神共病、物质滥用、住院治疗、医学共病、异常思维内容和思维紊乱的广泛语言改变，并观察到诊断后二十年反复出现的情绪相关语言变化，具有明显的12个月周期性。

Conclusion: 研究结果提供了双相障碍急性和慢性期语言改变的证据，验证并扩展了利用社交媒体进行可扩展心理健康监测的最新努力。

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [35] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: 本文介绍了BAREC 2025阿拉伯语细粒度可读性评估共享任务的获胜系统，在六个赛道中均获得第一名。通过四种Transformer模型的置信度加权集成、针对性数据增强和后处理技术，在句子和文档级别分别达到87.5%和87.4%的QWK分数。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语可读性评估中的严重类别不平衡和数据稀缺问题，提升细粒度可读性预测的准确性。

Method: 使用四种互补的Transformer模型（AraBERTv2、AraELECTRA、MARBERT、CAMeLBERT）进行集成，每种模型采用不同的损失函数；应用加权训练、高级预处理、SAMER语料库重新标注，并通过Gemini 2.5 Flash生成约10,000个稀有级别样本的合成数据；采用针对性后处理步骤校正预测分布偏差。

Result: 在六个赛道中全部获得第一名，句子级别QWK达到87.5%，文档级别QWK达到87.4%，后处理带来6.3%的QWK增益。

Conclusion: 证明了模型和损失函数的多样性、置信度信息融合以及智能数据增强在阿拉伯语可读性预测中的有效性，为处理类别不平衡和数据稀缺问题提供了有效解决方案。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [36] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文比较了传统心理测量问卷与生态效度问卷在测量大语言模型人格特质时的差异，发现传统问卷存在测量不稳定、产生误导性结果等问题，建议避免使用传统心理问卷评估LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用传统人类心理问卷（如BFI、PVQ）测量大语言模型的人格特质和价值观，但这些问卷缺乏生态效度，不能反映LLMs在真实用户查询场景中的文本生成特性，需要系统比较两种问卷的差异。

Method: 对传统心理问卷和生态效度问卷进行全面的比较分析，评估它们在测量LLMs人格特质时的表现差异和稳定性。

Result: 研究发现传统问卷：1）产生与生态效度问卷显著不同的人格特征剖面；2）项目数量不足导致测量不稳定；3）造成LLMs具有稳定人格结构的误导印象；4）对角色提示的LLMs产生夸张的人格剖面。

Conclusion: 传统心理问卷不适合用于测量大语言模型，建议避免使用，研究代码将在发表后公开。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [37] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 构建了一个气候科学领域的知识图谱，支持语义查询，帮助研究人员发现模型、数据集、区域和变量之间的精确联系，并与大语言模型集成提升问答系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 气候科学文献的复杂性和数量不断增加，研究人员难以跨模型、数据集、区域和变量找到相关信息，需要更智能的知识检索方式。

Method: 从气候出版物和科学文本构建领域特定的知识图谱，支持Cypher查询进行结构化语义搜索，并与RAG系统中的大语言模型集成。

Result: 知识图谱能够回答诸如特定区域验证的模型、与特定遥相关模式常用数据集等精确问题，提高了气候知识检索的准确性和效率。

Conclusion: 该知识图谱超越了传统构建工作，展示了其在气候研究、模型开发等实际应用中的价值，为依赖准确科学信息的用户提供了更好的工具。

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [38] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本研究提出了一种针对阿拉伯语医疗文本生成的LLM微调方法，通过收集社交媒体真实医患对话数据，微调Mistral-7B等模型，在医疗咨询、诊断和治疗建议方面取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有医院管理系统在实时医疗建议、不规则输入和少数语言支持方面的不足，特别是在阿拉伯语医疗文本生成方面的空白。

Method: 收集社交媒体医患对话数据集，预处理多阿拉伯方言数据，微调Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium等生成模型。

Result: 微调后的Mistral-7B模型表现最佳，BERT Score在精确率、召回率和F1分数上分别达到68.5%、69.08%和68.5%。

Conclusion: 生成式AI在医院管理系统中具有巨大潜力，特别是在语言文化多样化的医疗环境中提供可扩展的解决方案。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [39] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 本研究提出了一种可扩展的合成数据增强策略，使用ChatGPT-4o和Gemini 2.5 Pro生成了80,000个阿拉伯语医疗问答对，将训练语料扩展到100,000条记录，显著提升了阿拉伯医疗聊天机器人的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语医疗聊天机器人的发展受到大规模高质量标注数据集稀缺的限制，先前的研究虽然构建了20,000条医患互动数据，但模型的可扩展性和泛化能力仍然有限。

Method: 使用ChatGPT-4o和Gemini 2.5 Pro生成80,000个上下文相关且医学上连贯的合成问答对，经过语义过滤和人工验证后整合到训练流程中，对包括Mistral-7B和AraGPT2在内的五个大型语言模型进行微调。

Result: ChatGPT-4o生成的数据在所有模型中始终获得更高的F1分数和更少的幻觉现象，合成数据增强策略有效提升了模型性能。

Conclusion: 合成数据增强是增强低资源医疗NLP领域特定语言模型的可行解决方案，为更具包容性、可扩展性和准确性的阿拉伯语医疗聊天机器人系统铺平了道路。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [40] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本文研究通过结合重音检测和语音识别来开发奥地利德语对话中的重音感知自动语音识别系统，使用wav2vec2模型进行重音分类，并在大规模语料库中自动标注韵律重音。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发能够同时转录单词及其重音级别的重音感知ASR系统，为语言学研究和基于韵律的对话系统提供潜在应用。

Method: 通过微调wav2vec2模型开发词级重音检测器，自动标注大规模语料库中的韵律重音，然后训练能够同时处理单词转录和重音级别识别的新型ASR系统。

Result: 集成重音信息后ASR性能与基线系统相当，在识别正确的语句中重音检测准确率达到85.53%。

Conclusion: 基于transformer的模型能有效编码韵律信息，这是对韵律增强ASR的新贡献，具有重要的研究和应用价值。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [41] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出了一个系统框架，用于生成高质量、与人口分布对齐的LLM驱动社交模拟角色集，通过社交媒体数据生成、质量评估和重要性采样来减少偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM社交模拟研究主要关注代理框架和模拟环境设计，忽视了角色生成的复杂性和非代表性角色集引入的偏见问题，需要构建能真实反映现实人群多样性和分布的角色集。

Method: 利用LLM从长期社交媒体数据生成叙事角色，进行严格质量评估筛选低质量档案，应用重要性采样实现与参考心理测量分布（如大五人格）的全局对齐，并引入任务特定模块针对子群体进行适配。

Result: 大量实验表明，该方法显著减少了人口层面的偏见，能够为广泛的研究和政策应用提供准确、灵活的社交模拟。

Conclusion: 该框架为LLM驱动的社交模拟提供了高质量、人口对齐的角色集生成方法，解决了现有研究中角色集代表性不足的问题，具有重要的研究和应用价值。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [42] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: DocExplainerV0是一个即插即用的边界框预测模块，将答案生成与空间定位解耦，用于提升视觉语言模型在文档中的答案定位能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在文档理解方面表现出色，但准确在文档中定位答案仍然是一个主要挑战，这限制了模型的可解释性和实际应用。

Method: 引入DocExplainerV0模块，该模块可以与现有视觉语言模型（包括无法微调的专有系统）配合使用，通过解耦答案生成和空间定位来实现边界框预测。

Result: 系统评估显示文本准确性和空间定位之间存在差距，正确的答案往往缺乏可靠的空间定位。该框架揭示了这些不足并建立了未来研究的基准。

Conclusion: DocExplainerV0为构建更可解释和鲁棒的文档信息提取视觉语言模型提供了标准化框架和基准。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [43] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本研究使用Biber的多维分析(MDA)方法，比较人类写作与大型语言模型(LLM)生成文本的语域变异差异，创建了可解释的基准测试框架


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI生成文本在语域特征上的系统性差异，特别是前沿LLM在英语之外语言(如捷克语)的表现，因为非英语语言在训练数据中代表性不足

Method: 应用Biber的多维分析方法，使用AI-Brown语料库(与BE-21可比)和AI-Koditex捷克语料库，分析16个前沿LLM在不同设置和提示下的表现，重点比较基础模型和指令调优模型

Result: 识别出LLM与人类文本差异最显著和最系统的变异维度，建立了模型间可比性和可排名的基准测试框架

Conclusion: 研究为评估LLM文本生成质量提供了可解释的多维度基准，特别关注了非英语语言的LLM表现评估

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [44] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 论文研究情感支持对话中不恰当的积极回应问题，分析人类和LLM生成回应中的不协调积极性现象，特别是在高风险情境下LLM更容易产生不现实的乐观回应。


<details>
  <summary>Details</summary>
Motivation: 研究情感支持对话中善意但可能适得其反的积极回应现象，特别是在线支持对话中LLM生成回应可能显得轻描淡写或不切实际的乐观，需要更符合情感期望的支持方式。

Method: 收集Reddit真实用户-助手对话，按情感强度分类为轻度（关系紧张、一般建议）和重度（悲伤、焦虑）情境；使用大语言模型生成额外回应；微调LLM并开发弱监督多标签分类器集成（DeBERTa和MentalBERT）来检测不协调积极性类型。

Result: 发现LLM在高风险情境下更容易通过轻描淡写和最小化语气表现出不现实的积极性；开发的分类器集成在检测不同类型不协调积极性方面表现改善。

Conclusion: 需要超越生成通用积极回应，研究符合情境的支持措施来平衡积极情感与情感认同，为构建情境感知和保持信任的在线对话系统提供方向。

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [45] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 本文比较了多种大语言模型在处理长文本分类任务（特别是法律文件）时的性能，发现专门为长文本设计的Longformer模型并无明显优势，开源模型表现优于GPT变体。


<details>
  <summary>Details</summary>
Motivation: 现有主流语言模型（如BERT、RoBERTa）存在输入文本长度限制，无法有效处理长达数百页的法律文件分类任务，需要探索适合长文本处理的模型解决方案。

Method: 在5种语言上使用XLM-RoBERTa、Longformer、GPT-3.5、GPT-4等模型进行多类别分类实验，使用比较议程项目的21个政策主题标签代码本。

Result: Longformer模型在长文本处理方面没有显示出特别优势；开源模型表现优于GPT变体；类别间的支持度和内容重叠度是影响长文本分类性能的关键因素。

Conclusion: 专门为长文本预训练的模型不一定优于通用模型，开源模型在长文本分类任务中具有竞争力，类别间的语义特征对模型性能有重要影响。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [46] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 提出Self Improving Faithfulness Aware Contrastive Tuning框架，通过自指导机制自动生成对比学习数据，提升LLM在知识冲突任务中的忠实度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务中经常生成不忠实的响应，倾向于依赖内部参数知识而非提供的上下文，存在知识冲突问题。

Method: 使用自指导机制让基础LLM自动生成高质量的结构化对比学习数据（锚样本、语义等价正样本、模拟不忠实场景的负样本），然后应用对比学习训练模型。

Result: 在ECARE KRE和COSE KRE基准测试中，基于Llama3 8B Instruct的SI FACT模型将上下文召回率提高了6.2%，显著减少了对内部记忆的依赖。

Conclusion: SI FACT在增强LLM上下文忠实度方面提供了强有效性和高数据效率，为构建更主动和可信的语言模型提供了实用途径。

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [47] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN是一个无需重新训练的任务无关框架，通过专家剪枝和神经元重组来减少SMoE模型的内存使用，在50%专家稀疏度下性能提升5%以上


<details>
  <summary>Details</summary>
Motivation: SMoE架构虽然计算高效，但仍需加载所有专家参数，导致内存使用高和部署困难。现有方法主要关注专家级操作，忽略了神经元级结构

Method: DERN三步骤：1) 使用路由统计剪枝冗余专家；2) 将专家分解为神经元级片段并分配到最兼容的保留专家；3) 在保留专家内合并片段构建紧凑表示

Result: 在Mixtral、Qwen和DeepSeek SMoE模型上，50%专家稀疏度下常识推理和MMLU基准性能提升超过5%，显著减少专家数量和内存使用

Conclusion: DERN通过神经元级重组有效解决了专家间语义冲突问题，无需额外训练即可实现SMoE模型的高效压缩和部署

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [48] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文通过大规模实证分析发现，上下文学习(ICL)确实构成学习机制，但其学习能力有限，对未见任务的泛化能力不足，且对提示格式和分布变化敏感。


<details>
  <summary>Details</summary>
Motivation: 研究上下文学习(ICL)是否真正构成学习过程，而非仅仅是基于预训练知识的演绎推理，并系统分析其学习能力和局限性。

Method: 进行大规模ICL分析，通过消融实验控制记忆效应、预训练影响、分布偏移、提示风格和措辞等因素，考察不同条件下ICL的表现。

Result: ICL是一种有效的学习范式，但在学习未见任务和泛化方面能力有限；当示例数量足够多时，准确率对示例分布、模型、提示风格和语言特征不敏感，主要从提示中的规律性推断模式。

Conclusion: 自回归模型的临时编码机制不够鲁棒，表明其通用泛化能力有限，ICL对分布变化（特别是思维链提示）敏感，在形式相似任务上表现差异显著。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [49] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 研究评估了多种改进的Transformer模型来处理超长学生作文，解决传统模型因长度限制而需要截断文本的问题，使用Kaggle ASAP 2.0数据集进行测试。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型有固定长度限制，而高年级学生的作文经常超过这个限制。截断文本会严重影响自动作文评分系统的有效性，特别是对组织结构等需要长上下文评估的评分标准。

Method: 评估了多种改进架构的模型：XLNet、Longformer、ModernBERT、Mamba和Llama模型的微调版本，使用Kaggle ASAP 2.0数据集进行比较研究。

Result: 研究比较了这些改进模型在处理长文本作文评分方面的表现，但具体结果未在摘要中说明。

Conclusion: 需要采用能够处理长文本的改进架构模型来解决自动作文评分中的长度限制问题，确保评分系统能够全面评估作文的组织结构等长上下文相关要素。

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [50] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 提出云边协作的多智能体提示框架，包含GuideLLM、SolverLLM和JudgeLLM三个组件，并在RefactorCoderQA基准测试中达到76.84%的准确率。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型的推理和问题解决能力，解决现有基准测试的局限性。

Method: 云边协作架构，包含边缘部署的轻量级GuideLLM（提供方法指导）、云端部署的强大SolverLLM（生成代码解决方案）和自动评估器JudgeLLM。

Result: 微调模型RefactorCoder-MoE在RefactorCoderQA基准测试中达到76.84%的整体准确率，显著优于开源和商业基线模型。

Conclusion: 该架构在多个技术领域表现出色，人类评估验证了生成解决方案的可解释性、准确性和实用性，系统级指标也显示了良好的性能特征。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [51] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive通过自动合成复杂问题和多轮强化学习训练，提升了开源大语言模型在深度搜索任务中的表现，在BrowseComp基准上取得了新的开源竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型在深度搜索任务中表现不佳，主要受限于长时程推理能力和缺乏足够难度的监督数据。

Method: 1. 从开放知识图谱自动合成复杂、困难且难以找到的问题；2. 应用端到端多轮强化学习来增强LLM的深度搜索长时程推理能力。

Result: DeepDive-32B在BrowseComp基准上超越了WebSailor、DeepSeek-R1-Browse和Search-o1，多轮RL训练显著提升了深度搜索能力，并支持测试时的工具调用扩展和并行采样。

Conclusion: DeepDive通过自动数据合成和多轮强化学习的结合，有效提升了开源LLM在深度搜索任务中的性能，为构建更好的深度搜索智能体提供了可行方案。

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [52] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE是一种仅使用文本数据进行语音识别模型领域适应的深度监督方法，通过变分自编码器建模编码器输出并微调解码器，无需额外推理成本即可显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 预训练语音识别模型如Whisper在处理未见词汇和方言时需要领域适应，但在实际场景中收集语音数据往往不现实，因此需要仅使用文本数据的适应方法。

Method: 提出WhisTLE方法：1）训练变分自编码器(VAE)从文本建模编码器输出；2）使用学习的文本到潜在编码器微调解码器；3）可选结合文本到语音(TTS)适应；4）推理时恢复原始编码器，无额外运行时成本。

Result: 在4个域外数据集和4个ASR模型上，WhisTLE结合TTS相比仅使用TTS适应相对降低词错误率12.3%，在32个场景中的27个场景中优于所有非WhisTLE基线方法。

Conclusion: WhisTLE提供了一种有效的文本-only适应方法，能够显著提升预训练ASR模型在未见领域的性能，且不增加推理成本，具有很好的实用价值。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS是一个包含50种常见超市物品的高质量3D纹理网格数据集，专为机器人和计算机视觉基准测试设计，具有成本效益和现实世界适用性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多依赖合成模型或难以获取的专业物品，缺乏真实超市环境中常见物品的高质量3D数据，限制了在机器人抓取、物体检测等应用中的实用性。

Method: 使用运动结构重建技术，通过高分辨率成像采集物品的3D网格，生成水密网格模型，涵盖10个不同类别的物品，具有多样化的形状、尺寸和重量。

Result: 创建了一个包含50种易获取超市物品的全面3D数据集，所有物品均可从澳大利亚主要超市连锁店采购，提供了高质量的纹理网格模型。

Conclusion: ASOS数据集强调可访问性和现实世界适用性，为物体检测、姿态估计和机器人应用提供了有价值的基准测试资源，填补了现有数据集的空白。

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [54] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态检索增强生成框架(MM-RAG)，用于自然灾害后房屋损坏评估，通过双分支编码器和跨模态交互模块实现图像和文本的语义对齐，在检索准确率和损坏严重程度分类指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自然灾害后准确评估房屋损坏对于保险理赔和资源规划至关重要，需要结合图像和文本信息进行综合分析。

Method: 采用双分支多模态编码器结构：图像分支使用ResNet和Transformer提取建筑损坏特征，文本分支使用BERT检索器处理帖子和保险政策文本；集成跨模态交互模块通过多头注意力实现语义对齐；引入模态注意力门控机制动态控制生成过程中的视觉证据和文本先验信息作用。

Result: 在检索准确率和损坏严重程度分类指标上表现出色，Top-1检索准确率提高了9.6%。

Conclusion: 该MM-RAG框架通过端到端训练和多任务优化目标，成功实现了图像理解和政策匹配的协同学习，为灾后房屋损坏评估提供了有效的多模态解决方案。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [55] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的集成框架，通过多图像增强变体转录和Needleman-Wunsch对齐器融合，提高噪声历史文档文本提取的稳定性和准确性


<details>
  <summary>Details</summary>
Motivation: 解决噪声历史文档中LLM文本提取的不稳定性问题，提高转录准确性和可靠性

Method: 使用Gemini 2.0 Flash对每个图像的多个增强变体进行转录，通过自定义Needleman-Wunsch风格对齐器融合输出，生成共识转录和置信度分数

Result: 在622份宾夕法尼亚州死亡记录数据集上，相比单次转录基线准确率提升4个百分点；填充和模糊处理对提高准确性最有效，网格扭曲扰动最适合区分高低置信度情况

Conclusion: 该方法简单、可扩展，可立即部署到其他文档集合和转录模型，为历史文档数字化提供有效解决方案

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [56] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出了首个专门针对智能交通监控领域的大规模多模态基准数据集MITS，包含17万张真实交通监控图像和500万条指令跟随数据，显著提升了主流大模型在交通监控任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用大模型在智能交通监控领域表现有限，主要原因是缺乏专门的交通监控多模态数据集。为了解决这个问题，需要构建专门的数据集来提升模型在交通监控任务中的性能。

Method: 收集了170,400张真实交通监控图像，标注了8个主要类别和24个子类别的交通对象和事件。通过系统化的数据生成流程，生成了高质量的图像描述和500万条指令跟随的视觉问答对，涵盖5个关键交通监控任务。

Result: 在MITS数据集上微调后，主流大模型性能显著提升：LLaVA-1.5从0.494提升到0.905(+83.2%)，LLaVA-1.6从0.678到0.921(+35.8%)，Qwen2-VL从0.584到0.926(+58.6%)，Qwen2.5-VL从0.732到0.930(+27.0%)。

Conclusion: MITS数据集有效解决了智能交通监控领域缺乏专门多模态数据的问题，显著提升了模型性能，为交通监控和大模型研究提供了高价值资源，数据集、代码和模型均已开源。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [57] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 本文研究了基于决策树的结构化推理是否能提升视觉语言模型在细粒度分类任务中的性能，发现虽然模型能很好地理解树状知识，但树基推理始终不如标准零样本提示方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在零样本视觉分类方面表现出色，但在细粒度任务和大规模层次标签空间中的性能研究不足，需要探索结构化推理是否能提升性能。

Method: 引入一个框架，将分类分解为使用决策树的可解释决策，并在细粒度(GTSRB)和粗粒度(CIFAR-10)数据集上进行评估，同时探索使用LLM生成的类别和图像描述来增强树提示。

Result: 模型在理解树知识方面达到98.2%的准确率，但树基推理始终表现不如标准零样本提示。添加图像描述后，树基方法和零样本方法的性能都有所提升。

Conclusion: 研究结果突显了结构化推理在视觉分类中的局限性，为设计更可解释的视觉语言模型系统提供了见解。

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [58] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI是一个从数据中学习可控、可提示世界模型的系统，通过三步循环构建概率图模型，提取底层结构，并整合为新的控制信号。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从视频数据中学习丰富控制结构和灵活提示功能的世界模型系统，以支持视频预测、理解和底层结构提取。

Method: 三步循环：1) 概率预测 - 构建随机访问自回归序列模型；2) 结构提取 - 通过因果推断零样本提取底层低维属性；3) 整合 - 将结构转换为新token类型并重新融入训练。

Result: 在1.4万亿token互联网视频数据上训练PSI实例，实现了最先进的光流、自监督深度和对象分割，并支持完整的预测改进循环。

Conclusion: PSI系统通过循环增强的方式，不仅改善了底层数据建模能力，还创建了类似LLM的通用提示语言控制机制。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [59] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 本文首次分析了联邦学习中视频数据的梯度反演攻击风险，发现特征提取器能提供更好保护但仍有泄漏可能，超分辨率技术可提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过只交换模型更新而非原始数据来保护隐私，但梯度反演攻击能从中重建敏感数据。虽然已知对图像、文本和表格数据的威胁，但对视频数据的影响尚未研究。

Method: 评估两种视频分类方法：使用预训练特征提取器的方法和处理原始视频帧的简单变换方法。测试在不同攻击场景下（零参考帧、一个参考帧、多个参考帧）的梯度反演攻击效果，并应用图像超分辨率技术提升重建质量。

Result: 特征提取器对梯度反演攻击具有更强韧性，但若分类器复杂度不足仍可能泄漏。超分辨率技术能显著提升攻击者重建的视频质量。视频数据在联邦学习中存在可行的泄漏威胁。

Conclusion: 视频数据在联邦学习中的泄漏是真实存在的威胁，需要进一步研究其发生条件和防护措施，特别是在使用特征提取器但仍需确保分类器足够复杂的情况下。

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [60] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 提出一种半监督协同训练框架，用于密集零售环境中的目标检测，结合Faster R-CNN和YOLO进行伪标签交换，集成多种分类器提升鲁棒性，通过元启发式算法优化超参数，在SKU-110k数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 解决密集零售环境中有限标注数据和复杂条件（遮挡、重叠物体）带来的挑战，减少人工标注成本，适应零售场景中频繁的产品和布局变化

Method: 采用半监督协同训练框架：1) Faster R-CNN（ResNet主干）精确定位 + YOLO（Darknet主干）全局上下文，相互交换伪标签；2) 集成XGBoost、随机森林和SVM分类器；3) 元启发式算法优化超参数

Result: 在SKU-110k数据集上表现出色，证明了框架的可扩展性和实用性

Conclusion: 该框架适用于实际零售应用，如自动化库存跟踪、产品监控和结账系统，能够有效减少对人工标注的依赖并适应零售环境变化

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [61] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出Token Purging(PG)方法，一种无需反向传播的测试时自适应技术，通过移除受域偏移影响严重的token来提升3D点云分类性能


<details>
  <summary>Details</summary>
Motivation: 解决3D点云分类中因分布偏移导致的性能下降问题，现有TTA方法需要迭代更新，计算成本高

Method: 提出两种变体：PG-SP利用源域统计信息，PG-SF是完全无源版本，依赖CLS-token驱动的自适应。在注意力层前移除受域偏移影响的token

Result: 在ModelNet40-C、ShapeNet-C和ScanObjectNN-C数据集上，PG-SP比最先进的无反向传播方法平均准确率高10.3%，PG-SF为无源自适应设定了新基准。速度提升12.4倍，内存效率提高5.5倍

Conclusion: Token Purging是一种高效、内存友好的测试时自适应方法，特别适合实际部署，在保持高性能的同时显著降低了计算成本

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [62] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出了一种准确且高度可解释的跨视角细粒度定位方法，通过匹配地面图像与参考航拍图像的局部特征来估计3自由度位姿，避免了传统鸟瞰图转换中的信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将地面图像转换为鸟瞰图表示再与航拍图像对齐，这种转换常因透视畸变或高度信息压缩导致信息损失，影响对齐质量。

Method: 直接在原始图像间建立对应关系，仅将匹配的关键点使用单目深度先验提升到鸟瞰空间，支持度量和相对深度，采用尺度感知的Procrustes对齐来估计相机位姿。

Result: 实验结果表明，仅需相机位姿的弱监督，该方法就能学习准确的局部特征对应关系，在跨区域泛化和未知方向等挑战性条件下实现优越的定位性能。

Conclusion: 该方法兼容各种相对深度模型且无需针对每个模型进行微调，具有灵活性和强大的定位性能，非常适合实际部署。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [63] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 基于深度学习的智能手机应用KidsVisionCheck，通过红眼反射图像进行儿童视力筛查，准确率达90%，无需专业设备。


<details>
  <summary>Details</summary>
Motivation: 利用智能手机和人工智能技术重现传统Bruckner测试，使儿童视力筛查更加可达和普及。

Method: 使用深度神经网络训练模型，训练数据来自眼科医生收集标注的儿童眼粒图像。

Result: 在未见测试数据上达到90%的准确率，能够识别最佳数据收集条件并提供即时反馈。

Conclusion: 该研究为全球范围内实现可达的儿科视力筛查和早期干预视力异常奠定了基础。

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [64] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出DGFusion方法，通过深度引导的多模态融合提升自动驾驶语义感知性能，在挑战性条件下实现最先进的分割效果


<details>
  <summary>Details</summary>
Motivation: 现有传感器融合方法在空间上均匀处理传感器数据，在挑战性条件下性能受限，需要根据深度信息动态调整传感器融合策略

Method: 提出深度引导的多模态融合网络DGFusion，将多模态分割作为多任务问题，利用LiDAR测量作为输入和深度真值，学习深度感知特征并通过局部深度token和全局条件token动态调整传感器融合

Result: 在MUSES和DELIVER数据集上实现了最先进的全景分割和语义分割性能

Conclusion: 深度引导的传感器融合方法能有效适应不同深度条件下的传感器可靠性变化，提升自动驾驶语义感知的鲁棒性

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [65] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 本文提出基于图像块的自动酒渣鼻检测策略，使用ResNet-18深度学习框架，通过提取不同大小、形状和位置的面部图像块来提升检测性能，在保护患者隐私的同时实现竞争性或更优的准确率和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 酒渣鼻是一种慢性炎症性皮肤病，需要精确早期检测来提高治疗效果。传统全图像方法可能包含过多无关信息且存在隐私问题，因此需要开发更精准、隐私保护的检测方法。

Method: 使用ResNet-18深度学习框架，从面部图像中提取各种大小、形状和位置的图像块，研究局部视觉信息对模型性能的影响，并与全图像方法进行对比实验。

Result: 实验结果表明，基于图像块的策略能够引导深度学习模型关注临床相关区域，在准确率和灵敏度方面达到竞争性或优于全图像方法，同时增强鲁棒性和可解释性。

Conclusion: 提出的基于图像块的策略为改进自动化皮肤病诊断提供了实用见解，通过使用局部图像块既保护了患者隐私，又提高了检测性能，具有重要的临床应用价值。

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [66] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出了一种基于合成数据和临床先验知识的隐私保护型玫瑰痤疮自动检测方法，通过红色通道强度构建诊断相关区域掩码，使用ResNet-18在掩码合成图像上训练，在真实测试数据上取得了优于全脸基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮是一种常见但诊断不足的炎症性皮肤病，自动检测面临症状弥散性、标记数据稀缺以及面部图像隐私问题等挑战。需要开发既能保护隐私又能准确检测的方法。

Method: 1. 基于临床先验知识（玫瑰痤疮主要表现为面部中央红斑）构建固定红色信息掩码，选择面部图像中红色通道强度持续较高的区域；2. 使用ResNet-18深度学习模型在掩码合成图像上进行训练；3. 专注于脸颊、鼻子和额头等诊断相关区域，排除身份识别特征。

Result: 该方法在真实世界测试数据上表现出优于全脸基线方法的性能，在准确率、召回率和F1分数方面均有显著提升。

Conclusion: 合成数据和临床先验知识可以共同实现准确且符合伦理的皮肤病AI系统，特别适用于远程医疗和大规模筛查等隐私敏感应用场景。

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [67] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 本文对ULW腹腔镜图像去烟框架进行了全面的消融研究，评估了可学习维纳滤波器模块和复合损失函数中各损失项的贡献


<details>
  <summary>Details</summary>
Motivation: 为了严格评估ULW框架中各个组件的有效性和必要性，需要系统性地分析每个组件对整体性能的具体贡献

Method: 采用消融研究方法，包括：(1)移除可学习维纳滤波器模块；(2)选择性使用复合损失函数中的单个损失项（MSE、SSIM损失和感知损失）。所有变体在公开的腹腔镜图像数据集上进行基准测试

Result: 使用定量指标（SSIM、PSNR、MSE和CIEDE-2000）和定性视觉比较来评估不同变体的性能

Conclusion: 通过系统消融研究验证了ULW框架中各个组件的必要性和有效性，为腹腔镜图像去烟技术的优化提供了重要见解

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [68] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: WAVE-DETR是一个结合可见光RGB和声学信号的多模态无人机检测器，使用Deformable DETR和Wav2Vec2架构，在真实ARDrone数据集上通过门控融合机制将mAP提升11.1%-15.3%。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在复杂环境条件下鲁棒检测无人机的多模态检测系统，利用视觉和声学信号的互补性来提高检测性能。

Method: 基于Deformable DETR和Wav2Vec2架构，开发了四种融合配置（门控机制、线性层、MLP和交叉注意力），将声学嵌入与多分辨率特征映射融合。

Result: 门控融合方法表现最佳，在小无人机上mAP提升11.1%-15.3%，中大型无人机mAP提升3.27%-5.84%，在所有IoU阈值(0.5-0.9)上均有改进。

Conclusion: 声学信息能够显著提升无人机检测性能，特别是在小尺寸无人机检测方面，多模态融合在真实环境条件下具有重要应用价值。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [69] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 提出了一种称为代理监督的新训练范式，通过将估计的空间变换应用于代理图像，将输入域与监督域解耦，从而提高深度学习图像配准网络的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像配准虽然精度高，但对输入图像特性变化（如伪影、视野不匹配、模态差异）敏感，需要开发能提高网络鲁棒性和泛化性的通用训练方法。

Method: 引入代理监督方法，将输入域与监督域解耦，通过将估计的空间变换应用于代理图像来确保在相似性定义良好的域中进行监督计算，允许在异构输入上进行训练。

Result: 在三个代表性应用中（抗伪影脑MR配准、掩码无关肺CT配准、多模态MR配准）均表现出对输入变化的强韧性，包括不均匀场、不一致视野和模态差异，同时在精心整理的数据上保持高性能。

Conclusion: 代理监督提供了一个原则性框架，在不增加复杂性的情况下训练鲁棒且可泛化的深度学习配准模型，为更鲁棒的医学图像配准提供了实用途径。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [70] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 本研究提出了一个结合卷积自编码器和Vision Transformer的框架，用于提高牙齿年龄估计的准确性和可解释性，特别针对第二和第三磨牙的性能差异问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在法医年龄估计等高风险应用中面临'黑盒'问题，需要提高模型性能和透明度来支持专家决策。

Method: 使用卷积自编码器(AE)与Vision Transformer(ViT)结合的框架，通过分析AE的潜在空间度量和图像重建来提供多方面的诊断洞察。

Result: 分类准确率显著提升：牙齿37从0.712提高到0.815，牙齿38从0.462提高到0.543。分析表明剩余性能差距主要是由于牙齿38数据集的高类内形态变异性。

Conclusion: 该框架不仅提高了准确性，还提供了模型不确定性的证据，强调了单一可解释性模式的不足，为法医年龄估计提供了更强大的工具支持。

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [71] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: SCoDA是一种无需源域数据的自监督域适应方法，通过几何流形对齐和EMA更新解决传统SFDA方法丢失几何信息的问题，在多个基准数据集上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统SFDA方法依赖全监督预训练源模型，使用余弦相似度对齐实例级特征时会丢弃源模型潜在流形的关键几何信息

Method: 使用自监督预训练的教师模型初始化框架，通过几何流形对齐原则，结合实例级特征匹配和空间相似性损失训练学生模型，采用EMA更新教师参数以防止灾难性遗忘

Result: 在多个基准数据集上的广泛实验表明，SCoDA显著优于最先进的SFDA方法

Conclusion: SCoDA通过自监督预训练和几何流形对齐，有效解决了SFDA中的几何信息丢失问题，取得了优异的性能表现

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [72] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出了一种基于SAM2基础模型的零样本细胞追踪框架，无需人工标注训练数据，在2D和3D显微镜视频中实现竞争性精度


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖昂贵的人工标注数据集，且泛化能力有限，无法适应显微镜数据的巨大多样性

Method: 将Segment Anything 2 (SAM2)大型基础模型集成到追踪流程中，作为完全无监督方法，不依赖特定训练数据集

Result: 在2D和大规模3D延时显微镜视频中达到竞争性精度，无需数据集特定适配

Conclusion: 该方法克服了传统方法的局限性，实现了跨不同显微镜数据集的泛化能力

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [73] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 提出了一种将现有2D多摄像头跟踪系统扩展到3D空间的方法，通过深度信息重建目标点云并恢复3D边界框，在AI City Challenge中获得第三名


<details>
  <summary>Details</summary>
Motivation: 3D空间跟踪需要完全重建2D跟踪系统，这对现有MTMC系统不可行，需要一种能够利用现有2D系统扩展到3D的解决方案

Method: 利用深度信息重建目标点云空间，通过聚类和偏航角细化恢复3D边界框，引入增强的在线数据关联机制利用目标局部ID一致性分配全局ID

Result: 在2025 AI City Challenge的3D MTMC数据集上评估，获得排行榜第三名

Conclusion: 该方法成功将2D多摄像头跟踪系统扩展到3D空间，无需完全重建系统，在挑战赛中表现优异

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [74] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种零样本的指代表达理解方法，通过将REC任务重新定义为基于框的视觉语言验证，使用通用检测器和VLM进行True/False查询，无需特定训练即可达到竞争性甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的REC方法需要任务特定的训练，本文旨在探索零样本工作流是否能达到类似或更好的性能，减少对特定预训练的依赖。

Method: 使用COCO-clean通用检测器(YOLO-World)生成候选框，然后使用通用VLM对每个区域独立进行True/False查询验证，避免跨框干扰，支持弃权和多匹配。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上，该方法不仅超越了零样本GroundingDINO基线，还超过了经过REC训练的GroundingDINO和GroundingDINO+CRG的报告结果。

Conclusion: 工作流设计而非任务特定的预训练是实现强大零样本REC性能的关键因素。

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [75] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 提出RPCP增强技术解决小麦叶片病虫害分割中的像素级不平衡问题，通过随机几何变换和投影滤波来增强罕见类别的学习效果


<details>
  <summary>Details</summary>
Motivation: 小麦叶片病虫害分割中，虫害区域通常只占标注像素的极小部分，这种极端像素级不平衡会导致模型过拟合常见类别而忽略罕见类别，影响整体分割性能

Method: 随机投影复制粘贴(RPCP)增强技术：从训练图像中提取罕见虫害斑块，应用随机几何变换模拟变化，将变换后的斑块粘贴到合适区域，并使用随机投影滤波器优化局部特征，确保与背景自然融合

Result: 实验表明该方法显著提高了虫害类别的分割性能，同时保持甚至略微提升了其他类别的准确率

Conclusion: 目标增强方法能有效缓解极端像素不平衡问题，为农业分割问题提供了简单而有效的解决方案

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [76] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 提出基于隐马尔可夫模型的新框架，结合不确定身份信息和跟踪技术，解决长期多目标跟踪中的身份切换问题，在牲畜跟踪和标准数据集上均取得性能提升


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪方法在长时间视频中因身份切换导致性能下降，难以满足长期跟踪需求。但在畜牧业等实际应用中，可以通过喂食器等来源获得动物的零星身份信息

Method: 使用隐马尔可夫模型（HMM）框架，将不确定的身份信息与跟踪技术相结合，处理长期多目标跟踪中的身份不确定性

Result: 在10分钟猪只跟踪数据集上，即使只有21个喂食站身份识别，也提升了ByteTrack的F1分数；在MOT17和MOT20基准数据集上验证了框架的有效性，对身份识别不确定性具有鲁棒性

Conclusion: 提出的HMM框架能够有效利用零星身份信息改善长期多目标跟踪性能，为实际应用如牲畜行为分析提供了可行的解决方案

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [77] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 这篇综述论文探讨了事件相机与传统帧相机的融合技术，重点分析了这种融合在视频恢复和3D重建任务中的优势，系统回顾了深度学习在时空增强方面的贡献，并整理了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 事件相机作为新兴的生物启发式传感器，具有低延迟、低功耗和高捕获率等优势，但需要与传统帧相机融合才能充分发挥其潜力。本文旨在系统梳理事件流与帧流融合技术的最新进展。

Method: 采用系统性文献综述方法，从两个维度分析深度学习贡献：时间增强（帧插值、运动去模糊）和空间增强（超分辨率、低光增强、HDR增强、伪影减少）。同时探讨3D重建领域的融合进展。

Result: 论文全面回顾了事件相机融合技术的最新研究成果，展示了该融合在提升视觉质量方面的显著效果，特别是在挑战性条件下。还整理了公开可用的数据集资源。

Conclusion: 事件相机与传统相机的融合技术为视觉媒体恢复和增强提供了新的可能性，结合深度学习技术有望推动该领域的进一步发展，特别是在高级视觉应用方面。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [78] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: ISTASTrack是首个基于Transformer的ANN-SNN混合跟踪器，通过ISTA适配器实现RGB和事件数据的有效融合，在多个基准测试中达到SOTA性能并保持高能效。


<details>
  <summary>Details</summary>
Motivation: 现有ANN网络难以充分利用事件流的稀疏和异步特性，而ANN-SNN混合架构在RGB-Event感知中展现出潜力，但跨异构范式的特征融合仍具挑战。

Method: 采用双分支架构：视觉Transformer处理RGB输入，脉冲Transformer处理事件流；设计基于ISTA算法的适配器进行双向特征交互；加入时序下采样注意力模块对齐特征。

Result: 在FE240hz、VisEvent、COESOT和FELT等基准测试中实现了最先进的性能，同时保持高能效。

Conclusion: ISTASTrack证明了ANN-SNN混合设计在鲁棒视觉跟踪中的有效性和实用性，为跨模态跟踪提供了新解决方案。

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [79] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出基于多深度状态空间模型和FLARE损失函数的太阳耀斑预测方法，有效解决类别不平衡问题，在11年太阳活动周期数据上表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 当前太阳耀斑预测性能不足，现有方法难以有效处理耀斑类别间的严重不平衡问题，需要更可靠的预测模型来保护关键基础设施

Method: 使用多深度状态空间模型构建预测框架，并引入频率和局部边界感知可靠性损失函数（FLARE损失）来改善类别不平衡下的预测性能和可靠性

Result: 在覆盖完整11年太阳活动周期的多波长太阳图像数据集上，该方法在Gandin-Murphy-Gerrity分数和真实技能统计量两个标准指标上都优于基线方法

Conclusion: 所提出的多深度状态空间模型结合FLARE损失函数能够有效提升太阳耀斑预测的准确性和可靠性，特别是在处理类别不平衡问题上表现出色

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [80] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: TUNI是一个RGB-热成像语义分割模型，通过统一的编码器同时进行多模态特征提取和跨模态融合，减少了参数和计算成本，实现了实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-T语义分割模型中热特征提取有限、跨模态融合不理想以及编码器冗余导致的实时效率问题。

Method: 提出TUNI模型，使用堆叠块构建RGB-T编码器，通过大规模RGB和伪热数据预训练，整合特征提取和融合；采用精简的热分支架构；引入RGB-T局部模块，使用自适应余弦相似度选择性地强调跨模态的显著一致和不同局部特征。

Result: 在FMB、PST900和CART数据集上达到与最先进模型竞争的性能，参数更少、计算成本更低；在Jetson Orin NX上实现27 FPS的推理速度。

Conclusion: TUNI通过统一的编码器架构和局部特征融合模块，有效解决了RGB-T语义分割中的特征提取和融合问题，同时实现了高效的实时部署能力。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [81] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出了一种基于局部设计元素的少部分字形生成模型，只需输入部分形状而非完整字符即可生成整个字体


<details>
  <summary>Details</summary>
Motivation: 传统少样本字体生成需要完整字符形状，而本方法旨在通过部分设计元素提高字体创建效率，并探索局部设计细节对整体字符结构的影响

Method: 设计了一个基于局部形状输入的字体生成模型，利用部分设计元素来推断和生成完整的字符集

Result: 该方法不仅提高了字体创建的效率，还提供了关于局部设计细节如何影响整体字符结构的见解

Conclusion: 提出的少部分字形生成模型为字体设计提供了更高效的解决方案，同时深化了对字体设计中局部与整体关系的理解

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [82] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 本文提出了一种针对微型和纳米无人机优化的高效视觉惯性里程计(VIO)流水线，在超低功耗RISC-V SoC上实现了实时性能，相比基线流水线RMSE平均降低3.65倍


<details>
  <summary>Details</summary>
Motivation: 解决传统高精度VIO流水线计算需求大、无法在微型无人机等资源受限设备上运行的问题，填补高性能系统与微控制器适用实现之间的技术空白

Method: 采用最先进的特征检测和跟踪方法(SuperPoint、PX4FLOW、ORB)，进行优化和量化以适应RISC-V超低功耗并行SoC，并利用刚体运动模型减少估计误差

Result: 在GAP9低功耗SoC上，使用ORB特征跟踪器时RMSE平均降低3.65倍；PX4FLOW在移动速度低于24像素/帧时以更低运行时间实现与ORB相当的跟踪精度

Conclusion: 该优化流水线成功实现了在超低功耗设备上的实时VIO，为微型无人机等资源受限应用提供了高精度定位解决方案

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [83] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出基于卷积神经网络的层次化多级注意力网络(MLANet)，从单张野外图像重建3D人脸模型，预测几何、纹理、姿态和光照参数


<details>
  <summary>Details</summary>
Motivation: 解决从2D野外图像恢复3D人脸模型的挑战，包括缺乏真实标注数据集和复杂真实环境的困难

Method: 使用预训练层次化主干网络，引入多级注意力机制，采用半监督训练策略结合3DMM参数和可微分渲染器实现端到端训练

Result: 在AFLW2000-3D和MICC Florence基准数据集上进行广泛实验，包括对比和消融研究，定量和定性评估显示方法有效

Conclusion: 提出的MLANet方法能够有效从单张野外图像重建高质量的3D人脸模型，解决了现有挑战

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [84] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: LaV-CoT是一个语言感知的视觉思维链框架，通过多阶段推理流程和多方面奖励优化，显著提升多语言视觉问答性能，在多个数据集上超越开源和专有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本思维链，对多语言多模态推理支持有限，限制了在实际应用中的部署。需要开发能够同时处理视觉和语言信息的推理框架。

Method: 提出多阶段推理流程（文本摘要+边界框、语言识别、空间对象级描述、逐步逻辑推理），采用自动化数据标注方法，结合监督微调和语言感知组相对策略优化的两阶段训练范式。

Result: 在MMMB、Multilingual MMBench和MTVQA等数据集上，比同规模开源基线准确率提升约9.5%，甚至超越规模大2倍的模型约2.6%，优于GPT-4o-0513和Gemini-2.5-flash等专有模型。

Conclusion: LaV-CoT通过语言感知的视觉思维链框架和多方面奖励优化，有效提升了多语言视觉问答的性能和可解释性，具有工业部署的实际价值。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [85] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出无需训练的框架，利用大语言模型解析模糊颜色术语，在CIELAB色彩空间中优化文本嵌入，提升文本到图像生成的颜色准确性


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在处理复杂颜色术语（如蒂芙尼蓝、柠檬绿）时存在颜色对齐问题，无法准确理解人类意图，需要改进颜色保真度

Method: 使用大语言模型解析提示词中的模糊颜色描述，在CIELAB色彩空间基于空间关系优化文本嵌入，指导颜色混合操作

Result: 实验证明该方法在不影响图像质量的情况下显著改善了颜色对齐效果，填补了文本语义与视觉生成之间的差距

Conclusion: 该训练免费框架有效解决了文本到图像生成中的颜色模糊问题，无需额外训练或参考图像即可提升颜色准确性

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [86] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: AVI-Math是首个针对无人机航拍图像多模态数学推理的基准测试，包含3,773个高质量车辆相关问题，涵盖6个数学学科和20个主题，评估显示当前视觉语言模型在此类推理任务上表现不佳


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在无人机遥感领域的数学推理能力（如精确距离计算、轨迹估计和空间分析）尚未得到充分测试，需要专门的基准来评估这一领域的能力

Method: 构建AVI-Math基准数据集，包含从不同高度和角度采集的无人机航拍图像，涵盖几何、逻辑和代数等数学领域，对14个主流视觉语言模型进行全面评估，并探索思维链提示和微调技术

Result: 尽管这些模型在以往多模态基准测试中表现成功，但在AVI-Math的推理任务上表现挣扎，暴露出当前视觉语言模型在数学推理能力上的显著局限性

Conclusion: 研究不仅揭示了视觉语言模型在数学推理方面的局限性，还为推进无人机可信视觉语言模型在实际应用中的发展提供了有价值的见解，思维链提示和微调技术显示出解决这些推理挑战的潜力

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [87] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: BEVTraj是一个新颖的轨迹预测框架，直接在鸟瞰图空间中利用实时传感器数据进行轨迹预测，无需依赖预建高清地图，实现了与最先进地图模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预建高清地图或实时地图构建模块，但预建地图局限于特定区域且无法适应瞬时变化，而地图构建模块可能无法捕捉关键场景细节或引入错误，影响预测性能。

Method: 提出BEVTraj框架，在BEV空间中使用可变形注意力机制从密集BEV特征中提取相关上下文，并引入稀疏目标候选提议模块实现完全端到端预测，无需后处理步骤。

Result: 大量实验表明，BEVTraj实现了与最先进高清地图模型相当的性能，同时通过消除对预建地图的依赖提供了更大的灵活性。

Conclusion: BEVTraj框架成功克服了传统地图依赖方法的局限性，在保持高性能的同时提供了更好的适应性和灵活性。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [88] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 提出了一种利用多视角信息改进多人解析模型在遮挡场景下性能的训练框架，通过弱监督和一致性损失实现，在遮挡情况下相对基线模型提升4.20%


<details>
  <summary>Details</summary>
Motivation: 现有方法在重叠人体分割方面表现不佳，而多视角信息可以提供分离的重叠人体视图

Method: 提出基于弱监督人类实例和多视角一致性损失的新训练框架，使用半自动标注策略从多视角RGB+D数据和3D人体骨架生成标注

Result: 在遮挡场景下，相比基线模型实现了4.20%的相对性能提升

Conclusion: 多视角信息能有效改善多人解析模型在遮挡情况下的表现，提出的训练框架和标注策略具有实用价值

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [89] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0是一个开源的韩英双语视觉语言模型，相比前代模型能力提升，支持多图像理解、文档图表处理和布局感知OCR，在14B和1.7B两个规模版本上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个更强大的双语视觉语言模型，支持韩语和英语，提升多模态理解能力，特别是针对文档、图表和表格等复杂输入的处理，同时保持语言核心能力并提高安全性。

Method: 采用四阶段课程训练和内存高效技术，通过偏好优化提升安全性，支持多图像理解和布局感知OCR（预测文本内容及其空间位置）。

Result: 模型在多模态对齐方面表现增强，在空间定位基准测试中表现强劲，14B模型在OpenCompass VLM排行榜上位列同规模模型第8名，同时发布了适用于设备部署的1.7B轻量版本。

Conclusion: VARCO-VISION-2.0模型推动了双语视觉语言模型的发展及其实际应用，提供了完整规模和高效率两个版本供实际使用。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [90] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 提出了一种轻量级的人脸图像质量评估方法，结合MobileNetV3-Small和ShuffleNetV2网络，使用MSECorrLoss损失函数，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像质量评估方法要么无法捕捉人脸特有的退化特征，要么计算成本过高，限制了在实际应用中的部署。需要一种既准确又高效的解决方案。

Method: 集成两个紧凑卷积神经网络（MobileNetV3-Small和ShuffleNetV2），通过简单平均进行预测级融合，使用结合MSE和Pearson相关正则化的MSECorrLoss损失函数来更好地对齐人类感知判断。

Result: 在VQualA FIQA基准测试中取得了SRCC 0.9829和PLCC 0.9894的优秀性能，同时满足计算效率约束。

Conclusion: 该方法在准确性和计算成本之间实现了良好平衡，适合实际部署，为人脸识别系统在非受控环境中的应用提供了有效的质量评估解决方案。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [91] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出RCOD框架，通过潜在域分组策略和退化感知采样，实现单步扩散模型中保真度与真实感的灵活权衡控制


<details>
  <summary>Details</summary>
Motivation: 传统单步扩散方法在真实图像超分辨率任务中缺乏灵活的保真度-真实感权衡控制机制，无法像多步方法那样通过调整采样步骤来适应不同场景需求

Method: 提出RCOD框架，包含：1）潜在域分组策略实现噪声预测阶段的显式控制；2）退化感知采样策略对齐蒸馏正则化；3）视觉提示注入模块替代文本提示

Result: 在定量指标和视觉质量上均优于现有单步扩散方法，同时保持计算效率，在推理阶段具有灵活的逼真度控制能力

Conclusion: RCOD框架成功解决了单步扩散模型在保真度-真实感权衡方面的局限性，为真实图像超分辨率提供了高效且可控的解决方案

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [92] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL是一个源自由域适应框架，通过梯度引导伪标签优化和余弦相似度对比学习，在无需源数据的情况下提升视盘和视杯分割的跨域性能


<details>
  <summary>Details</summary>
Motivation: 解决视盘和视杯分割模型在不同成像协议或条件下性能显著下降的问题，特别是在无法访问原始源数据的情况下实现鲁棒的域适应

Method: 两阶段方法：第一阶段通过梯度机制提取类别特定特征，进行不确定性量化和原型估计来优化噪声伪标签；第二阶段使用基于余弦相似度的对比损失来增强视杯和视杯特征之间的类间可分离性

Result: 在具有挑战性的跨域眼底成像数据集上，Grad-CL优于最先进的无监督和源自由域适应方法，实现了优越的分割精度和改善的边界描绘

Conclusion: Grad-CL框架有效解决了医学图像分割中的域适应问题，特别是在保护数据隐私的场景下，为眼科疾病的早期诊断提供了可靠的技术支持

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [93] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: VQBridge方法解决了向量量化(VQ)训练中的不稳定问题，通过压缩-处理-恢复管道实现100%码本使用率，显著提升图像重建和生成性能


<details>
  <summary>Details</summary>
Motivation: 传统VQ训练存在直通估计偏差、滞后更新和稀疏梯度等问题，导致码本使用率低和重建性能不佳，需要更稳定高效的训练方法

Method: 提出VQBridge投影器，基于映射函数方法，通过compress-process-recover管道优化码向量，结合学习退火实现稳定训练

Result: 在262k大码本下实现100%使用率，达到SOTA重建性能，与LlamaGen集成后图像生成性能超越VAR 0.5 rFID和DiT 0.2 rFID

Conclusion: 高质量分词器对自回归图像生成至关重要，FVQ方法有效、可扩展且通用，为大规模码本训练提供了稳定解决方案

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [94] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock是一种通过渐进层冻结实现从像素预测到潜在预测过渡的自监督视觉表示学习方法，可加速MAE训练并避免表示崩溃


<details>
  <summary>Details</summary>
Motivation: 观察到视频掩码自编码(MAE)训练中ViT层按深度顺序收敛的现象，希望利用这一发现来加速训练并改进潜在预测方法

Method: 通过明确的进度表在训练过程中逐步冻结模型层，实现从像素预测到潜在预测的渐进过渡

Result: 在高达40亿参数的大型模型上应用LayerLock，在4DS感知套件上的表现超过了非潜在掩码预测方法

Conclusion: LayerLock提供了一种简单有效的方法来改进自监督视觉表示学习，能够处理大规模模型并避免表示崩溃问题

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [95] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本研究首次系统比较了隐式和显式新视角合成方法在太空3D物体重建中的表现，重点评估了外观嵌入的作用。发现嵌入虽然能提高光度保真度，但对几何精度提升有限，而凸面溅射比高斯溅射能提供更紧凑、无杂乱的表示。


<details>
  <summary>Details</summary>
Motivation: 太空机器人应用需要高精度的几何重建，而现有新视角合成方法中外观嵌入对几何准确性的实际贡献尚不明确，需要系统评估其在空间场景中的有效性。

Method: 使用SPEED+数据集，比较了K-Planes、高斯溅射和凸面溅射三种方法，分析外观嵌入对光度保真度和几何精度的影响，特别关注表示效率和几何准确性的权衡。

Result: 外观嵌入主要减少显式方法所需的图元数量而非提升几何保真度；凸面溅射相比高斯溅射能产生更紧凑、无杂乱的表示，更适合安全关键应用。

Conclusion: 外观嵌入在几何中心任务中的作用有限，研究明确了空间场景中重建质量与表示效率之间的权衡关系，为太空机器人应用提供了方法选择指导。

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [96] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA是一个新的训练框架，通过减少领域偏差和增强语义对齐来提升AI生成图像检测的泛化能力，在GenImage基准上实现了5.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在分布内生成图像上表现良好，但对未见生成模型的泛化能力有限，主要原因是它们依赖生成特定的伪影（如风格先验和压缩模式）。

Method: 提出GAMMA框架，引入多样化操作策略（基于修复的操作和语义保持扰动），采用多任务监督（双分割头和分类头），并引入反向交叉注意力机制让分割头指导分类分支。

Result: 在GenImage基准上达到最先进的泛化性能，准确率提升5.8%，并对新发布的生成模型（如GPT-4o）保持强鲁棒性。

Conclusion: GAMMA通过减少领域偏差和增强语义对齐，有效提升了AI生成图像检测的泛化能力，为应对日益复杂的生成模型提供了有效解决方案。

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [97] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 本研究比较了三种胎儿脑MRI超分辨率重建方法(NiftyMIC、SVRTK、NeSVoR)在140例扫描中的表现，发现NeSVoR重建成功率最高(>90%)，虽然不同方法间体积测量存在差异，但对脑室扩大诊断分类性能无影响。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI通常采用快速多视角2D切片采集以减少运动伪影，但这些图像分辨率低且可能受运动影响，无法充分捕捉3D解剖结构。现有超分辨率重建方法的比较性能，特别是在病理情况下，以及对下游体积分析和诊断任务的影响尚未充分探索。

Method: 应用三种最先进的SRR方法(NiftyMIC、SVRTK、NeSVoR)处理140例胎儿脑MRI扫描(包括健康对照和脑室扩大病理病例)，使用BoUNTi算法分割重建后的高分辨率体积以提取9个主要脑结构体积，评估视觉质量、重建成功率、体积测量一致性和诊断分类性能。

Result: NeSVoR在健康组和病理组均表现出最高且最一致的重建成功率(>90%)。虽然不同SRR方法间的体积估计存在显著差异，但脑室扩大的分类性能不受SRR方法选择的影响。

Conclusion: 研究结果突显了NeSVoR的鲁棒性，以及尽管SRR引起的体积变异性，诊断性能仍具有弹性。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [98] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 提出Mask Consistency Regularization (MCR)训练策略，通过掩码扩张和重塑扰动来解决图像修复中物体移除任务的两个关键问题：掩码幻觉和掩码形状偏差。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在物体移除任务中面临掩码幻觉（在掩码区域生成无关内容）和掩码形状偏差（填充内容模仿掩码形状而非周围内容）两大挑战。

Method: 提出MCR训练策略，在训练过程中引入两种掩码扰动：扩张和重塑，强制这些扰动分支的输出与原始掩码保持一致。扩张掩码帮助对齐模型输出与周围内容，重塑掩码鼓励模型打破掩码形状偏差。

Result: 实验证明MCR显著减少了幻觉和掩码形状偏差，在物体移除任务中实现了更好的性能。

Conclusion: MCR通过掩码一致性正则化策略有效解决了物体移除中的关键问题，能够产生更鲁棒和上下文连贯的图像修复结果。

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [99] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror是一个全面的文本到图像生成人工制品评估框架，包含首个大规模人工标注数据集MagicData340K、基于VLM的MagicAssessor评估模型和自动化基准MagicBench，揭示了当前顶级T2I模型仍存在严重人工制品问题


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中普遍存在的物理人工制品（如解剖和结构缺陷）问题，当前缺乏系统性和细粒度的评估框架

Method: 1)建立人工制品详细分类法；2)人工标注34万张生成图像的MagicData340K数据集；3)训练MagicAssessor视觉语言模型进行评估；4)设计新颖数据采样策略和多级奖励系统的GRPO方法；5)构建MagicBench自动化基准

Result: 评估发现即使GPT-image-1等顶级模型也存在显著人工制品问题，人工制品减少是未来T2I发展的关键前沿

Conclusion: MagicMirror框架填补了T2I生成人工制品评估的空白，为模型改进提供了系统性的评估工具和基准

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [100] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip是一个新的手语翻译框架，通过融合手动和非手动线索（特别是手势和唇部运动特征），并采用分层对比学习来提高翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法主要关注手动信号（手势），而忽视了非手动线索如口型动作。实际上，口型在手语中传递重要语言信息，对区分视觉相似的手势至关重要。

Method: 提出SignClip框架，融合空间手势和唇部运动特征；引入分层对比学习框架，具有多级对齐目标，确保手语-唇部和视觉-文本模态间的语义一致性。

Result: 在两个基准数据集PHOENIX14T和How2Sign上的实验显示，该方法优于现有技术。在PHOENIX14T的无注释设置下，BLEU-4从24.32提升到24.71，ROUGE从46.57提升到48.38。

Conclusion: SignClip通过有效融合手动和非手动线索，显著提升了手语翻译的准确性，证明了非手动线索在手语理解中的重要性。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [101] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文分析了开源和闭源大型视觉语言模型在文本篡改检测方面的性能，发现开源模型正在接近但仍落后于闭源模型如GPT-4o，并揭示了专门用于图像篡改检测的VLM在文本篡改检测中存在泛化问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注大型视觉语言模型在图像篡改检测方面的有效性，但文本篡改检测的研究相对缺失，本文旨在填补这一知识空白。

Method: 通过在不同文本篡改数据集上分析闭源和开源VLMs的性能，包括对野外场景文本和模拟真实世界滥用的幻想ID卡上的篡改进行基准测试。

Result: 开源模型在文本篡改检测方面正在进步但仍落后于闭源模型，专门用于图像篡改检测的VLM在文本篡改任务上存在泛化问题。

Conclusion: 文本篡改检测是VLM能力评估的重要维度，开源模型需要进一步改进以达到闭源模型的水平，且专门化模型需要更好的泛化能力。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [102] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD是一个新颖的多模态零样本3D异常检测框架，通过点云、RGB图像和文本语义的多模态协作学习实现优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本3D异常检测方法主要关注点云数据，忽略了RGB图像和文本先验等互补模态提供的丰富语义线索。

Method: 提出了多模态提示学习机制（MPLM）增强模态内表示能力和模态间协作学习，以及协作调制机制（CMM）充分利用点云和RGB图像的互补表示。

Result: 大量实验证明MCL-AD框架在零样本3D异常检测中达到了最先进的性能。

Conclusion: 多模态协作学习能够显著提升零样本3D异常检测的效果，证明了利用多种模态信息的价值。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [103] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 提出了一种基于Lipschitz引导的随机深度(DropPath)方法，通过深度相关的丢弃概率来控制网络的有效Lipschitz常数，在保持清洁精度的同时提升对抗鲁棒性并减少计算量。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和Vision Transformers在计算机视觉中表现优异但对抗扰动高度脆弱，标准防御方法计算成本高或缺乏形式化保证。

Method: Lipschitz引导的随机深度方法，丢弃概率随深度增加而增加，以控制网络的有效Lipschitz常数，正则化更深层。

Result: 在CIFAR-10和ViT-Tiny上的实验表明，该方法保持接近基线的清洁精度，在FGSM、PGD-20和AutoAttack下增强鲁棒性，并显著减少FLOPs。

Conclusion: 深度相关的DropPath调度是一种有效的正则化方法，能够同时提升对抗鲁棒性和计算效率。

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [104] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 提出基于能量地图的概率框架，用于精确定位复杂城市环境中的街道家具，通过随机生死优化算法整合地理空间信息，提高定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中街道家具的精确定位问题，这对公共基础设施的有效监控和维护至关重要，为地方当局和私人利益相关者提供技术支持。

Method: 使用基于能量地图的概率框架，将空间位置可能性编码为地图格式，整合GIS图层、道路地图等外部地理空间信息，采用随机生死优化算法推断最可能的资产配置。

Result: 通过在都柏林市中心街灯基础设施的地理定位数据集上进行真实模拟评估，证明了该方法在可扩展和准确的城市资产映射方面的潜力。

Conclusion: 该方法能够有效提高城市街道家具的定位精度，具有可扩展性和实用性，算法实现已在GitHub开源。

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [105] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ClusCa通过空间聚类和特征缓存技术，在保持生成质量的同时显著加速扩散变换器的推理过程，实现4.96倍加速且图像质量提升0.51%。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅利用时间维度相似性，忽略了空间维度的相似性，导致计算效率仍有提升空间。

Method: 对每个时间步的token进行空间聚类，每个聚类只计算一个代表性token，然后将信息传播给聚类内所有其他token，减少90%以上的token计算量。

Result: 在DiT、FLUX和HunyuanVideo上验证有效，FLUX实现4.96倍加速，ImageReward达到99.49%（比原模型提升0.51%），无需训练即可直接应用。

Conclusion: ClusCa作为正交互补的特征缓存方法，通过空间聚类有效利用空间相似性，显著提升扩散变换器的推理效率。

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [106] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter是首个完全整数化的ViT分割框架，通过系统替换浮点运算为整数运算，在保持合理精度损失的同时大幅提升效率


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现优异，但在资源受限设备上部署受限，量化虽然能提高效率但ViT分割模型在低精度下表现脆弱

Method: 基于Segmenter架构，系统替换浮点运算为整数运算；提出λ-ShiftGELU激活函数处理长尾分布；移除L2归一化层；用最近邻上采样替换双线性插值

Result: 在FP32基线5.1%精度损失内，模型大小减少3.8倍，推理速度提升1.2倍；单张校准图像的PTQ也能获得竞争性精度

Conclusion: I-Segmenter为ViT分割模型的现实部署提供了实用解决方案，在效率和精度间取得了良好平衡

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [107] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: GARD是一种基于伽马扩散模型的OCT图像去噪方法，通过噪声减少保真项和加速推理框架，在保持解剖结构的同时有效去除散斑噪声，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: OCT图像受散斑噪声影响严重，传统去噪方法难以在噪声去除和解剖结构保持之间取得平衡，需要更准确地模拟散斑统计特性的方法。

Method: 提出GARD方法：使用Denoising Diffusion Gamma Model替代传统高斯噪声假设，引入Noise-Reduced Fidelity Term利用预处理图像指导去噪，采用DDIM框架加速推理。

Result: 在配对噪声-低噪声OCT B扫描数据集上，GARD在PSNR、SSIM和MSE指标上显著优于传统方法和先进深度学习模型，定性结果显示边缘更清晰、解剖细节保持更好。

Conclusion: GARD通过伽马扩散模型和噪声减少保真项，有效解决了OCT图像去噪中噪声去除与结构保持的平衡问题，为医学图像分析提供了更准确的工具。

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [108] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: GLAM模型通过几何引导的全局和局部对齐方法，改进乳腺X线摄影多视图分析，在多个数据集上超越现有基线方法


<details>
  <summary>Details</summary>
Motivation: 现有乳腺X线摄影视觉语言模型忽视多视图关系特性，无法像放射科医生那样同时分析双侧视图，导致几何上下文丢失和预测效果不佳

Method: 提出GLAM模型，利用乳腺X线摄影多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉、视觉-语言的对比学习，学习局部跨视图对齐和细粒度局部特征

Result: 在EMBED数据集（最大公开乳腺X线摄影数据集之一）上预训练后，模型在多个数据集的不同设置下均优于基线方法

Conclusion: GLAM模型通过几何引导的多视图对齐方法，有效提升了乳腺X线摄影视觉语言模型的性能，为医学影像分析提供了新的解决方案

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [109] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 这篇综述论文回顾了现代通用视觉语言模型(VLMs)中视觉定位的研究进展，包括其重要性、核心组件、实际应用、基准评估以及与多模态思维链和推理的关系，并分析了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉定位能力使模型能够根据文本描述识别视觉输入中的特定区域，这种能力对于指代表达理解、细粒度视觉问答、实体引用字幕生成以及模拟和真实环境中的控制等广泛应用至关重要。

Method: 论文首先概述了视觉定位在VLMs中的重要性，然后详细阐述了开发具有定位能力的现代模型的核心组件范式，并考察了其实际应用，包括基准测试和评估指标。

Result: 通过系统梳理代表性工作，论文提供了对视觉定位技术发展现状的全面认识，并分析了该领域与其他相关技术(如多模态思维链和推理)的多方面相互关系。

Conclusion: 论文最后分析了视觉定位面临的内在挑战，并提出了未来研究的 promising 方向，为该领域的进一步发展提供了重要指导。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [110] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 提出一种针对文本图像编辑方法的视觉组件攻击——Attention Attack，通过使用自动生成的图像描述作为代理提示来破坏文本提示与视觉表示之间的交叉注意力，从而破坏编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的图像编辑方法容易受到对抗攻击，需要开发能够破坏编辑过程而不需要了解具体编辑方法或提示的攻击手段。

Method: 使用自动生成的图像描述作为代理提示，破坏文本提示与视觉表示之间的交叉注意力机制，打破图像内容与其文本描述之间的对齐关系。

Result: 在TEDBench++基准测试中，该攻击显著降低了编辑性能，同时保持攻击的不可感知性。

Conclusion: Attention Attack是一种有效的对抗攻击方法，能够破坏文本图像编辑系统的功能，同时提出了新的评估指标来更好地衡量攻击效果。

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [111] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 该论文研究如何通过知识蒸馏技术降低基于神经网络的图像压缩方法的计算资源需求，使深度学习压缩方法更适合资源受限平台的实时应用。


<details>
  <summary>Details</summary>
Motivation: 虽然基于深度学习的图像压缩方法在性能上优于传统编解码器，但需要大量计算资源，不适合在资源受限平台上进行实时应用，这限制了其在主流应用中的部署。

Method: 采用知识蒸馏训练范式，让较小的神经网络通过部分学习更大、更复杂模型的输出来实现更好的性能，而不是独立训练。

Result: 研究表明知识蒸馏可以有效地应用于图像压缩任务：适用于不同架构大小、实现不同的图像质量/比特率权衡，并节省处理和能源资源。

Conclusion: 知识蒸馏为降低神经网络图像压缩方法的资源需求提供了有效途径，未来研究可以探索不同教师模型和替代损失函数的影响，并将该方法扩展到基于transformer的模型。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [112] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 提出了一种无需训练的本征图像分解方法，仅使用可见光和热成像图像对，通过热成像检测吸收光来推断反射率和阴影的序数关系，实现自监督分解。


<details>
  <summary>Details</summary>
Motivation: 解决传统本征图像分解方法因缺乏真实世界地面实况数据而依赖合成数据或稀疏标注的问题，特别是在室外场景中数据稀缺的挑战。

Method: 利用热成像相机检测被吸收光产生的热量，建立可见光和热成像图像强度之间的序数关系，进而推导出阴影和反射率的序数关系，通过自监督方式优化神经网络进行分解。

Result: 在自然光和人工光照条件下对已知反射率和阴影进行定量评估，并在多样化室外场景进行定性实验，结果显示优于近期基于学习的方法。

Conclusion: 该方法为获取真实世界序数监督提供了一条可扩展的路径，解决了传统手动标注不可行的问题，在室外场景本征图像分解方面表现出色。

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [113] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本文提出了一种压缩视频质量增强(CVQE)的新分类法和统一基准测试框架，解决了现有综述在系统分类、架构比较和基准测试方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩视频质量增强综述缺乏系统性的分类方法，无法将具体方法与编码标准和伪影类型关联，缺乏跨编码类型的架构范式比较分析，以及基准测试实践不完善。

Method: 1) 提出了新的分类法，按架构范式、编码标准和压缩域特征利用对CVQE方法进行分类；2) 设计了统一的基准测试框架，整合现代压缩协议和标准测试序列；3) 系统分析了重建性能与计算复杂度之间的权衡关系。

Result: 建立了CVQE方法的系统性分类体系，提供了公平的多标准评估框架，揭示了当前最先进方法在性能与复杂度之间的关键权衡。

Conclusion: 该综述为CVQE研究和部署中的一致性评估和明智模型选择奠定了基础，并指出了未来研究的有前景方向。

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [114] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: MM SAM-adapter是一个新颖的多模态语义分割框架，通过适配器网络将融合的多模态特征注入到Segment Anything Model的RGB特征中，在保持RGB特征强泛化能力的同时选择性利用辅助模态信息，在多个挑战性基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割方法在恶劣光照、遮挡和恶劣天气等挑战性条件下表现脆弱，需要整合辅助传感器数据（如LiDAR、红外）来提供互补信息以增强鲁棒性。

Method: 提出MM SAM-adapter框架，使用适配器网络将融合的多模态特征注入到SAM的RGB特征中，实现多模态信息的平衡高效利用，仅在辅助模态提供额外线索时选择性整合。

Result: 在DeLiVER、FMB和MUSES三个挑战性基准测试中实现了state-of-the-art性能，在RGB-easy和RGB-hard子集上都优于竞争方法。

Conclusion: 多模态适配方法能够有效提升场景理解的鲁棒性，MM SAM-adapter框架在有利和不利条件下都表现出色，证明了多模态融合的有效性。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [115] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen是一种基于潜在扩散模型的第二代图像生成方法，通过用单步生成器替换VAE解码器，可以从固定大小的潜在表示生成任意分辨率的图像，显著降低计算复杂度，将4K图像生成时间从100多秒减少到10秒以内。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在生成高分辨率图像时计算需求呈二次增长，导致4K图像生成延迟超过100秒，需要一种更高效的任意分辨率图像生成方法。

Method: 将扩散模型生成的固定潜在表示作为内容表示，提出使用单步生成器解码任意分辨率图像，用新的生成器替换VAE解码器，无需重新训练扩散模型。

Result: 实验表明InfGen能够将多种模型升级到任意高分辨率时代，同时将4K图像生成时间缩短到10秒以内。

Conclusion: InfGen简化了高分辨率图像生成过程，显著降低了计算复杂度，可应用于任何使用相同潜在空间的模型，实现了高效的任意分辨率图像生成。

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [116] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本研究将三种先进的时序自监督学习方法应用于3D脑部MRI分析，通过处理可变长度输入和学习鲁棒空间特征，在阿尔茨海默病预测任务中表现优于监督学习。


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病预测中深度学习模型面临的标注数据不足、跨数据集泛化能力差以及对不同扫描次数和时间间隔缺乏灵活性的问题。

Method: 采用时序自监督学习（SSL）方法，包括时序顺序预测和对比学习，处理可变长度输入并学习鲁棒空间特征，使用四个公开数据集共3,161名患者进行预训练。

Result: 在七个下游任务中的六个任务上，自监督学习方法的表现优于监督学习，展示了跨任务和不同输入图像数量及时间间隔的适应性和泛化能力。

Conclusion: 时序自监督学习方法在阿尔茨海默病预测中具有优越性能，能够处理临床应用中多变的数据条件，代码和模型已公开。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [117] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: 提出了一种新颖的两阶段生成-估计框架，用于PBR材质生成，通过微调扩散模型合成纹理图像，然后使用链式分解方案预测SVBRDF通道，实现了高质量、灵活的用户控制。


<details>
  <summary>Details</summary>
Motivation: 传统材质创建和重建需要艺术家大量时间和专业知识，现有基于视觉基础模型的方法在质量、灵活性和用户控制方面存在不足。

Method: 两阶段框架：1）生成阶段使用微调扩散模型合成着色、可平铺的纹理图像；2）估计阶段采用链式分解方案，通过单步图像条件扩散模型顺序预测SVBRDF通道。

Result: 方法在质量和性能上优于现有材质生成和估计方法，对生成纹理和真实照片都表现出强鲁棒性，支持文本到材质、图像到材质、结构引导生成和材质编辑等多种应用。

Conclusion: 该框架高效、高质量且支持灵活用户控制，为材质创建提供了有效的解决方案，在多个应用场景中展现出优越性能。

Abstract: Material creation and reconstruction are crucial for appearance modeling but
traditionally require significant time and expertise from artists. While recent
methods leverage visual foundation models to synthesize PBR materials from
user-provided inputs, they often fall short in quality, flexibility, and user
control. We propose a novel two-stage generate-and-estimate framework for PBR
material generation. In the generation stage, a fine-tuned diffusion model
synthesizes shaded, tileable texture images aligned with user input. In the
estimation stage, we introduce a chained decomposition scheme that sequentially
predicts SVBRDF channels by passing previously extracted representation as
input into a single-step image-conditional diffusion model. Our method is
efficient, high quality, and enables flexible user control. We evaluate our
approach against existing material generation and estimation methods,
demonstrating superior performance. Our material estimation method shows strong
robustness on both generated textures and in-the-wild photographs. Furthermore,
we highlight the flexibility of our framework across diverse applications,
including text-to-material, image-to-material, structure-guided generation, and
material editing.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [118] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 评估大型语言模型对抗攻击的韧性，发现RoBERTa-Base和FlanT5表现优异（攻击成功率0%），而BERT-Base易受攻击（TextFooler成功率93.75%）。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在面对对抗攻击时的安全性和鲁棒性，识别现有防护机制的优缺点。

Method: 使用TextFooler和BERTAttack对Flan-T5、BERT和RoBERTa-Base进行系统性对抗测试。

Result: RoBERTa-Base和FlanT5展现出卓越韧性，攻击成功率为0%；BERT-Base准确率从48%降至3%，TextFooler攻击成功率达93.75%。

Conclusion: 某些LLM已具备有效防御机制但计算资源消耗大，研究为开发更高效防御策略提供实践建议。

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [119] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: 本文提出了一种无监督方法，通过筛选注意力头并使用字符而非词片段来从Whisper中提取更精确的词级时间戳，在20-100毫秒的严格容差下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要额外训练，要么性能不足，且评估标准宽松（通常使用200毫秒以上的容差）。研究发现Whisper中的某些注意力头能够捕捉准确的词对齐信息，且使用字符比词片段能产生更精细的对齐。

Method: 通过筛选注意力头，在教师强制模式下使用字符输入Whisper，提出无监督的词对齐提取方法，无需额外训练。

Result: 该方法在20-100毫秒的严格容差范围内，产生的词对齐比现有工作更准确。

Conclusion: 研究发现特定注意力头对词对齐有重要作用，使用字符输入可获得更精确的时间戳，提出的无监督方法在严格标准下优于现有技术。

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [120] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 提出了一种通用的2D卷积前端架构，用于语音识别特征提取，相比现有方法更加统一且参数高效，性能与现有监督学习方法相当


<details>
  <summary>Details</summary>
Motivation: 现有神经前端方法仍受传统方法影响较大，虽然这种归纳偏置有助于系统设计，但作者希望开发更通用的前端架构，并统一前端设计而不是组合不同来源的层拓扑

Method: 使用2D卷积神经网络作为前端特征提取器，通过系统实验减少现有技术的影响，实现参数高效的通用前端架构

Result: 该方法在计算资源有限的情况下表现良好，性能与现有监督学习特征提取器相当，证明了通用统一方法的可行性

Conclusion: 2D卷积前端是一种参数高效且性能优异的通用特征提取方案，特别适合计算资源有限的场景，相比大型预训练模型更具实用性

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [121] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本文分析了会议转录中的泄漏问题，发现在主要说话人活动区域存在显著的跨通道泄漏，但VAD会忽略这些泄漏部分，因此对最终性能影响不大。同时比较了不同分割方法，显示先进的二值化方法能将与oracle分割的差距缩小三分之一。


<details>
  <summary>Details</summary>
Motivation: 会议转录领域近年来取得显著进展但仍存在性能限制，需要分析语音分离中的泄漏问题及其对系统性能的影响。

Method: 扩展了先前提出的泄漏分析框架，增加了对时间局部性的敏感性分析，比较了不同分割方法（能量基VAD与先进二值化方法），并分析了影响性能的因素。

Result: 发现主要说话人活动区域存在显著跨通道泄漏，但VAD会忽略这些泄漏；先进二值化方法相比简单能量基VAD能将与oracle分割的差距缩小三分之一；在仅使用LibriSpeech数据训练的系统中达到了LibriCSS上的最先进性能。

Conclusion: 虽然存在跨通道泄漏问题，但由于VAD的过滤作用，对最终转录性能影响有限；先进的分割方法能显著提升系统性能，但仍存在改进空间。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [122] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出了ZADS方法，一种无需重新训练扩散模型的零样本自适应采样技术，通过测试时优化数据保真度权重来提升加速MRI重建质量


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解决逆问题时严重依赖精心调优的数据保真度权重，特别是在快速采样计划下表现不佳，需要一种能自适应调整权重的方法

Method: 将去噪过程视为固定的展开采样器，仅使用欠采样测量值以自监督方式优化保真度权重，无需重新训练扩散先验

Result: 在fastMRI膝盖数据集上的实验表明，ZADS在多种噪声计划和采集设置下均优于传统压缩感知和现有扩散方法

Conclusion: ZADS提供了一种通用的权重自适应方法，能够在不重新训练的情况下适应不同的测量条件和时间步计划，实现高质量重建

Abstract: Diffusion/score-based models have recently emerged as powerful generative
priors for solving inverse problems, including accelerated MRI reconstruction.
While their flexibility allows decoupling the measurement model from the
learned prior, their performance heavily depends on carefully tuned data
fidelity weights, especially under fast sampling schedules with few denoising
steps. Existing approaches often rely on heuristics or fixed weights, which
fail to generalize across varying measurement conditions and irregular timestep
schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling
(ZADS), a test-time optimization method that adaptively tunes fidelity weights
across arbitrary noise schedules without requiring retraining of the diffusion
prior. ZADS treats the denoising process as a fixed unrolled sampler and
optimizes fidelity weights in a self-supervised manner using only undersampled
measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS
consistently outperforms both traditional compressed sensing and recent
diffusion-based methods, showcasing its ability to deliver high-fidelity
reconstructions across varying noise schedules and acquisition settings.

</details>


### [123] [Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms](https://arxiv.org/abs/2509.09972)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Mohsen Mesgaran,Parastoo Farajpoor,Hamid Jafarbiglu*

Main category: eess.IV

TL;DR: 本研究结合无人机多光谱影像和LSTM深度学习网络，使用SMOTE技术处理类别不平衡，成功实现了番茄田中分枝列当的早期检测，最佳准确率达到88.37%，召回率95.37%。


<details>
  <summary>Details</summary>
Motivation: 分枝列当对加州番茄产业构成严重威胁，其地下生命周期使早期检测困难，传统化学防治方法成本高、环境危害大且效果有限。

Method: 在已知感染列当的番茄农场进行研究，使用无人机多光谱影像采集数据，通过LSTM深度学习网络处理时序生长阶段数据，并采用SMOTE技术解决类别不平衡问题。

Result: 在897生长度日时，检测准确率达到79.09%，召回率70.36%；整合所有生长阶段并使用SMOTE增强后，准确率提升至88.37%，召回率达到95.37%。

Conclusion: 时序多光谱分析和LSTM网络在早期列当检测方面具有强大潜力，无人机多光谱传感结合深度学习可为精准农业提供有力工具，减少损失并提高番茄生产的可持续性。

Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche
ramosa) to California's tomato industry, which supplies over 90 percent of U.S.
processing tomatoes. The parasite's largely underground life cycle makes early
detection difficult, while conventional chemical controls are costly,
environmentally harmful, and often ineffective. To address this, we combined
drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep
learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)
to handle class imbalance. Research was conducted on a known broomrape-infested
tomato farm in Woodland, Yolo County, CA, across five key growth stages
determined by growing degree days (GDD). Multispectral images were processed to
isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with
79.09 percent overall accuracy and 70.36 percent recall without integrating
later stages. Incorporating sequential growth stages with LSTM improved
detection substantially. The best-performing scenario, which integrated all
growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy
and 95.37 percent recall. These results demonstrate the strong potential of
temporal multispectral analysis and LSTM networks for early broomrape
detection. While further real-world data collection is needed for practical
deployment, this study shows that UAV-based multispectral sensing coupled with
deep learning could provide a powerful precision agriculture tool to reduce
losses and improve sustainability in tomato production.

</details>


### [124] [Polarization Denoising and Demosaicking: Dataset and Baseline Method](https://arxiv.org/abs/2509.10098)
*Muhamad Daniel Ariff Bin Abdul Rahman,Yusuke Monno,Masayuki Tanaka,Masatoshi Okutomi*

Main category: eess.IV

TL;DR: 提出用于偏振去噪和去马赛克的新数据集和方法，包含40个真实场景和三种噪声条件，采用先去噪后去马赛克的信号处理方法，在图像重建性能上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏合适的评估数据集和可靠的基线方法，偏振去噪和去马赛克的联合任务研究较少，而DoFP偏振仪的单次拍摄多偏振方向成像能力在许多应用中很有价值。

Method: 采用先去噪后去马赛克的方法，基于公认的信号处理组件构建可复现的方法，使用包含噪声马赛克输入和无噪声完整图像对的数据集。

Result: 实验结果表明，该方法在图像重建性能上优于其他替代方法，提供了一个可靠的基线。

Conclusion: 提出的数据集和方法为偏振去噪和去马赛克任务提供了有效的解决方案，填补了该领域的研究空白。

Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images
with multiple polarization orientations in one shot and thus it is valuable for
many applications using polarimetric information. The image processing pipeline
for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.
While polarization demosaicking for a noise-free case has increasingly been
studied, the research for the joint task of polarization denoising and
demosaicking is scarce due to the lack of a suitable evaluation dataset and a
solid baseline method. In this paper, we propose a novel dataset and method for
polarization denoising and demosaicking. Our dataset contains 40 real-world
scenes and three noise-level conditions, consisting of pairs of noisy mosaic
inputs and noise-free full images. Our method takes a
denoising-then-demosaicking approach based on well-accepted signal processing
components to offer a reproducible method. Experimental results demonstrate
that our method exhibits higher image reconstruction performance than other
alternative methods, offering a solid baseline.

</details>


### [125] [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](https://arxiv.org/abs/2509.10348)
*Yehudit Aperstein,Amit Tzahar,Alon Gottlib,Tal Verber,Ravit Shagan Damti,Alexander Apartsin*

Main category: eess.IV

TL;DR: 本研究提出了基于DenseNet-121的不确定性感知框架，通过熵拒绝和置信区间拒绝两种选择性预测机制，在胸部X光多标签分类中提高模型可靠性，避免过度自信预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像任务中存在过度自信风险，特别是在需要同时检测多种共发病变的胸部X光多标签分类中，不确定预测可能带来高风险。

Method: 使用DenseNet-121骨干网络，集成熵基拒绝和置信区间拒绝两种选择性预测机制，采用分位数校准程序调整全局或类特定的拒绝阈值。

Result: 在三个大型公开数据集上的实验表明，选择性拒绝改善了诊断准确性和覆盖范围之间的权衡，熵基拒绝在所有病理学中获得了最高的平均AUC。

Conclusion: 该研究支持将选择性预测整合到AI辅助诊断工作流程中，为深度学习在临床环境中更安全、不确定性感知的部署提供了实用步骤。

Abstract: Overconfidence in deep learning models poses a significant risk in
high-stakes medical imaging tasks, particularly in multi-label classification
of chest X-rays, where multiple co-occurring pathologies must be detected
simultaneously. This study introduces an uncertainty-aware framework for chest
X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective
prediction mechanisms: entropy-based rejection and confidence interval-based
rejection. Both methods enable the model to abstain from uncertain predictions,
improving reliability by deferring ambiguous cases to clinical experts. A
quantile-based calibration procedure is employed to tune rejection thresholds
using either global or class-specific strategies. Experiments conducted on
three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)
demonstrate that selective rejection improves the trade-off between diagnostic
accuracy and coverage, with entropy-based rejection yielding the highest
average AUC across all pathologies. These results support the integration of
selective prediction into AI-assisted diagnostic workflows, providing a
practical step toward safer, uncertainty-aware deployment of deep learning in
clinical settings.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [126] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: HYPOGENEAGENT是一个基于大语言模型的框架，将细胞聚类注释转化为可量化优化的任务，通过LLM生成GO假设并评估聚类一致性和区分度，在Perturb-seq数据中表现出比传统指标更好的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模单细胞和Perturb-seq研究中的聚类分辨率选择和功能注释通常依赖启发式方法和专家经验，具有主观性，需要更客观的自动化解决方案。

Method: 使用LLM作为基因集分析师生成GO假设和置信度评分，然后通过句子嵌入模型计算聚类内一致性（intra-cluster agreement）和聚类间区分度（inter-cluster separation），组合得到分辨率评分。

Result: 在K562 CRISPRi Perturb-seq数据集测试中，该方法选择的分辨率与已知通路对齐度优于传统指标如轮廓系数、模块度评分等。

Conclusion: LLM代理可以作为聚类分辨率和功能注释的客观裁决者，为单细胞多组学研究中的全自动化、上下文感知解释流程铺平道路。

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [127] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: 研究发现大型语言模型对话代理的人格表达水平与用户人格匹配度呈倒U型关系，中等表达水平在多个评价维度上表现最佳，人格对齐可进一步改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着LLM使对话代理能够表达独特个性，需要研究不同个性表达水平和用户-代理个性匹配如何影响用户在目标导向任务中的感知。

Method: 采用150名参与者的组间实验，使用新颖的Trait Modulation Keys框架控制对话代理在五大特质上的低、中、高表达水平，让参与者完成旅行规划任务。

Result: 结果显示倒U型关系：中等表达水平在智力、愉悦度、拟人化、采用意愿、信任和喜爱度等方面获得最积极评价；个性对齐进一步改善结果，外向性和情绪稳定性是最有影响力的特质；聚类分析识别出三种兼容性特征。

Conclusion: 个性表达和策略性特质对齐构成了对话代理个性的最佳设计目标，为基于LLM的对话代理设计提供了重要启示。

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [128] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: boldsea是一个基于语义事件的可执行本体架构，用于建模复杂动态系统，通过集成事件语义和数据流架构解决传统BPM系统和面向对象语义技术的局限性


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理系统和面向对象语义技术在动态系统建模中的局限性，提供运行时修改事件模型、时间透明度和数据与业务逻辑统一的能力

Method: 提出boldsea语义语言(BSL)及其BNF语法，设计boldsea-engine架构直接解释语义模型为可执行算法，无需编译

Result: 实现了语义模型作为动态结构直接控制流程执行，支持运行时模型修改，确保时间透明度，统一数据与业务逻辑

Conclusion: boldsea架构通过可执行本体和语义事件方法有效解决了复杂动态系统建模的关键挑战，为下一代BPM系统提供了新范式

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [129] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 本文研究了LLM在UNO游戏中作为助手角色的表现，发现虽然所有模型都能超越随机基准，但只有少数大模型能有效帮助其他玩家获胜。


<details>
  <summary>Details</summary>
Motivation: 测试LLM作为主动参与者是否能真正帮助人类完成目标，特别是在协作游戏场景中的表现。

Method: 构建工具让仅解码器LLM在RLCard游戏环境中作为代理参与UNO游戏，接收完整游戏状态信息，使用两种不同的提示策略，评估从1B到70B参数的不同规模模型。

Result: 所有模型在玩UNO时都能成功超越随机基准，但只有少数模型能够显著帮助其他玩家获胜。

Conclusion: 模型规模对性能有影响，但即使是大模型在协作帮助方面的能力仍然有限，LLM作为主动参与者的辅助能力有待进一步提升。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [130] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding框架通过结构化因果推理方法，将多智能体系统中的故障归因从模式识别任务转变为因果推理任务，显著提高了步骤级准确率（2.85倍提升）。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中的故障归因方法存在严重缺陷，步骤级准确率低于17%，无法进行有效的反事实推理来确定单个动作修正是否能避免任务失败。

Method: 提出Abduct-Act-Predict (A2P) Scaffolding框架，通过三个结构化步骤指导大语言模型进行推理：1)溯因推理推断隐藏的根本原因；2)定义最小修正干预；3)模拟后续轨迹验证干预效果。

Result: 在Algorithm-Generated数据集上达到47.46%的步骤级准确率（相比基线16.67%提升2.85倍），在Hand-Crafted数据集上达到29.31%准确率（相比基线12.07%提升2.43倍）。

Conclusion: 通过因果推理框架重构问题，A2P Scaffolding为自动化故障归因提供了更鲁棒、可验证且准确性显著更高的解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [131] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 提出了HHI-Assist数据集和基于Transformer的条件去噪扩散模型，用于预测物理交互场景中的人体运动，以提升辅助机器人的安全性和响应能力


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化需要辅助机器人，但机器人需要准确的人体运动预测来确保安全交互，而物理交互中的耦合动力学复杂性使得这一任务具有挑战性

Method: 收集人类-人类交互动作捕捉数据集HHI-Assist，并开发基于条件Transformer的去噪扩散模型来预测交互代理的姿态

Result: 模型有效捕捉了护理者和被护理者之间的耦合动力学，相比基线方法有所改进，并在未见场景中表现出强泛化能力

Conclusion: 通过推进交互感知的运动预测和引入新数据集，这项工作有潜力显著增强机器人辅助策略，数据集和代码已开源

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [132] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出了一种无需训练的视觉语言导航框架，通过将导航指令分解为空间约束图并进行约束优化求解，实现零样本适应新环境


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法主要针对离散环境或需要在连续模拟器环境中进行无监督训练，难以在真实世界场景中泛化和部署

Method: 构建空间约束库，将人类指令分解为有向无环图（包含路径点节点、对象节点和边），通过约束求解器进行图约束优化确定路径点位置，并采用导航树和回溯机制处理无解或多解情况

Result: 在标准基准测试中相比最先进的零样本VLN方法显著提高了成功率和导航效率，真实世界实验表明能有效泛化到新环境和指令集

Conclusion: 该训练免费框架为构建更鲁棒和自主的导航系统铺平了道路，通过约束驱动范式实现了零样本环境适应

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [133] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: 提出了一种将大型语言模型(LLM)与偏置随机密钥遗传算法(BRKGA)结合的新框架，用于解决最长运行子序列问题，通过LLM分析实例特定指标生成定制启发式偏置，显著提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多利用LLM进行代码生成来创建或改进特定启发式算法，但往往忽略了单个问题实例的结构特性，需要一种能够考虑实例特定特征的方法来增强元启发式算法的性能。

Method: 引入人类-LLM协作过程共同设计和实现计算高效指标，LLM分析这些实例特定指标生成定制启发式偏置，指导BRKGA算法在搜索空间中朝向有希望的区域。

Result: 在1,050个不同复杂度实例上的实验表明，最佳混合方法BRKGA+Llama-4-Maverick相比标准BRKGA基线取得了统计显著的改进，特别是在最复杂实例上表现突出。

Conclusion: 利用LLM产生先验的、实例驱动的启发式偏置是增强复杂优化领域中元启发式算法性能的有效方法。

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [134] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 该论文提出了语音风格适应(VSA)新任务，研究语音语言模型根据口语指令调整说话风格的能力，并发布了双语基准VStyle和评估框架LALM as a Judge。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型主要关注语义准确性和指令跟随，但在根据口语指令调整说话风格(如音色、韵律、角色扮演)方面的能力研究有限。

Method: 提出VStyle双语基准(中英文)，涵盖四个语音生成类别；开发LALM as a Judge评估框架，从文本忠实度、风格遵循度和自然度三个维度进行渐进式评估。

Result: 实验表明当前商业系统和开源SLM在可控风格适应方面存在明显局限性，验证了该任务的新颖性和挑战性。

Conclusion: 通过发布VStyle数据集和评估工具包，为推进以人为中心的语音交互研究提供了基础，揭示了语音风格适应这一重要研究方向的价值和挑战。

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [135] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中获胜，通过多模态检索管道和LLM幻觉控制技术，在三个任务中分别获得第2、第2和第1名。


<details>
  <summary>Details</summary>
Motivation: 解决CRAG-MM挑战赛中的多模态、多轮问答问题，需要处理图像索引知识图谱、网络资源和对话上下文，同时控制LLM的幻觉问题。

Method: 开发了领域特定的检索管道（处理图像知识图谱、网络资源和多轮对话）和先进的拒绝训练技术（SFT、DPO和RL）。

Result: 在Task 1和Task 2中获得第2名，在Task 3中获得第1名，最终赢得总冠军，特别在自我中心查询方面表现优异。

Conclusion: 该综合框架成功整合了多模态检索和LLM调优，有效控制了幻觉问题，在复杂多模态问答任务中取得了卓越成果。

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [136] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 本文介绍了text-2-SQL-4-PM，一个用于过程挖掘领域文本到SQL任务的双语（葡萄牙语-英语）基准数据集，包含1,655个自然语言语句和205个SQL语句，并进行了基线研究验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 为了解决过程挖掘领域中文本到SQL转换的独特挑战，包括专业词汇和基于事件日志的单表关系结构，提高非SQL专家用户的数据可访问性和专家用户的生产力。

Method: 采用专家手动整理、专业翻译和详细标注过程的方法构建数据集，并使用GPT-3.5 Turbo进行基线研究来验证数据集的实用性。

Result: text-2-SQL-4-PM数据集支持文本到SQL实现的评估，并在语义解析和其他自然语言处理任务中具有更广泛的适用性。

Conclusion: 该数据集为过程挖掘领域的文本到SQL任务提供了有效的基准工具，证明了其在提升数据库查询自然语言化方面的价值和实用性。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [137] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC推出AI助手系统，利用大语言模型和检索增强生成技术，为大型科学实验数据提供自然语言访问，支持数据保存和科学知识传承。


<details>
  <summary>Details</summary>
Motivation: 随着RHIC运行25年结束，保存其海量数据（约1EB）和嵌入的科学知识成为关键优先事项，需要支持可重复性、教育和未来发现。

Method: 基于大语言模型，采用检索增强生成和模型上下文协议，索引RHIC实验的结构化和非结构化内容，实现领域适应的交互。

Result: 系统已部署，报告了计算性能、多实验集成进展，以及为可持续和可解释的长期AI访问设计的架构特性。

Conclusion: 现代AI/ML工具可以显著提升科学遗产数据的可用性和可发现性，为大型科学实验的数据保存提供有效解决方案。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [138] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 提出使用冻结大语言模型提取用户文本表示，并通过微调小语言模型来构建高效的用户行为代理，解决推荐系统中用户行为模拟的挑战


<details>
  <summary>Details</summary>
Motivation: 解决用户行为模拟的复杂性，传统方法需要处理大规模表格数据、克服预训练偏见，并在百万级用户规模上实现有效模拟

Method: 使用冻结LLM提取鲁棒文本用户表示，通过微调SLMs构建资源高效的用户代理，采用多组低秩适配器训练用户群体persona

Result: 实验证明该方法能有效弥合离线指标与真实推荐系统性能之间的差距

Conclusion: 该方法在可扩展性和性能之间取得最优平衡，为用户行为代理开发提供了有效解决方案

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间中实施长度公平剪裁来解决PPO/GRPO方法在序列长度上的不公平问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式剪裁时存在固定剪裁范围对长短响应系统性重加权的问题，导致有效目标失真。

Method: 提出FSPO方法，采用高斯启发式解决方案：用KL校正漂移项和√L缩放的带对序列对数IS比率进行剪裁。

Result: FSPO在不同长度区间内平坦化剪裁率，稳定训练过程，在多个评估数据集上优于所有基线方法。

Conclusion: FSPO通过理论形式化的长度公平性概念和实际有效的剪裁策略，解决了序列级RL中的长度偏差问题，提升了模型性能。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [140] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出了一个推理时计算动态分配框架，同时考虑token成本和延迟，在保持部署实用性的同时实现更好的准确率-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时扩展方法主要关注并行生成技术（如best-of-N），忽略了增量解码方法（如beam search），并且大多只关注token使用而忽视了延迟问题，这对于用户体验和智能体工作流至关重要。

Method: 将推理时扩展建模为动态计算分配和方法选择问题，系统需要基于每个查询决定应用哪种策略以及分配多少计算资源，同时显式地考虑token成本和时钟延迟。

Result: 在推理基准测试上的实验表明，该方法始终优于静态策略，在保持部署实用性的同时实现了有利的准确率-成本权衡。

Conclusion: 提出的动态计算分配框架能够有效平衡推理性能与资源消耗，特别适用于需要高效处理多个查询的智能体工作流场景。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [141] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 提出LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习问题，并在开放世界场景下扩展为LoFT-OW，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法多从零训练模型，存在过度自信和伪标签质量低的问题，需要利用基础模型微调范式来改善

Method: 提出LoFT框架，通过参数高效微调预训练基础模型生成更可靠的伪标签；针对开放世界场景提出LoFT-OW，处理未标注数据中的分布外样本

Result: 在多个基准测试中取得优越性能，即使仅使用1%的未标注数据也能超越先前方法

Conclusion: 基础模型微调范式能有效提升长尾半监督学习性能，LoFT框架为解决现实世界中的长尾学习问题提供了有效方案

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [142] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出一种无需训练的token合并框架，通过自适应合并语义冗余token来压缩transformer表示，在保持精度的同时显著降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 大规模transformer在语义通信中计算和通信成本过高，难以部署在资源受限的边缘设备上

Method: 基于每层相似度阈值选择性合并冗余token，将合并策略发现建模为多目标优化问题，使用贝叶斯优化寻找帕累托最优解

Result: 在ImageNet分类中减少30% FLOPs和80%通信成本，在VQA任务中以1/3计算量和1/10带宽达到与完整模型竞争的性能

Conclusion: 该框架为在资源受限的边缘智能场景中部署强大transformer模型提供了实用且通用的解决方案

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>
