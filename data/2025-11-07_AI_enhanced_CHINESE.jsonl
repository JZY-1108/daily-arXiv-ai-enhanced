{"id": "2511.03738", "pdf": "https://arxiv.org/pdf/2511.03738", "abs": "https://arxiv.org/abs/2511.03738", "authors": ["Pranav Bhandari", "Nicolas Fay", "Sanjeevan Selvaganapathy", "Amitava Datta", "Usman Naseem", "Mehwish Nasim"], "title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models exhibit implicit personalities in their generation, but\nreliably controlling or aligning these traits to meet specific needs remains an\nopen challenge. The need for effective mechanisms for behavioural manipulation\nof the model during generation is a critical gap in the literature that needs\nto be fulfilled. Personality-aware LLMs hold a promising direction towards this\nobjective. However, the relationship between these psychological constructs and\ntheir representations within LLMs remains underexplored and requires further\ninvestigation. Moreover, it is intriguing to understand and study the use of\nthese representations to steer the models' behaviour. We propose a novel\npipeline that extracts hidden state activations from transformer layers using\nthe Big Five Personality Traits (Openness, Conscientiousness, Extraversion,\nAgreeableness and Neuroticism), which is a comprehensive and empirically\nvalidated framework to model human personality applies low-rank subspace\ndiscovery methods, and identifies trait-specific optimal layers across\ndifferent model architectures for robust injection. The resulting\npersonality-aligned directions are then operationalised through a flexible\nsteering framework with dynamic layer selection, enabling precise control of\ntrait expression in LLM outputs. Our findings reveal that personality traits\noccupy a low-rank shared subspace, and that these latent structures can be\ntransformed into actionable mechanisms for effective steering through careful\nperturbations without impacting the fluency, variance and general capabilities,\nhelping to bridge the gap between psychological theory and practical model\nalignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u7279\u8d28\u6765\u63a7\u5236LLM\u4e2a\u6027\u8868\u8fbe\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u9690\u85cf\u72b6\u6001\u6fc0\u6d3b\u3001\u53d1\u73b0\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u5e76\u5b9e\u73b0\u52a8\u6001\u5c42\u9009\u62e9\u548c\u7cbe\u786e\u7684\u4e2a\u6027\u63a7\u5236\u3002", "motivation": "LLM\u5728\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u9690\u542b\u4e2a\u6027\uff0c\u4f46\u53ef\u9760\u63a7\u5236\u6216\u5bf9\u9f50\u8fd9\u4e9b\u7279\u8d28\u4ee5\u6ee1\u8db3\u7279\u5b9a\u9700\u6c42\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u9700\u8981\u6709\u6548\u7684\u673a\u5236\u6765\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u6d41\u7a0b\uff1a\u4ecetransformer\u5c42\u63d0\u53d6\u9690\u85cf\u72b6\u6001\u6fc0\u6d3b\uff08\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u7279\u8d28\uff09\uff0c\u5e94\u7528\u4f4e\u79e9\u5b50\u7a7a\u95f4\u53d1\u73b0\u65b9\u6cd5\uff0c\u8bc6\u522b\u8de8\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u7279\u8d28\u7279\u5b9a\u6700\u4f18\u5c42\u8fdb\u884c\u7a33\u5065\u6ce8\u5165\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5c42\u9009\u62e9\u7684\u7075\u6d3b\u5f15\u5bfc\u6846\u67b6\u64cd\u4f5c\u5316\u3002", "result": "\u53d1\u73b0\u4eba\u683c\u7279\u8d28\u5360\u636e\u4f4e\u79e9\u5171\u4eab\u5b50\u7a7a\u95f4\uff0c\u8fd9\u4e9b\u6f5c\u5728\u7ed3\u6784\u53ef\u901a\u8fc7\u7cbe\u5fc3\u6270\u52a8\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u5f15\u5bfc\u673a\u5236\uff0c\u800c\u4e0d\u5f71\u54cd\u6d41\u7545\u6027\u3001\u65b9\u5dee\u548c\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u5f25\u5408\u5fc3\u7406\u5b66\u7406\u8bba\u4e0e\u5b9e\u9645\u6a21\u578b\u5bf9\u9f50\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e2a\u6027\u611f\u77e5LLM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.03739", "pdf": "https://arxiv.org/pdf/2511.03739", "abs": "https://arxiv.org/abs/2511.03739", "authors": ["Eugenius Mario Situmorang", "Adila Alfa Krisnadhi", "Ari Wibisono"], "title": "TextualVerifier: Verify TextGrad Step-by-Step", "categories": ["cs.CL"], "comment": null, "summary": "TextGrad is a novel approach to text-based automatic differentiation that\nenables composite AI systems to perform optimization without explicit numerical\nequations. However, it currently lacks self-verification mechanisms that ensure\nreasoning validity in text-based decision making. This research introduces\nTextualVerifier, a verification framework that leverages chain-of-thought\nreasoning and majority voting with large language models to address this\nverification gap. TextualVerifier implements a four-stage workflow:\nchain-of-thought decomposition, variant generation, majority voting, and\nconsensus aggregation. It integrates non-invasively with TextGrad at both the\nloss function and optimization result verification stages. Experimental\nevaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)\nstandalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad\non GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically\nsignificant improvements (p < 0.001). In phase one, TextualVerifier improves\nthe validity of reasoning steps by 29 percent. In phase two, integration into\nTextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4\npercent with a moderate overhead of 5.9 LLM calls on average. Further\nevaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92\npercentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.\nTextualVerifier thus presents the first self-verification framework for\nTextGrad through LLM-based techniques without requiring numerical gradients,\nenabling more reliable reasoning and opening new directions for verification in\ntext-based optimization.", "AI": {"tldr": "TextualVerifier\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u5206\u89e3\u3001\u53d8\u4f53\u751f\u6210\u3001\u591a\u6570\u6295\u7968\u548c\u5171\u8bc6\u805a\u5408\u56db\u9636\u6bb5\u5de5\u4f5c\u6d41\uff0c\u4e3aTextGrad\u63d0\u4f9b\u81ea\u6211\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6709\u6548\u6027\u548c\u4f18\u5316\u6027\u80fd\u3002", "motivation": "TextGrad\u4f5c\u4e3a\u6587\u672c\u81ea\u52a8\u5fae\u5206\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u6211\u9a8c\u8bc1\u673a\u5236\uff0c\u65e0\u6cd5\u786e\u4fdd\u6587\u672c\u63a8\u7406\u7684\u6709\u6548\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u590d\u5408AI\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u5de5\u4f5c\u6d41\uff1a\u601d\u7ef4\u94fe\u5206\u89e3\u3001\u53d8\u4f53\u751f\u6210\u3001\u591a\u6570\u6295\u7968\u548c\u5171\u8bc6\u805a\u5408\uff0c\u975e\u4fb5\u5165\u5f0f\u96c6\u6210\u5230TextGrad\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u7ed3\u679c\u9a8c\u8bc1\u9636\u6bb5\u3002", "result": "\u5728PRM800K\u4e0a\u63a8\u7406\u6b65\u9aa4\u6709\u6548\u6027\u63d0\u534729%\uff1b\u96c6\u6210\u5230TextGrad\u540e\u5728GPQA-Diamond\u3001MMLU-ML\u3001MMLU-CP\u57fa\u51c6\u4e0a\u5206\u522b\u83b7\u5f978.08\u300110.71\u30013.92\u4e2a\u767e\u5206\u70b9\u7684\u6027\u80fd\u63d0\u5347\uff0c\u635f\u5931\u51fd\u6570\u96c6\u6210\u4f7f\u51c6\u786e\u7387\u4ece68.2%\u63d0\u5347\u81f370.4%\u3002", "conclusion": "TextualVerifier\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u6280\u672f\u4e3aTextGrad\u63d0\u4f9b\u7684\u81ea\u6211\u9a8c\u8bc1\u6846\u67b6\uff0c\u65e0\u9700\u6570\u503c\u68af\u5ea6\u5373\u53ef\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u63a8\u7406\uff0c\u4e3a\u6587\u672c\u4f18\u5316\u4e2d\u7684\u9a8c\u8bc1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.03772", "pdf": "https://arxiv.org/pdf/2511.03772", "abs": "https://arxiv.org/abs/2511.03772", "authors": ["Stergios Chatzikyriakidis", "Dimitris Papadakis", "Sevasti-Ioanna Papaioannou", "Erofili Psaltaki"], "title": "GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the\nexisting GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern\nGreek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian\nGreek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a\ndataset with total size 6,374,939 words and 10 varieties. This is the first\ndataset with such variation and size to date. We conduct a number of\nfine-tuning experiments to see the effect of good quality dialectal data on a\nnumber of LLMs. We fine-tune three model architectures (Llama-3-8B,\nLlama-3.1-8B, Krikri-8B) and compare the results to frontier models\n(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).", "AI": {"tldr": "\u6269\u5c55\u5e0c\u814a\u65b9\u8a00\u6570\u636e\u96c6GRDD+\uff0c\u65b0\u589e6\u79cd\u65b9\u8a00\u53d8\u4f53\uff0c\u603b\u89c4\u6a21\u8fbe637\u4e07\u8bcd\uff0c\u5305\u542b10\u79cd\u5e0c\u814a\u65b9\u8a00\u53d8\u4f53\u3002\u5bf9\u591a\u4e2aLLM\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5728\u65b9\u8a00\u5904\u7406\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u6784\u5efa\u9996\u4e2a\u5177\u6709\u5982\u6b64\u89c4\u6a21\u548c\u591a\u6837\u6027\u7684\u5e0c\u814a\u65b9\u8a00\u6570\u636e\u96c6\uff0c\u7814\u7a76\u9ad8\u8d28\u91cf\u65b9\u8a00\u6570\u636e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6269\u5c55GRDD\u6570\u636e\u96c6\uff0c\u65b0\u589eCretan\u3001Cypriot\u7b49\u65b9\u8a00\u6570\u636e\uff0c\u5e76\u52a0\u5165Greco-Corsican\u3001Griko\u7b496\u79cd\u65b0\u53d8\u4f53\u3002\u5bf9Llama-3-8B\u3001Llama-3.1-8B\u3001Krikri-8B\u4e09\u79cd\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4e0e\u524d\u6cbf\u6a21\u578bClaude-3.7-Sonnet\u3001Gemini-2.5\u3001ChatGPT-5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b10\u79cd\u5e0c\u814a\u65b9\u8a00\u53d8\u4f53\u3001\u603b\u89c4\u6a216,374,939\u8bcd\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u9996\u4e2a\u5177\u6709\u5982\u6b64\u53d8\u5f02\u6027\u548c\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5e0c\u814a\u65b9\u8a00\u5904\u7406\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u5c55\u793a\u4e86\u65b9\u8a00\u6570\u636e\u5bf9LLM\u6027\u80fd\u7684\u79ef\u6781\u5f71\u54cd\uff0c\u4e3a\u65b9\u8a00\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.03823", "pdf": "https://arxiv.org/pdf/2511.03823", "abs": "https://arxiv.org/abs/2511.03823", "authors": ["Jan Koco\u0144", "Maciej Piasecki", "Arkadiusz Janz", "Teddy Ferdinan", "\u0141ukasz Radli\u0144ski", "Bart\u0142omiej Koptyra", "Marcin Oleksy", "Stanis\u0142aw Wo\u017aniak", "Pawe\u0142 Walkowiak", "Konrad Wojtasik", "Julia Moska", "Tomasz Naskr\u0119t", "Bartosz Walkowiak", "Mateusz Gniewkowski", "Kamil Szyc", "Dawid Motyka", "Dawid Banach", "Jonatan Dalasi\u0144ski", "Ewa Rudnicka", "Bart\u0142omiej Alberski", "Tomasz Walkowiak", "Aleksander Szcz\u0119sny", "Maciej Markiewicz", "Tomasz Berna\u015b", "Hubert Mazur", "Kamil \u017byta", "Mateusz Tykierko", "Grzegorz Chodak", "Tomasz Kajdanowicz", "Przemys\u0142aw Kazienko", "Agnieszka Karli\u0144ska", "Karolina Seweryn", "Anna Ko\u0142os", "Maciej Chrab\u0105szcz", "Katarzyna Lorenc", "Aleksandra Krasnod\u0119bska", "Artur Wilczek", "Katarzyna Dziewulska", "Paula Betscher", "Zofia Cie\u015bli\u0144ska", "Katarzyna Kowol", "Daria Miko\u015b", "Maciej Trzci\u0144ski", "Dawid Krutul", "Marek Koz\u0142owski", "S\u0142awomir Dadas", "Rafa\u0142 Po\u015bwiata", "Micha\u0142 Pere\u0142kiewicz", "Ma\u0142gorzata Gr\u0119bowiec", "Maciej Kazu\u0142a", "Marcin Bia\u0142as", "Roman Roszko", "Danuta Roszko", "Jurgita Vai\u010denonien\u0117", "Andrius Utka", "Pawe\u0142 Levchuk", "Pawe\u0142 Kowalski", "Irena Prawdzic-Jankowska", "Maciej Ogrodniczuk", "Monika Borys", "Anna Buli\u0144ska", "Wiktoria Gumienna", "Witold Kiera\u015b", "Dorota Komosi\u0144ska", "Katarzyna Krasnowska-Kiera\u015b", "\u0141ukasz Kobyli\u0144ski", "Martyna Lewandowska", "Marek \u0141azi\u0144ski", "Miko\u0142aj \u0141\u0105tkowski", "Dawid Mastalerz", "Beata Milewicz", "Agnieszka Anna Mykowiecka", "Angelika Peljak-\u0141api\u0144ska", "Sandra Penno", "Zuzanna Przybysz", "Micha\u0142 Rudolf", "Piotr Rybak", "Karolina Saputa", "Aleksandra Tomaszewska", "Aleksander Wawer", "Marcin Woli\u0144ski", "Joanna Wo\u0142oszyn", "Alina Wr\u00f3blewska", "Bartosz \u017buk", "Filip \u017barnecki", "Konrad Kaczy\u0144ski", "Anna Cichosz", "Zuzanna Deckert", "Monika Garnys", "Izabela Grabarczyk", "Wojciech Janowski", "Sylwia Karasi\u0144ska", "Aleksandra Kujawiak", "Piotr Misztela", "Maria Szyma\u0144ska", "Karolina Walkusz", "Igor Siek", "Jakub Kwiatkowski", "Piotr P\u0119zik"], "title": "PLLuM: A Family of Polish Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "83 pages, 19 figures", "summary": "Large Language Models (LLMs) play a central role in modern artificial\nintelligence, yet their development has been primarily focused on English,\nresulting in limited support for other languages. We present PLLuM (Polish\nLarge Language Model), the largest open-source family of foundation models\ntailored specifically for the Polish language. Developed by a consortium of\nmajor Polish research institutions, PLLuM addresses the need for high-quality,\ntransparent, and culturally relevant language models beyond the English-centric\ncommercial landscape. We describe the development process, including the\nconstruction of a new 140-billion-token Polish text corpus for pre-training, a\n77k custom instructions dataset, and a 100k preference optimization dataset. A\nkey component is a Responsible AI framework that incorporates strict data\ngovernance and a hybrid module for output correction and safety filtering. We\ndetail the models' architecture, training procedures, and alignment techniques\nfor both base and instruction-tuned variants, and demonstrate their utility in\na downstream task within public administration. By releasing these models\npublicly, PLLuM aims to foster open research and strengthen sovereign AI\ntechnologies in Poland.", "AI": {"tldr": "PLLuM\u662f\u4e13\u4e3a\u6ce2\u5170\u8bed\u5f00\u53d1\u7684\u6700\u5927\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u65e8\u5728\u89e3\u51b3\u82f1\u8bed\u4e3b\u5bfc\u7684AI\u751f\u6001\u4e2d\u5176\u4ed6\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u5f00\u53d1\uff0c\u5bf9\u5176\u4ed6\u8bed\u8a00\u652f\u6301\u6709\u9650\u3002PLLuM\u65e8\u5728\u4e3a\u6ce2\u5170\u8bed\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u900f\u660e\u4e14\u6587\u5316\u76f8\u5173\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u8d85\u8d8a\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u5546\u4e1a\u683c\u5c40\u3002", "method": "\u6784\u5efa\u4e861400\u4ebftoken\u7684\u6ce2\u5170\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u300177k\u81ea\u5b9a\u4e49\u6307\u4ee4\u6570\u636e\u96c6\u548c100k\u504f\u597d\u4f18\u5316\u6570\u636e\u96c6\uff0c\u91c7\u7528\u8d1f\u8d23\u4efbAI\u6846\u67b6\uff0c\u5305\u542b\u4e25\u683c\u6570\u636e\u6cbb\u7406\u548c\u6df7\u5408\u6a21\u5757\u8fdb\u884c\u8f93\u51fa\u6821\u6b63\u4e0e\u5b89\u5168\u8fc7\u6ee4\u3002", "result": "\u5f00\u53d1\u4e86\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u53d8\u4f53\uff0c\u5c55\u793a\u4e86\u5728\u516c\u5171\u7ba1\u7406\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00\u53d1\u5e03\u8fd9\u4e9b\u6a21\u578b\uff0cPLLuM\u65e8\u5728\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76\uff0c\u52a0\u5f3a\u6ce2\u5170\u7684\u4e3b\u6743AI\u6280\u672f\u3002"}}
{"id": "2511.03765", "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "categories": ["cs.CV", "cs.AR"], "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "LoRA-Edge\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\u548cTensor-Train\u5206\u89e3\uff0c\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0aCNN\u7684\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u5927\u5e45\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884cCNN\u5fae\u8c03\u4ee5\u5e94\u5bf9\u9886\u57df\u504f\u79fb\u5f88\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u4e25\u683c\u7684\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u80fd\u8017\u9884\u7b97\uff0c\u5b8c\u5168\u5fae\u8c03\u4e0d\u53ef\u884c\u3002", "method": "\u5bf9\u9884\u8bad\u7ec3\u5377\u79ef\u5c42\u5e94\u7528Tensor-Train SVD\u5206\u89e3\uff0c\u4ec5\u9009\u62e9\u6027\u66f4\u65b0\u8f93\u51fa\u4fa7\u6838\u5fc3\uff0c\u4f7f\u7528\u96f6\u521d\u59cb\u5316\u4fdd\u6301\u8f85\u52a9\u8def\u5f84\u521d\u59cb\u4e0d\u6d3b\u8dc3\uff0c\u6700\u540e\u5c06\u66f4\u65b0\u878d\u5408\u56de\u5bc6\u96c6\u6838\u4e2d\u3002", "result": "\u5728\u591a\u79cdHAR\u6570\u636e\u96c6\u548cCNN\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cLoRA-Edge\u4ec5\u66f4\u65b0\u6700\u591a1.49%\u7684\u53c2\u6570\uff0c\u5c31\u80fd\u8fbe\u5230\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u5dee4.7%\u4ee5\u5185\u7684\u51c6\u786e\u7387\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u53471.4-3.8\u500d\u3002", "conclusion": "LoRA-Edge\u5b9e\u73b0\u4e86\u7ed3\u6784\u5bf9\u9f50\u3001\u53c2\u6570\u9ad8\u6548\u7684\u8fb9\u7f18\u8bbe\u5907CNN\u81ea\u9002\u5e94\uff0c\u4e3a\u8fb9\u7f18\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03827", "pdf": "https://arxiv.org/pdf/2511.03827", "abs": "https://arxiv.org/abs/2511.03827", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb", "Mikhail Kuznetsov", "Muslum Ozgur Ozmen", "Z. Berkay Celik"], "title": "STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models", "categories": ["cs.CL"], "comment": "Presented at the 2nd Workshop on Frontiers in Probabilistic\n  Inference: Sampling Meets Learning (NeurIPS 2025)", "summary": "Aligning large language models with human values is crucial for their safe\ndeployment; however, existing methods, such as fine-tuning, are computationally\nexpensive and suboptimal. In contrast, inference-time approaches like Best-of-N\nsampling require practically infeasible computation to achieve optimal\nalignment. We propose STARS: Segment-level Token Alignment with Rejection\nSampling, a decoding-time algorithm that steers model generation by iteratively\nsampling, scoring, and rejecting/accepting short, fixed-size token segments.\nThis allows for early correction of the generation path, significantly\nimproving computational efficiency and boosting alignment quality. Across a\nsuite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)\nby up to 14.9 percentage points and Direct Preference Optimization (DPO) by up\nto 4.3 percentage points on win-rates, while remaining highly competitive with\nstrong Best-of-N baselines. Our work establishes granular, reward-guided\nsampling as a generalizable, robust, and efficient alternative to traditional\nfine-tuning and full-sequence ranking methods for aligning LLMs.", "AI": {"tldr": "STARS\u662f\u4e00\u79cd\u89e3\u7801\u65f6\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u3001\u8bc4\u5206\u548c\u62d2\u7edd/\u63a5\u53d7\u56fa\u5b9a\u957f\u5ea6\u7684token\u7247\u6bb5\u6765\u5f15\u5bfc\u6a21\u578b\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u5bf9\u9f50\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u5982\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u63a8\u7406\u65f6\u65b9\u6cd5\u5982Best-of-N\u91c7\u6837\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u8ba1\u7b97\u91cf\u624d\u80fd\u8fbe\u5230\u6700\u4f18\u5bf9\u9f50", "method": "STARS\u7b97\u6cd5\uff1a\u5728\u89e3\u7801\u65f6\u8fed\u4ee3\u91c7\u6837\u56fa\u5b9a\u957f\u5ea6\u7684token\u7247\u6bb5\uff0c\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u5bf9\u6bcf\u4e2a\u7247\u6bb5\u8fdb\u884c\u8bc4\u5206\uff0c\u7136\u540e\u57fa\u4e8e\u8bc4\u5206\u51b3\u5b9a\u63a5\u53d7\u6216\u62d2\u7edd\u8be5\u7247\u6bb5\uff0c\u4ece\u800c\u65e9\u671f\u7ea0\u6b63\u751f\u6210\u8def\u5f84", "result": "\u57286\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSTARS\u5728\u80dc\u7387\u4e0a\u6bd4\u76d1\u7763\u5fae\u8c03(SFT)\u9ad8\u51fa14.9\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u9ad8\u51fa4.3\u4e2a\u767e\u5206\u70b9\uff0c\u4e0e\u5f3aBest-of-N\u57fa\u7ebf\u8868\u73b0\u76f8\u5f53", "conclusion": "\u7ec6\u7c92\u5ea6\u3001\u5956\u52b1\u5f15\u5bfc\u7684\u91c7\u6837\u662f\u4f20\u7edf\u5fae\u8c03\u548c\u5168\u5e8f\u5217\u6392\u5e8f\u65b9\u6cd5\u7684\u901a\u7528\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2511.03819", "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "SILVI\u662f\u4e00\u4e2a\u5f00\u6e90\u89c6\u9891\u6807\u6ce8\u8f6f\u4ef6\uff0c\u4e13\u95e8\u7528\u4e8e\u6807\u6ce8\u52a8\u7269\u884c\u4e3a\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u884c\u4e3a\u6807\u6ce8\u548c\u4e2a\u4f53\u5b9a\u4f4d\u529f\u80fd\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6807\u6ce8\u5de5\u5177\u8981\u4e48\u53ea\u652f\u6301\u884c\u4e3a\u6807\u6ce8\u800c\u4e0d\u5b9a\u4f4d\u4e2a\u4f53\uff0c\u8981\u4e48\u53ea\u5b9a\u4f4d\u4e2a\u4f53\u800c\u4e0d\u80fd\u6355\u6349\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u5728\u52a8\u7269\u793e\u4ea4\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86SILVI\u8f6f\u4ef6\uff0c\u96c6\u6210\u884c\u4e3a\u6807\u6ce8\u548c\u4e2a\u4f53\u5b9a\u4f4d\u529f\u80fd\uff0c\u5141\u8bb8\u7814\u7a76\u8005\u5728\u89c6\u9891\u6570\u636e\u4e2d\u76f4\u63a5\u6807\u6ce8\u884c\u4e3a\u548c\u4ea4\u4e92\uff0c\u751f\u6210\u9002\u5408\u8bad\u7ec3\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "result": "SILVI\u6210\u529f\u8fde\u63a5\u4e86\u884c\u4e3a\u751f\u6001\u5b66\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u884c\u4e3a\u5206\u6790\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "SILVI\u586b\u8865\u4e86\u52a8\u7269\u884c\u4e3a\u5206\u6790\u4e2d\u4ea4\u4e92\u6807\u6ce8\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u867d\u7136\u4e3b\u8981\u9488\u5bf9\u52a8\u7269\u884c\u4e3a\u5f00\u53d1\uff0c\u4f46\u4e5f\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9700\u8981\u63d0\u53d6\u52a8\u6001\u573a\u666f\u56fe\u7684\u4eba\u7c7b\u4ea4\u4e92\u6807\u6ce8\u3002"}}
{"id": "2511.03830", "pdf": "https://arxiv.org/pdf/2511.03830", "abs": "https://arxiv.org/abs/2511.03830", "authors": ["Miko\u0142aj Langner", "Jan Eliasz", "Ewa Rudnicka", "Jan Koco\u0144"], "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification", "categories": ["cs.CL"], "comment": "9 pages, 8 figures", "summary": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5206\u51b3\u7b56\u5e8f\u5217\u7684\u9ad8\u6548\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5206\u7c7b\u4efb\u52a1\u91cd\u6784\u4e3a\u72ec\u7acb\u7684\u662f/\u5426\u67e5\u8be2\uff0c\u7ed3\u5408\u524d\u7f00\u7f13\u5b58\u673a\u5236\uff0c\u5728\u4e0d\u635f\u5931\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u77ed\u6587\u672c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u77ed\u6587\u672c\u63a8\u7406\u573a\u666f\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u7f13\u5b58\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06\u591a\u6807\u7b7e\u5206\u7c7b\u91cd\u6784\u4e3a\u5e8f\u5217\u5316\u4e8c\u5206\u51b3\u7b56\uff0c\u6bcf\u4e2a\u76ee\u6807\u7ef4\u5ea6\u72ec\u7acb\u67e5\u8be2\uff1b\u91c7\u7528LLM-to-SLM\u84b8\u998f\uff0c\u4f7f\u7528DeepSeek-V3\u751f\u6210\u6807\u6ce8\u6570\u636e\u6765\u5fae\u8c03\u5c0f\u6a21\u578b\uff1b\u7ed3\u5408\u524d\u7f00\u7f13\u5b58\u673a\u5236\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u7ef4\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u5728\u60c5\u611f\u6587\u672c\u5206\u6790\u768424\u4e2a\u7ef4\u5ea6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u5c06\u591a\u6807\u7b7e\u5206\u7c7b\u5206\u89e3\u4e3a\u4e8c\u5206\u67e5\u8be2\uff0c\u7ed3\u5408\u84b8\u998f\u548c\u7f13\u5b58\u611f\u77e5\u63a8\u7406\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u6846\u67b6\uff0c\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u53ef\u8de8\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2511.03855", "pdf": "https://arxiv.org/pdf/2511.03855", "abs": "https://arxiv.org/abs/2511.03855", "authors": ["Duong Mai", "Lawrence Hall"], "title": "Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets", "categories": ["cs.CV", "cs.AI"], "comment": "Abstract accepted for oral presentation at SPIE Medical Imaging 2026:\n  Computer-Aided Diagnosis", "summary": "Deep learned (DL) models for image recognition have been shown to fail to\ngeneralize to data from different devices, populations, etc. COVID-19 detection\nfrom Chest X-rays (CXRs), in particular, has been shown to fail to generalize\nto out-of-distribution (OOD) data from new clinical sources not covered in the\ntraining set. This occurs because models learn to exploit shortcuts -\nsource-specific artifacts that do not translate to new distributions - rather\nthan reasonable biomarkers to maximize performance on in-distribution (ID)\ndata. Rendering the models more robust to distribution shifts, our study\ninvestigates the use of fundamental noise injection techniques (Gaussian,\nSpeckle, Poisson, and Salt and Pepper) during training. Our empirical results\ndemonstrate that this technique can significantly reduce the performance gap\nbetween ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results\naveraged over ten random seeds across key metrics such as AUC, F1, accuracy,\nrecall and specificity. Our source code is publicly available at\nhttps://github.com/Duongmai127/Noisy-ood", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u6280\u672f\u63d0\u9ad8COVID-19\u80f8\u90e8X\u5149\u68c0\u6d4b\u6a21\u578b\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5c06ID\u4e0eOOD\u6027\u80fd\u5dee\u8ddd\u4ece0.10-0.20\u964d\u4f4e\u52300.01-0.06\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56fe\u50cf\u8bc6\u522b\u4e2d\u5bb9\u6613\u5b66\u4e60\u6e90\u7279\u5b9a\u4f2a\u5f71\u800c\u975e\u771f\u5b9e\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6cdb\u5316\u5931\u8d25\uff0c\u7279\u522b\u662f\u5728COVID-19\u80f8\u90e8X\u5149\u68c0\u6d4b\u4e2d\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u57fa\u7840\u566a\u58f0\u6ce8\u5165\u6280\u672f\uff08\u9ad8\u65af\u3001\u6563\u6591\u3001\u6cca\u677e\u548c\u6912\u76d0\u566a\u58f0\uff09\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86ID\u4e0eOOD\u8bc4\u4f30\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4ece0.10-0.20\u964d\u4f4e\u52300.01-0.06\uff0c\u57fa\u4e8eAUC\u3001F1\u3001\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u7279\u5f02\u6027\u7b49\u5173\u952e\u6307\u6807\u3002", "conclusion": "\u566a\u58f0\u6ce8\u5165\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8COVID-19\u68c0\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u6e90\u7279\u5b9a\u4f2a\u5f71\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.03880", "pdf": "https://arxiv.org/pdf/2511.03880", "abs": "https://arxiv.org/abs/2511.03880", "authors": ["Hellina Hailu Nigatu", "Bethelhem Yemane Mamo", "Bontu Fufa Balcha", "Debora Taye Tesfaye", "Elbethel Daniel Zewdie", "Ikram Behiru Nesiru", "Jitu Ewnetu Hailu", "Senait Mengesha Yayo"], "title": "Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens", "categories": ["cs.CL", "cs.CY"], "comment": "Paper Under Review", "summary": "As low-resourced languages are increasingly incorporated into NLP research,\nthere is an emphasis on collecting large-scale datasets. But in prioritizing\nquantity over quality, we risk 1) building language technologies that perform\npoorly for these languages and 2) producing harmful content that perpetuates\nsocietal biases. In this paper, we investigate the quality of Machine\nTranslation (MT) datasets for three low-resourced languages--Afan Oromo,\nAmharic, and Tigrinya, with a focus on the gender representation in the\ndatasets. Our findings demonstrate that while training data has a large\nrepresentation of political and religious domain text, benchmark datasets are\nfocused on news, health, and sports. We also found a large skew towards the\nmale gender--in names of persons, the grammatical gender of verbs, and in\nstereotypical depictions in the datasets. Further, we found harmful and toxic\ndepictions against women, which were more prominent for the language with the\nlargest amount of data, underscoring that quantity does not guarantee quality.\nWe hope that our work inspires further inquiry into the datasets collected for\nlow-resourced languages and prompts early mitigation of harmful content.\nWARNING: This paper contains discussion of NSFW content that some may find\ndisturbing.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u4e09\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u963f\u6cd5\u5c14\u5965\u7f57\u83ab\u8bed\u3001\u963f\u59c6\u54c8\u62c9\u8bed\u3001\u63d0\u683c\u91cc\u5c3c\u4e9a\u8bed\uff09\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u7537\u6027\u504f\u5411\u548c\u6709\u5bb3\u5185\u5bb9\u3002", "motivation": "\u968f\u7740\u4f4e\u8d44\u6e90\u8bed\u8a00\u88ab\u7eb3\u5165NLP\u7814\u7a76\uff0c\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u5f80\u5f80\u91cd\u6570\u91cf\u8f7b\u8d28\u91cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u6280\u672f\u6027\u80fd\u5dee\u548c\u4ea7\u751f\u6709\u5bb3\u504f\u89c1\u5185\u5bb9\u3002", "method": "\u8c03\u67e5\u4e09\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u96c6\u4e2d\u7684\u6027\u522b\u8868\u5f81\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u4e2d\u653f\u6cbb\u548c\u5b97\u6559\u9886\u57df\u6587\u672c\u5360\u4e3b\u5bfc\uff0c\u57fa\u51c6\u6570\u636e\u96c6\u96c6\u4e2d\u4e8e\u65b0\u95fb\u3001\u5065\u5eb7\u3001\u4f53\u80b2\u9886\u57df\uff1b\u5b58\u5728\u4e25\u91cd\u7684\u7537\u6027\u504f\u5411\uff08\u4eba\u540d\u3001\u52a8\u8bcd\u8bed\u6cd5\u6027\u522b\u3001\u523b\u677f\u5370\u8c61\uff09\uff1b\u53d1\u73b0\u4e86\u9488\u5bf9\u5973\u6027\u7684\u6709\u5bb3\u548c\u6bd2\u6027\u63cf\u8ff0\uff0c\u4e14\u6570\u636e\u91cf\u6700\u5927\u7684\u8bed\u8a00\u95ee\u9898\u6700\u7a81\u51fa\u3002", "conclusion": "\u6570\u91cf\u4e0d\u80fd\u4fdd\u8bc1\u8d28\u91cf\uff0c\u5e0c\u671b\u7814\u7a76\u80fd\u4fc3\u8fdb\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u96c6\u7684\u8fdb\u4e00\u6b65\u8c03\u67e5\uff0c\u53ca\u65e9\u7f13\u89e3\u6709\u5bb3\u5185\u5bb9\u3002"}}
{"id": "2511.03882", "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u53cc\u5e73\u9762X\u5c04\u7ebf\u5f15\u5bfc\u4e0b\u810a\u67f1\u624b\u672f\u4e2d\u5e94\u7528\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u4e86\u9ad8\u771f\u5b9e\u5ea6\u4eff\u771f\u73af\u5883\u5e76\u8bad\u7ec3\u4e86\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u63d2\u7ba1\u5bf9\u9f50\u7b56\u7565\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u89c6\u9891\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u91cd\u65b0\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u662f\u5426\u9002\u7528\u4e8eX\u5c04\u7ebf\u5f15\u5bfc\u7684\u810a\u67f1\u624b\u672f\uff0c\u56e0\u4e3a\u591a\u89c6\u56feX\u5c04\u7ebf\u89e3\u91ca\u590d\u6742\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u771f\u5b9e\u5ea6\u4eff\u771f\u73af\u5883\uff0c\u6536\u96c6\u6b63\u786e\u8f68\u8ff9\u548c\u5bf9\u5e94\u53cc\u5e73\u9762X\u5c04\u7ebf\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u89c4\u5212\u548c\u5f00\u73af\u63a7\u5236\u3002", "result": "\u7b56\u7565\u572868.5%\u7684\u60c5\u51b5\u4e0b\u9996\u6b21\u5c1d\u8bd5\u6210\u529f\uff0c\u4fdd\u6301\u4e86\u5b89\u5168\u7684\u690e\u5f13\u6839\u5185\u8f68\u8ff9\uff0c\u80fd\u6cdb\u5316\u5230\u590d\u6742\u89e3\u5256\u7ed3\u6784\uff08\u5305\u62ec\u9aa8\u6298\uff09\uff0c\u5e76\u5728\u771f\u5b9eX\u5c04\u7ebf\u4e0a\u4ea7\u751f\u5408\u7406\u8f68\u8ff9\u3002", "conclusion": "\u521d\u6b65\u7ed3\u679c\u6709\u524d\u666f\uff0c\u4f46\u5b58\u5728\u5165\u53e3\u70b9\u7cbe\u5ea6\u9650\u5236\uff0c\u5b8c\u5168\u95ed\u73af\u63a7\u5236\u9700\u8981\u66f4\u9891\u7e41\u7684\u53cd\u9988\u673a\u5236\uff0c\u7ed3\u5408\u66f4\u5f3a\u5148\u9a8c\u548c\u9886\u57df\u77e5\u8bc6\u53ef\u4e3a\u65e0CT\u7684\u673a\u5668\u4eba\u810a\u67f1\u5bfc\u822a\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.03900", "pdf": "https://arxiv.org/pdf/2511.03900", "abs": "https://arxiv.org/abs/2511.03900", "authors": ["Manh Nguyen", "Sunil Gupta", "Dai Do", "Hung Le"], "title": "GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Hallucination mitigation remains a persistent challenge for large language\nmodels (LLMs), even as model scales grow. Existing approaches often rely on\nexternal knowledge sources, such as structured databases or knowledge graphs,\naccessed through prompting or retrieval. However, prompt-based grounding is\nfragile and domain-sensitive, while symbolic knowledge integration incurs heavy\nretrieval and formatting costs. Motivated by knowledge graphs, we introduce\nGraph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds\ngeneration in corpus-derived evidence without retraining. GRAD constructs a\nsparse token transition graph by accumulating next-token logits across a small\nretrieved corpus in a single forward pass. During decoding, graph-retrieved\nlogits are max-normalized and adaptively fused with model logits to favor\nhigh-evidence continuations while preserving fluency. Across three models and a\nrange of question-answering benchmarks spanning intrinsic, extrinsic\nhallucination, and factuality tasks, GRAD consistently surpasses baselines,\nachieving up to 9.7$\\%$ higher intrinsic accuracy, 8.6$\\%$ lower hallucination\nrates, and 6.9$\\%$ greater correctness compared to greedy decoding, while\nattaining the highest truth--informativeness product score among all methods.\nGRAD offers a lightweight, plug-and-play alternative to contrastive decoding\nand knowledge graph augmentation, demonstrating that statistical evidence from\ncorpus-level token transitions can effectively steer generation toward more\ntruthful and verifiable outputs.", "AI": {"tldr": "GRAD\u662f\u4e00\u79cd\u89e3\u7801\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7a00\u758f\u6807\u8bb0\u8f6c\u79fb\u56fe\u6765\u7f13\u89e3LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u4e8b\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6e90\uff0c\u4f46\u57fa\u4e8e\u63d0\u793a\u7684\u63a5\u5730\u65b9\u6cd5\u8106\u5f31\u4e14\u5bf9\u9886\u57df\u654f\u611f\uff0c\u7b26\u53f7\u77e5\u8bc6\u96c6\u6210\u9700\u8981\u9ad8\u6602\u7684\u68c0\u7d22\u548c\u683c\u5f0f\u5316\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5728\u68c0\u7d22\u8bed\u6599\u4e0a\u7d2f\u79ef\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7684\u5bf9\u6570\u6982\u7387\u6765\u6784\u5efa\u7a00\u758f\u6807\u8bb0\u8f6c\u79fb\u56fe\uff0c\u5728\u89e3\u7801\u65f6\u81ea\u9002\u5e94\u878d\u5408\u56fe\u68c0\u7d22\u7684\u5bf9\u6570\u6982\u7387\u548c\u6a21\u578b\u5bf9\u6570\u6982\u7387\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\u548c\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRAD\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5185\u5728\u51c6\u786e\u7387\u63d0\u9ad89.7%\uff0c\u5e7b\u89c9\u7387\u964d\u4f4e8.6%\uff0c\u6b63\u786e\u6027\u63d0\u9ad86.9%\u3002", "conclusion": "GRAD\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8bc1\u660e\u8bed\u6599\u7ea7\u6807\u8bb0\u8f6c\u79fb\u7684\u7edf\u8ba1\u8bc1\u636e\u53ef\u4ee5\u6709\u6548\u5f15\u5bfc\u751f\u6210\u66f4\u771f\u5b9e\u548c\u53ef\u9a8c\u8bc1\u7684\u8f93\u51fa\u3002"}}
{"id": "2511.03888", "pdf": "https://arxiv.org/pdf/2511.03888", "abs": "https://arxiv.org/abs/2511.03888", "authors": ["Abdulmumin Sa'ad", "Sulaimon Oyeniyi Adebayo", "Abdul Jabbar Siddiqui"], "title": "Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages", "summary": "The global waste crisis is escalating, with solid waste generation expected\nto increase by 70% by 2050. Traditional waste collection methods, particularly\nin remote or harsh environments like deserts, are labor-intensive, inefficient,\nand often hazardous. Recent advances in computer vision and deep learning have\nopened the door to automated waste detection systems, yet most research focuses\non urban environments and recyclable materials, overlooking organic and\nhazardous waste and underexplored terrains such as deserts. In this work, we\npropose an enhanced real-time object detection framework based on a pruned,\nlightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)\nand specialized data augmentation strategies. Using the DroneTrashNet dataset,\nwe demonstrate significant improvements in precision, recall, and mean average\nprecision (mAP), while achieving low latency and compact model size suitable\nfor deployment on resource-constrained aerial drones. Benchmarking our model\nagainst state-of-the-art lightweight YOLO variants further highlights its\noptimal balance of accuracy and efficiency. Our results validate the\neffectiveness of combining data-centric and model-centric enhancements for\nrobust, real-time waste detection in desert environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8f7b\u91cf\u5316YOLOv12\u7ed3\u5408\u81ea\u5bf9\u6297\u8bad\u7ec3\u548c\u4e13\u7528\u6570\u636e\u589e\u5f3a\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u6c99\u6f20\u73af\u5883\u4e2d\u7684\u5e9f\u7269\u68c0\u6d4b\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "motivation": "\u5168\u7403\u5e9f\u7269\u5371\u673a\u52a0\u5267\uff0c\u4f20\u7edf\u5e9f\u7269\u6536\u96c6\u65b9\u6cd5\u5728\u504f\u8fdc\u73af\u5883\u6548\u7387\u4f4e\u4e0b\u4e14\u5371\u9669\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u73af\u5883\u548c\u53ef\u56de\u6536\u6750\u6599\uff0c\u5ffd\u89c6\u4e86\u6709\u673a\u5371\u9669\u5e9f\u7269\u548c\u6c99\u6f20\u7b49\u672a\u5145\u5206\u63a2\u7d22\u7684\u5730\u5f62", "method": "\u4f7f\u7528\u4fee\u526a\u8f7b\u91cf\u5316\u7684YOLOv12\u6a21\u578b\uff0c\u96c6\u6210\u81ea\u5bf9\u6297\u8bad\u7ec3(SAT)\u548c\u4e13\u7528\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5728DroneTrashNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cmAP\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u7d27\u51d1\u6a21\u578b\u5c3a\u5bf8\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u4e0a\u90e8\u7f72\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861", "conclusion": "\u9a8c\u8bc1\u4e86\u6570\u636e\u4e2d\u5fc3\u548c\u6a21\u578b\u4e2d\u5fc3\u589e\u5f3a\u76f8\u7ed3\u5408\u5bf9\u4e8e\u6c99\u6f20\u73af\u5883\u4e2d\u7a33\u5065\u5b9e\u65f6\u5e9f\u7269\u68c0\u6d4b\u7684\u6709\u6548\u6027"}}
{"id": "2511.03908", "pdf": "https://arxiv.org/pdf/2511.03908", "abs": "https://arxiv.org/abs/2511.03908", "authors": ["Alvin Wei Ming Tan", "Ben Prystawski", "Veronica Boyce", "Michael C. Frank"], "title": "Context informs pragmatic interpretation in vision-language models", "categories": ["cs.CL"], "comment": "Accepted at CogInterp Workshop, NeurIPS 2025", "summary": "Iterated reference games - in which players repeatedly pick out novel\nreferents using language - present a test case for agents' ability to perform\ncontext-sensitive pragmatic reasoning in multi-turn linguistic environments. We\ntested humans and vision-language models on trials from iterated reference\ngames, varying the given context in terms of amount, order, and relevance.\nWithout relevant context, models were above chance but substantially worse than\nhumans. However, with relevant context, model performance increased\ndramatically over trials. Few-shot reference games with abstract referents\nremain a difficult task for machine learning models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8fed\u4ee3\u53c2\u8003\u6e38\u620f\u4e2d\u4eba\u7c7b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53d1\u73b0\u5728\u7f3a\u4e4f\u76f8\u5173\u4e0a\u4e0b\u6587\u65f6\u6a21\u578b\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u6709\u76f8\u5173\u4e0a\u4e0b\u6587\u65f6\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u8bed\u8a00\u73af\u5883\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u654f\u611f\u8bed\u7528\u63a8\u7406\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u8fed\u4ee3\u53c2\u8003\u6e38\u620f\u8bc4\u4f30\u4eba\u7c7b\u548c\u673a\u5668\u6a21\u578b\u7684\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u8fed\u4ee3\u53c2\u8003\u6e38\u620f\u8bd5\u9a8c\uff0c\u6539\u53d8\u4e0a\u4e0b\u6587\u7684\u6570\u91cf\u3001\u987a\u5e8f\u548c\u76f8\u5173\u6027\uff0c\u6bd4\u8f83\u4eba\u7c7b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u65e0\u76f8\u5173\u4e0a\u4e0b\u6587\u65f6\u6a21\u578b\u8868\u73b0\u9ad8\u4e8e\u968f\u673a\u4f46\u8fdc\u5dee\u4e8e\u4eba\u7c7b\uff1b\u6709\u76f8\u5173\u4e0a\u4e0b\u6587\u65f6\u6a21\u578b\u6027\u80fd\u968f\u8bd5\u9a8c\u6b21\u6570\u663e\u8457\u63d0\u5347\uff1b\u62bd\u8c61\u6307\u79f0\u7684\u5c11\u6837\u672c\u53c2\u8003\u6e38\u620f\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ecd\u662f\u6311\u6218\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u867d\u7136\u6a21\u578b\u5728\u6709\u76f8\u5173\u4e0a\u4e0b\u6587\u65f6\u80fd\u663e\u8457\u6539\u5584\uff0c\u4f46\u5728\u590d\u6742\u8bed\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u8ddd\u3002"}}
{"id": "2511.03891", "pdf": "https://arxiv.org/pdf/2511.03891", "abs": "https://arxiv.org/abs/2511.03891", "authors": ["Hlali Azzeddine", "Majid Ben Yakhlef", "Soulaiman El Hazzat"], "title": "Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition", "categories": ["cs.CV", "cs.AI", "cs.DB"], "comment": null, "summary": "Small, imbalanced datasets and poor input image quality can lead to high\nfalse predictions rates with deep learning models. This paper introduces\nClass-Based Image Composition, an approach that allows us to reformulate\ntraining inputs through a fusion of multiple images of the same class into\ncombined visual composites, named Composite Input Images (CoImg). That enhances\nthe intra-class variance and improves the valuable information density per\ntraining sample and increases the ability of the model to distinguish between\nsubtle disease patterns. Our method was evaluated on the Optical Coherence\nTomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et\nal., 2024), which contains 2,064 high-resolution optical coherence tomography\n(OCT) scans of the human retina, representing seven distinct diseases with a\nsignificant class imbalance. We constructed a perfectly class-balanced version\nof this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout\ncomposite image. To assess the effectiveness of this new representation, we\nconducted a comparative analysis between the original dataset and its variant\nusing a VGG16 model. A fair comparison was ensured by utilizing the identical\nmodel architecture and hyperparameters for all experiments. The proposed\napproach markedly improved diagnostic results.The enhanced Dataset achieved\nnear-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared\nto a baseline model trained on raw dataset. The false prediction rate was also\nsignificantly lower, this demonstrates that the method can producehigh-quality\npredictions even for weak datasets affected by class imbalance or small sample\nsize.", "AI": {"tldr": "\u63d0\u51faClass-Based Image Composition\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u540c\u7c7b\u591a\u5f20\u56fe\u50cf\u878d\u5408\u6210\u7ec4\u5408\u89c6\u89c9\u56fe\u50cf(CoImg)\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u5c0f\u6837\u672c\u3001\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u548c\u56fe\u50cf\u8d28\u91cf\u5dee\u5bfc\u81f4\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bef\u5224\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5c0f\u6837\u672c\u3001\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u548c\u8f93\u5165\u56fe\u50cf\u8d28\u91cf\u5dee\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bef\u5224\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u7ec6\u5fae\u75be\u75c5\u6a21\u5f0f\u7684\u533a\u5206\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7c7b\u57fa\u56fe\u50cf\u7ec4\u5408\u65b9\u6cd5\uff0c\u5c06\u540c\u7c7b\u591a\u5f20\u56fe\u50cf\u878d\u5408\u6210\u7ec4\u5408\u89c6\u89c9\u56fe\u50cf(CoImg)\uff0c\u6784\u5efa\u5e73\u8861\u6570\u636e\u96c6Co-OCTDL\uff0c\u4f7f\u7528VGG16\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5728OCTDL\u6570\u636e\u96c6\u4e0a\uff0c\u6539\u8fdb\u65b9\u6cd5\u8fbe\u5230\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387(99.6%)\u3001F1\u5206\u6570(0.995)\u548cAUC(0.9996)\uff0c\u8bef\u5224\u7387\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5373\u4f7f\u5bf9\u4e8e\u53d7\u7c7b\u522b\u4e0d\u5e73\u8861\u6216\u5c0f\u6837\u672c\u5f71\u54cd\u7684\u5f31\u6570\u636e\u96c6\uff0c\u4e5f\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u9884\u6d4b\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6027\u80fd\u3002"}}
{"id": "2511.03915", "pdf": "https://arxiv.org/pdf/2511.03915", "abs": "https://arxiv.org/abs/2511.03915", "authors": ["Stefano M. Iacus", "Devika Jain", "Andrea Nasuto", "Giuseppe Porro", "Marcello Carammia", "Andrea Vezzulli"], "title": "The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023", "categories": ["cs.CL", "cs.CY", "stat.AP"], "comment": null, "summary": "Quantifying human flourishing, a multidimensional construct including\nhappiness, health, purpose, virtue, relationships, and financial stability, is\ncritical for understanding societal well-being beyond economic indicators.\nExisting measures often lack fine spatial and temporal resolution. Here we\nintroduce the Human Flourishing Geographic Index (HFGI), derived from analyzing\napproximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned\nlarge language models to classify expressions across 48 indicators aligned with\nHarvard's Global Flourishing Study framework plus attitudes towards migration\nand perception of corruption. The dataset offers monthly and yearly county- and\nstate-level indicators of flourishing-related discourse, validated to confirm\nthat the measures accurately represent the underlying constructs and show\nexpected correlations with established indicators. This resource enables\nmultidisciplinary analyses of well-being, inequality, and social change at\nunprecedented resolution, offering insights into the dynamics of human\nflourishing as reflected in social media discourse across the United States\nover the past decade.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4eba\u7c7b\u7e41\u8363\u5730\u7406\u6307\u6570(HFGI)\uff0c\u901a\u8fc7\u5206\u679026\u4ebf\u6761\u5730\u7406\u5b9a\u4f4d\u7684\u7f8e\u56fd\u63a8\u6587\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf948\u4e2a\u7e41\u8363\u6307\u6807\u8fdb\u884c\u5206\u7c7b\uff0c\u63d0\u4f9b\u6708\u5ea6/\u5e74\u5ea6\u7684\u53bf/\u5dde\u7ea7\u7e41\u8363\u76f8\u5173\u8bdd\u8bed\u6570\u636e\u3002", "motivation": "\u91cf\u5316\u4eba\u7c7b\u7e41\u8363\u8fd9\u4e00\u591a\u7ef4\u6982\u5ff5\u5bf9\u4e8e\u7406\u89e3\u8d85\u8d8a\u7ecf\u6d4e\u6307\u6807\u7684\u793e\u4f1a\u798f\u7949\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u6d4b\u91cf\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u7cbe\u7ec6\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u3002", "method": "\u5206\u67902013-2023\u5e74\u7ea626\u4ebf\u6761\u5730\u7406\u5b9a\u4f4d\u7684\u7f8e\u56fd\u63a8\u6587\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8868\u8fbe\u8fdb\u884c\u5206\u7c7b\uff0c\u6db5\u76d648\u4e2a\u4e0e\u54c8\u4f5b\u5168\u7403\u7e41\u8363\u7814\u7a76\u6846\u67b6\u4e00\u81f4\u7684\u6307\u6807\uff0c\u5305\u62ec\u5bf9\u79fb\u6c11\u6001\u5ea6\u548c\u8150\u8d25\u8ba4\u77e5\u3002", "result": "\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6708\u5ea6/\u5e74\u5ea6\u7684\u53bf/\u5dde\u7ea7\u7e41\u8363\u76f8\u5173\u8bdd\u8bed\u6307\u6807\uff0c\u7ecf\u9a8c\u8bc1\u80fd\u51c6\u786e\u4ee3\u8868\u57fa\u7840\u6982\u5ff5\uff0c\u5e76\u4e0e\u65e2\u5b9a\u6307\u6807\u663e\u793a\u9884\u671f\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u8d44\u6e90\u80fd\u591f\u4ee5\u524d\u6240\u672a\u6709\u7684\u5206\u8fa8\u7387\u8fdb\u884c\u798f\u7949\u3001\u4e0d\u5e73\u7b49\u548c\u793e\u4f1a\u53d8\u9769\u7684\u591a\u5b66\u79d1\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u8fc7\u53bb\u5341\u5e74\u7f8e\u56fd\u793e\u4ea4\u5a92\u4f53\u8bdd\u8bed\u4e2d\u53cd\u6620\u7684\u4eba\u7c7b\u7e41\u8363\u52a8\u6001\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.03912", "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u7684\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u6269\u5c55\u6b63\u5e38\u6837\u672c\u96c6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u66f4\u65b0\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u6837\u672c\u51c6\u5165\u673a\u5236\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u672a\u77e5\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u548c\u4e13\u5bb6\u76d1\u7763\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u5f02\u5e38\u6807\u7b7e\u7684\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u4e3b\u5e72\u7f51\u7edc\u914d\u5408\u5fae\u5c0f\u5377\u79ef\u9002\u914d\u5668\u8fdb\u884c\u5feb\u901f\u9886\u57df\u9002\u5e94\uff0c\u901a\u8fc7k\u8fd1\u90bb\u5f02\u5e38\u8bc4\u5206\u548c\u53cc\u91cd\u6982\u7387\u95e8\u63a7\u673a\u5236\uff08\u8ddd\u79bbz-score\u9608\u503c\u548cSWAG\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff09\u5b89\u5168\u6269\u5c55\u6b63\u5e38\u6837\u672c\u96c6\u3002", "result": "\u5728COVID-CXR\u6570\u636e\u96c6\u4e0aROC-AUC\u4ece0.9489\u63d0\u5347\u52300.9982\uff0cF1\u4ece0.8048\u63d0\u5347\u52300.9746\uff1b\u5728Pneumonia CXR\u4e0aROC-AUC\u4ece0.6834\u63d0\u5347\u52300.8968\uff1b\u5728Brain MRI ND-5\u4e0aROC-AUC\u4ece0.6041\u63d0\u5347\u52300.7269\uff0cPR-AUC\u4ece0.7539\u63d0\u5347\u52300.8211\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6807\u7b7e\u7a00\u7f3a\u7684\u771f\u5b9e\u4e16\u754c\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u7a33\u5b9a\u5730\u7cbe\u70bc\u6b63\u5e38\u6027\u6982\u5ff5\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.03945", "pdf": "https://arxiv.org/pdf/2511.03945", "abs": "https://arxiv.org/abs/2511.03945", "authors": ["Fu-Chun Yang", "Jason Eshraghian"], "title": "Direct Semantic Communication Between Large Language Models via Vector Translation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "9 pages, 1 figure, 2 tables", "summary": "In multi-agent settings, such as debate, reflection, or tool-calling, large\nlanguage models (LLMs) pass messages as plain tokens, discarding most latent\nsemantics. This constrains information transfer and adds unnecessary\ncomputational overhead. We form a latent bridge via vector translations, which\nuse learned mappings that enable direct semantic exchange between\nrepresentation spaces. A dual-encoder translator trained between Llama-2-7B and\nMistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the\ntranslated vectors at 30 percent blending strength steers the target model's\ngeneration without destabilizing logits. Bidirectional evaluation shows a\n2.01:1 transfer asymmetry, indicating that general-purpose models yield more\ntransferable representations than instruction-tuned variants. This conservative\ninjection preserves computational stability while demonstrating that\ncross-model latent communication is feasible, enabling collaborative AI systems\nthat share meaning rather than tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5411\u91cf\u7ffb\u8bd1\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u8de8\u6a21\u578b\u8bed\u4e49\u76f4\u63a5\u4ea4\u6362\u7684\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684token\u4f20\u9012\u65b9\u5f0f\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u5347\u4fe1\u606f\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0cLLM\u901a\u8fc7\u7eaftoken\u4f20\u9012\u6d88\u606f\u4f1a\u4e22\u5f03\u5927\u90e8\u5206\u6f5c\u5728\u8bed\u4e49\uff0c\u9650\u5236\u4e86\u4fe1\u606f\u4f20\u8f93\u5e76\u589e\u52a0\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u4f7f\u7528\u53cc\u7f16\u7801\u5668\u7ffb\u8bd1\u5668\u5728Llama-2-7B\u548cMistral-7B-Instruct\u6a21\u578b\u4e4b\u95f4\u5b66\u4e60\u6620\u5c04\uff0c\u5b9e\u73b0\u8868\u793a\u7a7a\u95f4\u7684\u76f4\u63a5\u8bed\u4e49\u4ea4\u6362\uff0c\u5e76\u4ee530%\u7684\u6df7\u5408\u5f3a\u5ea6\u6ce8\u5165\u7ffb\u8bd1\u540e\u7684\u5411\u91cf\u3002", "result": "\u83b7\u5f97\u4e860.538\u7684\u5e73\u5747\u4f59\u5f26\u5bf9\u9f50\u5ea6\uff0c\u53cc\u5411\u8bc4\u4f30\u663e\u793a2.01:1\u7684\u4f20\u8f93\u4e0d\u5bf9\u79f0\u6027\uff0c\u8868\u660e\u901a\u7528\u6a21\u578b\u6bd4\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4ea7\u751f\u66f4\u53ef\u8f6c\u79fb\u7684\u8868\u793a\u3002", "conclusion": "\u4fdd\u5b88\u7684\u5411\u91cf\u6ce8\u5165\u4fdd\u6301\u4e86\u8ba1\u7b97\u7a33\u5b9a\u6027\uff0c\u8bc1\u660e\u4e86\u8de8\u6a21\u578b\u6f5c\u5728\u901a\u4fe1\u7684\u53ef\u884c\u6027\uff0c\u4f7f\u534f\u4f5cAI\u7cfb\u7edf\u80fd\u591f\u5171\u4eab\u610f\u4e49\u800c\u975etoken\u3002"}}
{"id": "2511.03943", "pdf": "https://arxiv.org/pdf/2511.03943", "abs": "https://arxiv.org/abs/2511.03943", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Temporal action localization requires precise boundary detection; however,\ncurrent methods apply uniform computation despite significant variations in\ndifficulty across boundaries. We present two complementary contributions.\nFirst, Boundary Distance Regression (BDR) provides information-theoretically\noptimal localization through signed-distance regression rather than\nclassification, achieving 43\\% sharper boundary peaks. BDR retrofits to\nexisting methods with approximately 50 lines of code, yielding consistent 1.8\nto 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive\nTemporal Refinement (ATR) allocates computation via continuous depth selection\n$\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without\nreinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs,\ncompared to 53.6\\% at 198G for uniform processing, providing a 2.9\\%\nimprovement with 18\\% less compute. Gains scale with boundary heterogeneity,\nshowing 4.2\\% improvement on short actions. Training cost is mitigated via\nknowledge distillation, with lightweight students retaining 99\\% performance at\nbaseline cost. Results are validated across four benchmarks with rigorous\nstatistical testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8fb9\u754c\u8ddd\u79bb\u56de\u5f52(BDR)\u548c\u81ea\u9002\u5e94\u65f6\u5e8f\u7cbe\u5316(ATR)\u4e24\u4e2a\u4e92\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8fb9\u754c\u68c0\u6d4b\u7684\u8ba1\u7b97\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u5bf9\u6240\u6709\u8fb9\u754c\u91c7\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u8fb9\u754c\u96be\u5ea6\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "BDR\u4f7f\u7528\u5e26\u7b26\u53f7\u8ddd\u79bb\u56de\u5f52\u66ff\u4ee3\u5206\u7c7b\uff0c\u5b9e\u73b0\u4fe1\u606f\u7406\u8bba\u6700\u4f18\u5b9a\u4f4d\uff1bATR\u901a\u8fc7\u8fde\u7eed\u6df1\u5ea6\u9009\u62e9\u03c4\u2208[0,1]\u5206\u914d\u8ba1\u7b97\uff0c\u652f\u6301\u7aef\u5230\u7aef\u53ef\u5fae\u4f18\u5316\u3002", "result": "\u5728THUMOS14\u4e0a\uff0cATR\u5728162G FLOPs\u4e0b\u8fbe\u523056.5% mAP@0.7\uff0c\u76f8\u6bd4\u5747\u5300\u5904\u7406\u768453.6%(198G FLOPs)\uff0c\u6027\u80fd\u63d0\u53472.9%\u4e14\u8ba1\u7b97\u51cf\u5c1118%\u3002BDR\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u5e26\u67651.8-3.1%\u7684mAP@0.7\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u754c\u68c0\u6d4b\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u5bf9\u77ed\u52a8\u4f5c\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2511.04020", "pdf": "https://arxiv.org/pdf/2511.04020", "abs": "https://arxiv.org/abs/2511.04020", "authors": ["Shiyin Lin"], "title": "Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) enhanced with retrieval -- commonly referred to\nas Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance\nin knowledge-intensive tasks. However, RAG pipelines often fail when retrieved\nevidence is incomplete, leaving gaps in the reasoning process. In such cases,\n\\emph{abductive inference} -- the process of generating plausible missing\npremises to explain observations -- offers a principled approach to bridge\nthese gaps. In this paper, we propose a framework that integrates abductive\ninference into retrieval-augmented LLMs. Our method detects insufficient\nevidence, generates candidate missing premises, and validates them through\nconsistency and plausibility checks. Experimental results on abductive\nreasoning and multi-hop QA benchmarks show that our approach improves both\nanswer accuracy and reasoning faithfulness. This work highlights abductive\ninference as a promising direction for enhancing the robustness and\nexplainability of RAG systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5c06\u6eaf\u56e0\u63a8\u7406\u6574\u5408\u5230\u68c0\u7d22\u589e\u5f3aLLMs\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u8bc1\u636e\u4e0d\u8db3\u3001\u751f\u6210\u5019\u9009\u7f3a\u5931\u524d\u63d0\u5e76\u8fdb\u884c\u9a8c\u8bc1\uff0c\u63d0\u9ad8RAG\u7cfb\u7edf\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u4f20\u7edfRAG\u7ba1\u9053\u5728\u68c0\u7d22\u8bc1\u636e\u4e0d\u5b8c\u6574\u65f6\u4f1a\u5931\u8d25\uff0c\u7559\u4e0b\u63a8\u7406\u7a7a\u767d\u3002\u6eaf\u56e0\u63a8\u7406\u80fd\u63d0\u4f9b\u539f\u5219\u6027\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\u3002", "method": "\u68c0\u6d4b\u8bc1\u636e\u4e0d\u8db3\u3001\u751f\u6210\u5019\u9009\u7f3a\u5931\u524d\u63d0\u3001\u901a\u8fc7\u4e00\u81f4\u6027\u548c\u5408\u7406\u6027\u68c0\u67e5\u8fdb\u884c\u9a8c\u8bc1", "result": "\u5728\u6eaf\u56e0\u63a8\u7406\u548c\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u53ef\u4fe1\u5ea6", "conclusion": "\u6eaf\u56e0\u63a8\u7406\u662f\u589e\u5f3aRAG\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6709\u524d\u666f\u65b9\u5411"}}
{"id": "2511.03950", "pdf": "https://arxiv.org/pdf/2511.03950", "abs": "https://arxiv.org/abs/2511.03950", "authors": ["Zhejia Cai", "Puhua Jiang", "Shiwei Mao", "Hongkun Cao", "Ruqi Huang"], "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Reconstructing real-world objects from multi-view images is essential for\napplications in 3D editing, AR/VR, and digital content creation. Existing\nmethods typically prioritize either geometric accuracy (Multi-View Stereo) or\nphotorealistic rendering (Novel View Synthesis), often decoupling geometry and\nappearance optimization, which hinders downstream editing tasks. This paper\nadvocates an unified treatment on geometry and appearance optimization for\nseamless Gaussian-mesh joint optimization. More specifically, we propose a\nnovel framework that simultaneously optimizes mesh geometry (vertex positions\nand faces) and vertex colors via Gaussian-guided mesh differentiable rendering,\nleveraging photometric consistency from input images and geometric\nregularization from normal and depth maps. The obtained high-quality 3D\nreconstruction can be further exploit in down-stream editing tasks, such as\nrelighting and shape deformation. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u5904\u7406\u51e0\u4f55\u548c\u5916\u89c2\u4f18\u5316\u7684\u9ad8\u65af-\u7f51\u683c\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u5f15\u5bfc\u7684\u7f51\u683c\u53ef\u5fae\u5206\u6e32\u67d3\u540c\u65f6\u4f18\u5316\u7f51\u683c\u51e0\u4f55\u548c\u9876\u70b9\u989c\u8272\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u6e32\u67d3\u4e4b\u95f4\u6743\u8861\uff0c\u5c06\u51e0\u4f55\u548c\u5916\u89c2\u4f18\u5316\u89e3\u8026\uff0c\u8fd9\u963b\u788d\u4e86\u4e0b\u6e38\u7f16\u8f91\u4efb\u52a1\u3002\u9700\u8981\u7edf\u4e00\u7684\u51e0\u4f55\u548c\u5916\u89c2\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u540c\u65f6\u4f18\u5316\u7f51\u683c\u51e0\u4f55\uff08\u9876\u70b9\u4f4d\u7f6e\u548c\u9762\uff09\u548c\u9876\u70b9\u989c\u8272\uff0c\u5229\u7528\u9ad8\u65af\u5f15\u5bfc\u7684\u7f51\u683c\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u7ed3\u5408\u8f93\u5165\u56fe\u50cf\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u6cd5\u5411/\u6df1\u5ea6\u56fe\u7684\u51e0\u4f55\u6b63\u5219\u5316\u3002", "result": "\u83b7\u5f97\u4e86\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u7ed3\u679c\uff0c\u53ef\u7528\u4e8e\u4e0b\u6e38\u7f16\u8f91\u4efb\u52a1\u5982\u91cd\u5149\u7167\u548c\u5f62\u72b6\u53d8\u5f62\u3002", "conclusion": "\u63d0\u51fa\u7684\u9ad8\u65af-\u7f51\u683c\u8054\u5408\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u4e86\u51e0\u4f55\u548c\u5916\u89c2\u7684\u7edf\u4e00\u4f18\u5316\uff0c\u4e3a3D\u7f16\u8f91\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u57fa\u7840\u3002"}}
{"id": "2511.04035", "pdf": "https://arxiv.org/pdf/2511.04035", "abs": "https://arxiv.org/abs/2511.04035", "authors": ["Dongji Gao", "Chenda Liao", "Changliang Liu", "Matthew Wiesner", "Leibny Paola Garcia", "Daniel Povey", "Sanjeev Khudanpur", "Jian Wu"], "title": "WST: Weakly Supervised Transducer for Automatic Speech Recognition", "categories": ["cs.CL"], "comment": null, "summary": "The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in\nend-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily\non large-scale, high-quality annotated data, which are often costly and\ndifficult to obtain. To mitigate this reliance, we propose a Weakly Supervised\nTransducer (WST), which integrates a flexible training graph designed to\nrobustly handle errors in the transcripts without requiring additional\nconfidence estimation or auxiliary pre-trained models. Empirical evaluations on\nsynthetic and industrial datasets reveal that WST effectively maintains\nperformance even with transcription error rates of up to 70%, consistently\noutperforming existing Connectionist Temporal Classification (CTC)-based weakly\nsupervised approaches, such as Bypass Temporal Classification (BTC) and\nOmni-Temporal Classification (OTC). These results demonstrate the practical\nutility and robustness of WST in realistic ASR settings. The implementation\nwill be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u5f31\u76d1\u7763\u8f6c\u6362\u5668\uff08WST\uff09\u6765\u89e3\u51b3RNN-T\u5bf9\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u80fd\u591f\u5728\u8f6c\u5f55\u9519\u8bef\u7387\u9ad8\u8fbe70%\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684CTC\u57fa\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "RNN-T\u5728\u7aef\u5230\u7aef\u8bed\u97f3\u8bc6\u522b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u5f31\u76d1\u7763\u8f6c\u6362\u5668\uff08WST\uff09\uff0c\u96c6\u6210\u7075\u6d3b\u7684\u8bad\u7ec3\u56fe\u8bbe\u8ba1\uff0c\u80fd\u591f\u9c81\u68d2\u5730\u5904\u7406\u8f6c\u5f55\u9519\u8bef\uff0c\u65e0\u9700\u989d\u5916\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6216\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cWST\u5728\u8f6c\u5f55\u9519\u8bef\u7387\u9ad8\u8fbe70%\u65f6\u4ecd\u80fd\u6709\u6548\u4fdd\u6301\u6027\u80fd\uff0c\u4e00\u81f4\u4f18\u4e8eBTC\u548cOTC\u7b49\u73b0\u6709CTC\u57fa\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "WST\u5728\u5b9e\u9645ASR\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u5c06\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2511.03962", "pdf": "https://arxiv.org/pdf/2511.03962", "abs": "https://arxiv.org/abs/2511.03962", "authors": ["Zhong Chen", "Changfeng Chen"], "title": "A Linear Fractional Transformation Model and Calibration Method for Light Field Camera", "categories": ["cs.CV"], "comment": null, "summary": "Accurate calibration of internal parameters is a crucial yet challenging\nprerequisite for 3D reconstruction using light field cameras. In this paper, we\npropose a linear fractional transformation(LFT) parameter $\\alpha$ to decoupled\nthe main lens and micro lens array (MLA). The proposed method includes an\nanalytical solution based on least squares, followed by nonlinear refinement.\nThe method for detecting features from the raw images is also introduced.\nExperimental results on both physical and simulated data have verified the\nperformance of proposed method. Based on proposed model, the simulation of raw\nlight field images becomes faster, which is crucial for data-driven deep\nlearning methods. The corresponding code can be obtained from the author's\nwebsite.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u5206\u5f0f\u53d8\u6362\u53c2\u6570\u03b1\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u8026\u5149\u573a\u76f8\u673a\u7684\u4e3b\u955c\u5934\u548c\u5fae\u900f\u955c\u9635\u5217\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5185\u53c2\u6807\u5b9a\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u7684\u89e3\u6790\u89e3\u548c\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u68c0\u6d4b\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "motivation": "\u5149\u573a\u76f8\u673a\u5185\u90e8\u53c2\u6570\u7684\u7cbe\u786e\u6807\u5b9a\u662f3D\u91cd\u5efa\u7684\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u524d\u63d0\u6761\u4ef6\u3002\u9700\u8981\u89e3\u51b3\u4e3b\u955c\u5934\u548c\u5fae\u900f\u955c\u9635\u5217\u4e4b\u95f4\u7684\u8026\u5408\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u5206\u5f0f\u53d8\u6362\u53c2\u6570\u03b1\u89e3\u8026\u4e3b\u955c\u5934\u548c\u5fae\u900f\u955c\u9635\u5217\uff0c\u5305\u62ec\u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u7684\u89e3\u6790\u89e3\u548c\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u68c0\u6d4b\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u7269\u7406\u548c\u6a21\u62df\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u8be5\u6a21\u578b\uff0c\u539f\u59cb\u5149\u573a\u56fe\u50cf\u7684\u6a21\u62df\u901f\u5ea6\u66f4\u5feb\uff0c\u8fd9\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5149\u573a\u76f8\u673a\u5185\u53c2\u6807\u5b9a\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6807\u5b9a\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6570\u636e\u652f\u6301\u3002"}}
