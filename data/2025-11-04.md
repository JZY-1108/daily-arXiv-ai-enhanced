<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.CV](#cs.CV) [Total: 183]
- [cs.RO](#cs.RO) [Total: 6]
- [math.NA](#math.NA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.DL](#cs.DL) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个包含1k个挑战性可视化任务的基准测试，涵盖7个高级可视化任务和48种图表类型。研究发现现有LLMs在处理复杂可视化任务时存在明显缺陷，为此开发了SynthVis-30K数据集和PlotCraftor模型，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面表现出色，但在处理复杂可视化任务方面的能力尚未得到充分评估和发展，需要系统性地评估和改进LLMs在数据可视化方面的能力。

Method: 1. 创建PlotCraft基准测试，包含1k个可视化任务；2. 开发SynthVis-30K大规模高质量可视化代码数据集；3. 基于该数据集开发PlotCraftor代码生成模型。

Result: PlotCraftor在VisEval、PandasPlotBench和PlotCraft基准测试中表现与领先专有方法相当，在困难任务上性能提升超过50%。对23个领先LLMs的评估显示它们在处理复杂可视化任务时存在明显性能缺陷。

Conclusion: PlotCraft基准测试揭示了LLMs在复杂可视化任务中的不足，而基于SynthVis-30K数据集开发的PlotCraftor模型能够有效弥补这一差距，在多个基准测试中表现出色。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI框架通过原型理论改进MBTI人格识别，使用LLM增强语料库和轻量级编码器学习原型，通过检索-重用-修订-保留循环提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别方法采用硬标签分类，忽视了人格判断的渐进性和原型特性，需要更符合认知心理学的建模方法。

Method: 构建平衡语料库，LoRA微调轻量编码器学习原型，采用检索-重用-修订-保留推理循环，通过基于提示的投票机制聚合原型证据。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI维度和16种人格类型任务上均优于基线方法，并展现出强大的跨数据集泛化能力。

Conclusion: 将推理过程与心理原型理论对齐，能够提升基于文本的人格建模的准确性、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 开发了一个残差流解码器框架，用于探测语言模型在段落和文档尺度上的规划信息，发现可以解码相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够处理更长时间跨度的任务，现有的激活理解方法通常局限于测试特定概念或token，需要开发能够理解更长时程规划信息的方法。

Method: 开发了残差流解码器框架，测试了多种方法来探测模型激活中的段落和文档尺度规划信息。

Result: 在小型模型中，可以解码出相当于5个以上未来token上下文的信息。

Conclusion: 这些结果为更好地监控语言模型和理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出了一种挑战传统next-token预测训练方法的优化方案，通过预测信息丰富的token来更有效地训练大语言模型。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型的训练性能，在保持计算成本的同时提升模型表现，挑战传统的next-token预测训练方法。

Method: 提出预测信息丰富token的训练策略，在算术、多标签文本分类和自然语言生成三种任务中进行验证。

Result: 为LLM训练提供了原则性优化方法，提升了模型性能并深化了对目标token选择策略的理论理解。

Conclusion: 通过选择性地预测信息丰富的token，可以更有效地训练大语言模型，为训练优化提供了新的方向。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出了一个评估和改进LLM角色一致性的统一框架，通过强化学习微调LLM，显著减少了角色漂移问题。


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类用户时经常出现角色漂移、前后矛盾或放弃角色适当行为的问题，影响了模拟的真实性和可靠性。

Method: 定义了三种自动指标来评估角色一致性，并以此作为奖励信号，使用多轮强化学习对LLM进行微调。

Result: 该方法使不一致性降低了55%以上，产生了更连贯和忠实的模拟用户。

Conclusion: 该框架有效提升了LLM在角色扮演中的一致性，为可扩展的AI代理训练和评估提供了更可靠的模拟用户。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB是一个基于浏览器的网络安全桌面演练系统，集成LLM队友和检索增强副驾驶，提供按需认知提示，相比传统脚本化演练更轻量、可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全桌面演练存在脚本化、资源密集、难以扩展的问题，需要更轻量、可重复的实践方式。

Method: 重新设计Backdoors & Breaches游戏，集成大语言模型队友和检索增强副驾驶(C2D2)，将精选语料扩展为事实、概念、程序、元认知片段，使用提示工程代理和逐步淡出的脚手架阶梯。

Result: 在4名研究生的单人试点中，参与者更倾向于使用基于代理的版本，认为其更具可扩展性，但在简单知识测验中出现天花板效应。

Conclusion: 尽管存在样本小、单人模式、语料窄等限制，早期结果表明LLM增强的桌面演练可以提供轻量、可重复的实践，无需传统演练的后勤负担。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 该论文提出了IL-PCR语料库，为法律条文检索和先例检索任务提供统一测试平台，并开发了基于LLM的重排序方法以利用两个任务之间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究将法律条文检索和先例检索作为独立任务处理，但这两个任务本质相关，相似案件往往引用相似法律条文。

Method: 构建IL-PCR语料库，实验多种基线模型（词法模型、语义模型、基于GNN的集成模型），并开发基于LLM的重排序方法。

Result: 基于LLM的重排序方法在两个检索任务上取得了最佳性能。

Conclusion: IL-PCR语料库为法律检索研究提供了统一平台，利用任务间依赖关系的LLM重排序方法能显著提升检索性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方案，通过模板生成句子对训练，在How2Sign和iSign数据集上显著提升手语翻译性能，BLEU-4分数分别从1.97提升到4.56和从0.55提升到3.43。


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模句子对齐数据集稀缺的挑战，需要新的方法来提升低资源环境下的翻译性能。

Method: 提出POSESTITCH-SLT预训练方案，基于语言模板的句子生成技术，使用简单的基于Transformer的编码器-解码器架构。

Result: 在How2Sign数据集上BLEU-4从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越了基于姿态的无词汇表翻译的现有最佳方法。

Conclusion: 模板驱动的合成监督在低资源手语环境中具有显著效果，证明了该方法在解决数据稀缺问题上的有效性。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中性能媲美Transformer，在长上下文场景中表现出更优的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能利用训练时并行计算，又能在推理时保持恒定计算和内存复杂度的RNN架构，以在短上下文和长上下文场景中都实现竞争力。

Method: 基于Mamba-2构建Factorization Memory，开发稀疏形式仅更新部分循环状态，同时保持密集版本性能。

Result: Factorization Memory在短上下文任务中性能与Transformer相当，在长上下文场景中表现出更优的泛化能力，是首个成功结合稀疏内存激活的RNN架构。

Conclusion: 该工作提供了Factorization Memory与Transformer和Mamba-2架构的系统性实证分析，展示了其在短长上下文场景中的竞争力。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 论文形式化定义了因果语言建模目标的结构特性：反转不变性，即标准CLM预训练对语料及其反转赋予相同似然，说明预训练是方向盲的。作者认为这反映了当前预训练目标的局限性，并提出应从时间不对称性角度重新思考预训练。


<details>
  <summary>Details</summary>
Motivation: 揭示标准因果语言建模目标的反转不变性特性，指出这种对称性可能无法捕捉自然语言中的方向性依赖关系（如语音、形态或因果依赖），从而限制了模型对语言时间不对称性的建模能力。

Method: 通过理论形式化分析因果语言建模目标的结构特性，证明反转不变性的存在，并基于此提出从时间不对称性角度重新审视预训练目标。

Result: 发现标准CLM预训练对语料及其反转赋予相同似然，解释了为什么在反转文本上训练的模型能达到与正向文本训练模型相当的性能，尽管人类语言和推理本质上是时间不对称的。

Conclusion: 反转不变性反映了当前预训练目标的局限性，建议未来研究应开发能够显式建模语言时间箭头同时保留标准语言建模能力的损失函数和架构。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个评估LLMs元语言推理能力的新基准，使用跨语言注释文本和语法描述，测试模型在未见过的低资源语言和结构上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能在训练中未见过的低资源语言和结构上进行语言推理泛化，而不仅仅是特定下游任务。

Method: 使用从18种类型多样的参考语法中提取的跨语言注释文本和语法描述，设计了受控评估任务——词义推断任务，模型需要根据上下文推断缺失的单词和注释。

Result: 结果表明，结合结构化语言线索能持续提升所有模型的推理性能。

Conclusion: 这项工作凸显了使用LLMs进行类型学语言分析和低资源语言文档化的前景和当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文介绍了推理轨迹生成任务，开发了包含手动标注推理轨迹的调试问题数据集，并展示了前沿LLM能够生成91%正确的推理轨迹和98.7%有效的苏格拉底对话轮次。


<details>
  <summary>Details</summary>
Motivation: 苏格拉底调试通过引导学生自行发现和修复bug，而不是直接提供修复方案。大多数新手程序员的bug源于编程误解，苏格拉底调试可以引导推理轨迹，让学生通过认知失调识别并修正错误信念。

Method: 提出了推理轨迹生成任务，创建了手动标注推理轨迹的调试问题数据集，开发了基于LLM的推理轨迹生成和基于这些轨迹的苏格拉底对话生成解决方案。

Result: 大规模LLM作为评判者的评估显示，前沿模型能够生成高达91%正确的推理轨迹和98.7%有效的对话轮次。

Conclusion: 研究表明LLM能够有效生成苏格拉底调试所需的推理轨迹和对话，为编程教育中的自动调试指导提供了可行方案。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 本文研究了迭代式改写文本如何逃避AI生成文本检测器，揭示了检测器在对抗改写攻击时的脆弱性，并提出了首个系统评估检测器鲁棒性的基准PADBen。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器对直接LLM输出能达到90%以上准确率，但在面对迭代式改写内容时完全失效。本文旨在探究这种失效的根本原因，并建立系统性的评估基准。

Method: 通过内在机制分析揭示迭代式改写创建了语义位移但保留生成模式的中间清洗区域，提出了PADBen基准，包含五种文本类型分类和五个渐进检测任务，评估了11种最先进的检测器。

Result: 检测器表现出关键不对称性：能成功识别抄袭逃避问题，但在作者身份混淆情况下失败。现有检测方法无法有效处理中间清洗区域。

Conclusion: 当前检测方法无法有效应对中间清洗区域，需要在检测架构上进行根本性改进，超越现有的语义和风格判别方法。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是首个跨语言医疗错误修正基准，包含日语和英语版本，评估LLMs在医疗错误检测、定位和修正方面的能力。研究发现推理模型表现最佳，跨语言评估显示英语到日语存在性能差距，微调后模型在结构化医疗错误修正任务中超过人类专家。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗应用中显示出潜力，但其检测和修正临床文本错误的能力尚未得到充分评估，特别是在英语以外的语言中。为确保安全部署，需要评估LLMs处理医疗错误的能力。

Method: 从日本医师国家考试构建MedRECT基准，包含日语和英语版本，涵盖错误检测、错误定位和错误修正三个子任务。评估了9个当代LLMs，包括专有模型、开源模型和推理模型，并进行针对性的LoRA微调。

Result: 推理模型显著优于标准架构，在错误检测和句子提取方面分别有13.5%和51.0%的相对改进；跨语言评估显示英语到日语存在5-10%性能差距；微调后模型在日语和英语的错误修正性能分别提升0.078和0.168；微调模型在结构化医疗错误修正任务中超过人类专家表现。

Conclusion: MedRECT为开发更安全的跨语言医疗LLMs提供了可复现的框架和资源，推理模型在医疗错误处理中表现优异，微调能有效提升性能，且模型可以超越人类专家在特定任务上的表现。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出了G2方法，一种无需训练的即插即用技术，通过双引导机制在解码过程中增强LLM输出多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在输出多样性方面存在严重限制，多次尝试生成的内容高度相似，影响需要多样化输出的任务。现有方法如温度缩放虽然增强多样性但会损害输出质量。

Method: 使用基础生成器配合双引导机制，通过基于解码的干预来引导生成过程，在原始查询条件下鼓励更多样化的输出。

Result: 综合实验表明，G2能有效提高输出多样性，同时在多样性和质量之间保持最佳平衡。

Conclusion: G2是一种有效的训练免费方法，能够显著提升LLM输出多样性而不影响生成质量。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）记忆化对合著网络的影响，发现LLMs在生成合著网络时存在系统性偏见，倾向于高被引研究者，但这种偏见在不同学科和地区间存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术搜索和推荐系统中的广泛应用，其记忆化能力可能导致合著网络生成中的公平性和偏见问题，影响信息生态系统的完整性。

Method: 评估了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型的记忆化效应，分析记忆化驱动输出在学科和地区间的差异。

Result: 全球分析显示LLMs存在一致偏向高被引研究者的偏见，但临床医学等学科和非洲部分地区表现出更平衡的代表性，表明这些领域训练数据可能更具公平性。

Conclusion: 研究强调了在学术发现中部署LLMs的风险和机遇，需要关注记忆化带来的偏见问题，同时识别训练数据中存在的公平性差异。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文构建了首个大规模Bhili-Hindi-English平行语料库BHEPC，包含11万句经过专业翻译的句子，涵盖教育、行政和新闻等领域。通过评估多种多语言大模型，发现微调的NLLB-200模型在Bhili机器翻译任务中表现最佳，展示了多语言模型在低资源场景下的潜力。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性带来了机器翻译挑战，特别是对于Bhili等缺乏高质量语言资源的部落语言。本文旨在填补这一资源空白，促进低资源语言的包容性自然语言处理技术发展。

Method: 创建了包含11万句的Bhili-Hindi-English平行语料库BHEPC，由专业人工翻译完成。评估了专有和开源多语言大模型在双向翻译任务中的表现，包括微调方法和上下文学习策略。

Result: 微调的NLLB-200蒸馏600M变体模型在Bhili机器翻译任务中表现最优。通过上下文学习评估了多语言LLM的生成翻译能力，并分析了跨领域泛化性能和分布差异。

Conclusion: 这项工作填补了关键资源空白，为Bhili等低资源语言建立了有价值的机器翻译基准，展示了多语言模型在低资源场景中的潜力，促进了全球低资源和边缘化语言的包容性NLP技术发展。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次在差分隐私文本隐私化评估中引入数据集大小因素，通过在大规模数据集上进行效用和隐私测试，发现数据集大小对隐私-效用权衡有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私自然语言处理研究在评估文本重写机制时往往忽略数据集大小的影响，本文旨在填补这一空白。

Method: 设计在大规模数据集上进行的效用和隐私测试，使用动态分割大小，在包含多达100万个文本的不同规模数据集上运行测试。

Result: 研究发现数据集大小在评估差分隐私文本重写机制中起着关键作用，增加数据集大小会影响隐私-效用权衡。

Conclusion: 这些发现呼吁在差分隐私自然语言处理中采用更严格的评估程序，并为差分隐私自然语言处理在实践和大规模应用中的未来发展提供了启示。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出了ToM框架，通过树状MapReduce方法解决大语言模型在长文本推理中的逻辑连贯性问题，显著优于现有的分治框架和检索增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和分治框架在处理长文本推理时存在逻辑连贯性差、长距离依赖捕捉困难等问题，需要一种能保持文档层次结构的新方法。

Method: 通过层次语义解析构建文档树(DocTree)，采用树状MapReduce方法进行自底向上的递归推理：Map步骤在子节点生成推理依据，Reduce步骤在父节点聚合推理结果以解决冲突或达成共识。

Result: 在70B+大语言模型上的实验表明，ToM显著优于现有分治框架和检索增强生成方法，实现了更好的逻辑连贯性和长文本推理能力。

Conclusion: ToM框架通过利用文档的层次结构和树状MapReduce方法，有效解决了长文本推理中的逻辑连贯性问题，为长文本理解提供了新的解决方案。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过识别和修剪RAG外部语料库中的冗余知识，减少检索工作量并提高LLM内部知识的利用率，在保持性能的同时加速检索过程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM内部知识的扩展，外部语料库与LLM之间存在显著的知识冗余，这不仅增加了密集检索的工作量，反而损害了LLM能够自行回答问题的RAG性能。

Method: 提出Mastery-Score指标识别冗余知识进行修剪；使用Query Router和Noise-Tolerant Tuning避免不相关文档干扰，提高LLM对内部知识的利用。

Result: 实验结果显示，Zero-RAG将维基百科语料库修剪了30%，检索阶段加速了22%，且不影响RAG性能。

Conclusion: Zero-RAG有效解决了RAG中的知识冗余问题，通过修剪冗余知识和优化内部知识利用，实现了检索效率提升而不牺牲性能。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 在尼泊尔农村地区开发离线对话AI医疗助手，使用轻量级DialoGPT模型在合成医疗对话数据集上进行微调，支持10种常见疾病的医患交流。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限地区（如尼泊尔农村）缺乏互联网连接和云基础设施的问题，为这些地区提供可离线的医疗对话助手支持。

Method: 对轻量级生成对话模型DialoGPT进行微调，使用包含10种尼泊尔农村常见疾病（感冒、季节性发热、腹泻等）的合成医患交互数据集。

Result: 尽管训练数据有限且领域特定，微调后的模型能够生成连贯、上下文相关且医学上适当的回应，表现出对症状、疾病背景和同理心沟通的理解。

Conclusion: 紧凑型离线对话模型具有很好的适应性，针对性数据集在低资源医疗环境中的领域适应效果显著，为未来农村医疗对话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了BERT、ALBERT、RoBERTa和DistilBERT等Transformer模型中的性别偏见问题，提出了新的偏见度量指标MALoR，并通过基于反事实数据增强的继续预训练方法有效降低了性别偏见。


<details>
  <summary>Details</summary>
Motivation: 编码器Transformer模型在各种语言任务中表现出色，但继承了训练数据中的强烈性别偏见，这引起了自然语言处理领域的关注。

Method: 引入MALoR指标量化偏见程度，并提出基于反事实数据增强的继续预训练方法，在性别平衡的数据集上进行训练。

Result: 实验显示偏见分数显著降低，如BERT-base中"he-she"偏见从1.27降至0.08，"his-her"从2.51降至0.36；BERT-large中"male-female"偏见从1.82降至0.10。

Conclusion: 该方法有效减少了性别偏见，且不影响模型在下游任务中的性能表现。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [23] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 提出WordSaladChopper组件，通过检测大型推理模型中的无用自我重复（word salad）来减少输出token成本，实现显著长度节省且质量损失最小


<details>
  <summary>Details</summary>
Motivation: 大型推理模型输出token成本高昂，其中大量是无用的自我重复token，这些token消耗解码预算但不增加价值

Method: 利用<

>标记的隐藏状态模式检测word salad行为，使用单层线性分类器实时检测，检测后通过简单截断和重新生成提示来节省长度

Result: WSC组件实现了显著的长度节省，同时质量损失最小，是一个轻量级、即插即用的解决方案

Conclusion: WSC或类似组件是所有考虑用户体验的大型推理模型应用的必要组件，因为word salad token缺乏语义价值且去除成本低

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [24] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出CISEA-MRFE框架，结合上下文指令、语义增强增强和多细化特征提取，在四个基准数据集上显著提升情感分析性能


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和预训练语言模型的情感分析方法在细微情感线索、领域转移和不平衡情感分布场景下表现不佳，主要由于语义基础不足、泛化能力差和偏向主导情感类别

Method: CISEA-MRFE框架包含：上下文指令(CI)注入领域感知指令指导情感消歧；语义增强增强(SEA)通过情感一致释义增强提高鲁棒性；多细化特征提取(MRFE)结合尺度自适应深度编码器(SADE)和多尺度特征专业化与情感评估上下文编码器(EECE)进行情感感知序列建模

Result: 在四个基准数据集上持续优于强基线：IMDb相对准确率提升4.6%，Yelp提升6.5%，Twitter提升30.3%，Amazon提升4.1%

Conclusion: CISEA-MRFE方法在跨领域情感分类中展现出有效性和泛化能力

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [25] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: ISA（意图转移攻击）通过最小化编辑将有害请求伪装成良性信息查询，显著提高大语言模型的越狱成功率，现有防御方法对此无效。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要通过引入额外上下文或对抗性令牌来分散LLMs注意力，但未改变核心有害意图。ISA旨在通过意图混淆让LLMs误判攻击意图。

Method: 建立意图转换分类法，利用最小编辑将原始请求改写成看似无害的信息查询请求，生成自然、人类可读的提示。

Result: 在开源和商业LLMs上，ISA相比直接有害提示实现了超过70%的攻击成功率提升。使用ISA模板对良性数据进行微调后，成功率接近100%。

Conclusion: ISA揭示了LLMs在意图推断方面的根本性安全挑战，现有防御方法不足，需要更有效的防御策略。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [26] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是EVA高效注意力机制的高效实现，通过微调Transformer模型使其适配FlashEVA注意力，在仅使用15亿token进行微调的情况下，在推理时实现了6.7倍吞吐量提升和5倍GPU内存降低，但在检索任务上存在局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现出色，但其内存需求（特别是需要维护完整上下文）给推理带来了显著挑战。

Method: 提出了FlashEVA作为EVA高效注意力机制的高效实现，并展示了如何微调Transformer模型以适应FlashEVA注意力机制。

Result: FlashEVA在推理时实现了高达6.7倍的吞吐量提升和5倍的GPU峰值内存使用降低，同时通过可调节超参数提供了吞吐量与准确性之间的权衡控制。

Conclusion: 这项工作代表了向更高效和适应性更强的基于Transformer的推理模型迈出的重要一步，尽管在检索任务上存在局限性。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [27] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自学习推理框架，通过让LLM交替扮演教师和学生角色来生成和解决新颖问题，实现了开放式的数学发现能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM推理方法依赖带注释的数据集进行可验证奖励，这可能限制模型超越人类水平的能力。自学习提供了一种有前景的替代方案，但现有方法需要外部验证器或无法进行开放式学习。

Method: OpenSIR采用自学习框架，LLM交替扮演教师和学生角色：教师生成既困难又多样的问题，学生解决问题。通过优化难度和多样性来奖励挑战适当且探索不同概念的问题，实现开放式数学发现。

Result: 从单个简单种子问题开始，OpenSIR显著提升了指令模型性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提升到78.3，在College Math上从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升到58.7。

Conclusion: OpenSIR通过协同进化的教师-学生角色实现了开放式学习，这些角色自适应地校准难度并推动多样化探索，能够自主地从基础数学进展到高级数学。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [28] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2通过离散扩散作为非自回归草稿生成器解决推测解码中的两个关键瓶颈：自回归依赖和草稿令牌频繁拒绝问题，实现了55%的token每秒提升和5.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法受限于自回归依赖导致的并行性限制，以及草稿模型与验证模型不对齐导致的频繁令牌拒绝问题。

Method: 使用离散扩散作为非自回归草稿生成器，并开发新技术校准离散扩散草稿器与自回归验证器。

Result: 在推理、编码和数学基准测试中达到新SOTA，token每秒提升平均55%，相比标准解码获得5.5倍平均加速，且无精度损失。

Conclusion: SpecDiff-2有效解决了推测解码的两个关键瓶颈，显著加速LLM推理，为推测解码提供了新的发展方向。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [29] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 本文研究大型语言模型在概率场景中的不确定性量化问题，发现虽然模型在概率任务中能给出准确回答，但其token级别的概率分布与理论概率分布存在偏差。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型在决策支持等关键应用中可靠的不确定性量化，特别是在概率场景下需要模型输出概率与理论概率保持一致。

Method: 使用GPT-4.1和DeepSeek-Chat模型，评估它们对10个概率提示（如掷骰子）的响应，测量响应有效性和token级输出概率与理论概率的对齐度。

Result: 两个模型在所有提示场景中都实现了完美的领域内响应准确性，但其token级概率和熵值始终偏离相应的理论分布。

Conclusion: 当前基于token logits的不确定性量化方法在概率场景下存在不足，需要开发更可靠的方法来确保模型输出概率与理论概率的一致性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [30] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析方法研究法国侦探小说中侦探原型的演变，发现监督模型能够捕捉150年间侦探原型的统一性，并展示其从次要叙事角色到核心推理机器的演变过程。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型在150年间的演变轨迹，理解文学类型中人物角色的发展规律。

Method: 使用定量方法和字符级嵌入技术，通过监督模型分析从1866年M. Lecoq到2017年Commissaire Adamsberg的法国侦探小说。

Result: 模型成功捕捉到侦探原型的统一性，发现侦探角色从次要叙事功能发展为古典侦探故事的核心推理机器，二战后受硬汉派影响变得更加复杂，处理社会暴力和道德模糊性。

Conclusion: 法国侦探小说中的侦探原型经历了从功能性配角到复杂主角的演变，反映了文学类型与社会背景的互动关系。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [31] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个新的多语言问答基准，涵盖9个国家的地理、文化和历史问题，包含49,280个问题，使用7种语言，旨在评估多语言LLM的文化素养，发现模型在西方语言上表现更好但文化素养不一定更高，且跨语言知识迁移能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言问答基准虽然覆盖多种语言，但信息内容偏向西方中心主义，缺乏地区多样性，无法公平评估多语言模型对不同地理位置的现实信息理解能力。

Method: 构建XNationQA基准数据集，包含9个国家的地理、文化、历史问题，使用7种语言，共49,280个问题。评估8个标准多语言LLM，使用两种新的迁移指标进行分析。

Result: 发现模型在不同语言中获取文化特定事实的能力存在显著差异，模型在英语中对文化信息的了解往往优于该文化主导语言。模型在西方语言上表现更好，但这不意味着对西方国家的文化素养更高。开源模型在跨语言知识迁移方面能力非常有限。

Conclusion: 多语言LLM存在文化素养不平衡问题，英语优先倾向明显，跨语言知识迁移能力不足，需要改进模型设计以更好地处理多样化的文化信息。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [32] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 该论文首次系统评估了10种语言（涵盖高、中、低资源语言）在6个LLM上的越狱攻击和防御效果，发现攻击成功率和防御鲁棒性因语言而异，高资源语言在标准查询下更安全但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs经过安全对齐训练，但越狱攻击仍能绕过安全机制，而现有研究对跨语言泛化能力探索不足。

Method: 在HarmBench和AdvBench上评估两种越狱攻击类型（逻辑表达式基和对抗提示基），涵盖10种语言的6个LLM。

Result: 攻击成功率和防御鲁棒性因语言而异：高资源语言在标准查询下更安全，但对对抗性攻击更脆弱；简单防御有效但依赖语言和模型。

Conclusion: 需要开发语言感知和跨语言的安全基准测试来提升LLMs的安全性。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [33] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文系统分析了原生稀疏注意力(NSA)并提出改进方法，通过交替使用局部和全局注意力模式，结合潜在注意力机制，在减少KV缓存内存50%的同时提升了长文本理解能力。


<details>
  <summary>Details</summary>
Motivation: 改进原生稀疏注意力机制，解决长上下文建模中的依赖传播问题，同时降低内存消耗。

Method: 1. 在层间交替使用局部(滑动窗口)和全局(压缩、选择性)注意力模式；2. 滑动窗口分支采用多头潜在注意力(MLA)；3. 压缩和选择性分支采用分组头潜在注意力(GLA)。

Result: 在340M到1.3B参数的模型上实验表明，该方法在常识推理和长上下文理解任务中匹配或超越了全注意力和原生稀疏注意力的性能，同时将KV缓存内存减少50%。

Conclusion: 交替注意力模式和潜在注意力机制能有效提升长序列建模能力，在保持性能的同时显著降低内存需求。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [34] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair是一个对比学习框架，通过解耦的三重态和语言建模损失来消除LLM中的社会偏见，避免偏见样本与无偏见样本之间的负面耦合效应。


<details>
  <summary>Details</summary>
Motivation: 现有去偏见方法独立处理偏见和无偏见样本，忽略了它们之间的相互关联，导致改进一个群体时无意中损害另一个群体，使残余社会偏见持续存在。

Method: TriCon-Fair采用解耦损失函数，结合三重态对比学习和语言建模目标，为每个锚点分配明确的偏见负样本和无偏见正样本，解耦推拉动态。

Result: 实验结果显示TriCon-Fair在减少歧视性输出方面优于现有去偏见基线方法，同时保持强大的下游任务性能。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案，能有效消除LLM中的社会偏见。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [35] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出了一个评估LLM推理过程中知识基础的新框架，包含大规模知识库、基于知识的评估指标和轻量级评估器，能有效识别推理中的知识缺失或误用问题。


<details>
  <summary>Details</summary>
Motivation: 随着逐步推理成为LLM处理复杂任务的标准方法，需要验证LLM的推理是否准确基于知识，解决推理过程中的知识基础问题。

Method: 构建包含三个关键组件的框架：大规模原子知识库、基于知识的评估指标（衡量知识回忆和应用能力）、以及轻量级评估器LLM用于可靠且成本效益高的指标计算。

Result: 评估套件在识别缺失或误用知识元素方面表现出显著效果，为揭示LLM基本推理缺陷提供了关键见解。

Conclusion: 该知识基础评估框架不仅可用于评估，还可集成到偏好优化中，展示了知识基础评估的进一步应用潜力。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [36] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准上比现有检索模型提升3.61%。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法往往复制纯文本检索技术，在文档编码、训练目标和相似度计算方面存在局限性，需要专门针对多模态文档结构和视觉特性的方法。

Method: 使用基于OCR的预训练目标、自监督掩码对比学习目标和延迟交互评分机制，更好地适应多模态文档的结构和视觉特征。

Result: 在ViDoRe V2基准测试中获得3.61%的改进，对域外基准测试表现出更强的泛化能力。

Conclusion: ColMate成功弥合了多模态表示学习和文档检索之间的差距，为多模态文档检索提供了更有效的解决方案。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [37] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 评估大型语言模型在医疗诊断沟通中的表现，发现虽然能根据患者特征调整解释内容，但存在输出过于复杂和情感共情偏见的问题，导致可及性和支持不平等。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在支持临床医生进行诊断沟通方面的能力，特别是在生成患者可理解和共情的解释与指导方面的表现。

Method: 使用两个领先的LLMs在医疗诊断场景中进行测试，通过可读性指标评估可理解性，通过LLM-as-a-Judge评分与人工评估比较来评估共情能力。

Result: LLMs能够根据社会人口学变量和患者状况调整解释，但生成内容过于复杂，并显示出有偏见的情感共情，导致可及性和支持不平等。

Conclusion: 需要系统校准以确保公平的患者沟通，代码和数据已开源。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [38] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在七种印度语言中的文化推理能力，发现模型初始准确率与自我纠错能力呈负相关：表现最好的模型过度自信，而表现较差的模型反而更具自我意识。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在非英语语言中进行文化基础推理的能力，特别是在印度语言中的表现，这一领域目前研究较少。

Method: 创建多语言谜语数据集，评估5个LLM在7种印度语言中的表现，采用7种提示策略，分两阶段评估：谜语解答能力和自我评估能力。

Result: Gemini 2.5 Pro整体表现最佳，但少样本方法提升有限，准确率在不同语言间差异显著。关键发现：模型初始准确率与识别自身错误的能力呈负相关。

Conclusion: 多语言推理存在明显差距，需要开发既能有效推理又能识别自身局限性的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [39] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种易到难的增强框架来解决机器生成文本检测中的边界模糊问题，通过使用针对较长文本的简单监督器来增强目标检测器，在各种实际场景中显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签是"黄金标准"，但实际存在边界模糊问题，导致传统训练范式不精确。由于人类认知局限和检测器超智能，不精确学习普遍存在且不可避免。

Method: 提出易到难增强框架：使用针对较长文本检测任务的简单监督器（尽管能力较弱）来增强更具挑战性的目标检测器。通过将检测器结构性地整合到监督器中，理论上将监督器建模为检测器的性能下界。

Result: 在跨LLM、跨领域、混合文本和改写攻击等多种实际场景的广泛实验中，该框架展现出显著的检测效果提升。

Conclusion: 该框架通过易到难的监督策略，在不精确标签条件下提供可靠监督，有效解决了机器生成文本检测中的边界模糊问题，实现了接近潜在"黄金"标签的性能。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [40] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个新颖的多智能体框架，通过任务分解和交互式强化学习解决复杂自然语言到SQL的转换问题，在BIRD和Spider数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 复杂自然语言查询到SQL的转换仍然困难，这些查询通常需要环境交互和自我修正能力。

Method: 采用三智能体框架：基础智能体进行模式链接，生成智能体通过多轮强化学习策略生成查询，验证智能体进行最终选择。核心是生成智能体的ReAct式思考-行动-观察循环。

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率，均为最先进水平。

Conclusion: 该结构化工作流程将专门化智能体流水线化，结合交互式强化学习进行生成和生成式建模进行验证，为稳健准确的SQL生成提供了高效方法。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [41] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了IF-CRITIC模型，通过约束清单生成和多阶段过滤机制，为指令跟随提供高效可靠的评估，在性能上超越现有LLM-as-a-Judge基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随评估模型存在成本高、评估不可靠等缺陷，需要开发更高效可靠的评估方法。

Method: 开发约束清单生成器分解指令，通过多阶段批判过滤机制收集高质量训练数据，采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC的评估性能超越Deepseek-R1和o4-mini等强基线模型，在指令跟随优化中能以更低计算开销实现显著性能提升。

Conclusion: IF-CRITIC能够为指令跟随提供可扩展的奖励信号，在较低计算开销下实现大语言模型指令跟随能力的显著优化。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [42] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小规模LLM与大规模LLM协作，通过多轮提示交互解决复杂问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型面临用户无法提供准确有效提示的挑战，限制了模型在复杂问题上的表现。

Method: 采用强化学习框架，小规模LLM负责思考和生成提示，大规模LLM进行复杂推理，设计了双重约束奖励机制优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1提供了一个即插即用的协作框架，能够有效提升LLM在复杂任务中的表现。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [43] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个结合开源大语言模型与NOAA实时海洋数据的对话平台，通过API调用生成基于权威数据的可验证回答和可视化，避免AI幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决通用对话AI系统在科学领域产生未经验证的"幻觉"问题，确保海洋科学数据的准确性和可验证性。

Method: 将开源大语言模型的自然语言流畅性与NOAA实时参数化海洋数据流集成，通过API调用识别、解析和合成相关数据集。

Result: 在与三种广泛使用的AI聊天产品盲测比较中，只有OceanAI能够提供NOAA来源的数值和原始数据引用，其他产品要么拒绝回答要么提供无支持的结果。

Conclusion: OceanAI通过基于可验证观测的输出，提高了透明度、可重复性和可信度，为海洋领域的AI决策支持提供了可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [44] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个对话式系统，通过自然语言回答空气质量、气象和政策项目相关问题，提供可执行的Python代码和交互式可视化，使环境数据分析对决策者、研究人员和公民更加易用。


<details>
  <summary>Details</summary>
Motivation: 印度每年因空气污染导致约160万人过早死亡，但决策者难以将分散的数据转化为决策。现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。

Method: 开发VayuChat对话系统，整合中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，通过大语言模型提供统一接口。

Result: 系统能够通过简单对话执行复杂的环境分析，使数据科学对非专业人士更加可及。平台已在Hugging Face上公开部署。

Conclusion: VayuChat通过对话式界面成功降低了环境数据分析的门槛，使政策制定者、研究人员和公民能够更轻松地获取和利用空气质量数据。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [45] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 该研究创建了一个基于临床指南的标准化数据集，用于评估大型语言模型在医疗领域的临床推理能力，并测试了多个流行LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏评估基于指南的临床推理的标准基准，而LLM在医疗领域的应用日益增多，需要系统性的评估框架。

Method: 利用GPT辅助创建基于公开指南的数据集，包含真实患者场景和临床问题，并对多个流行LLM进行基准测试。

Result: 开发了验证数据集并展示了其有效性，为系统评估LLM的临床实用性和指南依从性提供了框架。

Conclusion: 该研究填补了医疗LLM评估的空白，为标准化评估临床推理能力提供了有效工具。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [46] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该项目创建了一个包含约200种语言、30万亿token的多语言LLM预训练数据集，是目前最大的公开多语言数据集合，并提供了完整的数据处理流程和评估基准。


<details>
  <summary>Details</summary>
Motivation: 为多语言大语言模型提供高质量、大规模、丰富标注的预训练数据，解决当前多语言数据资源不足的问题。

Method: 从不同来源的网络爬虫数据出发，通过完整的开源流程进行文档选择、HTML文本提取、语言识别、去重、标注（包括文本质量评估、个人信息识别等）和最终筛选。

Result: 构建了30万亿token的多语言数据集，训练了57个单语言编码器-解码器模型和GPT类参考模型，并提供了自动挖掘的平行文本和机器翻译合成的平行语料库。

Conclusion: 该项目为多语言NLP研究提供了宝贵的数据资源和评估框架，推动了多语言大语言模型的发展。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [47] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文研究罗马尼亚语预训练语料库的特征和覆盖范围，通过与英语数据对比，使用轻量级多任务模型分析并进行多级过滤，生成高质量预训练数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中高质量数据至关重要，特别是对于罗马尼亚语等资源稀缺语言，需要研究其语料库特征并改进数据质量。

Method: 训练轻量级多任务模型，使用LLM标注的罗马尼亚文本进行多级过滤（如教育价值、主题、格式），生成高质量预训练数据集。

Result: 实验显示罗马尼亚语和英语数据在主题分布上存在显著差异，通过数据过滤有效提升了LLM在多个基准测试中的预训练性能。

Conclusion: 数据过滤方法能显著提升罗马尼亚语预训练语料质量，为资源稀缺语言的高质量数据构建提供了有效途径。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [48] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个专注于时间序列证据下时序和数值推理的事实核查基准数据集，包含287个真实世界声明和400个时间序列数据，通过LLM辅助的多步骤标注过程实现高质量标注。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在结构化证据、判决理由充分性和合成声明方面存在局限，需要开发更高质量的时间序列事实核查基准。

Method: 采用LLM辅助的多步骤标注过程，从38个事实核查机构收集287个真实声明，构建包含400个时间序列的数据库，为每个声明标注时间框架、判决和证据使用理由。

Result: 实现了kappa=0.745的标注者间一致性，基线模型在判决准确性上仅达到63.37%，证据到理由的得分仅为48.63%，表明当前最先进模型在时间序列推理方面仍面临挑战。

Conclusion: TSVer填补了时间序列事实核查基准的空白，揭示了现有模型在时序和数值推理方面的局限性，为未来研究提供了重要基准。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [49] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 提出了MicroRemed基准和ThinkRemed多智能体框架，用于评估LLM在微服务故障修复中的端到端性能，无需人工编写提示。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖SRE人工编写提示，LLM仅将文本指令转换为可执行代码，未能充分发挥LLM在自主决策和系统级操作中的潜力。

Method: 引入MicroRemed基准评估LLM从诊断报告直接生成可执行Ansible剧本的能力；提出ThinkRemed多智能体框架，模拟SRE的反思和感知推理过程。

Result: MicroRemed对当前LLM构成显著挑战，而ThinkRemed通过迭代推理和系统反思提高了端到端修复性能。

Conclusion: 该研究为LLM在微服务修复领域的应用提供了首个基准和有效框架，展示了多智能体推理在复杂系统操作中的优势。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [50] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 开发了一个基于语言模型的停止代理，用于优化销售对话中的动态筛选决策，在保持销售业绩的同时大幅减少失败通话时间


<details>
  <summary>Details</summary>
Motivation: 销售人员在对话中需要动态决定是继续交谈还是放弃转向下一个潜在客户，但目前对这些决策的效率和改进方法了解甚少

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的顺序决策代理，通过模仿回顾性推断的最优停止策略来学习何时退出对话

Result: 应用于欧洲大型电信公司的通话数据，停止代理将失败通话时间减少54%，同时几乎保持所有销售额；重新分配节省的时间可使预期销售额增加37%

Conclusion: 销售人员倾向于过度关注少数明显的消费者不感兴趣表达，错误预测通话失败风险，表明其实时对话决策存在认知局限；AI算法有潜力纠正这些认知局限并提高销售团队效率

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [51] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [52] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: ZoFia是一个新颖的两阶段零样本假新闻检测框架，通过分层显著性量化实体重要性，使用SC-MMR算法选择关键词检索外部证据，并采用多LLM交互系统进行多视图协作分析和对抗性辩论。


<details>
  <summary>Details</summary>
Motivation: 假新闻的快速传播威胁社会稳定和公众信任，现有LLM存在知识覆盖时间限制和产生幻觉内容的问题，且在快速演变的新闻流中缺乏可靠性，现有静态数据集训练的模型也缺乏对新兴新闻主题的泛化能力。

Method: 1. 引入分层显著性量化新闻内容中实体的重要性；2. 提出SC-MMR算法选择信息丰富且多样化的关键词作为查询检索最新外部证据；3. 构建多LLM交互系统，每个代理承担不同角色，对新闻文本及相关信息进行多视图协作分析和对抗性辩论。

Result: 在两个公共数据集上的综合实验表明，ZoFia明显优于现有的零样本基线方法和大多数少样本方法。

Conclusion: ZoFia框架能够有效解决假新闻检测中的时效性和泛化性问题，通过外部证据检索和多LLM协作分析提供可解释且鲁棒的判断，代码将开源以促进相关社区发展。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [53] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一个无需标签的测试时强化学习框架，通过使用单一模型同时作为求解器和重构器，利用原始问题和其改写版本之间的答案稳定性来构建可靠的学习信号，采用调和平均而非多数投票来聚合答案。


<details>
  <summary>Details</summary>
Motivation: 标准方法如多数投票容易陷入虚假但流行的答案，需要构建可靠的测试时学习信号来避免这种陷阱。

Method: 使用单一模型同时扮演求解器和重构器两个角色，基于原始问题和其改写版本的答案稳定性，采用调和平均来聚合答案频率。

Result: 在多样化推理基准测试中，Self-Harmony在无标签测试时设置下达到最先进结果，在30个设置中的28个排名第一，且在所有实验中零训练失败。

Conclusion: Self-Harmony展示了前所未有的鲁棒性和稳定性，无需人工监督或辅助模型即可有效避免虚假答案。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [54] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出DEER框架，通过解耦专家混合架构和强化学习路由机制，解决机器生成文本检测在领域转移下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测方法在领域转移时性能显著下降，需要同时捕获领域特定和领域通用的检测模式。

Method: 两阶段解耦专家混合架构：1) 领域特定专家学习细粒度领域内区分特征，共享专家提取跨领域可迁移特征；2) 强化学习路由机制动态选择专家，解决推理时领域标签缺失问题。

Result: 在5个领域内和5个领域外基准数据集上，DEER显著优于现有方法，领域内F1提升1.39%，领域外F1提升5.32%，准确率分别提升1.35%和3.61%。

Conclusion: 解耦专家专业化和自适应路由对模型性能至关重要，DEER框架能有效应对机器生成文本检测的领域转移挑战。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [55] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文研究了领域特异性对阿拉伯语金融文本摘要的影响，提出了最大的阿拉伯语金融新闻数据集AraFinNews，并评估了领域适应模型在金融摘要中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究领域特异性在阿拉伯语金融文本摘要中的作用，填补阿拉伯语金融摘要数据集的空白，评估领域适应对摘要质量的影响。

Method: 构建AraFinNews数据集（21.25万篇文章-标题对），使用基于transformer的模型（mT5、AraT5、FinAraT5）进行实验，评估领域预训练对摘要质量的影响。

Result: 领域适应模型生成更忠实和连贯的摘要，特别是在处理定量和实体中心信息方面表现更好。

Conclusion: 领域特定适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [56] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新的检索增强推测解码框架，通过自适应决策机制解决现有方法中不必要的检索问题，显著提升大语言模型推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在局限性：基于模型的方法准确但成本高，检索增强方法依赖启发式切换策略导致不必要的检索。需要一种更智能的自适应方法来平衡效率和准确性。

Method: 1) 基于熵的自适应触发器：量化上下文可预测性，仅在不确定性低时启动检索；2) 反馈驱动的候选选择：利用历史反馈组织高质量候选进行并行验证；3) 源感知的宽松验证策略：对模型生成草稿严格检查，对检索草稿宽松验证。

Result: 在Spec-Bench上的实验表明，ReSpec实现了最先进的加速效果，分别比EAGLE-2和SAM-Decoding快33%和25%以上，同时保持输出质量。

Conclusion: ReSpec通过将启发式草稿切换转化为自适应决策，在保持输出质量的同时显著提升了推测解码的效率，为解决大语言模型推理加速提供了有效方案。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [57] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了AI辅助论文评审中的提示注入攻击威胁，提出了静态和迭代两种攻击方法，并探索了检测防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的广泛应用，出现了通过隐藏注入提示操纵AI评审者给出过高评价的安全威胁，需要对此进行系统性研究。

Method: 提出了两类攻击方法：静态攻击使用固定注入提示，迭代攻击通过模拟评审模型优化提示以最大化攻击效果；同时探索了基于检测的防御机制。

Result: 两种攻击方法都能显著提升评审分数，经常获得满分评价；检测防御能大幅降低攻击成功率，但自适应攻击者仍能部分规避防御。

Conclusion: AI辅助同行评审面临严重的提示注入威胁，需要更多关注和严格的安全保障措施。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [58] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 该论文提出了FirstAidQA数据集，包含5,500个高质量的一级急救和应急响应问答对，旨在支持在低连接环境下部署轻量级语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型计算密集，不适合急救人员使用的低端设备，且缺乏专门针对急救领域的高质量数据集，阻碍了轻量级解决方案的开发。

Method: 使用ChatGPT-4o-mini通过提示式上下文学习生成数据集，基于《Vital First Aid Book (2019)》文本，经过文本清理、上下文分块、过滤等预处理步骤，并进行人工验证以确保准确性、安全性和实用性。

Result: 创建了包含5,500个高质量问答对的FirstAidQA数据集，涵盖广泛的急救场景，支持LLM和SLM的指令调优和微调。

Conclusion: FirstAidQA数据集填补了急救领域数据集的空白，可推动在安全关键和资源受限环境中AI应用的研究，数据集已在Hugging Face平台公开。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [59] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强的RAG系统，专门用于5G标准文档问答，通过结构化和时间推理来解决跨引用和规范演化问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架依赖语义相似性，无法可靠解决5G标准中的跨引用问题或处理规范演化，需要专门针对5G标准的结构和时间特性进行建模。

Method: 构建三个元数据丰富的数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档），通过元数据查找递归检索引用条款，并通过挖掘变更和链接到记录设计原理的变更请求来追踪规范演化。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验确认显式跨引用解析和演化感知检索显著提高了答案质量。

Conclusion: 建模5G标准的结构和时间特性对于提高问答质量具有重要价值，显式的跨引用解析和演化感知检索是关键技术。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [60] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: DeepAmbigQA是一个新的开放域问答数据集，专门评估模型处理名称歧义和多步推理的能力，现有最先进模型在该数据集上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少同时评估名称歧义和多步推理两个挑战，而实际复杂问题往往需要同时解决这两个问题。

Method: 开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和链接知识图谱构建问答任务，系统性地嵌入名称歧义和多步推理。

Result: 构建了包含3600个问题的DeepAmbigQA数据集，其中一半需要显式名称消歧。GPT-5在歧义问题上准确率仅0.13，非歧义问题为0.21。

Conclusion: 当前问答系统在信息收集和答案完整性方面仍有不足，需要开发更鲁棒的问答系统。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [61] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展了DistilQwen模型系列，推出了四个专为工业需求设计的模型系列：慢思考模型、自适应思考模型和蒸馏奖励模型，在保持高推理效率的同时提供强大的推理性能。


<details>
  <summary>Details</summary>
Motivation: 为满足现实应用对小型高效推理模型的需求，平衡推理性能与推理速度，开发专门针对工业要求的知识蒸馏技术。

Method: 基于Qwen模型初始化，通过知识蒸馏技术开发四个模型系列：慢思考模型（高精度推理）、自适应思考模型（动态调整推理策略）、蒸馏奖励模型（支持强化学习）。

Result: 在多个基准测试中展现出高推理效率和强推理性能，蒸馏奖励模型具有实际应用价值，支持在阿里云PAI平台上的可扩展训练和推理。

Conclusion: 该模型系列成功满足了工业应用需求，在推理效率和性能之间取得了良好平衡，为行业实践者提供了实用的解决方案。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [62] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了MiniTruePrefixes模型，专门用于检测文本前缀中的事实不一致性，并将其集成到受控解码框架中，显著提高了抽象摘要的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然NLI模型被用于提高LLM输出的事实性，但传统的NLI模型是针对完整句子设计的，而自回归生成是在文本前缀级别进行决策的，因此需要专门针对文本前缀的蕴含检测模型。

Method: 将蕴含检测任务推广到任意文本前缀，提供相应的评估和训练数据集，训练专门的MiniTruePrefixes模型来检测文本前缀中的事实不一致性。

Result: MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型高出5-14个F1分数；集成到受控解码框架后，LLaMA-3.2-3B-Instruct在事实一致性和运行时方面与同系列的8B模型相当，但只使用一半内存。

Conclusion: 专门针对文本前缀的蕴含检测模型能有效提高生成文本的事实一致性，并在资源效率方面具有优势。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [63] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 构建Cancer-Myth-Indic基准，用于评估大型语言模型在印度语言中对癌症相关错误预设问题的回答准确性，填补多语言医疗LLM评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准几乎都是英文的，导致多语言LLM评估存在显著差距，特别是针对越来越多人使用LLM获取医疗建议的情况。

Method: 通过翻译Cancer-Myth的500个项目子集到5种印度语言，由母语译者遵循风格指南进行翻译，保留隐含预设，测试LLM在预设压力下的表现。

Result: 创建了包含2500个翻译项目的多语言基准，涵盖5种使用广泛但服务不足的印度语言。

Conclusion: Cancer-Myth-Indic基准有助于评估LLM在多语言医疗咨询中的准确性和有效性，填补了现有文献的空白。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [64] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型和推理模型基准测试的有效性问题，分析了模型性能提升是否真正反映推理能力，还是仅仅在追踪与声称能力脱节的数字。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和LRMs的快速发展，基准测试结果迅速饱和，需要不断更新更难的测试集。但作者质疑超越基准是否真正证明推理能力，还是只是追踪与声称能力脱节的数字。

Method: 调查了OpenAI、Anthropic和Google三个模型家族，分析它们在不同基准测试中推理能力随时间的演变，以及不同推理任务的性能趋势。

Result: 提供了基准测试和推理任务的全面概述，揭示了当前基准测试的现状和剩余挑战。

Conclusion: 本文旨在为未来推理评估和模型开发研究提供基础参考，强调需要更有效的基准测试方法来真正衡量模型的推理能力。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [65] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨语言形态特征对分词和语言建模的影响，指出先前研究存在混杂因素导致结论冲突，重新评估了关于黏着语和融合语建模难度的三个假设，并提出了无需专家标注的token二元组指标来预测语言建模难度。


<details>
  <summary>Details</summary>
Motivation: 现有研究关于形态特征对语言建模影响的结论相互矛盾，作者认为这是由于实验设置中的混杂因素导致的，需要重新评估这些结论并找到更可靠的分析方法。

Method: 识别先前分析中的混杂因素，重新评估Arnett & Bergen (2025)提出的三个假设（分词形态对齐、分词效率、数据集大小），引入token二元组指标作为预测语言建模难度的内在方法。

Result: 发现先前每个结论都包含混杂因素，提出的token二元组指标是形态复杂度的梯度代理指标，无需专家标注即可有效预测因果语言建模的难度。

Conclusion: 需要建立更可靠的分析框架来回答形态学如何影响语言建模的问题，token二元组指标为此提供了实用的工具。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [66] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过遗传算法在46,080个可行管道配置中进行端到端架构搜索，优化检索增强生成(RAG)系统的整体性能。


<details>
  <summary>Details</summary>
Motivation: RAG质量受多个相互影响的模块选择影响，孤立优化模块很脆弱，需要端到端的整体优化方法。

Method: 使用遗传搜索算法，在9个技术家族和46,080个可行配置中优化综合目标函数，结合检索指标(召回率、mAP、nDCG、MRR)和生成指标(LLM-Judge、语义相似度)。

Result: 在6个维基百科领域上，RAGSmith找到的配置平均比朴素RAG基线提升3.8%(范围1.2%-6.9%)，检索和生成分别提升达12.5%和7.5%。

Conclusion: 研究提供了实用的领域感知RAG系统构建指导，证明了进化搜索在全管道优化中的有效性，发现向量检索加后生成反思/修订的稳健骨干架构。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [67] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动化构建检索依赖基准的流水线，通过比较Wikidata快照差异生成基于最新知识的问题，评估LLM在动态知识环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM问答评估依赖静态基准，奖励记忆而非检索，无法反映世界知识的动态特性。需要能捕捉最新知识需求的评估方法。

Method: 计算Wikidata连续快照的差异，筛选高质量三元组，生成三个难度级别的自然语言问题，通过SPARQL验证确保答案唯一性。

Result: 实验显示模型在面对预训练后新事实时性能显著下降，多跳查询差距最大。检索增强方法和更大指令调优模型只能部分缓解但无法消除时效性差距。

Conclusion: LiveSearchBench将评估从静态记忆转向需要最新检索和推理的任务，为系统化长期评估LLM在演化知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [68] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 提出一种可重复的草稿精炼流程，使用开源LLM在拉丁语翻译任务上达到与顶级专有系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决形态丰富、资源匮乏语言（如拉丁语）的翻译挑战，提升开源模型性能以媲美专有系统。

Method: 使用微调的NLLB-1.3B生成高质量草稿，再用零样本LLM（Llama-3.3或Qwen3）进行精炼，可结合RAG增强上下文。

Result: 在两个基准测试中（标准域内和新的域外12世纪拉丁信件），开源RAG系统性能与GPT-5基线统计相当。

Conclusion: 无需任务特定LLM微调，开源系统即可达到顶级专有系统性能，并发布了完整流程、数据集和评估工具。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [69] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出了BARD框架，通过预算控制信号实现推理能力和推理长度的细粒度控制，在保持性能的同时提高计算效率


<details>
  <summary>Details</summary>
Motivation: 解决长链思维蒸馏中推理过程冗余和计算预算不可控的问题，提高资源使用效率

Method: 采用两阶段训练：第一阶段在教师生成的长CoT数据上进行监督微调，第二阶段使用强化学习同时优化推理性能和预算保真度

Result: 8B学生模型在AIME24、AIME25、GPQA等推理基准上表现优异，并能跨广泛预算范围精确自适应控制推理长度

Conclusion: BARD框架成功实现了推理能力和计算效率的平衡，为资源受限环境下的推理蒸馏提供了有效解决方案

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [70] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文探讨使用大型语言模型作为认知扭曲检测的标注器，提出多轮独立LLM标注可揭示稳定模式，并引入数据集无关的评估框架。


<details>
  <summary>Details</summary>
Motivation: 文本自动认知扭曲检测具有主观性，人类标注者间一致性低导致不可靠标注，需要寻找更一致的标注方法。

Method: 使用LLM作为标注器进行多轮独立标注，引入基于Cohen's kappa的数据集无关评估框架进行公平比较。

Result: GPT-4能产生一致性标注（Fleiss's Kappa=0.78），基于这些标注训练的模型在测试集上表现优于基于人类标注训练的模型。

Conclusion: LLM可为主观NLP任务提供可扩展且内部一致的训练数据生成方案，支持良好的下游性能。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [71] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究合成数据来源多样性对微调大语言模型的影响，重点关注分布崩溃、对抗鲁棒性和自偏好偏差三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要。

Method: 通过分析三种关键维度：分布崩溃、对抗鲁棒性和自偏好偏差，研究不同来源合成数据对微调模型的影响。

Result: 使用多来源合成数据微调可以缓解分布崩溃，保持输出分布广度；合成数据微调在移除安全防护时能保持更高输出质量；微调能减少自偏好偏差，人类数据效果最佳。

Conclusion: 合成数据来源多样性对模型行为有重要影响，多来源合成数据在缓解分布崩溃方面表现良好，但需注意合成数据可能产生更危险的输出。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [72] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 提出了一种结合帕累托优化大语言模型和思维链提示的孟加拉语文本去毒新方法，并构建了包含68,041个句子的BanglaNirTox数据集。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语在线环境中的有毒语言普遍存在，但缺乏有效的预防措施，且由于资源有限，孟加拉语文本去毒研究相对不足。

Method: 使用帕累托优化的大语言模型和思维链提示生成去毒句子，构建包含毒性标签、推理和去毒改写的人工生成平行语料库。

Result: 帕累托优化的大语言模型结合思维链提示显著提高了孟加拉语文本去毒的质量和一致性。

Conclusion: 该方法有效解决了孟加拉语文本去毒的资源限制问题，为低资源语言的文本净化提供了可行方案。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [73] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种可控难度的干扰项生成框架，通过数据增强和多任务学习策略，能够生成不同难度级别的高质量干扰项。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多项选择完形填空的干扰项时缺乏适应性和难度控制能力，且缺乏难度标注数据集阻碍了进展。

Method: 采用两阶段方法：1）通过双向干扰项生成创建难度标注数据集；2）利用多任务学习训练可控难度生成模型，包含辅助任务增强语义理解和难度估计能力。

Result: 实验结果表明，该方法能生成各难度级别的高质量干扰项，在干扰项难度与人类感知对齐方面显著优于GPT-4o。

Conclusion: 该框架成功解决了干扰项生成的难度控制问题，为语言能力评估提供了更有效的工具。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [74] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究使用行为形式心智网络框架，探索心理学本科生对数学和焦虑相关概念的认知差异，并将人类学生与GPT模拟学生进行比较。研究发现人类学生的积极情绪评分和网络度能预测数学焦虑，但这些模型不适用于GPT数据。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑严重影响心理学专业学生的职业选择和福祉，需要深入了解学生对数学相关概念的认知结构和情感感知差异。

Method: 采用行为形式心智网络框架，对4组实验（2组人类学生样本和2组GPT模拟学生样本）进行个体层面和群体层面的网络特征分析，预测数学焦虑量表得分。

Result: 人类学生中，对"焦虑"的积极情绪评分和较高网络度，以及对"数学"的消极评分，能预测更高的总体和评估性数学焦虑。这些模型在GPT数据中不适用，因为模拟网络和心理测量得分与人类存在差异。

Conclusion: 研究强调了理解概念感知和关联在管理学生数学焦虑中的重要性，高数学焦虑学生对"焦虑"的情感极化框架与低焦虑学生不同，"科学"被积极评价但与"数学"的消极感知形成对比。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: ECO解码方法通过基于熵的动态控制强度调整，在保持流畅性的同时提升对话生成的可控性，优于现有解码方法。


<details>
  <summary>Details</summary>
Motivation: 传统加权解码方法使用固定常数控制属性概率偏差，难以同时满足可控性和流畅性需求。

Method: 提出ECO解码方法，根据语言模型和属性分类器概率分布的熵值，在每个生成步骤动态调整控制强度。

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在保持流畅性和语法性的同时，持续提升可控性，优于现有解码方法。

Conclusion: ECO解码解决了多属性生成中的概率插值问题，在单属性和多属性场景下都表现出色。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: BIRD数据集为青铜器铭文修复和断代提供标准化标注，提出异体字感知的掩码语言建模框架，通过字形网络连接字素和异体字，提升修复和断代效果。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜器铭文存在碎片化和断代困难的问题，需要系统性的数据集和建模方法来解决这一挑战。

Method: 构建BIRD数据集，提出异体字感知的掩码语言建模框架，集成领域和任务自适应预训练，使用字形网络连接字素和异体字。

Result: 实验表明字形网络改进了铭文修复效果，字形偏置采样在断代任务中取得了增益。

Conclusion: BIRD数据集和异体字感知建模框架为青铜器铭文研究提供了有效的技术支撑，字形网络和偏置采样策略显著提升了修复和断代性能。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究通过分析西班牙语母语者的语言错误，探讨大型语言模型对语言错误的解释、复制和纠正能力，旨在开发更具认知意识的自然语言处理系统。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是语法偏差，更能揭示语言认知架构和人工智能系统的局限性。通过研究母语者的真实语言错误，可以推动更贴近人类语言处理方式的NLP系统发展。

Method: 整合理论语言学、神经语言学和自然语言处理三个视角，构建包含500+西班牙语母语者真实错误的语料库，并测试GPT、Gemini等AI模型对这些错误的解释准确性和泛化能力。

Result: 项目将评估AI模型对人类语言错误模式的识别和处理能力，为理解西班牙语作为母语的特征提供实证基础。

Conclusion: 该研究不仅有助于理解西班牙语母语特征，还将推动开发能够处理人类语言不完美、多变和模糊特性的认知启发式NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语四种斯拉夫语言的议会语音语料库，总计6000小时，通过自动方式从ParlaMint转录本构建，并丰富了多种自动注释层。


<details>
  <summary>Details</summary>
Motivation: 构建一个大规模、多语言的议会语音语料库，为跨学科研究提供丰富的语音和文本数据资源。

Method: 从ParlaMint转录本自动对齐到相应的议会录音，并添加多种自动注释层，包括语言注释、情感预测、填充停顿检测、词级对齐和重音位置标注。

Result: 成功构建了包含6000小时语音的四个斯拉夫语言语料库，显著增强了语料库的实用性，并通过情感声学相关性分析展示了其应用价值。

Conclusion: ParlaSpeech语料库通过丰富的自动注释大大提升了其研究价值，为多学科下游研究提供了重要资源，并以JSONL、TextGrid格式和检索工具形式开放使用。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探索了在基于图的检索增强生成架构中使用大语言模型进行能效问答，通过从能源领域文档自动提取知识图谱，并利用图谱导航和推理来提供多语言准确答案。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决能源效率领域的专业问答需求，通过结合知识图谱和LLMs来提高问答系统的准确性和多语言能力，以更好地服务于能源领域的指导性和监管性文档。

Method: 系统首先从能源领域的指导和监管文档中自动提取知识图谱，然后在图谱上进行导航和推理，结合检索增强生成架构和大语言模型来生成多语言答案。

Result: 验证结果显示系统在约75.2±2.7%的情况下能正确回答问题，在通用能效问题上的准确率更高（达81.0±4.1%），多语言能力表现良好（翻译导致的准确率损失仅为4.4%）。

Conclusion: 该基于图的RAG架构在能效问答中展现出良好潜力，特别是在通用问题和多语言场景下表现优异，同时也识别了系统的优缺点，为未来改进提供了方向。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 该研究提出了一个认知基准框架，用于评估大型语言模型处理和应用特定文化知识的能力。该框架将布鲁姆分类学与检索增强生成相结合，在六个层次认知领域评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在特定文化知识处理方面的能力，特别是在台湾客家数字文化档案中的应用表现。

Method: 整合布鲁姆分类学和检索增强生成技术，使用台湾客家数字文化档案作为主要测试平台，评估模型生成的语义准确性和文化相关性。

Result: 通过框架评估了LLM在六个认知领域的表现，包括记忆、理解、应用、分析、评估和创造能力。

Conclusion: 该认知基准框架为评估LLM的文化知识处理能力提供了系统方法，有助于理解模型在特定文化语境下的表现。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个用于评估大语言模型在工程领域复杂推理能力的基准测试，包含90个跨3个工程分支的问题，通过符号模板生成确保多样性和避免污染风险。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估语言理解、事实回忆、数学或代码生成能力，但缺乏对工程领域所需的整合性推理能力的评估，该能力需要融合科学原理、定量建模和实际约束。

Method: 使用符号模板生成高度随机化的问题，采用两阶段评估：首先定量验证每个推理步骤的数值和语义有效性，然后引入LLM-As-A-Judge系统对识别出的推理错误进行定性分类。

Result: 开发了EngChain基准测试，包含90个问题，涵盖3个工程分支、9个领域和20个不同领域，确保问题多样性和避免数据污染。

Conclusion: EngChain填补了现有基准测试在评估工程领域复杂推理能力方面的空白，通过两阶段评估方法提供了更全面的模型能力分析。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）以及英语和中文的大型音频语言模型，支持多模态输入和多任务处理，在音频理解任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对东南亚语言缺乏专门的音频语言模型，开发支持多语言、多模态的音频AI系统，促进该地区的研究和产业发展。

Method: 在大规模音频语料库上训练，支持音频、文本或两者结合的输入，涵盖音频分析、语音对话等多种任务。

Result: 在SeaBench-Audio基准测试中，相比其他LALM在东南亚语言上取得了有竞争力的性能表现。

Conclusion: SeaLLMs-Audio是推进东南亚音频LLM发展的重要一步，预计将惠及该地区的研究社区和产业界。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了首个开源的AI助手角色训练方法，通过宪法AI和合成内省数据来塑造助手角色，比系统提示约束或激活引导更有效，且不影响通用能力。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的AI助手角色会影响交互质量和价值观对齐，但角色训练在学术界尚未得到充分研究，需要更有效和可控的方法。

Method: 使用宪法AI和合成内省数据管道，对三个流行开源模型进行微调，采用11种示例角色（如幽默、关怀、恶意等），并通过揭示偏好分析来追踪效果。

Result: 该方法比系统提示约束和激活引导更抗对抗性提示，生成更连贯和真实的文本，且对通用基准测试能力影响极小。

Conclusion: 开源的角色训练方法能有效塑造AI助手角色，提高鲁棒性和生成质量，同时保持通用能力，为AI助手角色训练提供了实用解决方案。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的rank-2投影子空间方法，用于更准确地解构大语言模型中参数知识(PK)和上下文知识(CK)的贡献，并首次对长自然语言解释序列中的多步知识交互进行分析。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型决策时参数知识和上下文知识的交互对于评估自然语言解释的可靠性至关重要，但现有研究仅关注单步生成和二元交互，忽略了更丰富的知识交互形式。

Method: 采用rank-2投影子空间方法解构PK和CK贡献，在四个QA数据集和三个开源指令调优LLM上进行实验，分析多步知识交互模式。

Result: 实验表明rank-1子空间无法有效表示多样知识交互，而rank-2方法能有效捕捉；幻觉NLEs与PK方向强相关，上下文忠实NLEs平衡PK和CK，思维链提示能减少PK依赖并转向CK。

Conclusion: 本研究首次提供了通过更丰富的rank-2子空间解构来系统研究LLM中多步知识交互的框架，为理解模型决策机制提供了新视角。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 提出了一个基于Qwen3模型和LoRA适配器的多专家系统，用于创建能够进行自然对话和执行上下文动作的NPC角色。该系统在计算效率方面表现良好，在CPDC挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时进行自然对话和执行上下文动作的NPC角色，以满足交互式环境中的实际需求。

Method: 使用Qwen3作为基础模型，通过LoRA适配器实例化三个专家模块：工具调用、工具响应解释和直接对话。

Result: 系统在L40S GPU上实现了快速响应和适度的资源使用，在Commonsense Persona-Grounded Dialogue Challenge 2025中总体排名第二。

Conclusion: 基于多专家架构和LoRA适配器的系统能够有效创建具有对话和动作执行能力的NPC，在计算效率和性能方面表现良好。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在长时间对话和阅读过程中，其信念配置文件会显著改变，导致行为不一致和偏离原始对齐，存在潜在风险。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在自主应用中广泛使用，上下文积累可能导致模型信念无声改变，造成用户体验不一致和行为偏离原始对齐。

Method: 通过多轮讨论道德困境、政治议题以及设计工具使用任务来测量模型信念变化，分析对话和阅读对模型信念的影响。

Result: GPT-5在10轮道德讨论后信念改变54.7%，Grok 4在阅读对立立场文本后政治信念改变27.2%，行为变化与信念变化一致。

Conclusion: 语言模型在长时间对话和阅读过程中信念高度可塑，这种信念漂移会反映在代理系统的实际行为中，带来可靠性风险。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出一种无需模型重新训练即可实现精确长度控制的提示工程方法，通过结构化引导让LLM在文档摘要任务中更好地遵循长度约束


<details>
  <summary>Details</summary>
Motivation: 解决LLM中长度控制这一重要但未被充分解决的问题，现有方法需要昂贵的模型重新训练或复杂推理工具，而生产环境中这些往往不切实际

Method: 基于提示工程的结构化引导方法，在提示中实现精心规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度约束

Result: 在六个先进LLM上的评估显示，该方法显著提高了长度保真度，某些模型在长度遵循方面提升了37.6%，同时保持或提升了输出质量

Conclusion: 该方法为需要精确长度控制的应用提供了立即可部署的解决方案，特别适用于模型重新训练不切实际或成本过高的生产环境

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级变换编码器，通过PCA特征解相关、自适应量化和熵编码来压缩KV缓存，实现高达20倍压缩比，同时保持推理和长上下文准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务中，KV缓存占用大量GPU内存，跨对话轮次的可重用缓存会变得陈旧，导致内存浪费、需要卸载或重新计算。

Method: 结合经典媒体压缩技术，使用PCA特征解相关、自适应量化和熵编码，仅需简短初始校准且不改变模型参数。

Result: 在Llama 3、Mistral NeMo和R1-Qwen 2.5模型上测试，在AIME25、LiveCodeBench等基准测试中实现高达20倍压缩，特定用例可达40倍以上，优于令牌驱逐、量化和SVD基线方法。

Conclusion: KVTC是内存高效LLM服务的实用构建模块，支持可重用KV缓存。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一个针对国际数学奥林匹克竞赛水平的数学推理基准套件，包含答案基准和证明基准，帮助评估基础模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估要么太简单，要么只关注简短答案，缺乏针对高级数学推理能力的有效评估标准。

Method: 开发IMO-Bench基准套件，包含IMO-AnswerBench（400个多样化的奥林匹克问题）和IMO-Proof Bench（证明写作能力评估），并建立自动评分系统。

Result: Gemini Deep Think模型在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别比最佳非Gemini模型高出6.9%和42.4%。

Conclusion: IMO-Bench为社区提供了强大的数学推理评估工具，有助于推动基础模型在高级数学推理能力方面的发展。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具及其父代理，并利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，显著提升了多代理系统中的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法通常将查询与粗粒度的代理级描述进行匹配，这会掩盖细粒度的工具功能，导致代理选择不理想。

Method: 在共享向量空间中嵌入工具及其父代理，通过元数据关系连接它们，支持细粒度的工具级或代理级检索。

Result: 在LiveMCPBench基准测试中，使用8种嵌入模型，Tool-to-Agent Retrieval在Recall@5和nDCG@5上分别比现有最优代理检索方法提升了19.4%和17.7%。

Conclusion: Tool-to-Agent Retrieval通过显式表示工具能力并利用元数据关系，有效解决了多代理系统中检索的粒度问题，显著提升了检索性能。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [91] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 该论文提出了一个基于运动捕捉数据的交互式舞蹈生成模型，通过扩散模型、运动修复和运动风格转换技术，生成既模仿人类动作又具有创造性增强的舞蹈动作。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在创造性任务中表现出色，但缺乏具身交互能力。舞蹈作为人类表达的基本形式，可以弥补这一不足，探索创造性的人机交互体验。

Method: 结合两种扩散模型、运动修复和运动风格转换技术，利用单人运动数据和高层次特征生成时间连贯且对选定运动参考有响应的动作表示，不依赖低层次人-人交互数据。

Result: 通过定量评估生成样本特征分布与测试集的收敛性，证明模型成功生成多样化且逼真的舞蹈动作，在保持真实性的同时展现出与人类舞伴的各种偏离。

Conclusion: 该模型是创造性AI舞蹈的初步尝试，生成的舞蹈动作既多样化又真实，为人类与AI的创造性舞蹈互动奠定了基础。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [92] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 提出基于机器学习的珊瑚白化分类系统，使用CNN模型在多样化全球数据集上达到88%准确率，优于ResNet和ViT模型。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海洋酸化和海水温度异常等威胁，亟需高效的保护和监测方法。

Method: 使用包含不同环境条件下健康和白化珊瑚样本的全球数据集，比较ResNet、ViT和CNN三种先进模型，并进行全面的超参数调优。

Result: CNN模型表现最佳，达到88%的分类准确率，超越了现有基准。

Conclusion: 该研究为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [93] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 评估YOLOv8深度学习管道在肯尼亚和坦桑尼亚珊瑚礁视频样线中自动识别鱼类科级分类的性能，为西印度洋地区提供首个自动化珊瑚礁鱼类监测基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋珊瑚礁监测受限于水下视觉普查的劳动力需求，需要开发自动化方法来提高监测效率。

Method: 使用YOLOv8深度学习管道，在肯尼亚和坦桑尼亚收集的视频样线数据上测试24个鱼类科的识别性能。

Result: 最佳模型mAP@0.5达到0.52，对丰富鱼类科识别准确率高，但对稀有或复杂类群的检测较弱。

Conclusion: 深度学习可作为传统监测方法的可扩展补充工具，具有在珊瑚礁监测中应用的潜力。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [94] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 提出了一种基于互信息的数据选择方法，用于改进对比学习中的数据增强策略，通过在自然扰动下选择具有高互信息的场景补丁作为正样本。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在数据选择和增强方面仍依赖人工假设或工程化设计，可能不是最优的。作者希望利用真实世界分布计算的互信息来选择训练数据，以提高学习特征在开放环境中的泛化能力。

Method: 考虑在自然扰动（如颜色变化和运动）下表现出高互信息的场景补丁作为对比学习的正样本，提出互信息指导的数据增强方法。

Result: 在多个基准测试和最先进的表示学习框架上评估了该方法，证明了其有效性。

Conclusion: 该方法为未来研究提供了一个有前景的方向，表明基于互信息的数据选择可以改进表示学习性能。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [95] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本研究对三种联邦学习框架(NVIDIA FLARE、Flower、Owkin Substra)在医学影像应用中的性能进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗AI中具有重要价值，能够在机构间实现协作模型训练而无需直接共享数据。本研究旨在评估不同FL框架在真实医疗环境中的适用性。

Method: 使用PathMNIST数据集，对三种FL框架(NVIDIA FLARE、Flower、Owkin Substra)进行基准测试，评估多个关键指标。

Result: NVIDIA FLARE在生产可扩展性方面表现最佳，Flower在原型设计和学术研究方面更具灵活性，Owkin Substra在隐私和合规性方面表现出色。

Conclusion: 每种框架都有针对不同使用场景优化的优势，强调了它们在医疗环境实际部署中的相关性。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [96] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对水稻叶片图像进行了图像去噪和对比度增强方法的比较研究，结合CLAHE技术评估不同去噪方法的效果。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理中的重要预处理步骤，对于水稻叶片分析（如疾病检测、营养评估）至关重要。去噪和对比度增强是主要步骤，需要找到有效的方法组合。

Method: 采用多种知名图像去噪方法结合CLAHE（对比度受限自适应直方图均衡化）技术，在水稻叶片图像数据集上进行实验比较。

Result: 通过多种评估指标全面测试了增强方法的效果，为评估数字图像处理方法有效性提供了坚实基础。

Conclusion: 该研究为农业研究和其他领域的图像处理方法评估提供了有价值的见解，有助于未来方法的适应性改进。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [97] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 该论文研究了不同LiDAR扫描模式对路边感知系统性能的影响，通过CARLA仿真环境创建了InfraLiDARs' Benchmark数据集，发现非重复扫描LiDAR与128线重复扫描LiDAR在检测性能上相当，且非重复扫描LiDAR成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LiDAR的优化部署，但不同扫描模式（重复扫描vs非重复扫描）对感知性能的深层影响尚未得到充分研究，这对智能交通系统的性能至关重要。

Method: 在CARLA仿真环境中创建InfraLiDARs' Benchmark数据集，使用同时运行的基础设施LiDAR收集数据，涵盖两种扫描模式，并进行全面的统计分析，评估不同扫描模式对多种3D物体检测算法性能的影响。

Result: 研究发现非重复扫描LiDAR和128线重复扫描LiDAR在各种场景下表现出相当的检测性能。尽管非重复扫描LiDAR感知范围有限，但其成本效益更高。

Conclusion: 该研究为设置具有最优LiDAR扫描模式和兼容算法的路边感知系统提供了见解，并公开发布InfraLiDARs' Benchmark数据集以促进进一步研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [98] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5是Cosmos世界基础模型的最新版本，基于流式架构统一了文本、图像和视频到世界的生成，通过强化学习后训练显著提升了视频质量和指令对齐能力。


<details>
  <summary>Details</summary>
Motivation: 开发更可靠的合成数据生成、策略评估和闭环仿真工具，为机器人和自主系统提供更好的物理AI支持。

Method: 采用流式架构，结合Cosmos-Reason1物理AI视觉语言模型进行文本接地和世界仿真控制，在2亿个精选视频片段上训练，并通过强化学习进行后训练优化。

Result: 相比Cosmos-Predict1，在视频质量和指令对齐方面取得显著提升，同时Cosmos-Transfer2.5虽然比前代小3.5倍，但提供了更高保真度和鲁棒的长时视频生成。

Conclusion: Cosmos-Predict2.5和Cosmos-Transfer2.5成为扩展具身智能的多功能工具，通过开源代码、预训练模型和基准测试加速物理AI的研究和部署。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [99] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 该研究使用深度学习进行高山栖息地变化检测，比较了后分类变化检测和直接变化检测两种范式，评估了地理空间基础模型在复杂自然环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统面临快速气候变化，需要频繁的栖息地监测，但人工测绘成本过高。研究旨在填补地理空间基础模型在复杂自然环境应用中的空白。

Method: 使用后分类变化检测（评估Prithvi-EO-2.0、Clay v1.0和U-Net CNN）和直接变化检测（测试ChangeViT和U-Net基线），基于高分辨率多模态数据（RGB、近红外、LiDAR、地形属性）。

Result: Clay v1.0在多类栖息地变化检测中达到51%准确率，U-Net为41%；二元变化检测两者均为67%。直接变化检测在二元检测中IoU更高（0.53 vs 0.35），但多类检测准确率仅28%。集成LiDAR将语义分割准确率从30%提升至50%。

Conclusion: 虽然整体准确率低于同质景观，但反映了复杂高山栖息地的真实性能。未来工作将集成基于对象后处理和物理约束以增强适用性。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [100] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一个无需训练的高效扩散视频生成加速框架，通过词典最小最大路径优化策略显著提升推理速度和生成质量，在多个基准测试中实现2.9倍加速和0.05 LPIPS分数。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略主要关注减少局部启发式误差，但忽略了全局误差的累积，导致加速视频与原始视频之间存在明显的内容退化问题。

Method: 将缓存调度建模为带误差权重边的有向图，引入词典最小最大路径优化策略来明确限制最坏情况路径误差。

Result: 在多个文本到视频基准测试中，LeMiCa在推理速度和生成质量上实现双重提升，在Latte模型上达到2.9倍加速，在Open-Sora上获得0.05 LPIPS分数，优于现有缓存技术。

Conclusion: LeMiCa为加速扩散视频生成提供了一个鲁棒且可泛化的范式，能够以最小的感知质量损失实现显著性能提升，可作为未来高效可靠视频合成研究的坚实基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [101] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD是一个三阶段即插即用框架，通过残差强化学习和分布感知数据收集来改进视觉语言动作模型，无需依赖昂贵的人工演示，实现了在多个任务上的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 监督微调依赖昂贵的人工演示，限制了视觉语言动作模型的可扩展性和泛化能力，需要一种更高效的改进方法。

Method: 三阶段框架：1) 训练轻量级残差actor探测VLA通用模型的失败区域；2) 使用混合rollout方案收集与通用模型部署分布对齐的轨迹；3) 通过标准SFT将精选轨迹蒸馏回通用模型。

Result: 在LIBERO上达到99%任务成功率，在SimplerEnv上提升超过50%，在真实世界Franka和YAM机械臂操作任务上实现100%成功率。

Conclusion: 残差探测和分布感知回放是收集部署对齐数据的关键，为自改进VLA模型提供了可扩展的路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [102] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: SpinalSAM-R1是一个用于脊柱CT图像分割的多模态视觉语言交互系统，通过集成微调的SAM模型和DeepSeek-R1，结合解剖学引导的注意力机制和语义驱动的交互协议，实现了高效的分割性能。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像分割面临低对比度和复杂椎体边界的挑战，现有模型如SAM在脊柱CT成像中受限于高标注要求和差的领域适应性。

Method: 提出SpinalSAM-R1系统，集成微调的SAM与DeepSeek-R1，引入解剖学引导的注意力机制和语义驱动的交互协议，使用LoRA进行高效微调。

Result: 在脊柱解剖结构CT图像上验证，实现了优越的分割性能，开发了基于PyQt5的交互软件，支持点、框和文本提示，11种临床操作达到94.3%解析准确率和亚800毫秒响应时间。

Conclusion: SpinalSAM-R1系统有效解决了脊柱CT图像分割的挑战，提供了高效准确的解决方案，并发布了开源软件。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [103] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 提出了一种用于共聚焦激光内镜（CLE）视频序列的过滤功能，以减少自监督学习（SSL）训练中的数据冗余，提高训练效率和收敛性。


<details>
  <summary>Details</summary>
Motivation: CLE图像对非专业医生难以解读，机器学习可作为辅助工具，但面临数据稀缺和过拟合问题。SSL可用于大规模未标记数据，但CLE视频帧间相关性高导致数据分布不均匀。

Method: 使用四种最先进的基线网络和基于视觉变换器小骨干的SSL师生网络，在鼻窦肿瘤和皮肤鳞状细胞癌数据集上进行评估，并应用视频过滤功能减少冗余。

Result: 在过滤后的SSL预训练模型上获得最高测试准确率：鼻窦肿瘤数据集67.48%，皮肤鳞状细胞癌数据集73.52%，均显著优于非SSL基线。训练时间减少67%。

Conclusion: SSL是CLE预训练的有效方法，提出的CLE视频过滤功能可显著提高自监督学习场景下的训练效率。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [104] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders是一种无需训练、模态无关的方法，通过在推理过程中部分估计Concept Sliders公式，实现细粒度概念控制生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像、音频和视频生成方面表现出色，但实现细粒度可控生成仍然具有挑战性。现有的Concept Sliders方法需要针对每个概念进行训练和架构特定的微调，限制了扩展到新模态的能力。

Method: 提出FreeSliders方法，完全无需训练且模态无关，通过部分估计CS公式在推理过程中实现概念控制。还提出了两阶段程序来自动检测饱和点并重新参数化遍历，实现感知均匀、语义有意义的编辑。

Result: 扩展了CS基准测试以包含视频和音频，建立了首个多模态细粒度概念生成控制套件。提出了三个评估属性和新指标来改进评估质量。实验表明该方法能够实现即插即用、无需训练的概念控制，优于现有基线方法。

Conclusion: FreeSliders为可控生成提供了新的工具，实现了跨模态的无训练概念控制，并建立了多模态评估基准。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [105] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI是一个新颖的分层文本到视频生成框架，通过组合场景解析、时空注意力机制和渐进式视频优化，解决了现有方法在时间一致性、组合理解和细粒度控制方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在保持时间一致性、组合理解和视觉叙事细粒度控制方面存在困难，需要更先进的框架来提升生成质量。

Method: 提出三个关键创新：组合场景解析器将文本分解为带时间标注的层次场景图；时空注意力机制确保连贯运动动态并保留空间细节；渐进式视频优化模块通过多尺度时间推理迭代提升视频质量。

Result: 在标准基准测试中，MOVAI实现了最先进性能，LPIPS指标提升15.3%，FVD指标提升12.7%，用户偏好研究提升18.9%，特别擅长生成复杂多对象场景。

Conclusion: MOVAI框架在生成具有真实时间动态和细粒度语义控制的复杂多对象场景方面表现出色，为文本到视频生成提供了有效解决方案。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [106] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出了一种名为"时间链"的认知启发方法，通过在模拟过程中生成一系列中间图像来改进和解释视觉语言模型中的物理模拟，无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 受机器学习中的上下文推理和人类心理模拟启发，旨在提升视觉语言模型的物理模拟能力并理解其内部动态。

Method: 在推理时生成模拟过程中的中间图像序列，应用于2D图形模拟和3D自然视频，测试速度、加速度、流体动力学和动量守恒等物理属性。

Result: 时间链方法显著提升了最先进图像生成模型的性能，分析揭示了模型能够模拟随时间展开的物理属性（如速度、重力和碰撞），但也发现模型在某些情况下难以从输入图像推断特定物理参数。

Conclusion: 时间链方法不仅能改进物理模拟性能，还能提供对模型内部模拟动态的深入理解，揭示了传统评估方法无法发现的物理推理能力。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [107] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出了首个结合生成AI和深度强化学习的端到端框架，用于实现自主、可重复的心脏超声扫描，解决了传统方法的操作依赖性和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 心脏超声诊断存在操作者依赖性、时间限制和人为错误等问题，特别是在偏远地区缺乏专业医生。现有AI方法缺乏可重复性、依赖专有数据且使用简化模型。

Method: 框架包含两个组件：(1) 结合GAN和VAE的条件生成模拟器，产生真实的心脏超声图像；(2) 深度强化学习模块，利用模拟器学习自主扫描策略。

Result: VAE-GAN在定性和定量评估中优于现有GAN变体，深度强化学习扫描系统在不同配置下均表现有效，并发布了公开可用的心脏超声数据集。

Conclusion: 该框架提供了AI驱动的指导，支持真实超声图像的条件生成，建立了可扩展到其他器官的可重复基础，为解决心脏超声扫描的挑战提供了有效方案。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [108] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D提出了一种新颖的双流架构，利用RGB-D输入的视觉和几何数据优势，通过集成DINOv2视觉Transformer和PointNet++点云编码器，在严重遮挡和光照变化下实现鲁棒的6D物体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 当前6D物体姿态估计方法在从合成数据泛化到真实场景时存在困难，特别是在光照变化、无纹理物体和严重遮挡等挑战性条件下表现脆弱。

Method: 采用双流架构：使用自监督Vision Transformer（DINOv2）处理RGB模态，利用预训练的视觉理解能力；同时使用PointNet++编码器处理深度数据生成的3D点云。两个互补特征流融合后输入多任务预测头。

Result: 在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能，验证了其优越的鲁棒性和准确性。

Conclusion: VLM6D通过有效整合视觉和几何信息，在复杂真实场景下实现了鲁棒且精确的6D物体姿态估计。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [109] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出了一种结合ConvNeXt和Vision Transformer的混合架构，用于面部图像年龄估计，在多个基准数据集上取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 年龄估计是计算机视觉中的复杂挑战，需要同时利用CNN的局部特征提取能力和Transformer的全局注意力机制。

Method: 使用预训练模型，结合ConvNeXt和ViT的混合架构，采用线性层和高级正则化技术优化模型，并通过消融研究验证各组件的重要性。

Result: 在MORPH II、CACD和AFAD数据集上，该混合方法在平均绝对误差(MAE)指标上优于传统方法。

Conclusion: 混合架构展现了将CNN和Transformer无缝集成的潜力，为年龄估计和相关视觉任务提供了稳健的基础。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [110] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC是一个基于设施位置函数的高效视觉token压缩框架，通过选择紧凑且具有代表性的视觉token子集来解决长视频理解中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中的大型多模态模型面临视觉token数量过多导致可扩展性受限的问题，需要一种高效的token压缩方法。

Method: 使用设施位置函数和惰性贪婪算法，在预定义预算内快速选择具有代表性和多样性的视觉token子集。

Result: 在Video-MME、MLVU和LongVideoBench等大规模基准测试中，FLoC始终优于现有压缩技术，显著减少视觉token数量同时保持接近最优性能。

Conclusion: FLoC提供了一个无需训练、模型无关且查询无关的通用解决方案，能有效解决长视频理解中的关键挑战，并具有高效的处理速度。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [111] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种增强图像保护方法鲁棒性的简单技术，通过在噪声上应用自适应高斯模糊来调整频率谱，使其不仅不可感知而且难以逆转。


<details>
  <summary>Details</summary>
Motivation: 现有的图像保护方法通过在图像中添加对抗性噪声来防止恶意编辑，但这些噪声容易被简单的技术（如JPEG压缩）逆转，限制了方法的实用性。

Method: 提出一种简单方法：在噪声上应用自适应的高斯模糊来调整频率谱，增强对抗性噪声的不可逆转性。

Result: 实验表明该方法能显著提升现有保护方法对各种逆转技术的防护性能，同时减少噪声引起的图像质量下降。

Conclusion: 通过调整频率谱的自适应高斯模糊技术，可以有效增强图像保护方法的鲁棒性，使其在实际应用中更加实用。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [112] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 提出了CompAgent，首个用于视觉合规验证的智能体框架，通过增强多模态大语言模型(MLLMs)的视觉工具能力来解决复杂政策规则的视觉内容合规问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定任务的深度学习模型，构建成本高且泛化能力有限；而MLLMs虽然具备广泛知识，但难以处理细粒度视觉细节和结构化合规规则。

Method: CompAgent框架包含规划代理和验证代理：规划代理根据合规政策动态选择视觉工具（如目标检测器、人脸分析器、NSFW检测器等），验证代理整合图像、工具输出和政策上下文进行多模态推理。

Result: 在公共基准测试中，CompAgent优于专用分类器、直接MLLM提示和精心设计的路由基线，在UnsafeBench数据集上达到76%的F1分数，比现有最佳方法提升10%。

Conclusion: 结果表明，基于智能体规划和工具增强推理的方法能够实现可扩展、准确且适应性强的视觉合规验证。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [113] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo是一个基于多智能体协作的训练免费框架，通过模拟人类取证调查流程来检测AI生成图像，显著优于传统分类器和现有视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法存在根本性限制：传统分类器缺乏可解释性且难以适应不断进化的生成模型，而视觉语言模型仅限于单次分析和像素级推理。

Method: 采用多智能体协作框架，整合反向图像搜索、元数据提取、预训练分类器和VLM分析等取证工具，通过专门的LLM智能体协调收集、综合和推理跨源证据，并引入结构化多智能体辩论机制和记忆增强推理模块。

Result: 在包含6,000张图像的全面评估中，AIFo在受控实验室环境和真实世界场景下均达到97.05%的准确率，显著优于传统分类器和最先进的VLM方法。

Conclusion: 基于智能体的程序推理为AI生成图像检测提供了一个更鲁棒、可解释和适应性强的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [114] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于能量的多提示学习方法(EMPL)，通过从能量分布中采样生成多个提示嵌入，实现了视觉语言模型在有限资源下对下游任务的高效适应，并在领域内和领域外开放词汇泛化之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要围绕单提示范式，很少探索多提示学习的技术潜力。本文旨在为视觉语言多提示学习提供理论基础，并通过理论和实验证明多提示增强在视觉语言迁移中的优越性。

Method: 提出基于能量的多提示学习(EMPL)，通过从隐式定义的能量分布中采样实例来生成多个提示嵌入。该方法参数高效，且严格保证了领域内和领域外开放词汇泛化之间的平衡。

Result: 综合实验验证了多提示学习的优势以及EMPL方法的卓越性能，证明了该方法在视觉语言迁移任务中的有效性。

Conclusion: EMPL方法不仅参数高效，而且通过多提示学习实现了更好的泛化能力，为视觉语言模型的快速适应提供了新的技术路径。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [115] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的迁移学习方法，用于检测地面终端组件的细粒度天气状况，以提升LEO卫星互联网的可靠性。


<details>
  <summary>Details</summary>
Motivation: 天气事件对卫星互联网性能有显著影响，特别是地面终端组件（如卫星天线）容易受到雪、雨等恶劣天气干扰，需要细粒度的天气检测能力来协助故障诊断和缓解。

Method: 采用高效的迁移学习方法，使地面组件能够本地检测代表性的天气相关状况，包括雪、湿和其他恶劣天气条件。

Result: 所提出的迁移学习方法在检测雪、湿和其他天气条件方面表现出色，性能优于典型的深度学习方法如YOLOv7、YOLOv9、Faster R-CNN和R-YOLO。

Conclusion: 该迁移学习方法不仅性能优越，而且具有良好的泛化能力，能够适应各种实际部署场景，为卫星互联网的可靠运行提供了有效的天气检测解决方案。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [116] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: DM-QPMNet是一种双编码器网络，通过多模态注意力融合机制，有效利用偏振强度图像和相位图的互补信息进行细胞分割。


<details>
  <summary>Details</summary>
Motivation: 传统阈值方法对噪声和细胞密度敏感，而简单的通道拼接深度学习方法无法充分利用偏振强度图像和相位图的互补特性。

Method: 采用双编码器分别处理偏振强度图像和相位图，通过多头注意力在中间深度融合模态特定特征，使用双源跳跃连接和每模态归一化。

Result: 相比单模态基线和简单拼接方法，该方法在细胞分割方面表现出显著改进。

Conclusion: 模态特定编码与可学习融合能有效利用ssQPM同时捕获的互补照明和相位线索，实现稳健的细胞分割。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [117] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出了基于VQ-VAE的电子显微镜数据压缩框架，支持16x到1024x压缩比，具有按需解码功能，并引入ROI驱动的工作流程进行选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 海量电子显微镜数据集对存储、传输和下游分析提出了挑战，需要高效的压缩解决方案。

Method: 使用向量量化变分自编码器(VQ-VAE)进行压缩，结合Transformer先验模型通过特征线性调制(FiLM)和拼接恢复纹理，并采用ROI驱动的工作流程。

Result: 实现了16x到1024x的可扩展压缩比，支持仅解码顶部层级的极端压缩，以及通过Transformer先验恢复纹理细节。

Conclusion: 该框架为海量EM数据提供了高效的压缩和选择性重建方案，显著降低了存储和传输需求。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [118] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种在双曲空间中计算最优传输映射的新算法，通过将欧几里得和球面几何的方法扩展到双曲设置来解决分层数据、网络和多亏格黎曼曲面中的最优传输问题。


<details>
  <summary>Details</summary>
Motivation: 现有的最优传输计算方法主要针对欧几里得空间和球面，但在涉及分层数据、网络和多亏格黎曼曲面的应用中，双曲空间中的最优传输问题自然出现，需要专门的计算方法。

Method: 使用几何变分技术，将欧几里得和球面几何的方法扩展到双曲设置，提出了一种新颖高效的计算双曲空间最优传输映射的算法。

Result: 在合成数据和多亏格曲面模型上进行了实验，验证了所提出方法的有效性。

Conclusion: 该算法成功解决了双曲空间中的最优传输问题，为处理分层数据、网络和多亏格黎曼曲面等应用提供了有效的计算工具。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [119] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出MSDI框架，通过3D高斯表示和运动扩散先验，结合LLM语义信息生成物理合理的4D人体运动，无需重新训练即可处理分布外对象感知运动。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成的视频存在不现实变形、语义违规和物理不一致问题，主要原因是缺乏3D物理先验。

Method: 使用预生成的3D人体和对象，结合LLM的空间和提示语义信息，通过提出的运动扩散分数蒸馏采样(MSDS)进行空间感知运动优化，从预训练运动扩散模型蒸馏分数梯度来优化人体运动。

Result: 实验表明该框架能生成自然且物理合理的人体运动，尊重3D空间上下文，为真实4D生成提供可扩展解决方案。

Conclusion: MSDI框架通过零样本方法成功解决了视频生成中的物理一致性问题，无需在有限交互数据集上进行联合训练，具有良好的泛化能力。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [120] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文介绍了L48数据集，一个从鸟类声音记录中提取的细粒度多标签数据集，用于评估单正多标签（SPML）方法。相比之前基于合成数据的方法，L48提供了更真实的SPML场景，并揭示了现有方法在细粒度分类中的弱点。


<details>
  <summary>Details</summary>
Motivation: 现有的SPML方法主要基于从完全标注数据集随机采样单正标签的合成数据集进行开发和评估，这种方法无法反映真实世界的复杂性，特别是细粒度分类中的困难误分类问题。

Method: 构建L48数据集——一个基于鸟类声音记录的细粒度多标签数据集，提供自然的单正标注设置，以及两个基于领域先验的扩展设置（提供额外负标签）。在L48上对现有SPML方法进行基准测试。

Result: 在L48数据集上的基准测试显示，现有SPML方法的性能与在合成数据集上相比存在显著差异，暴露出方法在细粒度分类中的弱点。

Conclusion: 需要更真实和困难的基准数据集来推动SPML方法的发展，L48数据集为此提供了有价值的评估平台。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [121] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 开发了一个3阶段的自动化管道，用于处理大规模甲虫图像数据，包括检测、裁剪和形态分割，以提高生物学研究效率。


<details>
  <summary>Details</summary>
Motivation: 在昆虫学和生态学研究中，生物学家需要处理数千个甲虫托盘图像，手动处理效率低下，需要自动化管道来处理大规模数据。

Method: 使用基于transformer的开放词汇目标检测器和视觉语言模型进行迭代检测，然后对670张甲虫图像进行手动标注，并微调两个基于transformer的分割模型变体进行精细分割。

Result: 构建了一个集成多种深度学习方法、专门用于甲虫图像处理的管道，能够相对准确地实现甲虫的细粒度分割。

Conclusion: 该管道能够显著提高处理大规模甲虫数据的效率，加速生物学研究进程。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [122] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出MambaNetLK，一种基于Mamba状态空间模型的无对应点3D配准框架，结合临床数据集C3VD-Raycasting-10k，在结肠镜导航中显著提升配准精度。


<details>
  <summary>Details</summary>
Motivation: 解决生物组织重复纹理和局部均匀几何特征导致的特征退化问题，以及术前解剖与术中观察之间的域偏移对配准稳定性的影响。

Method: 将Mamba SSM作为跨模态特征提取器集成到PointNetLK架构中，利用Lucas-Kanade算法进行迭代配准，线性时间复杂度捕获长程依赖。

Result: 在临床数据集上相比次优方法减少56.04%的中值旋转误差和26.19%的RMSE平移误差，在ModelNet40上表现出强泛化能力和初始位姿扰动鲁棒性。

Conclusion: MambaNetLK为手术导航中的3D配准提供了稳健基础，结合全局表达性SSM特征提取器和大规模临床数据集，实现了更准确可靠的微创手术导航系统。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [123] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 提出了Spot The Ball基准测试，用于评估视觉语言模型在视觉社交推理方面的能力，发现人类在定位被移除的体育球方面比最先进的模型准确2-3倍。


<details>
  <summary>Details</summary>
Motivation: 人类擅长通过微妙的行为线索（如注视、姿势和朝向）进行视觉社交推理，这对开发更类人AI代理至关重要。

Method: 使用足球、篮球和排球图像创建基准测试，要求定位被移除的球，评估了四种最先进的视觉语言模型和三种提示策略。

Result: 人类准确率（20-34%）比模型（≤17%）高2-3倍，模型依赖表面空间启发式方法，而人类利用社交线索如注视方向和身体姿势。

Conclusion: 揭示了视觉社交推理中持续存在的人机差距，需要明确编码结构化行为线索的架构来实现稳健的类人推理。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [124] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出了一种结合预训练CLIP视觉变换器和轻量级分类器的联邦学习框架，用于农业分类任务，在保护隐私的同时显著提升准确率和降低通信开销


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式训练的数据隐私问题，以及标准联邦学习在非独立同分布数据和通信成本方面的挑战

Method: 使用冻结的CLIP视觉变换器进行特征提取，仅训练轻量级分类器，并共享1%的CLIP特征表示来对齐类别分布

Result: 在农业分类任务上达到86.6%的准确率，比基线联邦学习方法提升4倍以上

Conclusion: 结合视觉语言模型特征与联邦学习，为隐私保护和可扩展的农业智能提供了有效且高效的解决方案

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [125] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: PersonalView是一个轻量级适配方法，仅需100个训练样本即可让现有模型获得多视角生成能力，通过条件架构设计和语义对应对齐损失实现多视角一致性和身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型无法控制生成图像视角，也无法生成一致的多视角人物图像，限制了应用场景。

Method: 设计条件架构利用预训练扩散变换器的上下文学习能力，并引入语义对应对齐损失保持原始生成能力。

Result: 在100个训练样本下，PersonalView在多视角一致性、文本对齐、身份相似度和视觉质量方面显著优于需要大量多视角数据训练的基线方法。

Conclusion: PersonalView证明了通过轻量级适配即可为现有模型添加多视角生成能力，大大降低了数据需求。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [126] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: LITHOS是一个大规模、多样化的公开实验框架，用于自动化岩石学分析，包含211,604个高分辨率RGB偏振光斑块和105,802个专家标注的矿物颗粒，涵盖25种矿物类别。


<details>
  <summary>Details</summary>
Motivation: 传统岩石学分析是劳动密集型任务，需要专家通过光学偏振显微镜进行详细视觉检查，这限制了可扩展性，因此需要自动化技术来解决这一挑战。

Method: 提出双编码器变换器架构，整合两种偏振模态作为强基线，评估了多种深度学习技术用于矿物分类。

Result: 该方法持续优于单偏振模型，证明了偏振协同在矿物分类中的价值。

Conclusion: LITHOS基准测试已公开，包括数据集、代码和预训练模型，以促进自动化岩石学分析的复现性和进一步研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [127] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 本文首次系统评估了11种轻量级视觉模型在7个不同数据集上的跨数据集性能，提出了跨数据集评分(xScore)指标，发现ImageNet准确率不能可靠预测细粒度或医学数据集性能，并识别了促进泛化的关键架构组件。


<details>
  <summary>Details</summary>
Motivation: 当前轻量级视觉模型主要在ImageNet上评估，但缺乏对其在其他领域泛化能力的系统研究，需要量化跨数据集鲁棒性并识别驱动泛化的架构要素。

Method: 在7个不同数据集上以固定100轮训练计划评估11种轻量级视觉模型(250万参数)，引入跨数据集评分(xScore)作为统一指标来量化模型性能的一致性和鲁棒性。

Result: 1) ImageNet准确率不能可靠预测细粒度或医学数据集性能；2) xScore可作为移动模型性能的可扩展预测指标，仅需4个数据集即可估计；3) 各向同性卷积、高空间分辨率和通道注意力等组件促进泛化，而Transformer块带来额外参数开销但益处有限。

Conclusion: 本研究提供了在ImageNet之外评估轻量级视觉模型的可复现框架，突出了移动友好架构的关键设计原则，指导开发能在多样化应用领域中稳健泛化的未来模型。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [128] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种结合深度算子网络和神经正切核的混合方法，用于解决复杂的逆问题，包括纳维-斯托克斯方程控制的源定位和图像重建。


<details>
  <summary>Details</summary>
Motivation: 解决非线性、稀疏性和噪声数据带来的挑战，确保物理一致性和准确性。

Method: 将物理信息约束和任务特定正则化整合到损失函数中，结合DeepONet和NTK的混合方法。

Result: 在多种合成和真实数据集上的验证表明该方法具有鲁棒性、可扩展性和精确性。

Conclusion: 该方法在计算物理和成像科学领域具有广泛的应用潜力。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [129] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC是一个联邦学习框架，用于解决多模态情感识别中模态缺失问题，通过扩散模型恢复缺失模态并保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中不可预测的模态缺失会显著降低现有多模态情感识别方法的性能，传统基于完整数据训练的恢复方法在极端数据分布下容易出现语义失真。

Method: 提出FedDISC框架，集成联邦学习到模态恢复中，使用DISC-Diffusion模块确保恢复模态与可用模态在上下文、说话者身份和语义上的一致性，并采用交替冻结聚合策略。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上的实验表明，FedDISC在不同缺失模态模式下实现了优越的情感分类性能，优于现有方法。

Conclusion: FedDISC通过联邦扩散模型有效解决了多模态情感识别中的模态缺失问题，实现了语义一致的模态恢复和鲁棒的情感分类。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [130] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen是一个生成框架，直接从原始OpenStreetMap数据创建逼真的卫星图像，能够生成前后一致的图像对，用于解决训练数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 准确和最新的地理空间数据对城市规划、基础设施监测和环境管理至关重要，但自动化城市监测仍然困难，因为特定城市特征及其变化的策划数据集稀缺。

Method: 使用完整的OSM JSON数据（包括矢量几何、语义标签、位置和时间），通过生成框架从原始OpenStreetMap数据直接创建卫星图像，能够生成前后一致的图像对。

Result: OSMGen能够产生逼真的卫星图像，用户对OSM输入的编辑会转化为有针对性的视觉变化，同时保持场景其余部分不变，解决了训练数据稀缺和类别不平衡问题。

Conclusion: OSMGen为静态和变化状态生成配对的（JSON，图像）数据，为实现卫星图像自动驱动结构化OSM更新的闭环系统铺平了道路。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [131] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 提出基于扩散模型重建动态的AI生成图像检测方法，通过分析不同噪声强度下的重建指标变化来区分真实和合成图像。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测方法无法应对现代文本到图像系统生成的逼真无伪影图像，需要新的检测技术。

Method: 利用多强度图像重建动态（扩散回弹）分析LPIPS、SSIM和PSNR等重建指标在不同噪声强度下的演化，提取可解释的流形特征。

Result: 在4000张图像的平衡数据集上达到0.993 AUROC，对压缩和噪声等常见失真具有鲁棒性。

Conclusion: 该方法展示了强大的泛化能力和可解释性，为可扩展、模型无关的合成媒体取证提供了基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [132] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 使用迁移学习和轻量级UNet模型在FOREST-2立方星上实现热红外云分割，通过预训练和微调将宏F1分数从0.850提升至0.877，并在NVIDIA Jetson Nano上实现5秒内全图像推理。


<details>
  <summary>Details</summary>
Motivation: 立方星任务受限于硬件和光谱信息，通常只有单一热红外波段且缺乏标注数据，传统云掩膜技术不可行，需要开发适用于热红外云分割的高效方法。

Method: 使用UNet架构配合轻量级MobileNet编码器，先在Landsat-7云覆盖评估数据集上预训练，然后用少量任务特定样本进行联合训练微调。

Result: 宏F1分数从0.850提升至0.877，模型转换为TensorRT引擎后在NVIDIA Jetson Nano上实现5秒内全图像推理。

Conclusion: 利用公共数据集和轻量级架构可以在轨道上实现准确高效的热红外云掩膜，支持数据有限的地球观测任务中的实时决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [133] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D是一个免费的生成式AI框架，利用Google街景图像重建孟加拉国文化遗产的3D模型，解决了传统3D数字化方法成本高、技术要求高的难题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产修复面临资源有限和技术专家稀缺的双重挑战。传统3D数字化方法需要昂贵硬件、专家操作和大量现场访问，这在发展中国家往往不可行。

Method: 采用两阶段流程：1）使用Gemini 2.5 Flash Image进行多模态视觉推理，合成结构纹理；2）通过Hexagen进行神经图像到3D生成，恢复几何结构。利用公开的Google街景图像。

Result: 系统在几秒钟内生成照片级真实、度量一致的重建结果，相比传统Structure-from-Motion流程显著提速，且无需专业硬件或专家监督。在Ahsan Manzil、Choto Sona Mosque和Paharpur等地标上的实验证明了其视觉和结构保真度。

Conclusion: 通过将开放图像转化为数字遗产，这项工作将保护重新定义为资源有限国家社区驱动、AI辅助的文化连续性行为，大幅降低了经济和技术门槛。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [134] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习的视频时刻检索模型，通过多智能体系统和证据学习解决模型间定位冲突，无需额外训练即可判断查询是否超出视频范围。


<details>
  <summary>Details</summary>
Motivation: 当前视频时刻检索方法未考虑不同模型定位结果间的冲突，导致模型无法有效整合。需要开发能够处理冲突并提高检索性能的新方法。

Method: 使用强化学习模型扫描整个视频定位时刻边界并生成位置证据，提出多智能体系统框架利用证据学习解决智能体间定位输出冲突。

Result: 在基准数据集上的实验表明，该方法相比最先进方法具有更好的效果，能够有效处理多智能体系统的竞争和冲突。

Conclusion: 建模多智能体系统的竞争和冲突是提高强化学习在时刻检索中性能的有效方法，证据学习在多智能体框架中发挥了新作用。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [135] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD是一个基于视觉的放射学辅助框架，通过摄像头系统直接从显示屏捕获医学图像，避免了与医院IT基础设施集成的挑战。


<details>
  <summary>Details</summary>
Motivation: 计算机辅助诊断系统在临床广泛部署受到与现有医院IT基础设施集成困难的阻碍。

Method: 采用自动化流水线检测、恢复和分析屏幕上的医学图像，将摄像头捕获的视觉数据转换为适合自动分析和报告生成的诊断质量图像。

Result: 在多种医学影像数据集上验证，诊断性能与传统CAD系统相当，分类任务F1分数下降通常小于2%，自动报告的自然语言生成指标与原图相比差异在1%以内。

Conclusion: VisionCAD仅需摄像头设备和标准计算资源，为AI辅助诊断提供了可访问的方法，可在不修改现有基础设施的情况下在各种临床环境中部署诊断能力。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [136] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了FERBench基准测试，系统评估了20个先进多模态大语言模型在面部表情识别任务上的表现，并开发了UniFER-7B模型，通过后训练策略显著提升了面部表情推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域取得成功，但在面部表情识别任务上的性能尚未得到充分探索，现有模型在推理和可解释性方面存在明显局限。

Method: 构建FERBench基准测试评估20个MLLM模型；开发后训练策略，包括创建UniFER-CoT-230K数据集用于冷启动初始化和UniFER-RLVR-360K数据集用于强化学习；最终构建UniFER-7B统一可解释FER基础模型。

Result: MLLM模型在分类性能上表现良好，但在推理和可解释性方面存在显著局限；UniFER-7B模型超越了多个开源和闭源通用MLLM模型，包括Gemini-2.5-Pro和Qwen2.5-VL-72B。

Conclusion: 通过系统化的基准测试和专门的后训练策略，可以显著提升MLLM在面部表情识别任务中的推理能力和可解释性，UniFER-7B模型为FER领域提供了有效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [137] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练框架解决现有视觉语言模型在代码生成任务中的局限性，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在代码生成任务中依赖单任务训练，限制了通用视觉代码智能的发展，需要更统一的解决方案。

Method: 采用两阶段训练框架：1）构建160万图像-代码对的监督微调语料库；2）提出视觉强化学习策略，通过从粗到细的奖励机制计算局部和全局图像块的视觉相似度。

Result: 在多个多模态代码生成基准测试中达到最先进的性能表现。

Conclusion: VinciCoder通过两阶段训练框架和从粗到细的视觉强化学习策略，有效提升了多模态代码生成的性能，证明了该方法的有效性。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [138] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出了首个统一框架，通过将SOD、CoSOD和SIS三个异构显著性任务转化为视觉语言模型中的思维链推理过程，解决了任务异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对单个显著性任务设计专用模型，缺乏统一处理异构任务的能力。本文旨在开发一个能够同时处理多种显著性任务的统一框架。

Method: 采用两阶段训练范式：监督微调（SFT）和强化学习（RL）。提出置信度引导策略优化（CGPO）算法，利用奖励与模型置信度差异作为优势信号，避免GRPO的局限性。同时引入"输出到推理"策略构建高质量SFT数据。

Result: 模型在所有任务上匹配或超越专用SOTA方法和强闭源VLM，特别是在CoSOD任务上CoCA的S-measure达到0.899，比之前最佳方法提升8.0个百分点，且使用更少训练数据。

Conclusion: 提出的统一框架通过思维链推理有效解决了显著性任务的异质性问题，CGPO算法显著提升了强化学习效果，证明了该方法在多个任务上的优越性能。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [139] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出LGCA框架解决CLIP模型在图像裁剪时引入错误信息的问题，通过局部特征捕获和显著区域扩展，结合局部和全局特征提升零样本图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过图像裁剪和文本描述增强来提升CLIP性能，但随机裁剪会引入错误信息和偏差，因为小尺度图像特征相似。

Method: LGCA框架先捕获图像局部特征，然后重复选择最显著区域并扩展，设计相似度评分结合原始和扩展图像，同时捕获局部和全局特征。

Result: 理论分析显示LGCA时间复杂度与原始模型相同，实验证明在多个数据集上显著提升零样本性能，优于现有最优基线。

Conclusion: LGCA通过局部-全局交叉对齐有效减少错误信息，在保持效率的同时显著提升零样本图像分类性能。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [140] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出ITEM方法，利用图像-文本不对齐作为检测线索，通过CLIP空间测量图像与描述的不对齐度，结合全局和局部语义不对齐来检测生成图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注视觉线索，容易过拟合特定图像模式，无法泛化到未见过的生成模型。发现生成图像与对应描述无法正确对齐，这提供了多模态检测线索。

Method: 在预训练CLIP空间中测量图像与描述的不对齐度，然后训练MLP头进行检测。提出分层不对齐方案：先关注整张图像，再关注描述中的每个语义对象，探索全局和局部语义不对齐。

Result: 在多个最新生成模型上表现出优越性能，具有出色的泛化能力和鲁棒性，优于其他最先进方法。

Conclusion: 利用图像-文本不对齐作为检测线索是有效的，多模态方法能提高生成图像检测的泛化性和鲁棒性。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [141] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出一种基于频率伪造线索(F^2C)的扩散模型图像检测方法，通过增强所有频段的频率特征来提升检测器的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像质量很高，但可能被恶意使用。现有检测器难以捕捉不同模型和设置下的判别性线索，限制了其对未见扩散模型的泛化能力和对各种扰动的鲁棒性。

Method: 观察到扩散生成图像与自然图像在低到高频段存在渐进性差异，提出频率选择性函数作为加权滤波器，抑制判别性较弱的频段，增强信息量更大的频段。

Result: 在多个扩散生成图像数据集上的实验表明，该方法优于现有最先进的检测器，具有更好的泛化性和鲁棒性。

Conclusion: 基于频率伪造线索的方法能够有效检测来自未见扩散模型的图像，并对各种扰动具有强鲁棒性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [142] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: ToxicTextCLIP是一个针对CLIP模型预训练阶段的文本模态攻击框架，通过背景感知选择和背景驱动增强生成高质量对抗文本，在分类和检索任务中达到95.83%的中毒成功率和98.68%的后门命中率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注图像模态攻击，而CLIP训练中同样重要的文本模态攻击研究不足。CLIP依赖未筛选的互联网数据，使其面临数据中毒和后门风险。

Method: ToxicTextCLIP框架迭代应用：1）背景感知选择器，优先选择与目标类别背景一致文本；2）背景驱动增强器，生成语义连贯且多样化的中毒样本。

Result: 在分类和检索任务中，ToxicTextCLIP达到95.83%的中毒成功率和98.68%的后门命中率，并能绕过RoCLIP、CleanCLIP和SafeCLIP等防御方法。

Conclusion: 该研究揭示了CLIP文本模态的脆弱性，提出的ToxicTextCLIP框架在攻击效果和防御规避方面表现出色，为多模态模型安全提供了重要见解。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [143] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出基于弱监督深度学习的肺炎分类和定位框架，使用Grad-CAM生成热力图，无需像素级标注，在Kermany CXR数据集上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像分析中需要昂贵像素级标注的问题，开发仅需图像级标签的肺炎诊断方法，提高AI在放射诊断中的透明度和临床可信度。

Method: 使用七种ImageNet预训练架构（ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V2/V3、ViT-B16），采用焦点损失和患者级数据分割，结合Grad-CAM生成解释性热力图。

Result: ResNet-18和EfficientNet-B0表现最佳：测试准确率98%、ROC-AUC=0.997、F1=0.987；MobileNet-V2在准确性和计算成本间达到最优平衡；Grad-CAM可视化确认模型聚焦临床相关肺区域。

Conclusion: 弱监督可解释模型能够有效提升肺炎筛查的透明度，增强AI辅助医学成像的临床信任，具有重要应用潜力。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [144] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，通过单张图像实现外观和人体部位语义的联合建模，在3D人体部位分割和重建任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在3D人体重建方面取得了高保真度，但在特定任务（如人体3D分割）上的实用性仍然受限。

Method: 集成人体几何先验于重建阶段，自监督语义先验于分割阶段；开发交互式标注程序生成高质量数据标签对；采用像素对齐聚合实现跨任务协同；多任务目标同时优化纹理建模保真度和语义一致性。

Result: 大量实验表明，HumanCrafter在3D人体部位分割和单图像3D人体重建方面均超越了现有最先进方法。

Conclusion: HumanCrafter通过统一框架有效解决了3D人体重建和分割的联合建模问题，展现了跨任务协同的优势。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [145] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出了一种基于自举深度学习的迭代分割和质量优化框架，用于MRI中前庭神经鞘瘤的自动分割，显著提高了分割精度并提升了标注效率。


<details>
  <summary>Details</summary>
Motivation: MRI中前庭神经鞘瘤的准确分割对患者管理至关重要，但传统手动标注耗时且依赖专家。现有深度学习方法在多样化数据集和复杂临床案例中仍存在鲁棒性挑战。

Method: 采用自举式深度学习框架进行迭代分割和质量优化，结合多中心数据并依赖专家共识确保标注可信度，实现人类在环的模型训练方法。

Result: 在目标内部验证数据集上，Dice相似系数从0.9125显著提升至0.9670，在代表性外部数据集上保持稳定性能，专家评估143个扫描发现需要专家干预的细微案例，效率比传统手动标注提升约37.4%。

Conclusion: 该人类在环模型训练方法实现了高分割精度，展示了作为临床适应性强、可推广的自动前庭神经鞘瘤分割策略的潜力，数据集已公开可用。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [146] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP是一种用于视觉语言模型的个性化联邦提示学习新范式，通过多组文本和视觉提示来捕捉细粒度语义特征，采用基于相似度的动态提示聚合策略，在保持参数效率的同时实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦学习中客户端数据异构性问题，同时保持视觉语言模型的个性化能力和泛化性能，需要一种既能捕捉本地特征又能有效聚合共享知识的方法。

Method: 为每个客户端配备多组配对的文本和视觉提示，引入多样性损失使各组专注于不同语义方面，采用基于余弦相似度的概率采样进行动态提示聚合。

Result: 在多个联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均优于现有方法，且在所有联邦提示学习方法中通信参数最低。

Conclusion: FedMGP通过多组提示和动态聚合策略有效平衡了共享知识和客户端特定特征，理论分析表明该方法能增强共享语义并抑制客户端特定噪声。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [147] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种前馈方法，可从单张图像合成可控的显式4D场景，结合视频扩散模型的生成先验与4D数据集学习到的几何和运动约束，无需测试时优化即可直接预测可变形3D高斯场。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要测试时优化或后处理，效率较低。Diff4Splat旨在实现单次前向传播即可合成高质量4D场景，提高动态场景合成的效率。

Method: 使用视频潜在变换器增强视频扩散模型，联合捕捉时空依赖关系并预测时变3D高斯基元，通过外观保真度、几何精度和运动一致性的目标进行训练。

Result: 在30秒内合成高质量4D场景，在视频生成、新视角合成和几何提取方面匹配或超越基于优化的方法，同时显著提高效率。

Conclusion: Diff4Splat提供了一种高效的单次前向传播方法，用于从单张图像合成可控的4D场景，在保持质量的同时大幅提升效率。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [148] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个大规模胸部X光数据集，用于可解释的医学视觉问答，包含17,597个问答对和4,394张图像，具有放射科医生验证的边界框和临床推理解释。


<details>
  <summary>Details</summary>
Motivation: 推动可重现和临床基础的医学视觉问答研究，解决医学VQA中幻觉问题和缺乏空间定位能力的问题。

Method: 构建包含六种诊断类型问题的平衡数据集（41.7%阳性，58.3%阴性样本），使用MedGemma-4B-it模型进行基准测试。

Result: 与基线相比性能提升11.8%（F1=0.624），同时实现了病变定位能力。

Conclusion: VinDr-CXR-VQA数据集能够显著提升医学VQA的性能和可靠性，为临床可解释的AI系统开发提供了重要资源。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [149] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++是一个用于全景图像多目标跟踪的创新方法，通过反馈驱动框架解决360度视野带来的失真、大搜索空间和身份模糊问题，在JRDB和EmboTrack基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统多目标跟踪方法在全景图像中表现不佳，因为360度视野带来分辨率稀释、视角相关失真等独特挑战，需要专门设计的方法来处理这些全景特定问题。

Method: 采用反馈驱动框架，包含DynamicSSM块稳定全景特征、FlexiTrack实例进行灵活定位和短期关联、ExpertTrack内存通过专家混合设计整合外观线索、Tracklet管理模块根据场景动态自适应切换跟踪模式。

Result: 在JRDB和EmboTrack基准测试中取得最先进性能，相比原始OmniTrack在JRDB上HOTA提升25.5%，在QuadTrack上提升43.07%。

Conclusion: OmniTrack++为全景多目标跟踪提供了有效的解决方案，通过创新的反馈驱动框架和专门设计的组件成功解决了全景图像特有的挑战，并建立了全面的EmboTrack基准数据集来支持该领域的研究。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [150] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer是一个用于多主体视频生成的新框架，通过文本提示和参考图像生成视频，解决了现有模型在可控性和适用性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常仅基于文本或单张图像，限制了可控性和应用范围。需要开发能够从文本提示和多个参考图像生成多主体视频的方法。

Method: 设计了分层身份保持注意力机制来聚合主体内和跨主体的特征；利用预训练视觉语言模型进行语义理解；采用在线强化学习阶段来优化训练目标。

Result: 实验表明，ID-Composer在身份保持、时间一致性和视频质量方面优于现有方法。

Conclusion: ID-Composer通过创新的注意力机制、语义理解和强化学习策略，有效解决了多主体视频生成中的身份保持和语义整合挑战。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [151] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 提出了一种无需训练或偏置标注的测试时去偏方法，使用预训练分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别文本提示均匀相似，从而消除混淆视觉区域的偏置信号。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常需要训练数据和明确的组标签进行微调或调整嵌入，限制了实际应用。测试时方法虽然避免这一约束，但仍依赖数据集特定偏置的先验知识，在开放集设置中泛化性有限。

Method: 使用预训练分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别文本提示均匀相似，保留目标属性同时消除混淆区域的偏置信号。

Result: 在Waterbirds和CelebA数据集上的实验表明，该方法在组鲁棒性指标和Attention IoU方面优于现有测试时去偏方法。

Conclusion: 分割引导的干预方法在视觉语言模型中实现了可扩展且无需标注的偏置缓解，证明了其有效性。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [152] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了T-VAD框架，基于大型视觉语言模型，通过异常热图解码器和区域感知异常编码器实现细粒度视频异常检测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法输出有限（仅正常或异常），且需要人工评估。需要更细粒度和交互式的异常检测方法。

Method: T-VAD框架包含异常热图解码器（AHD）进行像素级视觉-文本特征对齐生成热图，以及区域感知异常编码器（RAE）将热图转换为可学习文本嵌入，指导LVLM准确识别和定位异常事件。

Result: 在UBnormal数据集上达到94.8% AUC，异常热图准确率67.8%/76.7%（RBDC/TBDC）；在ShanghaiTech数据集上BLEU-4得分62.67/88.84，Yes/No准确率97.67%；在UBnormal数据集上BLEU-4得分50.32/78.10，Yes/No准确率89.73%。

Conclusion: T-VAD显著提升了异常检测的粒度和交互性，在多个指标上达到最先进性能，为视频异常检测提供了更精细和可解释的解决方案。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [153] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了Real-IAD Variety基准，这是最大最全面的工业异常检测数据集，包含198,960张高分辨率图像和160个类别，覆盖28个行业、24种材料和22种颜色变化。实验显示传统多类无监督方法在扩展到160类别时性能显著下降，而视觉语言模型表现出对类别扩展的强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测基准存在类别多样性不足、规模有限的问题，导致度量饱和和模型在真实场景中泛化能力差。需要更大更全面的基准来推动该领域发展。

Method: 构建了Real-IAD Variety数据集，包含160个对象类别、198,960张高分辨率图像，覆盖28个行业、24种材料类型和22种颜色变化。在多种设置下进行实验评估：多类无监督、多视图以及零/少样本设置。

Result: 传统多类无监督异常检测方法在从30类别扩展到160类别时性能显著下降，而视觉语言模型对类别扩展表现出强鲁棒性，性能变化最小，显著提升了在多样化工业环境中的泛化能力。

Conclusion: Real-IAD Variety作为首个大规模综合性工业异常检测基准，将加速超越领域特定限制的研究，促进可扩展通用异常检测系统的发展，为下一代基础模型的训练和评估提供关键资源。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [154] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 提出了一种从单张图像中精确学习和合成多实例语义的方法，通过惩罚性注意力优化和框控制来解决相似语义分离问题


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像学习多实例语义时训练数据有限的问题，特别是在实例具有相似语义或外观时的挑战

Method: 使用惩罚性注意力优化在学习阶段分离相似语义，在合成阶段引入并优化注意力层的框控制来减轻语义泄漏并精确控制输出布局

Result: 实验结果表明该方法实现了分离且高质量的语义学习和合成，在可编辑性和实例一致性之间取得了良好平衡

Conclusion: 该方法在处理语义或视觉相似实例或罕见对象时保持鲁棒性，代码已公开

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [155] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D-NVS结合体素表示与神经高斯泼溅，通过紧凑的神经体素和变形场来建模动态场景，大幅减少内存消耗并加速训练，同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在动态场景中由于需要为每帧复制高斯函数而导致内存开销巨大，需要更高效的动态场景建模方法。

Method: 使用紧凑的神经体素集合和学习的变形场来建模时间动态，避免为每个时间戳生成单独的高斯集合，并引入视图细化阶段选择性优化挑战性视角。

Result: 实验表明该方法在显著减少内存和加速训练的同时，超越了现有最优方法，实现了实时渲染和卓越的视觉保真度。

Conclusion: 4D-NVS通过神经体素和变形场的结合，为动态场景建模提供了高效且高质量的解决方案，在内存效率和渲染质量方面均表现出色。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [156] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出了一种频率引导的广义类别发现框架(FREE)，用于解决域偏移下的广义类别发现问题，通过频域信息增强模型在分布偏移下发现类别的能力。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法在标准条件下表现良好，但在存在分布偏移时性能显著下降。本文探索了一个更现实的任务：域偏移广义类别发现(DS_GCD)，其中未标记数据不仅包含未知类别，还包含来自未知域的样本。

Method: 1. 基于频率的域分离策略，通过测量幅度差异将样本划分为已知域和未知域；2. 两种频域扰动策略：跨域策略（通过交换域间幅度分量适应新分布）和域内策略（增强对未知域内变化的鲁棒性）；3. 扩展自监督对比目标和语义聚类损失；4. 聚类难度感知重采样技术。

Result: 大量实验表明，该方法有效减轻了分布偏移的影响，在各种基准数据集上实现了发现已知和未知类别的优越性能。

Conclusion: FREE框架通过利用频域信息，显著提升了模型在分布偏移条件下发现类别的能力，为域偏移广义类别发现问题提供了有效的解决方案。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [157] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 提出了一种基于记忆增强的上下文感知零样本异常检测方法，通过融合时序特征和视觉嵌入，结合上下文相似性评分实现实时异常分类。


<details>
  <summary>Details</summary>
Motivation: 视频异常通常依赖于上下文信息和时序演化，但现有异常检测器大多忽略这种上下文依赖，限制了其在真实场景中的泛化能力。

Method: 采用记忆增强管道，通过交叉注意力关联时序信号与视觉嵌入，并使用上下文相似性评分进行实时零样本异常分类。

Result: 在UCF-Crime数据集上达到90.4% AUC，在XD-Violence数据集上达到83.67% AP，创下零样本模型的新最佳性能。

Conclusion: 通过融合交叉注意力时序融合和上下文记忆，实现了高保真度的异常检测，为零样本模型在真实世界监控和基础设施监测中的应用迈出重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [158] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench是首个面向上下文感知视频异常理解的基准测试，通过事件中心的分层分类法定义了14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性，用于统一评估识别、时序定位、检测和预测等任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型对真实世界视频异常的理解仍停留在表面，缺乏对复杂原理和微妙上下文的理解能力，如区分有安全装备和无安全装备攀岩的差异。

Method: 提出了Cue-R1方法，基于R1风格的强化微调，采用可验证、任务对齐和层次细化的奖励机制，以统一生成方式处理各种异常理解任务。

Result: 在CueBench上的广泛实验表明，现有视觉语言模型在真实世界异常理解方面表现不佳，而Cue-R1方法平均超过最先进方法24%以上。

Conclusion: 当前视觉语言模型距离满意的真实世界异常理解仍有很大差距，CueBench为这一领域提供了严格的评估框架，而Cue-R1展示了显著的性能提升。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [159] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出了triplet segmentation新任务，通过仪器实例分割实现手术动作三元组的空间定位，并开发了TargetFusionNet架构和CholecTriplet-Seg数据集。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作识别方法仅限于帧级分类，无法可靠地将动作与特定仪器实例关联；之前的空间定位方法主要依赖类激活图，缺乏精确性和鲁棒性。

Method: 提出triplet segmentation统一任务，结合仪器实例分割；开发TargetFusionNet架构，扩展Mask2Former并引入目标感知融合机制，融合弱解剖先验与仪器实例查询。

Result: 在识别、检测和三元组分段的各项指标上，TargetFusionNet始终优于现有基线方法，证明强实例监督与弱目标先验结合显著提升了手术动作理解的准确性和鲁棒性。

Conclusion: 三元组分割为手术动作三元组的空间定位建立了统一框架，所提出的基准和架构为更可解释的手术场景理解铺平了道路。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [160] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究介绍了首个大规模多光谱激光雷达基准数据集FGI-EMIT，用于个体树木分割，并比较了传统无监督算法和深度学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 个体树木分割在林业应用中很重要，但缺乏大规模多光谱激光雷达基准数据集，且现有方法难以充分利用多光谱反射率信息。

Method: 创建了包含1,561棵人工标注树木的多光谱激光雷达数据集，在532、905和1550nm波长采集数据。比较了4种传统无监督算法和4种深度学习方法的性能。

Result: 深度学习方法的F1分数显著高于无监督方法，ForestFormer3D达到73.3%的最高分。在底层树木分割上，深度学习比传统方法高出25.9个百分点。

Conclusion: 深度学习方法在个体树木分割任务上明显优于传统无监督算法，但当前方法尚未能有效利用多光谱反射率信息。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [161] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个元数据引导的框架，通过将MRI体积图像与其DICOM采集参数对齐来学习MRI对比度表示，解决了MRI数据异质性和缺乏标准化对比标签的问题。


<details>
  <summary>Details</summary>
Motivation: MRI存在严重的数据异质性，不同扫描仪、协议和机构之间缺乏标准化的对比标签，这限制了大尺度自动化分析。需要统一的MRI对比度表示来实现自动序列识别、协调和质量控制等下游应用。

Method: 引入MR-CLIP框架，通过将体积图像与其DICOM采集参数对齐来学习MRI对比度表示，利用常规可用的采集元数据作为监督信号。

Result: 生成的嵌入显示了MRI序列的明显聚类，在少样本序列分类中优于监督的3D基线方法。此外，MR-CLIP通过图像-元数据嵌入距离识别损坏或不一致的元数据，实现无监督数据质量控制。

Conclusion: MR-CLIP通过将常规采集元数据转化为监督信号，为跨不同临床数据集的标签高效MRI分析提供了可扩展的基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [162] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了一种用于图像超分辨率网络后训练量化的新方法，通过双区域量化策略和敏感度感知微调来提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法在图像超分辨率中性能不佳，主要原因是忽略了激活值中的异常值影响，这些异常值与图像颜色信息高度相关且直接移除会导致性能显著下降。

Method: 1. 双区域量化策略：将激活值划分为异常值区域和密集区域，分别进行均匀量化以优化比特分配；2. 敏感度感知微调：针对不同网络层对量化的敏感度差异，让模型更关注高敏感层。

Result: 在多种超分辨率网络和数据集上的实验表明，该方法优于现有后训练量化方法，在大多数场景下达到与量化感知训练相当的性能，同时获得至少75倍的加速。

Conclusion: 所提出的双区域量化策略和敏感度感知微调有效解决了图像超分辨率后训练量化中的异常值问题和层间敏感度差异，实现了高性能的推理加速。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [163] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER是一种基于新颖性搜索的方法，通过LLM进行语义演化，使用CLIP嵌入量化新颖性，生成多样化的图像集。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型输出多样性有限的问题，特别是在创意视觉领域的应用受限。

Method: 使用LLM进行语义演化，CLIP嵌入量化新颖性，应用发射器引导搜索到不同的提示空间区域。

Result: 在多样性指标上显著优于现有的进化提示优化基线，消融研究证实了发射器的有效性。

Conclusion: WANDER能够从单一输入提示生成多样化的图像集，提升了扩散模型在创意任务中的应用潜力。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [164] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 该论文分析了低剂量CT图像增强中不同损失函数与图像质量指标之间的一致性，发现现有损失函数与感知质量指标存在不一致性，强调开发新损失函数时需要考虑图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT图像常受噪声和伪影影响，虽然深度学习模型在PSNR和SSIM指标上表现良好，但这些指标难以反映医学图像的感知质量，需要研究损失函数与图像质量指标的一致性。

Method: 对深度学习架构中不同损失函数进行客观分析，评估它们在低剂量CT图像质量增强中的相关性及其与图像质量指标的一致性。

Result: 研究发现损失函数与质量指标之间存在不一致性，特别是感知质量方面，现有损失函数在医学图像增强中的有效性受到限制。

Conclusion: 开发新的图像质量增强损失函数时，必须充分考虑图像质量指标，以确保增强结果在感知质量上的有效性。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [165] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 深度学习模型在ADNI数据集上对阿尔茨海默病诊断表现优异，但在拉丁美洲FLENI队列上性能显著下降，揭示了重要的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习模型在阿尔茨海默病诊断中从北美人群到拉丁美洲人群的泛化能力，探索模型在不同人群中的表现差异。

Method: 使用卷积和Transformer模型在ADNI数据集上训练，并在FLENI拉丁美洲临床队列上测试泛化性能，进行消融研究和遮挡敏感性分析。

Result: 所有模型在ADNI上AUC高达0.96-0.97，但在FLENI上降至0.80-0.82，显示显著性能下降。不同架构表现相似，图像归一化和采样选择是影响泛化的关键因素。

Conclusion: 需要基于人群的AI模型验证，未来应关注领域适应和队列多样化，确保诊断模型在不同人群中的可靠性。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [166] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 将地点识别重新定义为多类分类问题，通过为LiDAR扫描分配离散位置标签，使用编码器-解码器模型直接分类位置，在NuScenes数据集上取得与对比学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖对比学习，本文探索将地点识别作为多类分类问题的替代方法，以提高训练效率和稳定性。

Method: 为LiDAR扫描分配离散位置标签，训练编码器-解码器模型直接对每个扫描的位置进行分类。

Result: 在NuScenes数据集上评估，该方法取得了与基于对比学习方法相竞争的性能。

Conclusion: 多类分类方法在地点识别任务中具有竞争力，同时在训练效率和稳定性方面具有优势。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [167] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该研究分析了生成式AI模型如何编码'美'并消除'丑'，发现模型存在严重的肤色、年龄和性别偏见，86.5%生成图像为浅肤色，74%为年轻人群，非二元性别个体被过度性化，负面美特征提示产生更多NSFW内容。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方美容标准的推广，导致负面自我形象和身体畸形恐惧症。随着AI生成内容增多，担忧这些标准被夸大，需要研究生成式AI如何编码美丑概念及其社会影响。

Method: 创建两个图像生成流程：文本到图像模型和文本到语言模型到图像模型。开发结构化美容分类法，使用三个语言模型和两个文本到图像模型生成5984张图像，招募女性和非二元社交媒体用户通过李克特量表评估1200张图像。

Result: 86.5%生成图像为浅肤色，22%包含明确内容（尽管SFW训练），74%被评定为年轻年龄段。非二元个体图像被评定为更年轻和过度性化。带有'负面'美特征的提示（如'宽鼻子'）始终产生更高NSFW评分。

Conclusion: 生成式AI模型存在与美容标准相关的普遍人口偏见，这些偏见通过模型开发者（如负面提示）积极延续，导致数据流污染和不符合开发者美丽刻板印象的特征被主动消除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [168] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 开发了一个基于物联网的动物检测系统，结合YOLOv5和SSD算法，用于榴莲种植园中检测入侵动物，并通过Telegram通知和声音威慑机制实现实时监控。


<details>
  <summary>Details</summary>
Motivation: 榴莲种植园面临动物入侵导致作物损坏和经济损失的问题，传统农业实践缺乏无人干预的监控手段，现有系统依赖单一检测算法且通知平台和威慑机制有限。

Method: 集成YOLOv5和SSD目标检测算法，结合物联网技术，提供实时监控，检测到入侵时通过Telegram自动通知农民，并触发自动声音威慑机制（如虎啸）。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别为90%、85%和70%，白天准确率最高，夜间下降，对静态图像和视频均有效。

Conclusion: 本研究提供了一个结合检测、通知和威慑的综合实用框架，为自动化农业解决方案的未来创新铺平了道路。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [169] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种粒度一致的自适应2D掩码跟踪方法，通过保持跨帧的时间对应关系来消除冲突的3D伪标签，结合三阶段课程学习框架，从碎片化的单视图数据逐步训练到统一的多视图标注，最终实现全局一致的全场景监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将基础模型的2D掩码转移到3D来生成伪标签，但由于视频帧被独立处理，导致分割粒度不一致和冲突的3D伪标签，从而降低最终分割的准确性。

Method: 粒度一致的自适应2D掩码跟踪方法，结合三阶段课程学习框架：从碎片化单视图数据到统一多视图标注，再到全局一致的全场景监督。

Result: 实验结果表明，该方法能有效生成一致且准确的3D分割，在标准基准测试中达到最先进水平，并具备开放词汇能力。

Conclusion: 该方法能够从最初碎片化和矛盾的2D先验中稳健地提取出一致的3D表示，解决了现有方法中的一致性问题。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [170] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench是一个用于隐私保护联邦学习的可重现基准测试平台，使用合成肿瘤CT扫描数据评估分割性能和隐私泄露，揭示了隐私与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护隐私敏感数据方面具有重要价值，但现有系统仍面临成员推理攻击和数据异构性等脆弱性问题，需要标准化评估平台。

Method: 开发FedOnco-Bench基准测试，使用合成肿瘤CT扫描数据，评估FedAvg、FedProx、FedBN和FedAvg+DP-SGD等联邦学习方法的分割性能和隐私保护能力。

Result: FedAvg性能最佳(Dice约0.85)但隐私泄露最多(攻击AUC约0.72)，DP-SGD隐私保护最好(AUC约0.25)但性能下降(Dice约0.79)，FedProx和FedBN在异构数据下表现平衡。

Conclusion: FedOnco-Bench为医学图像分割的隐私保护联邦学习方法提供了标准化开源基准测试平台，有助于方法开发和比较。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [171] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: 提出了Med-Banana-50K数据集，这是一个包含5万张图像的医疗图像编辑数据集，涵盖三种模态和23种疾病类型，通过Gemini-2.5-Flash-AI生成双向编辑，并采用医疗质量控制和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医疗图像编辑方面取得显著进展，但缺乏大规模、高质量、开放可访问的医疗图像编辑数据集，特别是具有严格解剖和临床约束的数据集。

Method: 利用Gemini-2.5-Flash-Image从真实医疗图像生成双向编辑（病灶添加和移除），采用LLM-as-Judge进行医疗质量评估，包含指令符合度、结构合理性、真实性和保真度保留等标准，并进行最多五轮迭代优化。

Result: 构建了包含5万张图像的Med-Banana-50K数据集，涵盖胸片、脑部MRI和眼底摄影三种模态，23种疾病类型，还包括3.7万次失败尝试的完整对话记录。

Conclusion: Med-Banana-50K为训练和评估下一代医疗图像编辑模型提供了大规模、医疗验证和完整文档化的基础资源，数据集和代码已公开。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [172] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA是一个基于注意力机制的坐标无关GUI定位框架，通过监督微调激活MLLMs的固有定位能力，在3B参数模型上仅用85k截图就实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MLLMs的GUI定位方法将任务视为基于文本的坐标生成，但直接从视觉输入生成精确坐标具有挑战性且计算量大。直觉上，GUI定位应该先选择与指令相关的视觉块，然后在块内确定精确点击位置。

Method: 提出GUI-AIMA框架，通过多模态注意力对齐将MLLMs的内在多模态注意力与块级定位信号对齐。这些信号通过多头聚合在简化的查询-视觉注意力矩阵上自适应计算。采用坐标无关方式，可轻松集成即插即用的放大阶段。

Result: GUI-AIMA-3B模型仅用85k截图训练，在ScreenSpot-Pro上达到58.6%的平均准确率，在OSWorld-G上达到62.2%的平均准确率，在3B模型中实现了最先进的性能。

Conclusion: 验证了轻量级训练可以触发MLLMs的固有定位能力，GUI-AIMA在数据效率和性能方面都表现出色，为GUI定位任务提供了有效的解决方案。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [173] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 提出TA-LSDiff模型，结合拓扑感知扩散概率模型和水平集能量，无需显式几何演化即可实现胰腺分割，在四个公共数据集上达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 胰腺分割面临尺寸小、对比度低和拓扑变化大的挑战。传统水平集方法忽略点状拓扑效应，而深度学习网络牺牲结构细节，需要结合两者优势。

Method: 结合拓扑感知扩散概率模型和水平集能量，通过四个互补项整合输入图像和深度特征引导隐式曲线演化，并引入像素自适应细化模块通过邻域证据的亲和权重局部调节能量函数。

Result: 在四个公共胰腺数据集上的评估显示TA-LSDiff达到最先进精度，优于现有方法。消融研究系统量化了各组件贡献。

Conclusion: TA-LSDiff为胰腺分割提供了实用且准确的解决方案，成功结合了拓扑感知和水平集能量方法。

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [174] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA是一个新颖的位置编码框架，通过模态特定位置编码和全局自适应编码步长缩放，解决了当前视觉语言模型中位置编码策略不区分文本和视觉模态结构特性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型采用统一的1D或2D位置索引策略，没有考虑文本和视觉token在结构特性和连续性上的差异，限制了模型性能。

Method: 提出OMEGA框架：1）模态特定位置编码（MSPE）在不同坐标维度上为不同模态分配位置索引；2）全局自适应编码步长缩放（GAESS）根据两种模态的嵌入熵自适应调整视觉token的位置编码步长。

Result: OMEGA在各种架构和VQA基准测试中持续提升VLM性能。在视觉密集型任务上，Qwen2.5-VL-3B相比基线位置编码策略提升达3.43%，在Qwen2.5-VL-7B和LLaVA-v1.5-7B等更大模型上也观察到一致增益。

Conclusion: OMEGA通过模态特定的位置编码策略有效提升了视觉语言模型的性能，证明了考虑模态结构差异的重要性。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [175] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为LSSA的新型多模态对抗攻击方法，通过局部图像块随机打乱和采样来增强对抗样本的迁移性，解决了现有方法因输入多样性不足导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言预训练模型虽然在下游任务中表现优异，但对对抗样本仍然脆弱。先前的方法通过跨模态交互提升多模态对抗样本的迁移性，但由于过度依赖单模态信息而缺乏输入多样性，导致过拟合问题。

Method: LSSA攻击方法：1）随机打乱局部图像块以扩展原始图像-文本对；2）生成对抗图像并在其周围采样；3）利用原始图像和采样图像共同生成对抗文本。

Result: 在多个模型和数据集上的广泛实验表明，LSSA显著提升了多模态对抗样本在不同VLP模型和下游任务中的迁移性，并且在大型视觉-语言模型上优于其他先进攻击方法。

Conclusion: LSSA通过引入输入多样性和采样策略，有效解决了多模态对抗攻击中的过拟合问题，显著提升了对抗样本的迁移性能。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [176] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出Visual-Contrast Attention (VCA)作为MHSA的替代方案，通过视觉对比机制减少计算复杂度，在图像识别和生成任务中均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的MHSA层对所有token对进行二次方交互，计算大量消耗在视觉上弱相关或冗余的关联上，需要更高效的注意力机制。

Method: VCA将每个头的密集查询场蒸馏为少量空间池化的视觉对比token，分为可学习的正负流，通过差分交互突出区域间的真正差异，将复杂度从O(N²C)降至O(NnC)。

Result: 在ImageNet-1K上，DeiT-Tiny准确率从72.2%提升至75.6%；在图像生成任务中，FID-50K降低2.1-5.2点；仅增加不到0.3M参数且无需额外FLOPs。

Conclusion: VCA为构建更快更锐利的Vision Transformers提供了简单有效的路径，通过空间池化和双重位置嵌入实现对比推理的最佳协同效果。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [177] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 提出参数插值对抗训练(PIAT)框架，通过插值前后epoch参数来缓解对抗训练中的振荡和过拟合问题，并使用归一化均方误差(NMSE)进一步提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法存在模型鲁棒性振荡和过拟合问题，影响防御效果。

Method: PIAT框架在epoch间插值模型参数，使决策边界变化更平滑；使用NMSE对齐干净样本和对抗样本的logits相对大小。

Result: 在多个基准数据集上的实验表明，该框架能显著提升CNN和ViT的鲁棒性。

Conclusion: PIAT通过参数插值和NMSE有效解决了对抗训练中的振荡和过拟合问题，实现了更高的模型鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [178] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench是首个专门用于评估多模态大语言模型在脑成像分析中多模态理解能力的综合基准，包含15种脑成像模态、9,527个验证问答对和31,706张图像，涵盖15个多阶段临床任务。


<details>
  <summary>Details</summary>
Motivation: 当前脑成像视觉问答基准要么覆盖的成像模态有限，要么仅限于粗粒度的病理描述，无法全面评估MLLMs在整个临床连续体中的表现。

Method: 构建包含15种脑成像模态的综合多模态VQA基准，模拟临床工作流程，包含31,706张图像和9,527个验证问答对，涵盖15个多阶段临床任务。

Result: 评估24个最先进模型发现：(1)专有MLLMs优于开源和医疗模型但落后于医生；(2)医疗MLLMs性能差异大；(3)开源MLLMs整体落后但在特定任务表现优异；(4)MLLMs在复杂术前任务中表现明显不足，存在视觉到临床推理的差距。

Conclusion: OmniBrainBench为评估和推进MLLMs在脑成像分析中设立了新标准，揭示了与专家临床推理相比的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [179] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种遮挡感知扩散模型（ODM），用于在遮挡场景下预测行人过街意图，通过重建被遮挡的运动模式来指导未来意图预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在预测行人过街意图方面取得了显著成功，但很少考虑遮挡场景下的不完整观察问题。

Method: 采用遮挡感知扩散变换器架构，在去噪阶段估计与遮挡模式相关的噪声特征；引入遮挡掩码引导的反向过程，有效利用观察信息减少预测误差积累。

Result: 在PIE和JAAD基准数据集上的广泛实验表明，该方法在各种遮挡场景下比现有方法具有更鲁棒的性能。

Conclusion: 提出的ODM方法能够有效处理遮挡场景下的行人意图预测问题，通过重建被遮挡的运动模式提升了预测准确性。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [180] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: LMD是一种后处理、模型无关的可解释性方法，能够分解预训练融合模型中各层的模态特定信息，用于自动驾驶感知模型的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，感知模型的决策透明度至关重要，因为即使是单个误判也可能导致灾难性后果。多传感器输入时，由于传感器信息在融合网络中纠缠，难以确定每个模态对预测的贡献。

Method: 提出层间模态分解(LMD)方法，通过后处理方式解耦预训练融合模型中所有层的模态特定信息，支持相机-雷达、相机-LiDAR和相机-雷达-LiDAR等多种传感器融合设置。

Result: 在预训练融合模型上评估LMD，通过结构化扰动指标和模态可视化分解验证其有效性，证明该方法适用于解释高容量多模态架构。

Conclusion: LMD是首个能够将感知模型预测归因于自动驾驶传感器融合系统中单个输入模态的方法，为多模态融合模型提供了实用的可解释性工具。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [181] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo是一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位，通过类型化边建模不同的辩论关系，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法受限于数据库覆盖范围和质量，而现有的大视觉语言模型在复杂地理场景中表现不佳，多智能体系统缺乏有效处理冲突预测的机制。

Method: 提出异构图神经网络建模多样化的辩论关系，包括支持性协作、竞争性论证和知识传递；引入双级辩论机制，结合节点级细化和边级论证建模；采用跨级拓扑优化策略实现图结构和智能体表示的共同演化。

Result: 在多个基准测试中，GraphGeo显著优于现有最先进方法。

Conclusion: 该框架通过结构化辩论将智能体间的认知冲突转化为增强的地理定位精度。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [182] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL是一个统一的多模态大语言模型框架，专门用于处理异构医学数据（2D图像、3D体积扫描、时序视频），通过数据中心的预训练、微调和评估策略，在多个医学视觉理解基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有异构性（包含2D图像、3D体积扫描、时序视频等多种模态），存在显著的领域差距和数据格式不一致问题，阻碍了统一医学MLLMs的发展。

Method: 采用三种关键策略：1）整合自然和医学特定领域的长上下文数据进行预训练扩展；2）使用罕见医学数据（包括整体视频分析和代表性不足的2D模态）进行微调补充；3）扩展评估框架以纳入3D体积和视频理解基准。通过监督微调和组相对策略优化开发多个模型规模。

Result: 在多个基准测试中（包括医学VQA、视频QA和3D医学图像理解）实现了最先进的性能。

Conclusion: Fleming-VL是一个有效的统一端到端框架，能够处理异构医学模态，推动了医学AI的透明、可重现和可审计进展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [183] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 提出动态多级加权对齐网络解决零样本草图图像检索问题，通过多级权重对齐和加权四元组损失提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在模态样本不平衡和低质量信息不一致问题，导致性能不佳

Method: 包含三个模块：单模态特征提取（CLIP文本编码器和ViT）、跨模态多级加权（局部和全局聚合块生成对齐权重）、加权四元组损失（改进领域平衡）

Result: 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上优于现有最先进的ZS-SBIR方法

Conclusion: 该方法通过动态多级加权对齐有效解决了零样本草图图像检索中的模态不平衡问题，取得了优越性能

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [184] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR是一个端到端的虚拟试穿模型，仅需源图像和目标服装作为输入，无需复杂预处理，通过两阶段训练策略和参考图像机制提升试穿准确性和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖复杂输入（如不可知人像、人体姿态、密集姿态或关键点），导致实施困难且不实用。EVTAR旨在简化输入要求，提高现实应用可行性。

Method: 采用两阶段训练策略，仅需源图像和目标服装输入，无需掩码、密集姿态或分割图。利用额外参考图像（不同人穿着相同服装）来更好地保留服装纹理和细节。

Result: 在两个广泛使用的基准测试和多样化任务上评估，结果一致验证了方法的有效性。

Conclusion: EVTAR通过简化输入要求和引入参考图像机制，实现了更实用和高质量的虚拟试穿效果，模拟了人类选择服装时的参考行为。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [185] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一个统一的推理框架，通过链式测试时推理过程连接时间检测、空间定位和文本解释任务，实现零样本视频异常分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常研究大多停留在帧级检测，缺乏空间和语义上下文，无法解释异常原因。现有方法虽然提高了可解释性，但仍依赖数据和特定任务。

Method: 基于链式测试时推理过程，通过任务内推理优化时间检测，任务间链接实现空间和语义理解，无需额外训练。

Result: 在多个视频异常检测、定位和解释基准测试中实现了最先进的零样本性能，无需额外数据或梯度更新。

Conclusion: 精心设计的提示与任务链式连接能够释放基础模型的推理能力，实现实用、可解释的零样本视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [186] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一个专门用于2D血管分割的高效框架，通过集成卷积适配器、多提示编码器和轻量级掩码解码器，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的血管分割对临床诊断和手术规划至关重要，但现有基础模型如SAM在血管结构上表现不佳，需要专门优化的解决方案。

Method: VesSAM框架包含三个核心组件：卷积适配器增强局部纹理特征；多提示编码器融合解剖学提示（骨架、分叉点、段中点）；轻量级掩码解码器减少锯齿伪影。

Result: 实验结果显示VesSAM在8个数据集上比最先进的PEFT-based SAM变体提升超过10%的Dice和13%的IoU，参数更少且泛化能力更强。

Conclusion: VesSAM为血管分割提供了高效且泛化性强的解决方案，在保持竞争力的同时显著减少了参数数量。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [187] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的自监督多模态迭代去噪框架MID，该框架通过将噪声数据建模为非线性噪声积累过程中的状态，迭代学习噪声步骤和噪声增量，无需配对干净-噪声数据集即可有效去除复杂非线性噪声。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据经常被复杂的非线性噪声污染，使得传统的基于规则的去噪方法不足。需要开发能够处理复杂非线性噪声且无需配对训练数据的去噪方法。

Method: MID框架将噪声数据建模为非线性噪声积累过程中的状态，通过迭代引入更多噪声来学习两个神经网络：一个估计当前噪声步骤，另一个预测并减去相应的噪声增量。对于复杂非线性污染，使用一阶泰勒展开局部线性化噪声过程。

Result: 在四个经典计算机视觉任务上的实验表明MID具有鲁棒性、适应性和最先进的性能。在生物医学和生物信息学领域的任务中也表现出强大的性能和适应性。

Conclusion: MID是一种有效的自监督去噪框架，能够处理复杂非线性噪声，无需配对数据集，在多个领域都表现出优异的性能。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [188] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态机器学习框架，将相同的特征提取技术应用于戈雅画作的视觉图像和X射线图像，在艺术认证中实现了97.8%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 戈雅作品的艺术认证面临复杂计算挑战，包括其异质性的风格演变和广泛的历史伪造模式。

Method: 使用统一特征提取流水线（包括灰度共生矩阵描述符、局部二值模式、熵度量、能量计算和颜色分布分析），通过优化的单类支持向量机处理视觉和X射线图像特征。

Result: 在24幅认证戈雅画作数据集上，使用80/20训练测试配置和10折交叉验证，框架达到97.8%分类准确率和0.022假阳性率。案例研究显示对《巨人》作品认证置信度达92.3%。

Conclusion: 多模态方法相比单模态方法有显著性能提升，证明在艺术认证应用中同时对视觉和放射图像应用相同计算方法的有效性。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [189] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: 提出HyFormer-Net混合CNN-Transformer模型，用于乳腺癌超声图像的同时分割和分类，具有内在可解释性，在BUSI数据集上表现优异，并通过交叉数据集研究验证了泛化能力。


<details>
  <summary>Details</summary>
Motivation: B超乳腺癌诊断面临斑点噪声、操作者依赖性和边界模糊等挑战，现有深度学习方法存在单任务学习、架构限制（CNN缺乏全局上下文、Transformer缺乏局部特征）和黑盒决策等问题，阻碍了临床应用。

Method: HyFormer-Net采用双分支编码器集成EfficientNet-B3和Swin Transformer，通过多尺度分层融合模块结合两者优势，使用注意力门控解码器提供精确性和可解释性，并引入双管道可解释性机制。

Result: 在BUSI数据集上达到Dice分数0.761±0.072和准确率93.2%，优于U-Net、Attention U-Net和TransUNet。恶性召回率92.1±2.2%确保最小假阴性。集成模型达到Dice 90.2%、准确率99.5%和100%恶性召回率。交叉数据集研究中，使用50%目标域数据达到77.3% Dice，超过源域性能。

Conclusion: HyFormer-Net成功解决了乳腺癌超声诊断的关键挑战，提供了高性能和内在可解释性，并通过交叉数据集验证证明了其泛化能力，为临床采用奠定了基础。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [190] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost是一种参数高效的神经网络架构，通过动态缩放渐进注意力机制在CIFAR基准测试中达到最先进性能，实现了参数减少2.1倍的同时准确率提升3.2个百分点。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备部署深度学习模型时面临的参数效率与准确率之间的权衡问题，旨在实现不损失准确率的前提下大幅减少模型参数。

Method: 采用动态缩放渐进注意力机制，包含自适应融合、阶段缩放和残差自适应三个核心创新，结合增强的MBConv模块构建双注意力路径架构。

Result: 在CIFAR-10上达到95.57%准确率（0.85M参数）和93.80%（0.37M参数），在CIFAR-100上达到81.37%准确率（0.92M参数）和74.85%（0.44M参数），相比MobileNetV3参数减少2.1倍且准确率提升3.2个百分点。

Conclusion: FastBoost通过动态注意力与高效卷积操作的协同优化，实现了前所未有的参数-准确率权衡，为资源受限边缘设备的深度学习部署提供了可行解决方案。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [191] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 该论文提出了T-MLA攻击框架，这是首个针对神经图像压缩(NIC)的目标多尺度对数-指数攻击方法，通过在小波域中精心设计对抗扰动来显著降低重建图像质量，同时保持视觉不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有对NIC的对抗攻击大多是像素空间方法的简单适配，忽视了压缩管道的独特结构化特性。作者旨在揭示NIC系统中更深层次的安全漏洞。

Method: 提出T-MLA攻击框架，在小波域中直接针对攻击后重建图像的质量来构建对抗扰动，将扰动策略性地限制在特定小波子带中，实现最大化失真同时保证感知隐蔽性。

Result: 在多个最先进NIC架构上的广泛评估显示，重建质量大幅下降而扰动仍保持视觉不可感知，揭示了生成和内容传输管道核心的安全缺陷。

Conclusion: 该研究揭示了神经图像压缩系统存在的关键安全漏洞，表明需要开发更鲁棒的压缩方法来抵御此类针对性攻击。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [192] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出一种分层序列预测方法用于图像地理定位，使用S2网格单元从粗到细逐步预测位置，借鉴语言模型的自回归生成思想，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位中因视觉相似性和大搜索空间带来的挑战，模仿人类从大区域到具体地址的定位方式。

Method: 使用S2网格单元构建分层结构，采用自回归方式逐步预测更精细的网格单元，结合beam search和多样本推理等测试时计算扩展技术。

Result: 在Im2GPS3k和YFCC4k数据集上，无MLLM时比可比基线提升13.9%，有MLLM时在所有指标上均达到最优。

Conclusion: 分层自回归预测方法能有效处理地理定位问题，结合测试时计算策略可显著提升性能。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [193] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I是一个用于网络切片特征可视化研究的合成数据集，将多变量KPI向量通过四种编码方法转换为视觉表示，包含12万样本，适用于视觉学习和网络状态分类等任务。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络中网络切片需要精细的识别方法，但缺乏支持稳健数据集。该研究旨在为下一代网络系统提供网络切片特征可视化的研究数据集。

Method: 使用四种编码方法将多变量KPI向量转换为RGB图像：物理启发映射、Perlin噪声、神经壁纸和分形分支。每种方法生成3万个样本，模拟真实网络条件和噪声。

Result: 创建了包含12万个样本的数据集，每个样本包含原始KPI向量和对应的低分辨率RGB图像，公开可用。

Conclusion: SliceVision-F2I数据集适用于视觉学习、网络状态分类、异常检测等任务，可在多变量时间序列分析和特征到图像转换等研究场景中重用。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [194] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 提出一种结合Epanechnikov核密度估计和双峰逻辑回归分类器的新方法，用于基于医学图像诊断呼吸系统疾病。


<details>
  <summary>Details</summary>
Motivation: 利用EKDE在建模数据分布时的灵活性（无需假设特定形状）和对像素强度变化的适应性，从医学图像中提取关键特征，提高呼吸系统疾病的诊断准确性。

Method: 采用基于统计模型的学习方案，结合Epanechnikov非参数核密度估计（EKDE）和双峰逻辑回归分类器。在COVID-19放射影像数据集的13808张随机选择的胸部X光片上进行了测试。

Result: 准确率70.14%，敏感性59.26%，特异性74.18%。在检测呼吸系统疾病方面表现出中等性能，但敏感性方面还有改进空间。

Conclusion: 虽然临床专业知识对于进一步改进模型仍然至关重要，但本研究强调了基于EKDE的方法在提高医学影像诊断准确性和可靠性方面的潜力。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [195] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 提出ViACT框架，将解剖先验整合到视频transformer中，通过掩码自编码仅重建解剖区域，提高超声心动图分析的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视频transformer在超声心动图分析中容易学习非诊断区域（如图像背景）的虚假相关性，需要克服这一限制。

Method: 将变形解剖结构表示为点集，编码其空间几何和对应图像块为transformer token，采用掩码自编码策略仅重建解剖区域。

Result: ViACT能够将transformer注意力集中在心肌区域，生成与已知病理区域对齐的可解释注意力图，并能泛化到心肌点跟踪任务。

Conclusion: ViACT通过整合解剖约束，显著提升了超声心动图分析的性能和可解释性，且具有良好的泛化能力。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [196] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 本文提出在边缘计算中使用嵌入式GPU设备来提升计算机视觉应用的性能，解决移动设备资源受限问题。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的计算机视觉和增强现实应用对资源需求高，边缘计算虽然能卸载计算任务，但边缘设备容量有限，影响用户体验。

Method: 在边缘计算环境中使用带有图形处理单元(GPU)的嵌入式设备来处理高强度的计算机视觉任务。

Result: 实验表明，与仅使用CPU相比，GPU能够获得显著的性能提升。

Conclusion: 使用GPU的嵌入式设备能够为计算机视觉应用用户提供更好的体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [197] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种无需概念标注的弱监督概念预测框架PCP，通过类别级概念先验作为弱监督，在医学影像中实现可解释预测，性能优于零样本基线。


<details>
  <summary>Details</summary>
Motivation: 医学影像中可解释AI需要概念标注，但获取成本高昂。现有零样本方法难以捕捉医学领域特征，可靠性差。

Method: PCP框架利用类别级概念先验作为弱监督，结合KL散度和熵正则化进行预测精炼，无需显式监督或语言模型。

Result: 在PH2和WBCatt数据集上，概念级F1分数比零样本基线提高33%以上，在四个医学数据集上分类性能与全监督方法相当。

Conclusion: PCP提供了一种无需标注的可解释预测方法，在医学影像中实现了可靠的概念级预测性能。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [198] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv是一种用于惯性传感器人体活动识别的类别等变神经网络，通过编码时间、幅度和结构对称性来提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的HAR方法在处理分布外扰动时鲁棒性不足，需要一种能够系统性地编码数据对称性结构的方法来提升模型的泛化性能。

Method: 引入类别对称性乘积，结合循环时间平移、正增益和传感器层次偏序集来捕捉数据的类别对称性结构，构建等变神经网络。

Result: 在UCI-HAR数据集上，CatEquiv在分布外扰动下显著优于循环填充CNN和普通CNN，表现出更高的鲁棒性。

Conclusion: 强制实施类别对称性可以在不增加模型容量的情况下实现强大的不变性和泛化能力。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [199] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出MicroAUNet，一种轻量级注意力分割网络，结合深度可分离扩张卷积和通道-空间注意力块，通过两阶段知识蒸馏实现高精度实时结肠息肉分割


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习息肉分割模型在临床应用中存在的问题：要么分割边缘模糊影响临床决策，要么模型复杂度过高无法满足实时内窥镜应用需求

Method: 使用深度可分离扩张卷积和单路径参数共享的通道-空间注意力块增强多尺度边界特征，并采用渐进式两阶段知识蒸馏从高容量教师模型迁移语义和边界信息

Result: 在基准测试中展现出最先进的准确率，同时保持极低的模型复杂度，适合实时临床息肉分割应用

Conclusion: MicroAUNet在保证高精度的同时实现了轻量化设计，能够满足结肠内窥镜实时分割的临床需求

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [200] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER是一个评估统一多模态模型跨模态推理能力的新基准，包含1312个任务和1876张图像，重点关注文本和视觉模态之间的相互引导和验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将多模态能力孤立对待，缺乏对跨模态相互推理能力的测试，而这是实现真正统一多模态智能的核心能力。

Method: 构建包含两个互补设置的人工标注基准：1）语言增强的视觉生成推理；2）视觉增强的语言生成推理，通过17个统一模型进行实验验证。

Result: 发现跨模态推理决定视觉生成质量，交织模型显著优于非交织模型；模型在物理和符号推理间存在分离，能解释感知概念但无法构建符号任务的视觉抽象。

Conclusion: 相互跨模态推理是实现真正全模态生成的关键前沿，现有模型在此能力上仍有显著不足。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [201] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 该论文提出了一个从YouTube视频自动提取动物视频片段的流程，构建了包含30K视频（200万帧）的大规模数据集，并创建了Animal-in-Motion基准用于4D动物重建任务评估。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模有限（仅2.4K个15帧片段），缺乏动物中心3D/4D任务所需的关键处理，需要开发非侵入性的大规模数据收集方法。

Method: 开发自动化流程从YouTube挖掘视频并处理成对象中心片段，附带姿态估计、跟踪和3D/4D重建等下游任务的有用注释。创建包含230个手动筛选序列的Animal-in-Motion基准。

Result: 收集了30K视频（200万帧），比先前工作多一个数量级。在Animal-in-Motion基准上评估发现，基于模型的方法在2D指标上表现更好但3D形状不真实，而无模型方法产生更自然的重建但得分较低。

Conclusion: 通过提出的流程、基准和基线方法，旨在推进从野外视频进行大规模、无标记的4D动物重建及相关任务的发展。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [202] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 提出基于图像小波谱的扩散Transformer超分辨率模型(DTWSR)，通过多级离散小波变换分解图像，使用金字塔标记化方法嵌入小波谱，设计双解码器处理低频和高频子带，实现更一致和真实的超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的超分辨率方法大多忽略了多尺度频率子带间的相互关系，导致重建图像存在不一致性和不自然伪影。

Method: 使用多级离散小波变换(MDWT)分解图像为小波谱，提出金字塔标记化方法将谱嵌入为token序列，设计双解码器分别处理低频和高频子带，结合扩散模型和Transformer捕获多尺度频率子带间的关系。

Result: 在多个基准数据集上的广泛实验表明，该方法在感知质量和保真度方面都表现出高性能。

Conclusion: DTWSR模型通过有效捕获多尺度频率子带间的相互关系，实现了更一致和真实的超分辨率图像重建。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [203] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出了GCN-PSN框架，利用图卷积网络建模人体骨架拓扑结构，通过孪生架构和对比回归目标学习判别性姿态嵌入，在动作质量评估任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估需要精细理解人体运动和精确评估姿态相似性，传统基于坐标的方法难以有效捕捉骨架拓扑结构信息。

Method: 使用拓扑感知的图卷积网络将人体骨架建模为图结构，采用孪生架构和对比回归目标训练，学习对拓扑敏感的判别性姿态嵌入。

Result: 在AQA-7和FineDiving基准测试中超越了基于坐标的基线方法，取得了有竞争力的性能表现。

Conclusion: 实验验证了利用骨架拓扑结构进行姿态相似性和动作质量评估的有效性，图卷积网络能够更好地捕捉人体运动的拓扑特征。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [204] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个新颖的分层运动生成框架，通过粗到细的可扩展生成过程增强VQ-GT范式，在Motion-X数据集上实现FID 0.06的SOTA性能，推理时间减少27%。


<details>
  <summary>Details</summary>
Motivation: 传统方法在文本驱动的3D人体运动生成中效率较低，需要改进VQ-GT范式以提升生成质量和速度。

Method: 提出多尺度令牌保留策略(MTPS)和分层残差向量量化变分自编码器(RQ-VAE)，结合可扩展自回归建模(SAR)和轻量级卷积注意力混合VQ-VAE(CAQ-VAE)。

Result: 在Motion-X数据集上FID达到0.06（优于MoMask的0.20），推理时间减少27%，在下游任务如运动编辑中表现良好。

Conclusion: MoSa在3D人体运动生成中实现了SOTA的生成质量和效率，具有良好的泛化能力。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [205] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一个多模态视觉-语言-动作模型，通过整合红外相机、毫米波雷达和麦克风阵列等新型传感模态，超越了仅依赖RGB相机的限制，显著提升了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型主要依赖RGB相机，限制了感知能力和操作能力。需要整合更多传感模态来增强物理空间智能。

Method: 提出传感器掩码图像的统一表示方法，将传感器数据以空间基础且物理意义明确的掩码叠加到RGB图像上。基于RGB预训练的VLA骨干网络构建多感官架构，使用轻量级传感器投影器进行数据高效学习。

Result: 在需要传感器模态感知的挑战性真实世界任务中，OmniVLA达到84%的平均任务成功率，分别比仅RGB和原始传感器输入基线模型高出59%和28%，同时展现出更高的学习效率和更强的泛化能力。

Conclusion: OmniVLA通过多模态传感器整合显著提升了VLA模型的感知和操作能力，证明了传感器掩码图像表示的有效性，为机器人操作任务提供了更强大的空间智能。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [206] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 本文针对印度食品VQA系统，提出通过自动生成推理链来改进答案准确性，使用强化学习训练小模型，在基准上平均提升10%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统偏向西方食品，无法处理印度食品的复杂文化和烹饪背景，需要多步推理过程。

Method: 创建自动验证的推理链，对小规模LLM和VLM进行微调，并使用强化学习进行训练。

Result: 通过推理链增强，在印度食品VQA任务上平均准确率提升10个百分点。

Conclusion: 推理链能有效提升印度食品VQA系统的性能，特别是在理解复杂烹饪背景和食物关系方面。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [207] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 本文探索了使用翻转数据预训练和微调的方法来改进自动驾驶模型的领域适应性，特别是针对左舵驾驶条件。研究发现翻转数据预训练单独使用会降低性能，但结合微调后能显著提升模型对左舵驾驶环境的适应能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要良好的领域适应性来应对不同的道路条件。本文旨在研究如何通过训练方法改进PilotNet模型对左舵驾驶条件的适应性。

Method: 评估了四种训练方法：1) 基于美国右舵数据的基线模型；2) 基于翻转美国数据的模型；3) 在美国数据上预训练后在澳大利亚高速公路上微调；4) 在翻转美国数据上预训练后在澳大利亚高速公路上微调。使用显著性分析来测量注意力转移。

Result: 单独使用翻转数据预训练会降低预测稳定性，但随后进行微调能显著改善适应性，导致更低的预测误差和更强的左侧线索关注。在ResNet上的实验也证实了类似的适应趋势。

Conclusion: 翻转数据预训练结合微调能有效改善模型适应性，且需要最少的重新训练要求，强调了预处理技术的重要性。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [208] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文回顾了自动语音驱动3D手势生成中的人类评估实践，发现缺乏标准化和频繁使用有缺陷的实验设置，导致无法比较不同方法或了解最新技术水平。为解决这些问题，作者提出了BEAT2数据集的人类评估协议，并对6个最新手势生成模型进行了大规模众包评估。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音驱动3D手势生成领域的人类评估实践缺乏标准化，实验设置存在缺陷，使得无法准确比较不同方法的性能或确定最新技术水平。

Method: 作者提出了BEAT2数据集的人类评估协议，并对6个最新手势生成模型进行了大规模众包评估，评估维度包括运动真实性和语音-手势对齐度。

Result: 评估结果显示：1）新模型并不总是优于早期方法；2）已发表的高运动真实性或语音-手势对齐度声明在严格评估下可能不成立；3）领域需要采用解耦的运动质量和多模态对齐评估来进行准确基准测试。

Conclusion: 该研究强调了标准化评估的重要性，并发布了大量合成运动数据、渲染视频刺激和人类偏好投票数据，以推动标准化和新的评估研究。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [209] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个包含1333个英语Rebus谜题的大型多样化基准数据集，并开发了RebusDescProgICE框架，通过结合非结构化描述和基于代码的结构化推理，显著提升了视觉语言模型在Rebus谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题需要图像识别、认知技能、常识推理、多步推理和基于图像的文字游戏等多种能力，这对当前的视觉语言模型具有挑战性。

Method: 提出了RebusDescProgICE框架，结合非结构化描述和基于代码的结构化推理，并改进了基于推理的上下文示例选择方法。

Result: 相比Chain-of-Thought推理，该框架在闭源模型上提升了2.1-4.1%，在开源模型上提升了20-30%的性能。

Conclusion: RebusDescProgICE框架有效提升了视觉语言模型在复杂Rebus谜题上的推理能力，特别是在开源模型上取得了显著改进。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [210] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出Eyes on Target框架，通过将人眼注视特征注入Vision Transformer的注意力机制，在自我中心视频中实现基于视觉注意力的目标检测。


<details>
  <summary>Details</summary>
Motivation: 人类注视为理解复杂视觉环境中的注意力提供了丰富的监督信号，传统目标检测器对所有区域同等对待，而人类注视可以指导模型关注更重要的区域。

Method: 将注视特征注入ViT的注意力机制，偏向人类关注区域的空间特征选择，并引入注视感知的注意力头重要性指标来解释模型行为。

Result: 在自定义模拟器数据集和公共基准测试（Ego4D Ego-Motion和Ego-CH-Gaze）上，相比无视注视的基线方法，检测准确率持续提升。

Conclusion: 该方法有效利用注视信号增强目标检测性能，在模拟场景中评估人类表现具有潜力，并揭示了注视线索如何调节transformer注意力动态。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [211] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的黑盒梯度可迁移攻击方法AFA，通过对抗平坦性(AF)解决欺骗性平坦问题，结合蒙特卡洛对抗采样(MCAS)提升攻击能力，在ImageNet数据集上优于6个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有可迁移攻击方法虽然关注平坦损失，但仍陷入次优区域（特别是平坦但尖锐的欺骗性平坦区域），导致对抗样本可迁移性不足。

Method: 从双阶信息角度提出对抗平坦性(AF)理论，设计AFA攻击方法解决梯度符号改变问题，并使用MCAS提升内环采样效率。

Result: 在ImageNet兼容数据集上优于6个基线方法，生成的对抗样本位于更平坦区域，跨模型架构的可迁移性显著提升，在输入变换攻击和百度云API测试中也表现优异。

Conclusion: 提出的AFA方法通过对抗平坦性和高效采样机制，有效解决了欺骗性平坦问题，显著提升了黑盒可迁移攻击的性能。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [212] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 本研究提出了Viewpoint Learning任务来评估和改进多模态大语言模型的空间推理能力，通过Viewpoint-100K数据集和两阶段微调策略，显著提升了模型在3D推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在2D视觉理解方面取得了显著进展，但它们在捕捉复杂3D推理所需的空间信息方面仍存在不足，特别是跨视角一致性这一关键要求。

Method: 使用Viewpoint-100K数据集（包含10万对物体中心图像和问答对），采用两阶段微调策略：首先通过监督微调注入基础知识，然后使用GRPO算法进行强化学习以增强泛化能力，并引入混合冷启动初始化方法。

Result: 实验结果表明，该方法显著激活了MLLM的空间推理能力，在领域内和领域外推理任务上的性能均有明显提升。

Conclusion: 开发MLLM的基础空间技能对于推动机器人、自主系统和3D场景理解的未来发展具有重要价值。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [213] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM是一个用于脑部病灶分割的端到端框架，通过冻结预训练主干网络并仅训练轻量级适配器实现高效微调，采用创新的3x3角-轴-中心短序列扫描策略和内存驱动的结构提示生成器，在公共基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 脑部病灶分割面临小病灶、低对比度、各向异性采样和跨切片不连续性等挑战，现有方法在这些问题上表现不佳。

Method: 1) CenterMamba编码器采用3x3角-轴-中心短序列扫描策略；2) 内存驱动的结构提示生成器维护跨相邻切片的原型库；3) 内存增强的多尺度解码器集成多级内存注意力模块。

Result: 在公共基准测试中，CenterMamba-SAM实现了脑部病灶分割的最先进性能。

Conclusion: 该框架通过中心优先、轴强化和对角线补偿的信息聚合，增强了对弱边界和微小病灶的敏感性，同时保持稀疏而有效的特征表示，显著提升了脑部病灶分割的准确性和鲁棒性。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [214] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种轻量级几何感知适配器，通过方位对齐和水平循环填充来保护邻域连续性，使用局部K近邻计算几何特征，在训练时驱动区域感知正则化以稳定结构脆弱区域的预测。


<details>
  <summary>Details</summary>
Motivation: LiDAR语义分割在恶劣天气下性能下降，因为折射、散射和点丢失会破坏几何结构。现有方法忽略了边界、角落和稀疏区域的结构脆弱性。

Method: 使用方位对齐和水平循环填充保持邻域连续性，局部窗口K近邻收集附近点并计算简单统计量，压缩为紧凑的几何感知线索，在训练时驱动区域感知正则化。

Result: 在跨天气设置下，相比数据增强基线提升mIoU 7.9个百分点，相比类别中心正则化基线提升0.6个百分点。

Conclusion: 几何驱动的正则化是全天气LiDAR分割的关键方向，该适配器即插即用，训练时启用，推理成本可忽略。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [215] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream是一个实时运动条件视频生成系统，通过将双向教师模型蒸馏为因果学生模型，实现亚秒级延迟和高达29 FPS的流式生成，支持无限长度视频的实时交互。


<details>
  <summary>Details</summary>
Motivation: 现有运动条件视频生成方法存在分钟级延迟和非因果处理问题，无法实现实时交互。需要开发能够实时生成高质量视频的系统。

Method: 1. 增强文本到视频模型以支持运动控制；2. 通过自强制分布匹配蒸馏将双向教师蒸馏为因果学生；3. 引入滑动窗口因果注意力与注意力汇；4. 使用自展开和KV缓存滚动训练模拟推理时外推。

Result: 在运动跟随和视频质量方面达到最先进水平，生成速度比现有方法快两个数量级，能够以恒定速度生成任意长度视频，实现实时交互体验。

Conclusion: MotionStream通过创新的蒸馏方法和因果注意力机制，成功解决了实时视频生成的挑战，为运动控制视频生成提供了实用的交互解决方案。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [216] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor是一个基于先验引导的颜色转换器，通过学习明清时期绘画来恢复唐宋时期古画的色彩，通过亮度增强和色调校正两个子任务实现色彩恢复。


<details>
  <summary>Details</summary>
Motivation: 中国古代绘画是宝贵的文化遗产，但受到不可逆的色彩退化影响。由于复杂的化学机制和缺乏高质量数据集，色彩恢复非常困难，阻碍了端到端数字修复工具的开发。

Method: 将色彩恢复分解为亮度增强和色调校正两个顺序子任务：使用两个变分U-Net和多尺度映射模块进行亮度增强；设计双分支颜色查询模块，在局部色调先验引导下进行色调校正。

Result: 与最先进的着色方法进行广泛实验，在定量和定性评估中都表现出优越性能。

Conclusion: PRevivor能够有效恢复古画色彩，为文化遗产保护提供了有效的数字修复工具。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [217] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 这篇综述论文系统评估了基础模型在医学影像分析中的适应策略，包括监督微调、参数高效微调、自监督学习等方法，并指出了当前面临的领域偏移、数据稀缺等挑战，同时提出了持续学习、联邦学习等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像分析中具有巨大潜力，但实际临床应用仍面临领域偏移、高质量标注数据稀缺、计算资源需求和隐私保护等关键挑战，需要开发有效的适应策略。

Method: 论文采用系统性综述方法，评估了监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态框架等多种适应策略。

Result: 研究发现不同适应策略在性能提升、临床适用性和局限性方面各有优劣，现有技术仍存在未解决的挑战和权衡问题。

Conclusion: 论文为开发适应性强、可信赖且能集成到临床实践的基础模型提供了路线图，强调了持续学习、隐私保护方法和系统性基准测试等未来研究方向的重要性。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [218] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出了一种基于数据流形几何差异的图像生成检测框架，利用自然图像和生成图像在流形结构上的正交性，通过自监督模型的损失变化来识别生成图像。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像真实感的提升，迫切需要可靠的检测方法。现有方法依赖大量生成图像训练二元分类器，存在数据依赖性强的问题。

Method: 利用自然图像和生成图像数据流形的几何差异，设计一对函数使其对自然图像输出一致而对生成图像输出发散，通过自监督模型在图像变换时的损失变化进行检测，并使用归一化流放大可检测差异。

Result: 大量实验证明了该方法的有效性，代码已开源。

Conclusion: 该方法提供了一种简单有效的生成图像检测方案，不依赖大量生成图像数据，能够应对先进生成模型带来的挑战。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [219] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了UniREditBench基准测试，用于评估基于推理的图像编辑模型性能，包含2700个样本，涵盖真实和游戏世界场景，并引入多模态双参考评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在处理需要隐式推理的复杂图像编辑任务时表现不佳，现有基准主要关注单对象属性变换，忽略了多对象交互和游戏场景，且仅依赖文本参考评估可能导致误判。

Method: 构建包含2700个样本的统一基准，涵盖8个主要维度和18个子维度；引入多模态双参考评估（文本和真实图像参考）；设计自动化数据合成流程，创建包含10万样本的大规模合成数据集UniREdit-Data-100K，并基于此微调Bagel模型。

Result: 开发的UniREdit-Bagel模型在领域内和领域外设置下均取得显著改进；通过对开源和闭源图像编辑模型的全面基准测试，揭示了它们在不同方面的优势和弱点。

Conclusion: UniREditBench为基于推理的图像编辑提供了系统评估框架，多模态双参考评估提高了可靠性，UniREdit-Bagel展示了在复杂推理任务上的优越性能。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [220] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出REASON框架，通过两阶段概率图引导的双分支融合方法，自动评估胃内容物以进行术前误吸风险评估，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统胃内容物评估方法依赖手动追踪和经验公式，在效率和准确性方面存在显著局限性，需要更自动化和准确的解决方案。

Method: 两阶段框架：第一阶段使用分割模型生成抑制伪影、突出胃解剖结构的概率图；第二阶段通过双分支分类器融合右侧卧位和仰卧位两个标准视图的信息，提升特征判别能力。

Result: 在自收集数据集上的实验结果表明，该框架显著优于当前最先进方法，在胃内容物评估方面表现出色。

Conclusion: 该框架为自动化术前误吸风险评估提供了更稳健、高效和准确的解决方案，在临床实践中具有巨大应用潜力。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [221] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种用于全玻片病理图像表示的三阶段解缠学习框架，通过潜在因子分组、聚类推理实例解缠和实例效应重加权来解决空间、语义和决策纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 多实例学习(MIL)在全玻片病理图像表示中应用广泛，但实例间的空间、语义和决策纠缠限制了其表示能力和可解释性。

Method: 三阶段方法：1)正半定潜在因子分组映射实例到潜在子空间缓解空间纠缠；2)实例概率反事实推理和聚类推理实例解缠缓解语义纠缠；3)广义线性加权决策通过实例效应重加权解决决策纠缠。

Result: 在多中心数据集上的广泛实验表明，该模型优于所有最先进模型，并通过解缠表示和透明决策过程实现了病理学家对齐的可解释性。

Conclusion: 该框架有效解决了MIL中的空间、语义和决策纠缠问题，在保持高性能的同时实现了病理学家级别的可解释性。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [222] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出了一种名为APDM的新框架，通过将保护目标从图像转移到扩散模型本身来防止特定主体的个性化，解决了现有对抗扰动方法在存在干净图像或简单图像变换时失效的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在特定主体（如身份或物体）的高质量合成方面取得了进展，但这种能力也带来了严重的隐私风险，个性化技术可能被恶意用户滥用以生成未经授权的内容。现有基于对抗扰动的保护方法依赖不现实的假设，在少量干净图像或简单图像变换下就会失效。

Method: 提出了Anti-Personalized Diffusion Models (APDM)框架，包括：1）理论分析显示现有损失函数无法确保鲁棒反个性化的收敛；2）引入Direct Protective Optimization (DPO)损失函数，在不影响生成质量的前提下有效干扰目标模型的主体个性化；3）提出Learning to Protect (L2P)双路径优化策略，通过在个性化和保护路径之间交替，模拟未来个性化轨迹并在每一步自适应地加强保护。

Result: 实验结果表明，该框架优于现有方法，在防止未经授权的个性化方面达到了最先进的性能。

Conclusion: APDM框架通过将保护目标转移到扩散模型本身，并引入DPO损失函数和L2P优化策略，有效解决了现有方法在现实场景中的局限性，为防止恶意个性化提供了更可靠的解决方案。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [223] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba是首个基于Mamba架构的多视角立体视觉网络，通过动态Mamba模块实现高效的全局特征聚合，在DTU和Tanks-and-Temples基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的MVS方法存在二次复杂度问题，难以平衡性能与效率。Mamba架构具有全局建模能力和线性复杂度，为解决这一问题提供了新思路。

Method: 提出MVSMamba网络，采用动态Mamba模块，基于参考中心动态扫描策略，实现视图内和视图间特征交互、全方位多视角特征表示和多尺度全局特征聚合。

Result: 在DTU数据集和Tanks-and-Temples基准测试中，MVSMamba在性能和效率方面均优于现有最先进的MVS方法。

Conclusion: MVSMamba成功展示了Mamba架构在MVS任务中的潜力，为高效全局特征建模提供了新方向。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [224] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 提出一种基于CLIP模型的生成对抗攻击方法，通过结合文本语义和视觉表示生成视觉不可察觉的对抗扰动，在多目标环境中欺骗多标签分类器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到对抗攻击的影响，现有方法在保持视觉保真度和攻击效果方面存在不足，需要开发更有效的对抗攻击技术。

Method: 整合CLIP模型的文本-图像对齐能力、SSAE的集中扰动策略和GAMA的文本嵌入差异，使用引导损失函数生成对抗样本。

Result: 在多种黑盒受害者模型上的实验表明，该方法在攻击效果上具有竞争力，同时保持了更高的视觉保真度。

Conclusion: 该方法成功地将自然语言语义与视觉表示相结合，生成了既有效又视觉不可察觉的对抗扰动，在多目标场景中表现出色。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [225] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet是一种医学图像分割网络，通过结合局部建模和全局上下文来增强边界描绘和细节保留，在Synapse和BUSI数据集上取得了可比较的分割精度和边界质量。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对于计算机辅助诊断和治疗规划至关重要，但由于解剖结构的高度变异性和边界模糊性，精细结构的可靠描绘面临挑战。

Method: RDTE-UNet采用混合ResBlock细节感知Transformer主干网络，包含三个模块：ASBE用于自适应边界增强，HVDA用于细粒度特征建模，EulerFF用于基于欧拉公式的融合加权。

Result: 在Synapse和BUSI数据集上，RDTE-UNet在分割精度和边界质量方面达到了可比较的水平。

Conclusion: 该网络通过统一局部建模与全局上下文，改善了跨形态、方向和尺度的结构一致性和边界准确性。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [226] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D是一个多实例3D医学图像分割框架，通过竞争性查询优化策略实现从单点-单对象到单点-多实例的转变，能够从单个示例点分割所有语义相似的病变。


<details>
  <summary>Details</summary>
Motivation: 现有SAM交互式分割方法大多遵循单点-单对象范式，限制了多病灶分割能力；同时ViT骨干网络虽然能捕获全局上下文，但往往缺失高保真局部细节。

Method: 1) 提示条件化实例查询生成器将单点提示转换为多个专门化查询；2) 混合CNN-Transformer编码器通过空间门控将CNN边界显著性注入ViT自注意力；3) 竞争优化查询解码器通过查询间竞争实现端到端并行多实例预测。

Result: 在LiTS17和KiTS21数据集上，MIQ-SAM3D达到了可比较的性能水平，并对提示具有强鲁棒性。

Conclusion: 该方法为临床相关多病灶病例的高效标注提供了实用解决方案。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [227] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 提出了一种通过内容-风格子空间混合和平衡损失来扩展内容-风格边界的新方法，解决了风格强度增加时内容特征丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在单一风格强度下评估内容相似性，但实验发现增加风格强度会导致内容特征显著丢失，形成次优的内容-风格边界。

Method: 采用内容-风格子空间混合技术和内容-风格平衡损失函数，在保持风格多样性的同时增强内容特征的保留。

Result: 在定性和定量评估中均优于现有技术，实现了更优的内容-风格权衡，显著降低了倒置生成距离和生成距离分数。

Conclusion: 该方法有效扩展了内容-风格边界，在多种风格强度下都能保持更好的内容相似性，为文本到图像生成提供了更优的解决方案。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [228] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出了CMI-MTL框架，通过跨模态交互和多任务学习解决医学视觉问答中的语义对齐和自由形式答案多样性问题


<details>
  <summary>Details</summary>
Motivation: 现有自注意力方法难以有效处理视觉与语言之间的跨模态语义对齐，分类方法依赖预定义答案集，无法适应自由形式答案的多样性

Method: 包含三个关键模块：细粒度视觉文本特征对齐(FVTA)、跨模态交错特征表示(CIFR)、自由形式答案增强多任务学习(FFAE)

Result: 在VQA-RAD、SLAKE和OVQA三个Med-VQA数据集上优于现有最先进方法

Conclusion: CMI-MTL框架通过跨模态交互和多任务学习有效提升了医学视觉问答的性能和适应性

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [229] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种用于生成水下环境中AUV搭载事件相机合成数据的管道，用于训练视觉模型，并在岩石检测任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光照条件差、高动态范围场景等挑战，传统视觉技术难以适应。事件相机通过逐帧跟踪变化来缓解这些问题，但缺乏真实数据用于训练。

Method: 开发了一个管道，可以生成AUV搭载事件相机在水下环境中的逼真合成数据，特别针对能见度差和悬浮颗粒物的情况。

Result: 在岩石检测任务中验证了管道的有效性，该方法可以推广到其他水下任务。

Conclusion: 该合成数据生成管道为水下事件相机视觉模型的训练提供了可行的解决方案。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [230] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: SEPS框架通过语义增强的补丁精简方法，有效解决了跨模态对齐中的补丁冗余和模糊性问题，显著提升了细粒度视觉-语言对齐性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理视觉补丁冗余和模糊性方面存在挑战，主要源于不同模态间的信息密度差异。多模态大语言模型虽然能生成丰富语义，但其密集文本输出可能与原始稀疏描述产生冲突。

Method: 提出语义增强补丁精简(SEPS)框架，采用两阶段机制整合密集和稀疏文本的统一语义，识别显著视觉补丁，并通过相关性感知选择和均值计算来突出关键补丁-词汇对应关系。

Result: 在Flickr30K和MS-COCO数据集上的实验表明，SEPS在不同模型架构下rSum指标超越现有方法23%-86%，在文本到图像检索场景中表现尤为突出。

Conclusion: SEPS框架通过系统性解决补丁冗余和模糊性问题，显著提升了细粒度跨模态对齐性能，为视觉问答等应用提供了有效解决方案。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [231] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 该研究提出了Fire-ART数据集和全景图像重建方法，用于消防资产的语义化BIM建模，解决了传统方法在资产识别和重建方面的自动化能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 消防资产库存管理对应急准备、风险评估和现场灭火响应至关重要，但传统方法在自动化资产识别和重建方面能力有限，效率低下。

Method: 开发了Fire-ART数据集（包含15种基础资产、2626张图像和6627个实例），并提出了整合改进的立方体贴图转换和基于半径的球面相机投影的全景图像重建方法。

Result: 通过两个真实案例验证，该方法分别实现了73%和88%的F1分数，以及0.620米和0.428米的定位误差。

Conclusion: Fire-ART数据集和重建方法为消防设备的精确数字化管理提供了宝贵资源和稳健的技术解决方案。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [232] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出一种无需训练的解释方法，用平滑可调轮廓替代密集扰动掩码，通过星凸区域参数化和傅里叶级数优化，在保持极值保真度的同时生成紧凑、可解释的区域。


<details>
  <summary>Details</summary>
Motivation: 现有密集扰动掩码通常碎片化且过拟合，需要复杂的后处理，缺乏紧凑性和可解释性。

Method: 使用截断傅里叶级数参数化星凸区域，在分类器梯度指导下优化极值保留/删除目标，保证单一连通掩码并大幅减少参数数量。

Result: 在ImageNet分类器上匹配密集掩码的极值保真度，产生紧凑可解释区域，运行一致性更高，在基准测试中比梯度方法和扰动基线获得更高的相关性质量和更低复杂度。

Conclusion: 该方法通过低维平滑轮廓实现鲁棒解释，特别在自监督DINO模型上表现优异，相关性质量提升超过15%，并保持正相关性。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [233] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于蒸馏的高效因果视频生成框架，通过对抗自蒸馏和首帧增强策略，在极少数去噪步骤下实现高质量视频合成


<details>
  <summary>Details</summary>
Motivation: 现有混合视频生成模型结合自回归时序动态和基于扩散的空间去噪，但其顺序迭代特性导致错误累积和长推理时间，需要更高效的生成方法

Method: 基于分布匹配蒸馏框架，提出对抗自蒸馏策略，将学生模型n步去噪输出与(n+1)步版本在分布层面对齐；同时采用首帧增强策略，为首帧分配更多去噪步骤以减少错误传播

Result: 在VBench上的广泛实验表明，该方法在一步和两步视频生成中均超越现有最佳方法，且单一蒸馏模型可灵活支持多种推理步骤设置

Conclusion: 该框架通过对抗自蒸馏和首帧增强，显著提高了极少数步骤场景下的训练稳定性和生成质量，实现了高效高质量的视频合成

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [234] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 提出了UniSOT统一跟踪器，能够同时处理三种参考模态（边界框、自然语言或两者结合）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件），使用统一参数实现跨模态目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常针对单一或少数几种视频模态和参考模态设计，导致模型分离且限制实际应用。实际需要统一跟踪器来处理各种需求，但目前尚无能够同时处理上述所有参考模态和视频模态的跟踪器。

Method: 设计UniSOT统一跟踪器，采用统一参数架构，支持三种参考模态（边界框、自然语言、两者结合）和四种视频模态（RGB、RGB+深度、RGB+热成像、RGB+事件）的不同组合。

Result: 在18个视觉跟踪、视觉语言跟踪和RGB+X跟踪基准测试中，UniSOT表现出优于特定模态对应方法的性能。在TNL2K上所有三种参考模态上超过先前方法3.0% AUC，在三种RGB+X视频模态上超过Un-Track 2.0%主要指标。

Conclusion: UniSOT证明了统一跟踪器在处理多种参考模态和视频模态组合方面的有效性和优越性，为实际应用提供了更灵活的解决方案。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [235] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出一种分辨率感知的token解码器，在低分辨率瓶颈进行大部分计算，通过门控交叉注意力注入精细细节，仅对稀疏的不确定性选择像素进行细化，平衡全局语义、局部一致性和边界保真度。


<details>
  <summary>Details</summary>
Motivation: 解决越野语义分割中存在的边界模糊、稀疏监督和标签噪声问题。传统方法在低分辨率融合时会模糊边缘并传播局部错误，而保持高分辨率路径或重复高分辨率融合成本高且对噪声敏感。

Method: 1. 分辨率感知token解码器：在低分辨率瓶颈进行主要计算；2. 门控交叉注意力注入精细尺度细节；3. 仅对稀疏不确定性选择像素进行细化；4. 边界带一致性正则化器训练时增强边界一致性。

Result: 实现了竞争性性能和在过渡区域的改进稳定性，在越野语义分割任务中表现出色。

Conclusion: 该方法通过精心设计的组件协同工作，有效平衡了全局语义、局部一致性和边界保真度，在存在不完美监督的情况下仍能保持良好性能。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [236] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 提出一种仅训练阶段使用的多目标方法，通过增强热红外图像的特征表示来解决夜间检测中的重复检测、小目标漏检和类别混淆问题，无需测试时的RGB输入。


<details>
  <summary>Details</summary>
Motivation: 解决热红外夜间检测中的低对比度、弱高频线索导致的重复检测框、小目标漏检和类别混淆问题，避免现有方法需要RGB-TIR融合或图像转换带来的额外传感器需求、校准成本和性能脆弱性。

Method: 在训练阶段引入两个目标：1）通过类内特征聚合和类间特征分离来锐化实例级决策边界；2）通过RGB训练教师模型的多级金字塔特征对齐来注入跨模态语义先验，增强纹理贫乏的热红外特征。

Result: 在实验中优于现有方法，达到了最先进的性能。

Conclusion: 提出的训练阶段多目标方法有效提升了热红外夜间检测性能，无需测试时的多模态输入，解决了热红外检测的核心挑战。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [237] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出了一种模型无关的序数元学习算法(MAOML)，用于训练小型视觉语言模型，通过元学习解决数据稀疏性问题并利用标签的序数性，在水果新鲜度分类任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于专家标注成本高导致数据稀缺，而闭源视觉语言模型存在数据隐私问题，开源模型性能不佳，需要一种能在数据有限情况下有效训练小型视觉语言模型的方法。

Method: 采用模型无关的序数元学习算法(MAOML)，结合元学习处理数据稀疏性，并利用标签的序数关系来提升模型性能。

Result: 在零样本和少样本设置下实现了92.71%的行业标准准确率，在所有水果类型上平均表现优异。

Conclusion: MAOML算法能够有效训练小型视觉语言模型，在数据稀缺情况下达到与专有模型相当的性能，同时解决了数据隐私问题。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [238] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出了Reg-DPO方法，通过GT-Pair自动构建高质量偏好对，结合SFT损失作为正则化项，提升视频生成的训练稳定性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要基于图像领域，在视频任务中面临数据构建成本高、训练不稳定和内存消耗大的挑战。

Method: 使用GT-Pair自动构建偏好对（真实视频为正样本，模型生成视频为负样本），在DPO目标中加入SFT损失作为正则化项，结合FSDP框架和内存优化技术。

Result: 在I2V和T2V任务上优于现有方法，训练容量比单独使用FSDP提高近三倍，生成质量显著提升。

Conclusion: Reg-DPO方法有效解决了视频生成中的关键挑战，无需外部标注即可实现高质量视频生成。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [239] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该论文提出QA-SNNE方法，通过将问题语义融入预测置信度来改进手术视觉问答中的不确定性估计，提高模型安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在手术环境中部署视觉问答系统时，安全性和可靠性至关重要，因为错误或模糊的回答可能对患者造成伤害。现有研究大多关注准确性或语言质量，而忽视了安全行为如模糊意识、转诊专家或触发二次意见。

Method: 提出Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE)，一种黑盒不确定性估计器，通过在医学文本嵌入空间中比较生成答案与最近邻来测量语义熵，并以问题为条件。

Result: 在EndoVis18-VQA和PitVQA数据集上评估五个模型，QA-SNNE在大多数模板内设置中提高了AUROC，零样本模型的AUROC提升了15-38%，并在模板外压力下保持增益。

Conclusion: QA-SNNE通过将语义不确定性与问题上下文联系起来，为手术视觉问答中的自动故障检测提供了实用且可解释的步骤。结合LVLM骨干和问题对齐的不确定性估计可以提高安全性和临床医生信任度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [240] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉Transformer的后训练量化方法，通过将激活量化误差和权重量化误差建模为独立高斯噪声，并采用噪声注入优化来获得平坦的最小值，从而改善低比特量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练量化方法通常忽略了训练好的神经网络与量化模型之间的关系，导致较大的量化误差。本文旨在解决如何高效训练与预定义精度低比特模型适配的模型无关神经网络。

Method: 提出一个框架，主动通过测量和解耦误差源来预处理模型。具体将激活量化误差(AQE)和权重量化误差(WQE)统计建模为独立高斯噪声，并研究多种噪声注入优化方法以获得平坦最小值。

Result: 实验结果证明了该方法的有效性，为获得低比特后训练量化模型开辟了新途径。

Conclusion: 平坦的全精度神经网络对于低比特量化至关重要，通过提出的误差建模和噪声注入优化方法，能够有效改善视觉Transformer的后训练量化性能。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [241] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 提出HMVLM框架，通过MoE LoRA策略解决3D人体运动与语言模型集成中的灾难性遗忘问题，使用零专家保护预训练参数，并通过身体部位特定标记化改进姿态表示。


<details>
  <summary>Details</summary>
Motivation: 解决人体运动与文本模态之间的差距导致的灾难性遗忘问题，以及开发在异构下游任务中保持泛化能力的自回归兼容姿态表示的技术障碍。

Method: 基于MoE LoRA策略的统一框架，使用门控网络根据输入提示动态分配LoRA专家权重；引入零专家保护预训练参数；通过身体部位特定标记化将人体划分为不同关节组。

Result: 实验表明该方法有效缓解指令调优期间的知识遗忘，在多样化人体运动下游任务中取得显著性能。

Conclusion: HMVLM框架成功解决了人体运动与语言模型集成中的关键挑战，为多模态理解提供了有效解决方案。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [242] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff是一个基于扩散模型的即插即用解码框架，通过伪逆引导采样和自适应权重控制，显著提升了深度联合源信道编码在对抗性无线环境下的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度联合源信道编码框架容易受到物理层对抗威胁（如导频欺骗和子载波干扰），影响语义保真度，需要增强其安全性和鲁棒性。

Method: 采用伪逆引导采样和自适应指导权重实现灵活步长控制和高效语义重建；针对干扰攻击使用基于功率的子载波掩码策略；针对导频欺骗将信道估计重构为盲逆问题，开发EM驱动的重建算法。

Result: 在OFDM信道对抗条件下的广泛实验表明，SecDiff在重建质量和计算成本之间实现了良好平衡，优于现有的安全和生成式JSCC基线方法。

Conclusion: SecDiff为实现实用、低延迟且抗攻击的语义通信迈出了有前景的一步。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [243] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 本文提出了增强行人对齐网络(EPAN)，用于物联网监控环境下的人员重识别，通过双分支架构处理视角和环境变化，在Inspection-Personnel数据集上取得了90.09%的Rank-1准确率和78.82%的mAP。


<details>
  <summary>Details</summary>
Motivation: 解决物联网智能环境中监控和安全应用的人员重识别问题，特别是在多样化监控条件下应对视角和环境变化带来的挑战。

Method: 采用双分支架构来减轻视角和环境变化的影响，在不同尺度和视角下提取对齐信息，具备强大的特征提取能力。

Result: 在Inspection-Personnel数据集上取得了优异性能：Rank-1准确率90.09%，平均精度均值(mAP)78.82%。

Conclusion: EPAN在真实物联网应用中具有巨大潜力，能够在监控和安全系统中实现跨摄像头的高效可靠人员重识别。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [244] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 提出一种基于SE(3)流匹配的概率框架，用于估计6D物体姿态分布，解决姿态模糊性问题，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决物体姿态估计中的部分可观测性、遮挡和物体对称性导致的姿态模糊问题，传统确定性方法无法捕捉姿态分布的多模态特性。

Method: 使用SE(3)流匹配技术建模完整的姿态分布，提供基于样本的姿态估计，能够处理对称物体和严重遮挡等模糊情况。

Result: 在Real275、YCB-V和LM-O数据集上达到最先进性能，并展示了在机器人操作任务中的实际应用价值。

Conclusion: 提出的概率框架能够有效建模姿态分布的不确定性，在模糊情况下提供更可靠的姿态估计，适用于下游机器人任务。

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [245] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 提出DiMoDE框架，通过区分对待运动分量并利用其刚性流几何规律，改进深度和自运动联合学习，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度和自运动学习方法通常将自运动作为辅助任务，混合所有运动类型或排除与深度无关的旋转运动，限制了强几何约束的引入，降低了在多样化条件下的可靠性和鲁棒性。

Method: 引入运动分量的区分处理，通过相机光轴和成像平面对齐，将光流转换并量化偏差，对每个自运动分量单独施加几何约束。进一步将联合学习重新表述为同轴和共面形式，通过闭式几何关系实现深度与平移分量的相互推导。

Result: DiMoDE框架在多个公共数据集和新收集的多样化真实世界数据集上达到最先进性能，特别是在挑战性条件下表现优异。

Conclusion: 通过区分处理运动分量并利用其几何规律，能够引入更针对性的约束，提高深度和自运动估计的可靠性和鲁棒性。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [246] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出Luminance-Aware Statistical Quantification (LASQ)框架，将低光图像增强重新定义为基于分层亮度分布的统计采样过程，解决了传统方法在重建保真度和跨场景泛化能力之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法主要关注低光/正常光图像对之间的确定性像素级映射，忽略了真实环境中亮度转换的连续物理过程，导致在缺乏正常光参考时性能下降。

Method: LASQ将亮度转换重新概念化为强度坐标空间中的幂律分布，用分层幂函数近似，用概率采样替代确定性映射。设计了扩散前向过程来自主发现亮度层间的最优转换路径，实现无需正常光参考的无监督分布模拟。

Result: 该框架显著提升了实际场景中的性能，实现了更适应和通用的光照恢复。在有正常光参考的情况下，在领域特定数据集上取得优越性能，同时在非参考数据集上具有更好的泛化能力。

Conclusion: LASQ通过统计量化方法有效解决了低光图像增强中的保真度与泛化能力平衡问题，为实际应用提供了更实用的解决方案。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [247] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出一个完整的纹理编辑系统，能够生成具有局部特征（如污渍、撕裂、孔洞等）的逼真纹理，采用基于学习的方法，无需手动标注，通过无监督异常检测和聚类来实现条件生成。


<details>
  <summary>Details</summary>
Motivation: 自然界中材料表面普遍存在各种瑕疵和局部特征变化，将这些特征纳入纹理合成过程对于生成逼真纹理至关重要。传统方法需要大量手动标注，因此需要开发无需人工标注的自动化系统。

Method: 采用基于学习的方法，利用无标注样本进行无监督异常检测，自动识别外观改变特征，然后将纹理特征聚类成语义连贯的组别，用于指导条件图像生成。系统包含扩散式编辑和无限平稳纹理生成算法。

Result: 开发了一个完整的流程，从少量图像集合到多功能生成模型，用户可以交互式地在任意大小的纹理上创建和绘制特征。系统能够生成具有各种局部特征的逼真纹理。

Conclusion: 提出的系统实现了从图像收集到生成模型的完整工作流程，无需手动标注即可生成具有局部特征的逼真纹理。其中的扩散式编辑和无限纹理生成算法具有通用性，可应用于其他场景。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [248] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 提出了一种新颖的对比学习框架NSYNC，通过生成负样本合成图像来改进大型文本到图像扩散模型的风格化能力，使用正交梯度更新方法消除正负样本中的共同特征。


<details>
  <summary>Details</summary>
Motivation: 现有文本条件图像生成方法虽然能生成逼真图像，但难以捕捉特定风格特征。即使对目标风格数据集进行微调，仍然难以充分掌握风格特征。

Method: 利用合成图像生成技术，创建负样本合成集，与真实正样本一起进行对比训练。通过计算正梯度在负梯度上的投影，获得正交分量来更新参数，消除正负样本中的共同属性。

Result: 在多种画家和插画师风格上的实验表明，该方法在定量和定性评估上都优于基线方法。

Conclusion: NSYNC框架通过对比学习和正交梯度更新，有效提升了文本到图像扩散模型的风格化能力，能够更好地捕捉独特风格特征。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [249] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出一个五层模型来改进罕见驾驶场景的评估和生成，使用数据增强策略结合基础模型生成新场景，并引入多样性和原创性指标来评估合成数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 罕见和具有挑战性的驾驶场景对自动驾驶开发至关重要，但由于难以遇到，需要通过生成模型进行模拟或生成。

Method: 提出结构化五层模型，为场景中的每个智能体引入子类和特征，使用特定嵌入进行比较，并采用数据增强策略结合基础模型生成新驾驶场景。

Result: 展示了在不同生成设置下多样性和原创性指标的应用，以及对结构化场景描述生成的合成视频进行定性评估。

Conclusion: 提出的五层模型和评估指标能够有效改进罕见驾驶场景的生成和评估，为自动驾驶开发提供重要支持。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [250] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 提出PCD-ReID算法解决基站环境中被遮挡行人重识别问题，使用Transformer网络提取共享组件特征，在真实巡逻监控数据集上训练，相比ResNet50方法Rank-1准确率提升15.9%。


<details>
  <summary>Details</summary>
Motivation: 传统ResNet-based重识别算法难以有效处理遮挡问题，遮挡会掩盖关键身体特征，增加识别复杂度，需要新的重识别方法。

Method: 设计基于Transformer的PCD网络提取共享组件特征（如头盔、制服），收集真实巡逻监控图像数据集（6个月、1万人、5万张图像）进行训练以避免过拟合。

Result: 模型达到79.0%的mAP和82.7%的Rank-1准确率，相比基于ResNet50的方法Rank-1准确率提升15.9%。

Conclusion: PCD-ReID在塔台巡检场景中有效实现了遮挡感知的行人重识别性能，在监控安防应用中具有实际部署潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [251] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: 开发了Napari Organoid Analyzer (NOA)，一个用于简化基于AI的类器官分析的通用图形用户界面工具，解决了现有工具对非编程人员不友好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具对没有编程经验的生物学家来说难以使用，导致工作流程仍然主要依赖人工操作，且现有工具大多只针对特定任务。

Method: 开发NOA作为开源napari插件，集成了检测、分割、追踪、特征提取、自定义特征标注和基于机器学习的特征预测等多个模块，并整合了多种最先进算法。

Result: 通过三个案例研究展示了NOA的多功能性：量化类器官分化过程中的形态变化、评估光毒性效应、预测类器官活性和分化状态。

Conclusion: NOA在可访问和可扩展的框架内实现了全面的AI驱动类器官图像分析。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [252] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA是首个支持像素级推理和多模态提示的视觉-语言-动作模型，通过两阶段自动标注流程生成Pixel-160K数据集，在三个标准VLA基准测试中比OpenVLA提升10.1%-17.8%的成功率，仅需1.5%的预训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在两个主要限制：(i) 难以进行像素级场景理解，(ii) 过度依赖文本提示，在现实环境中灵活性不足。

Method: 提出新的视觉运动指令调优框架，集成多尺度像素感知编码器和视觉提示编码器，采用两阶段自动标注流程生成Pixel-160K大规模像素级标注数据集。

Result: 在三个标准VLA基准测试和两个VLA模型变体上，PixelVLA比OpenVLA提升10.1%-17.8%的操作成功率，同时仅需1.5%的预训练成本。

Conclusion: PixelVLA可以集成到现有VLA中，在复杂环境中实现更准确、高效和通用的机器人控制。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [253] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出基于DC-GAN的深度学习方法生成合成MRI数据，并使用CNN分类器评估合成图像质量，证明GAN生成的医学图像在下游任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI数据有限，需要生成合成医学图像来弥补数据不足问题。

Method: 使用深度卷积生成对抗网络（DC-GAN）生成合成MRI数据，并采用卷积神经网络（CNN）分类器对真实和合成图像进行脑肿瘤分类。

Result: 在真实和合成图像上的分类结果表现相当，验证了GAN生成图像在下游任务中的实用性。

Conclusion: GAN生成的合成医学图像可以有效解决数据有限问题，并在下游分类任务中表现出与真实数据相当的性能。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [254] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 提出连续-离散二元视觉分词器(CDD-VT)，通过自适应分配图像基元数量来解决多模态大模型中理解与生成的统一问题，简单实例使用少量基元(类似离散分词)，复杂实例使用大量基元(类似连续分词)。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型中理解与生成的统一挑战，克服连续分词器(CT)的复杂流水线和离散分词器(DT)的信息损失问题，受光的波粒二象性启发，质疑CT和DT的二元选择。

Method: 设计CDD-VT，包含两个核心组件：1)多样化量化基元，促进基元正交性以更好填充信息空间；2)动态基元分配器，根据样本复杂度确定最优基元集合。

Result: 在重建、检索和分类任务上的广泛实验表明，CDD-VT在性能和简洁性方面均优于专门的CT和DT方法。

Conclusion: CDD-VT在简洁可扩展的多模态大模型中实现了强大性能，有效统一了理解与生成，克服了传统分词方法的局限性。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [255] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM是一个轻量级架构，用于从带有RECIST标注的CT扫描中高效进行肿瘤体积分割，在MICCAI FLARE 2025比赛中取得了60.7%的Dice系数和63.6%的NSD分数。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤治疗评估主要依赖RECIST v1.1标准的单平面最长直径测量，但体积测量更可靠。然而，手动体积标注耗时费力，限制了临床应用。

Method: 提出了Lite ENSAM架构，这是ENSAM架构的轻量级适配版本，专门用于从RECIST标注的CT扫描中进行高效的肿瘤体积分割。

Result: 在MICCAI FLARE 2025任务1的子任务2中，在隐藏测试集上获得60.7%的Dice相似系数和63.6%的标准化表面Dice，在公共验证集上平均总RAM时间为50.6GB，CPU推理时间为14.4秒。

Conclusion: Lite ENSAM能够高效地从RECIST标注中生成肿瘤体积分割，为临床采用体积测量提供了可行的解决方案。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [256] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的自监督视觉基础模型训练框架，结合了DINO系列的核心原理，支持多种Transformer架构和训练策略，显著降低计算成本并保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练流程存在不灵活、领域特定或计算成本高的问题，限制了在不同领域和资源环境下的可用性。

Method: 采用统一配置驱动系统，支持多种Transformer架构，包含LoRA、层冻结、知识蒸馏等训练策略，兼容DDP和FSDP分布式训练，支持自然和专用数据类型。

Result: 在多样化数据集上实现竞争力性能，显著降低计算成本，提供可解释性工具和标签引导数据增强方法。

Conclusion: DINO-MX为开发和基准测试自监督视觉模型提供了可重现、可扩展的基础，适用于研究和实际应用。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [257] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 提出PSPooling（预计算结构池化）方法用于3D解剖形状分类，通过自监督图自编码器学习解剖感知表示，并在MedShapeNet19基准数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解剖3D形状分类进展受限于网格数据复杂性和缺乏标准化基准，需要鲁棒的学习方法和可复现的评估。

Method: 提出非学习型网格池化算子PSPooling，基于几何邻近度预计算节点对应集，实现并行化可逆的池化和反池化操作；集成到自监督图自编码器中学习未标记表面网格的解剖感知表示。

Result: 在MedShapeNet19数据集上，PSPooling显著提高了重建保真度和低标签情况下的分类准确率，为医学3D形状学习建立了强基线。

Conclusion: PSPooling方法有效解决了现有方法的稀疏性和重建问题，MedShapeNet19数据集有望成为解剖形状分类的广泛采用基准。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [258] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: ViC是一个无需训练的训练框架，将列表重排序和融合重新定义为视觉语言模型的零样本推理任务，通过序列化内容证据和检索器元数据，在跨模态视频检索中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构检索器候选融合的长期挑战，特别是在复杂多模态数据（如视频）中。传统融合方法仅依赖排名或分数信号，忽略了候选表示。

Method: 引入Vote-in-Context框架，将内容证据和检索器元数据序列化到VLM提示中，使模型能自适应权衡检索器共识与视觉语言内容。使用S-Grid紧凑序列化映射表示视频。

Result: 在MSR-VTT上达到87.1%(t2v)/89.0%(v2t)的Recall@1，在VATEX上达到99.6%(v2t)，相比之前最先进方法提升高达+40 Recall@1。

Conclusion: ViC是将现代VLM转变为强大零样本重排序器和融合器的简单、可复现且高效的方案。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [259] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种将强化学习有效整合到基于扩散的图像修复模型中的新框架，通过使用图像质量评估模型作为奖励函数，并针对远离真实值的困难样本进行优化，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法直接应用于基于扩散的图像修复模型效果不佳，因为修复任务更强调保真度而非纯生成，需要专门针对修复特点的RL策略。

Method: 使用图像质量评估模型作为奖励函数，针对困难样本进行强化学习优化，采用MLLM-based IQA模型进行分布对齐，并根据样本难度自适应结合监督微调。

Result: 该方法在各种修复任务的多个基准测试中表现出有效性，能够无缝应用于基于扩散的修复模型并提升其性能。

Conclusion: 提出的强化学习框架为基于扩散的图像修复模型提供了一种有效的优化策略，通过自适应权重调整和困难样本重点优化实现了性能提升。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [260] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos是一个统一的图像和视频重光照框架，通过将RGB空间的几何反馈引入流匹配主干网络，解决了现有扩散模型在语义潜在空间中优化导致的物理不一致问题。该方法使用深度和法线图进行监督，结合路径一致性学习实现高效训练，并提出了六维标注协议和LumosBench基准进行精细控制和评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的重光照方法在语义潜在空间中优化，虽然能产生丰富的光照效果，但缺乏物理正确性，经常产生过曝高光、错位阴影和不正确遮挡等不真实结果。

Method: 提出UniLumos框架，将RGB空间几何反馈引入流匹配主干网络，使用深度和法线图进行监督以对齐光照效果与场景结构。采用路径一致性学习减少计算开销，设计六维标注协议实现精细光照控制，并构建LumosBench基准进行自动评估。

Result: UniLumos在重光照质量上达到最先进水平，显著改善了物理一致性，同时实现了图像和视频重光照20倍的加速。

Conclusion: UniLumos通过几何反馈和路径一致性学习，成功解决了重光照中的物理一致性问题，在保持高质量的同时大幅提升了效率，为可控重光照提供了有效解决方案。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [261] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出了一种渐进式网络架构，用于从H&E染色图像合成IHC等效图像，通过分阶段优化结构、颜色和细胞边界来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: IHC染色虽然能提供高分辨率蛋白定位信息，但成本高、耗时长且难以多重标记。现有染色转换技术使用线性加权损失函数，无法同时保持结构真实性和颜色保真度。

Method: 基于ASP框架，提出渐进式网络架构，分阶段优化结构、颜色和细胞边界生成。引入DAB色原浓度和图像梯度损失函数来增强颜色保真度和细胞边界清晰度。

Result: 在HER2和ER数据集上的实验表明，该模型显著提升了视觉质量，获得了更精细的结构细节。

Conclusion: 渐进式机制有效解决了现有染色转换技术中结构真实性和颜色保真度难以兼顾的问题，为病理诊断提供了更高效的替代方案。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [262] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出LFRD2框架，结合神经网络表达能力和物理模型可解释性，解决屏下ToF成像中的信号衰减、多路径干扰和时序噪声问题。


<details>
  <summary>Details</summary>
Motivation: 屏下ToF成像中，透明OLED层会引入严重的信号衰减、多路径干扰和时序噪声，显著降低深度感知质量。

Method: 采用可学习分数阶反应-扩散动力学框架，结合时间分数阶反应-扩散模块实现迭代深度优化，并通过系数预测和重复微分的连续卷积算子提升恢复质量。

Result: 在四个基准数据集上的实验证明了该方法的有效性。

Conclusion: LFRD2框架成功解决了屏下ToF成像中的深度质量退化问题，代码已开源。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [263] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 提出了PRBench基准测试，首次系统评估不同鲁棒性训练方法在概率鲁棒性(PR)方面的改进，发现对抗训练(AT)方法在提升AR和PR方面更通用，而PR针对性训练方法在泛化误差和干净准确率方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型对微小扰动很脆弱，现有研究主要关注对抗鲁棒性(AR)，而概率鲁棒性(PR)作为AR的实用补充，其针对性训练方法研究相对不足，且存在评估协议不可比、与强AT基线比较有限、缺乏统一泛化框架等问题。

Method: 引入PRBench基准测试，使用全面指标集（干净准确率、PR和AR性能、训练效率、泛化误差）对常见AT和PR针对性训练方法进行实证比较，并提供PR性能泛化误差的理论分析。

Result: AT方法在不同超参数设置下提升AR和PR性能方面更通用，而PR针对性训练方法始终产生更低的泛化误差和更高的干净准确率。建立了包含222个训练模型、覆盖7个数据集和10种模型架构的排行榜。

Conclusion: PRBench为PR训练方法提供了首个系统评估基准，揭示了不同类型训练方法的互补优势，AT在鲁棒性提升方面更通用，而PR针对性方法在泛化和准确率方面更优。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [264] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 开发了一个任务探索管道，使用聚类技术、因子分析和字符串编辑距离自动识别完成任务的全局和局部策略，以及有意义的子任务。


<details>
  <summary>Details</summary>
Motivation: 推进机器对用户知识、技能和行为的理解，以实现隐式协调，在预期性人机交互中促进优势互动。

Method: 开发任务探索管道，结合聚类技术、因子分析和字符串编辑距离，自动识别全局策略（通用动作集）和局部策略（相似动作组合序列），并识别各种长度的有意义的子任务。

Result: 任务探索管道能够自动识别完成任务的关键策略，并用层次化子任务结构编码用户运行轨迹。还开发了Task Explorer应用程序来轻松查看管道结果。

Conclusion: 该管道可轻松适应任何基于动作的时间序列数据，识别的策略和子任务有助于人类和机器了解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [265] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: CGF-DETR是一种改进的实时检测变换器，专门用于胸部X光肺炎检测，通过XFABlock、SPGA模块和GCFC3等创新模块，在RSNA肺炎检测数据集上达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因，需要准确高效的自动检测系统。虽然基于变换器的检测器在目标检测任务中表现出色，但在医学影像特别是胸部X光肺炎检测中的应用仍待探索。

Method: 提出CGF-DETR模型：1）在骨干网络中引入XFABlock，通过卷积注意力机制与CSP架构结合改进多尺度特征提取；2）提出SPGA模块，用动态门控机制和单头自注意力替代标准多头注意力；3）设计GCFC3用于颈部，通过多路径卷积融合增强特征表示，同时通过结构重参数化保持实时性能。

Result: 在RSNA肺炎检测数据集上，CGF-DETR达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的推理速度。完整模型在mAP@[0.5:0.95]上达到50.4%。消融研究证实每个模块都对性能提升有贡献。

Conclusion: CGF-DETR通过创新的模块设计在肺炎检测任务中实现了显著的性能提升，同时保持了实时推理能力，为医学影像中的目标检测提供了有效的解决方案。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [266] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED是一个大规模多平台3D视觉语言基准数据集，包含车辆、无人机和四足机器人平台的RGB和LiDAR数据，提供128,000个对象和22,000个验证的指代表达式，比现有数据集大10倍。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉语言基准局限于室内场景、单一平台和小规模，无法满足开放世界环境中具身智能体的需求。

Method: 开发了可扩展的标注流程，结合视觉语言模型提示和人工验证；提出平台感知归一化和跨模态对齐技术；建立域内和跨平台评估协议。

Result: 构建了大规模多平台数据集，发现显著的性能差距，突显了可泛化3D视觉语言定位的挑战和机遇。

Conclusion: 3EED数据集和基准工具包的发布将推动语言驱动的3D具身感知研究的未来发展。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [267] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 提出HGFreNet，一种结合图注意力和Transformer的架构，通过跳数混合特征聚合和频域3D轨迹一致性来解决2D到3D人体姿态提升中的深度模糊和时间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要约束相邻帧间差异，但忽略了骨骼关节运动的全局时空相关性，导致3D轨迹不连贯。需要同时考虑全局空间和时间相关性来提升姿态估计的准确性和一致性。

Method: 设计HGFreNet架构，包含跳数混合图注意力模块和Transformer编码器。HGA模块将所有k跳邻居分组为混合组以扩大感受野，应用注意力机制发现这些组的潜在相关性。通过频域约束轨迹一致性来利用全局时间相关性，并使用初步网络估计3D姿态。

Result: 在Human3.6M和MPI-INF-3DHP基准数据集上的实验表明，HGFreNet在位置准确性和时间一致性方面优于现有最先进方法。

Conclusion: HGFreNet通过结合跳数混合图注意力和频域轨迹一致性约束，有效提升了2D到3D人体姿态估计的性能，证明了全局时空相关性建模的重要性。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [268] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++是一种从单视图图像高效生成高质量纹理网格的新方法，通过跨域扩散模型生成多视角法线图和彩色图像，在约3分钟内完成高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在效率低（SDS方法需要逐形状优化）或质量差（直接网络推理方法缺乏几何细节）的问题，需要同时提升单视图重建任务的质量、一致性和效率。

Method: 提出跨域扩散模型生成多视角法线图和对应彩色图像，使用多视角跨域注意力机制确保生成一致性，采用从粗到细的级联3D网格提取算法。

Result: 方法实现了高质量的重建结果，具有良好的泛化能力和效率，相比先前工作有明显优势。

Conclusion: Wonder3D++在单视图3D重建任务中实现了质量、一致性和效率的全面提升，代码已开源。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [269] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION是一个统一的自动驾驶模型，使用线性组RNN算子高效处理大规模LiDAR点云、多视角图像和时间序列，无需显式的时间或多模态融合模块，在多种核心任务中达到竞争性甚至最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决transformer在处理长序列数据时二次注意力机制带来的计算开销问题，简化多模态多任务自动驾驶系统的设计。

Method: 基于线性组RNN算子，构建单一通用架构，支持LiDAR-only、时序LiDAR、多模态和多模态时序融合等多种配置。

Result: 在3D感知（目标检测、跟踪、占据预测、BEV地图分割）、预测（运动预测）和规划（端到端规划）等任务中表现优异。

Conclusion: UniLION为自动驾驶3D基础模型的发展提供了新视角，简化系统设计的同时保持卓越性能。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [270] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo是首个专家策划的手术视频生成模型评估基准，通过四层Surgical Plausibility Pyramid框架评估模型输出，发现Veo-3模型在视觉感知层面表现优秀，但在更高层次的器械操作、环境反馈和手术意图理解方面存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在通用物理世界模拟方面表现出色，但在需要深度专业因果知识的高风险领域如手术中应用仍存在关键空白，需要专门评估框架。

Method: 开发SurgVeo基准和四层Surgical Plausibility Pyramid评估框架，让Veo-3模型在腹腔镜和神经外科手术剪辑上进行零样本预测任务，由四位认证外科医生按SPP标准评估生成视频。

Result: 发现明显的"可信度差距"：Veo-3在视觉感知可信度方面表现优异，但在器械操作可信度、环境反馈可信度和手术意图可信度等更高层次上严重失败。

Conclusion: 这项工作首次量化证明了手术AI中视觉逼真模仿与因果理解之间的鸿沟，为开发能够应对专业医疗领域复杂性的未来模型奠定了重要基础。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [271] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一个提示驱动的GraphRAG框架，强调提示设计在实体提取、事实选择和段落重排序中的重要性，用于提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于图的RAG方法在复杂推理方面已有研究，但提示设计对检索和推理过程的影响仍未充分探索。

Method: 从文本数据创建符号知识图，将实体和事实关系编码为结构化三元组；在线检索时选择性使用LLM进行语义过滤和答案生成；通过个性化PageRank实现基于知识图的实体引导图遍历。

Result: 在HotpotQA和2WikiMultiHopQA数据集上达到最先进性能，F1分数分别为80.7%和78.9%，Recall@5分别为97.1%和98.1%。

Conclusion: 提示设计是提高检索准确性和响应质量的重要部分，为更高效和可理解的多跳问答系统奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [272] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: Scitextures数据集是一个大规模的科学纹理和视觉模式集合，包含1200多个模型和10万张图像，涵盖物理、化学、生物、社会学、技术、数学和艺术等领域，用于探索视觉模式与生成机制之间的联系。


<details>
  <summary>Details</summary>
Motivation: 连接视觉模式与形成过程代表了最深层次的视觉理解能力，如云层纹理、城市生长等模式都源于底层机制。

Method: 通过自主AI流水线收集和实现标准化模型，使用Scitextures评估AI模型将视觉模式与生成模型和代码关联的能力，并测试AI推断和重建视觉模式背后机制的能力。

Result: 基准测试显示视觉语言模型能够理解和模拟超越视觉模式的物理系统。

Conclusion: 该数据集为探索世界视觉模式与生成机制之间的联系提供了途径，展示了AI在理解复杂系统方面的潜力。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [273] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: TIR-Bench是一个用于评估智能体在图像处理中思考能力的综合基准测试，包含13个多样化任务，测试模型使用工具进行图像处理和操作的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分捕捉像OpenAI o3这样的模型在图像处理中的高级思考能力，即使是常见的Visual Search基准也只测试基本操作，无法评估复杂、动态和工具依赖的推理能力。

Method: 开发了TIR-Bench基准测试，包含13个多样化任务，每个任务都需要在思维链中使用新颖工具进行图像处理和操作。评估了22个多模态大语言模型，包括领先的开源和专有模型，以及具有明确工具使用增强的模型。

Result: 结果显示TIR-Bench具有普遍挑战性，强大的性能需要真正的图像思考能力。进行了直接与智能体微调的对比研究。

Conclusion: TIR-Bench是一个全面评估智能体图像思考能力的基准测试，填补了现有基准在复杂图像推理能力评估方面的空白。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [274] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep是一个端到端的深度学习框架，通过将平面扫描算法适配用于声纳和视觉数据的跨模态融合，解决了水下环境中3D重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 水下视觉退化环境中的3D重建面临重大挑战：基于视觉的方法因能见度差和几何约束而失效，声纳方法存在高程模糊和低分辨率的固有缺陷。现有的融合技术依赖启发式方法和有缺陷的几何假设，导致显著伪影且无法建模复杂场景。

Method: SonarSweep采用端到端深度学习框架，将原理性的平面扫描算法适配用于声纳和视觉数据的跨模态融合。

Result: 在高保真仿真和真实环境中的广泛实验表明，SonarSweep能够持续生成密集且准确的深度图，在各种挑战性条件下（特别是高浊度环境）显著优于最先进方法。

Conclusion: SonarSweep成功克服了水下3D重建的限制，作者将公开发布代码和首个包含同步立体相机与声纳数据的新型数据集，以促进进一步研究。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [275] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay是一个端到端的零样本视觉语言导航框架，使用三个前视RGB-D图像和自然语言指令，无需全景视图和路径点预测器，显著降低延迟并提升实际应用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的零样本导航方法依赖全景观测和两阶段流水线，导致显著延迟并限制实际应用。

Method: 仅使用三个前视RGB-D图像结合语言指令，让MLLM直接预测动作；引入不确定性感知推理模块，包括消歧模块和未来-过去双向推理机制。

Result: 在仿真和真实机器人环境中，显著降低每步延迟，同时达到或超越全景视图基线的性能。

Conclusion: Fast-SmartWay展示了在真实世界零样本具身导航中的实用性和有效性。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [276] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 提出LiDAR-VGGT框架，通过两阶段粗到精融合将LiDAR惯性里程计与VGGT模型紧密耦合，解决现有方法对校准敏感和缺乏度量尺度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR惯性视觉里程计对校准敏感，而3D视觉基础模型在大规模环境中可扩展性有限且缺乏度量尺度，需要克服这些限制。

Method: 两阶段粗到精融合：预融合模块通过鲁棒初始化细化估计VGGT位姿和点云；后融合模块增强跨模态3D相似变换，使用边界框正则化减少尺度失真。

Result: 在多个数据集上的实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，优于VGGT方法和LIVO基线。

Conclusion: 提出的框架有效解决了校准敏感性和度量尺度缺失问题，将发布开源彩色点云评估工具包。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [277] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify是一个从RGB图像或文本提示自动合成关节物体的框架，解决了高自由度物体的运动学拓扑推断和关节参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 关节物体对于机器人操作和物理模拟至关重要，但现有方法依赖运动序列或手动标注数据集，难以扩展到复杂系统。

Method: 结合MCTS搜索进行结构推断和基于几何的优化进行关节推理，生成物理一致且功能有效的描述。

Result: 在合成和真实环境中的多样化输入上评估，在配准和运动学拓扑准确性方面优于现有方法。

Conclusion: Kinematify能够从静态几何自动生成关节物体模型，为机器人操作和模拟提供可扩展的解决方案。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [278] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: MARS是一个基于多模态大语言模型的多智能体机器人系统，专为智能家居机器人设计，通过四个智能体的协作实现风险感知、个性化辅助和可执行技能规划。


<details>
  <summary>Details</summary>
Motivation: 现有系统在风险感知规划、用户个性化和将语言计划转化为可执行技能方面存在困难，特别是在杂乱的家庭环境中。

Method: 系统集成四个智能体：视觉感知智能体提取环境语义和空间特征，风险评估智能体识别和优先处理危险，规划智能体生成可执行动作序列，评估智能体进行迭代优化。

Result: 在多个数据集上的实验表明，该系统在风险感知规划和协调多智能体执行方面优于最先进的多模态模型。

Conclusion: 该方法展示了协作AI在实际辅助场景中的潜力，并为在真实环境中部署MLLM驱动的多智能体系统提供了可推广的方法论。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [279] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 提出了统一扩散VLA模型和联合离散去噪扩散过程(JD3P)，通过同步去噪过程联合优化图像生成和动作预测，在CALVIN等基准上达到SOTA性能，推理速度比自回归方法快4倍。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型要么依赖外部专家进行模态统一，要么将图像生成和动作预测作为独立过程处理，限制了这两个任务之间的直接协同效益。

Method: 采用联合扩散过程，将多种模态整合到单一去噪轨迹中，基于统一令牌化空间和混合注意力机制，提出两阶段训练流程和推理时优化技术。

Result: 在CALVIN、LIBERO和SimplerEnv等基准上达到最先进性能，推理速度比自回归方法快4倍，并通过深入分析和真实世界评估验证了有效性。

Conclusion: 通过同步去噪过程实现理解、生成和行动的固有协同，为VLA模型提供了更高效统一的解决方案。

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [280] [Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach](https://arxiv.org/abs/2511.00508)
*Renjun Gao,Xiangjie Kong,Dongting Cai,Boyi Fu,Junxiang Yang*

Main category: math.NA

TL;DR: 本文提出了基于拉格朗日乘子法的Allen-Cahn模型重建算法，用于从点云数据重建物体表面，通过设计边缘检测函数确保能量稳定性，并实现了无条件稳定的全离散方案。


<details>
  <summary>Details</summary>
Motivation: 点云重建在假肢制造、医学成像和计算机视觉等领域至关重要，需要开发高效稳定的重建算法来处理复杂几何形状。

Method: 采用拉格朗日乘子法重构控制方程，结合Crank-Nicolson时间离散和有限差分空间近似，设计基于无符号距离函数的边缘检测函数来确保能量稳定性。

Result: 数值实验验证了算法在复杂3D体积（如《星球大战》角色）重建中的准确性、稳定性和有效性，并分析了参数选择对重建细节的影响。

Conclusion: 该算法为点云重建提供了一种稳定有效的解决方案，代码和数据已在GitHub上开源供研究使用。

Abstract: Reconstruction of an object from points cloud is essential in prosthetics,
medical imaging, computer vision, etc. We present an effective algorithm for an
Allen--Cahn-type model of reconstruction, employing the Lagrange multiplier
approach. Utilizing scattered data points from an object, we reconstruct a
narrow shell by solving the governing equation enhanced with an edge detection
function derived from the unsigned distance function. The specifically designed
edge detection function ensures the energy stability. By reformulating the
governing equation through the Lagrange multiplier technique and implementing a
Crank--Nicolson time discretization, we can update the solutions in a stable
and decoupled manner. The spatial operations are approximated using the finite
difference method, and we analytically demonstrate the unconditional stability
of the fully discrete scheme. Comprehensive numerical experiments, including
reconstructions of complex 3D volumes such as characters from \textit{Star
Wars}, validate the algorithm's accuracy, stability, and effectiveness.
Additionally, we analyze how specific parameter selections influence the level
of detail and refinement in the reconstructed volumes. To facilitate the
interested readers to understand our algorithm, we share the computational
codes and data in https://github.com/cfdyang521/C-3PO/tree/main.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [281] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: ORANGE是一个在线自演化框架，通过解析翻译日志中的SQL查询构建数据库特定知识库，积累领域知识来缩小语义鸿沟，提高SQL翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言转SQL方面进展显著，但其通用知识与数据库领域特定语义之间存在语义鸿沟。历史翻译日志包含丰富的领域内知识，现有方法主要增强单个翻译的推理过程，但未能从过往翻译中积累领域知识。

Method: 提出ORANGE框架，通过解析翻译日志中的SQL查询构建数据库特定知识库。采用嵌套思维链SQL到文本策略与元组语义追踪，确保知识生成的可靠性。

Result: 在多个基准测试上的实验证实了ORANGE的实用性，特别是在处理复杂和领域特定查询方面表现出有效性。

Conclusion: ORANGE框架通过积累领域知识有效缩小语义鸿沟，提高了SQL翻译的准确性，特别适用于现实世界的文本到SQL部署场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [282] [GeneFlow: Translation of Single-cell Gene Expression to Histopathological Images via Rectified Flow](https://arxiv.org/abs/2511.00119)
*Mengbo Wang,Shourya Verma,Aditya Malusare,Luopin Wang,Yiyang Lu,Vaneet Aggarwal,Mario Sola,Ananth Grama,Nadia Atallah Lanman*

Main category: q-bio.QM

TL;DR: GeneFlow是一个基于校正流的新型框架，能够将空间转录组数据映射到配对的细胞图像，生成不同染色方法的高分辨率图像，并解决转录组与图像之间的多对一映射问题。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术可以将转录组与组织病理形态对齐，为生物分子发现提供新机会。然而，转录组与图像之间存在固有的多对一关系，需要开发新方法来有效映射。

Method: 结合基于注意力的RNA编码器和由校正流引导的条件UNet，使用高阶ODE求解器创建转录组与图像流形之间的连续双射映射。

Result: 该方法能够从观察到的基因表达谱生成真实的细胞形态特征和空间分辨的细胞间相互作用，在所有实验中优于基于扩散的基线方法。

Conclusion: GeneFlow框架为整合遗传/化学扰动提供了潜力，并通过揭示成像表型中的失调模式实现疾病诊断，在空间转录组数据分析方面表现出优越性能。

Abstract: Spatial transcriptomics (ST) technologies can be used to align transcriptomes
with histopathological morphology, presenting exciting new opportunities for
biomolecular discovery. Using ST data, we construct a novel framework,
GeneFlow, to map transcriptomics onto paired cellular images. By combining an
attention-based RNA encoder with a conditional UNet guided by rectified flow,
we generate high-resolution images with different staining methods (e.g. H&E,
DAPI) to highlight various cellular/tissue structures. Rectified flow with
high-order ODE solvers creates a continuous, bijective mapping between
transcriptomics and image manifolds, addressing the many-to-one relationship
inherent in this problem. Our method enables the generation of realistic
cellular morphology features and spatially resolved intercellular interactions
from observational gene expression profiles, provides potential to incorporate
genetic/chemical perturbations, and enables disease diagnosis by revealing
dysregulated patterns in imaging phenotypes. Our rectified flow-based method
outperforms diffusion-based baseline method in all experiments. Code can be
found at https://github.com/wangmengbo/GeneFlow.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [283] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 该论文首次证明了在μP参数化的线性多层感知机中，学习率随宽度变化的可迁移性，显示在无限宽度极限下最优学习率收敛到非零常数。


<details>
  <summary>Details</summary>
Motivation: 研究μP参数化下学习率随网络宽度变化的可迁移特性，为特征学习最大化提供理论解释。

Method: 使用μP参数化线性多层感知机，理论分析最优学习率在无限宽度极限下的收敛行为，并与标准参数化(SP)和神经正切参数化(NTP)进行对比。

Result: 在μP参数化下，最优学习率随宽度增加收敛到非零常数，而在SP和NTP参数化下该性质不成立。

Conclusion: μP参数化能够实现学习率的可迁移性，为无限宽度神经网络的特征学习提供了理论基础。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>


### [284] [Few-Shot Multimodal Medical Imaging: A Theoretical Framework](https://arxiv.org/abs/2511.01140)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: stat.ML

TL;DR: 本文提出了一个统一的理论框架，用于解决医学影像在低资源条件下的学习和推理问题，通过PAC学习和PAC-Bayesian理论为样本效率、不确定性量化和可解释性提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域面临数据获取困难、数据系统分散、数据集不平衡等结构性障碍，导致诊断不确定性增加、模型鲁棒性降低和诊断决策偏差。现有方法缺乏在数据稀缺情况下成功或失败的理论依据。

Method: 首先形式化少样本条件下的学习目标并计算样本复杂度约束，然后基于PAC学习和PAC-Bayesian理论解释多模态集成如何促进泛化并量化稀疏监督下的不确定性，最后提出解释稳定性的形式化度量。

Result: 该框架为构建可靠、数据高效的诊断系统建立了原则性基础，能够联合表征样本效率、不确定性量化和可解释性。

Conclusion: 提出的统一理论框架为低资源医学影像条件下的学习提供了坚实的理论基础，解决了现有方法缺乏理论支撑的问题。

Abstract: Medical imaging relies heavily on large, labeled datasets. But,
unfortunately, they are not always easily accessible in clinical settings.
Additionally, many practitioners often face various structural obstacles like
limited data availability, fragmented data systems, and unbalanced datasets.
These barriers often lead to the increased diagnostic uncertainty,
underrepresentation of certain conditions, reduced model robustness, and biased
diagnostic decisions. In response to these challenges, approaches such as
transfer learning, meta-learning, and multimodal fusion have made great
strides. However, they still need a solid theoretical justification for why
they succeed or fail in situations where data is scarce. To address this gap,
we propose a unified theoretical framework that characterizes learning and
inference under low-resource medical imaging conditions. We first formalize the
learning objective under few-shot conditions and compute sample complexity
constraints to estimate the smallest quantity of data needed to achieve
clinically reliable accuracy. Then based on ideas from PAC-learning and
PAC-Bayesian theory, we explain how multimodal integration encourages
generalization and quantifies uncertainty under sparse supervision. We further
propose a formal metric for explanation stability, offering interpretability
guarantees under low-data conditions. Taken together, the proposed framework
establishes a principled foundation for constructing dependable, data-efficient
diagnostic systems by jointly characterizing sample efficiency, uncertainty
quantification, and interpretability in a unified theoretical setting.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [285] [Real-time and Zero-footprint Bag of Synthetic Syllables Algorithm for E-mail Spam Detection Using Subject Line and Short Text Fields](https://arxiv.org/abs/2511.00118)
*Stanislav Selitskiy*

Main category: cs.CR

TL;DR: 提出了一种基于合成音节袋算法的轻量级垃圾邮件检测方法，专门针对邮件主题行等短文本字段，无需额外存储或硬件升级。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习架构资源消耗大且需要离线处理，不适合实时垃圾邮件过滤。大部分垃圾邮件使用简单算法即可检测，只有小部分智能垃圾邮件需要复杂架构。

Method: 使用合成音节袋算法为每个邮件主题行生成约200维稀疏哈希向量，通过余弦或欧几里得距离计算与已知垃圾邮件主题的相似度。

Result: 算法在真实SMTP流量的一天数据上进行了性能测试。

Conclusion: 该算法能有效卸载复杂架构的压力，实现近实时、零占用空间的垃圾邮件检测。

Abstract: Contemporary e-mail services have high availability expectations from the
customers and are resource-strained because of the high-volume throughput and
spam attacks. Deep Machine Learning architectures, which are resource hungry
and require off-line processing due to the long processing times, are not
acceptable at the front line filters. On the other hand, the bulk of the
incoming spam is not sophisticated enough to bypass even the simplest
algorithms. While the small fraction of the intelligent, highly mutable spam
can be detected only by the deep architectures, the stress on them can be
unloaded by the simple near real-time and near zero-footprint algorithms such
as the Bag of Synthetic Syllables algorithm applied to the short texts of the
e-mail subject lines and other short text fields. The proposed algorithm
creates a circa 200 sparse dimensional hash or vector for each e-mail subject
line that can be compared for the cosine or euclidean proximity distance to
find similarities to the known spammy subjects. The algorithm does not require
any persistent storage, dictionaries, additional hardware upgrades or software
packages. The performance of the algorithm is presented on the one day of the
real SMTP traffic.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [286] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 本文提出GrowthHacker基准测试框架，使用LLM和基于LLM的智能体通过代码优化来改进离线策略评估(OPE)性能，在真实世界数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试需要大量资源且可能对用户产生负面影响，而离线策略评估(OPE)使用日志数据评估技术，在医疗、推荐系统等高成本高风险领域尤为重要。但现有研究很少探索如何利用LLM优化OPE结果。

Method: 提出GrowthHacker基准测试框架，包含智能体和基线方法，在Open Bandit Pipeline和Scope-RL上迭代优化代码、评估结果并开始新的优化周期。开发了two_agent框架来降低系统复杂度同时保持优化效果。

Result: two_agent框架实现了100%的可靠性和106.7%的平均改进率。two_agent和CrewAI都达到了45%的成功率，优于AutoGen的34%。

Conclusion: 基于LLM的智能体可以作为自动化的"增长黑客"来增强OPE系统，这对在生产环境中扩展数据驱动决策具有重要意义。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [287] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM是一个两阶段训练框架，使LLM能够编写测试工具代码，通过生成输入和验证输出来提高测试多样性和调试能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的测试生成方法主要产生输入-输出对，测试多样性有限且无法提供足够的调试信息。

Method: 采用两阶段训练：SFT（监督微调）后接RLVR（强化学习与验证奖励），使用定制奖励设计训练LLM编写测试工具代码。

Result: HarnessLLM在错误发现和测试策略多样性方面优于基于输入-输出的测试方法，并能通过测试时扩展提升代码生成性能。

Conclusion: HarnessLLM通过生成测试工具代码实现了更复杂的测试用例和灵活的输出验证，显著提升了测试效果。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [288] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 本研究通过分析超过9000个Java开源软件仓库中的225,000条SATD注释，将技术债务注释与其周围的源代码结构关联起来，发现SATD主要出现在内联代码、定义、条件语句和异常处理等位置。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注检测和优先处理自承认技术债务(SATD)，但很少关注受SATD影响的源代码。本研究旨在将SATD注释与其周围的源代码结构连接起来。

Method: 利用包含9000多个Java开源软件仓库代码注释的PENTACET数据集，定量推断SATD最常见出现的位置以及最常影响的代码结构/语句。

Result: 大规模研究显示，SATD主要出现在内联代码、定义、条件语句和异常处理附近，这些位置开发者面临不确定性和权衡决策。

Conclusion: SATD是开发者在变更过程中有意识认知的信号，而不仅仅是疏忽的体现，揭示了开发者在面对不确定性时的权衡决策。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [289] [Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements](https://arxiv.org/abs/2511.00449)
*Xiaolong Li,Zhi-Qin John Xu,Yan Ren,Tianming Qiu,Xiaowen Wang*

Main category: eess.IV

TL;DR: 本文提出了一种改进的nnU-Net框架，专门用于儿科高级别胶质瘤的脑肿瘤分割，在BraTS 2025 Task-6验证排行榜上获得第一名。


<details>
  <summary>Details</summary>
Motivation: 儿科脑肿瘤分割面临数据有限、解剖变异大和机构间成像异质性等独特挑战，需要专门的方法来提高分割准确性。

Method: 采用改进的nnU-Net框架，包括加宽残差编码器与SE注意力机制、3D深度可分离卷积、特异性驱动的正则化项、小尺度高斯权重初始化，以及两个后处理步骤。

Result: 在验证集上获得优异的Dice分数：CC 0.759、ED 0.967、ET 0.826、NET 0.910、TC 0.928、WT 0.928。

Conclusion: 该方法在儿科脑肿瘤分割任务中表现出色，为诊断、治疗计划和监测提供了有效的解决方案。

Abstract: Accurate segmentation of pediatric brain tumors in multi-parametric magnetic
resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and
monitoring, yet faces unique challenges due to limited data, high anatomical
variability, and heterogeneous imaging across institutions. In this work, we
present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the
largest public dataset of pre-treatment pediatric high-grade gliomas. Our
contributions include: (1) a widened residual encoder with
squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions;
(3) a specificity-driven regularization term; and (4) small-scale Gaussian
weight initialization. We further refine predictions with two postprocessing
steps. Our models achieved first place on the Task-6 validation leaderboard,
attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910
(NET), 0.928 (TC) and 0.928 (WT).

</details>


### [290] [Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation](https://arxiv.org/abs/2511.00477)
*Aditya Parikh,Sneha Das,Aasa Feragen*

Main category: eess.IV

TL;DR: 本文研究了医学图像分割中的算法偏见问题，特别是在乳腺癌分割中针对年轻患者的性能差异。研究发现存在"偏见标尺效应"，即系统性的错误标签会误导模型偏见的评估。通过控制实验证明，偏见源于年轻患者病例本质上的学习难度，而非标签质量或病例难度不平衡。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的算法偏见可能加剧健康不平等，特别是在分割任务中偏见原因尚不清楚。虽然分类任务中的公平性已被广泛研究，但分割任务在临床重要性下仍未被充分探索。乳腺癌分割模型对年轻患者表现出显著性能差异，通常归因于乳腺密度的生理差异。

Method: 审计MAMA-MIA数据集，建立年龄相关偏见的定量基线；通过控制实验系统性地反驳偏见源于标签质量敏感性或病例难度不平衡的假设；平衡训练数据的难度来测试是否能缓解差异；分析系统性偏见在训练有偏见的机器生成标签时如何被学习和放大。

Result: 发现关键的"偏见标尺效应"，系统性的错误验证标签会歪曲模型的实际偏见；平衡训练数据难度无法缓解差异，表明年轻患者病例本质更难学习；提供直接证据表明系统性偏见在训练有偏见的机器生成标签时会被学习和放大。

Conclusion: 这项工作为诊断医学分割中的算法偏见引入了系统性框架，并证明实现公平性需要解决定性的分布差异，而不仅仅是平衡病例数量。这对于自动化标注流程具有重要意义。

Abstract: Algorithmic bias in medical imaging can perpetuate health disparities, yet
its causes remain poorly understood in segmentation tasks. While fairness has
been extensively studied in classification, segmentation remains underexplored
despite its clinical importance. In breast cancer segmentation, models exhibit
significant performance disparities against younger patients, commonly
attributed to physiological differences in breast density. We audit the
MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in
its automated labels, and reveal a critical Biased Ruler effect where
systematically flawed labels for validation misrepresent a model's actual bias.
However, whether this bias originates from lower-quality annotations (label
bias) or from fundamentally more challenging image characteristics remains
unclear. Through controlled experiments, we systematically refute hypotheses
that the bias stems from label quality sensitivity or quantitative case
difficulty imbalance. Balancing training data by difficulty fails to mitigate
the disparity, revealing that younger patient cases are intrinsically harder to
learn. We provide direct evidence that systemic bias is learned and amplified
when training on biased, machine-generated labels, a critical finding for
automated annotation pipelines. This work introduces a systematic framework for
diagnosing algorithmic bias in medical segmentation and demonstrates that
achieving fairness requires addressing qualitative distributional differences
rather than merely balancing case counts.

</details>


### [291] [Image-based ground distance detection for crop-residue-covered soil](https://arxiv.org/abs/2511.00548)
*Baochao Wang,Xingyu Zhang,Qingtao Zong,Alim Pulatov,Shuqi Shang,Dongwei Wang*

Main category: eess.IV

TL;DR: 提出一种基于图像的方法，通过结合3D相机和RGB相机，在覆盖作物残留物的土壤上精确测量地面距离，以解决保护性农业中播种深度控制的难题。


<details>
  <summary>Details</summary>
Motivation: 保护性农业中土壤表面覆盖作物残留物，但现有距离测量技术无法区分残留物和土壤的距离信息，导致播种深度控制不精确。

Method: 同时获取深度图像和彩色图像，利用彩色图像区分残留物和土壤区域生成掩码图像，然后将掩码应用于深度图像，仅使用土壤区域的深度信息计算地面距离。

Result: 该方法可实现实时测量，测量误差在±3mm以内。

Conclusion: 该方法适用于保护性农业机械的精确深度播种，以及其他需要深度控制的应用如移植或耕作。

Abstract: Conservation agriculture features a soil surface covered with crop residues,
which brings benefits of improving soil health and saving water. However, one
significant challenge in conservation agriculture lies in precisely controlling
the seeding depth on the soil covered with crop residues. This is constrained
by the lack of ground distance information, since current distance measurement
techniques, like laser, ultrasonic, or mechanical displacement sensors, are
incapable of differentiating whether the distance information comes from the
residue or the soil. This paper presents an image-based method to get the
ground distance information for the crop-residues-covered soil. This method is
performed with 3D camera and RGB camera, obtaining depth image and color image
at the same time. The color image is used to distinguish the different areas of
residues and soil and finally generates a mask image. The mask image is applied
to the depth image so that only the soil area depth information can be used to
calculate the ground distance, and residue areas can be recognized and excluded
from ground distance detection. Experimentation shows that this distance
measurement method is feasible for real-time implementation, and the
measurement error is within plus or minus 3mm. It can be applied in
conservation agriculture machinery for precision depth seeding, as well as
other depth-control-demanding applications like transplant or tillage.

</details>


### [292] [GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations](https://arxiv.org/abs/2511.00598)
*Zixuan Sun,Shuaifeng Zhi,Ruize Li,Jingyuan Xia,Yongxiang Liu,Weidong Jiang*

Main category: eess.IV

TL;DR: 提出GDROS框架，通过几何引导的密集配准方法解决光学-SAR图像配准中的模态差异问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 光学和SAR遥感图像配准是图像融合和视觉导航的基础任务，但由于严重的非线性辐射差异、几何畸变和噪声变化等模态差异，现有基于模板和稀疏关键点的方法在大几何变换下难以实现可靠配准。

Method: 使用CNN-Transformer混合特征提取模块提取跨模态深度特征，构建多尺度4D相关体并迭代优化建立像素级密集对应关系，通过最小二乘回归模块对预测的光流场施加几何约束。

Result: 在WHU-Opt-SAR、OS和UBCv2三个不同空间分辨率的数据集上进行广泛实验，定性和定量结果均显示GDROS在所有指标上显著优于当前最先进方法。

Conclusion: GDROS框架通过几何引导的密集配准策略有效解决了光学-SAR图像配准中的模态差异问题，在不同成像分辨率下均表现出鲁棒性能。

Abstract: Registration of optical and synthetic aperture radar (SAR) remote sensing
images serves as a critical foundation for image fusion and visual navigation
tasks. This task is particularly challenging because of their modal
discrepancy, primarily manifested as severe nonlinear radiometric differences
(NRD), geometric distortions, and noise variations. Under large geometric
transformations, existing classical template-based and sparse keypoint-based
strategies struggle to achieve reliable registration results for optical-SAR
image pairs. To address these limitations, we propose GDROS, a geometry-guided
dense registration framework leveraging global cross-modal image interactions.
First, we extract cross-modal deep features from optical and SAR images through
a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D
correlation volume is constructed and iteratively refined to establish
pixel-wise dense correspondences. Subsequently, we implement a least squares
regression (LSR) module to geometrically constrain the predicted dense optical
flow field. Such geometry guidance mitigates prediction divergence by directly
imposing an estimated affine transformation on the final flow predictions.
Extensive experiments have been conducted on three representative datasets
WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial
resolutions, demonstrating robust performance of our proposed method across
different imaging resolutions. Qualitative and quantitative results show that
GDROS significantly outperforms current state-of-the-art methods in all
metrics. Our source code will be released at:
https://github.com/Zi-Xuan-Sun/GDROS.

</details>


### [293] [Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars](https://arxiv.org/abs/2511.00652)
*Ali Khalid,Jaiaid Mobin,Sumanth Rao Appala,Avinash Maurya,Stephany Berrio Perez,M. Mustafa Rafique,Fawad Ahmad*

Main category: eess.IV

TL;DR: DejaView是一种针对自动驾驶车辆3D点云数据的压缩系统，通过利用车辆在相同路线上重复行驶产生的长期时间冗余性，实现高效的压缩效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆每天产生数TB的传感器数据，其中3D点云数据占很大比例。这些数据需要传输到云端存储，用于机器学习训练或事故调查分析。为了降低网络和存储成本，需要更有效的压缩方法。

Method: DejaView的核心是一个diff操作，将点云数据表示为相对于过去3D数据的增量。系统利用自动驾驶车辆运行区域有限且经常重复相同路线的特点，在更大时间尺度（天和月）上寻找冗余性进行压缩。

Result: 使用两个月的LiDAR数据进行测试，DejaView能够以仅15厘米的重建误差将点云压缩210倍。

Conclusion: DejaView通过利用长期时间冗余性，为自动驾驶车辆的3D点云数据提供了高效的压缩解决方案，显著降低了网络和存储成本。

Abstract: An autonomous vehicle can generate several terabytes of sensor data per day.
A significant portion of this data consists of 3D point clouds produced by
depth sensors such as LiDARs. This data must be transferred to cloud storage,
where it is utilized for training machine learning models or conducting
analyses, such as forensic investigations in the event of an accident. To
reduce network and storage costs, this paper introduces DejaView. Although
prior work uses interframe redundancies to compress data, DejaView searches for
and uses redundancies on larger temporal scales (days and months) for more
effective compression. We designed DejaView with the insight that the operating
area of autonomous vehicles is limited and that vehicles mostly traverse the
same routes daily. Consequently, the 3D data they collect daily is likely
similar to the data they have captured in the past. To capture this, the core
of DejaView is a diff operation that compactly represents point clouds as delta
w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end
implementation of DejaView can compress point clouds by a factor of 210 at a
reconstruction error of only 15 cm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [294] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文提出Agent-REINFORCE框架，通过LLM代理增强的强化学习方法，在固定计算预算下搜索最优的多LLM协作图架构，以提升测试时扩展的性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常采用固定的协作架构和单一模型，忽略了不同任务需要不同的最优架构和模型组合，因此需要研究在固定预算下搜索计算最优的模型组合和架构。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，将REINFORCE流程映射为采样-反馈-更新，其中反馈作为文本梯度来更新概率图，从而高效搜索多LLM协作图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能够有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: Agent-REINFORCE框架成功解决了测试时扩展中多LLM协作图搜索的挑战，为动态调整模型组合和架构提供了有效解决方案。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [295] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 该研究评估稀疏自编码器(SAEs)在识别和控制LLM中种族与污名化概念关联的能力，发现SAEs可识别与黑人相关的潜在特征，但通过SAE引导减轻偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，需要开发方法检测模型是否不当依赖患者种族信息进行预测。

Method: 使用稀疏自编码器识别Gemma-2模型中与黑人个体相关的潜在特征，并通过激活这些潜在特征来引导模型输出，评估其对偏见的影响。

Result: 发现与黑人相关的潜在特征不仅对合理输入序列激活，也对问题词汇如"监禁"激活；激活该潜在特征会增加模型将患者评估为"好斗"的风险；SAE引导在简单场景中可改善偏见，但在复杂临床任务中效果不佳。

Conclusion: SAEs可作为识别LLM在临床应用中不当依赖人口统计信息的工具，但通过SAE引导减轻偏见在现实任务中效用有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [296] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLM的校准能力在网络深度中演化，上层存在置信度校正阶段，识别出残差流中的低维校准方向，扰动该方向可改善校准指标而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM校准能力在网络深度中的演化过程，理解置信度调节机制如何在整个前向传播过程中形成，而不仅仅是在最终投影层。

Method: 分析多个开源模型在MMLU基准上的表现，研究校准在网络各层的演化，识别置信度校正阶段和残差流中的校准方向。

Result: 发现上层存在主动重新校准模型置信度的阶段，识别出低维校准方向，扰动该方向可显著改善ECE和MCE等校准指标。

Conclusion: 校准是一个分布式现象，在整个网络前向传播过程中形成，而不仅仅在最终投影层，这为理解LLM中的置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [297] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 本文提出了一种新的解码策略——Pivot-Aware Speculative Decoding，通过只拒绝会导致最终输出效用下降的关键token（pivot tokens），在保持任务性能的同时显著提高了接受率，实现了最高2.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统的Speculative Decoding要求输出分布与目标模型完全匹配，这导致接受率过低，限制了加速潜力。作者认为实际应用中任务特定性能（如代码正确性、事实准确性）比采样分布更重要。

Method: 提出Pivot-Aware Speculative Decoding策略，训练轻量级分类器识别关键token（pivot tokens），只拒绝那些会导致效用下降的token，而不是所有不匹配的token。

Result: 在多个数据集上的评估表明，该方法在保持可比效用的同时，实现了最高2.5倍的加速。

Conclusion: 通过放宽分布匹配要求，专注于任务特定效用，可以显著提高解码效率，同时保持模型性能。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [298] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 本文提出了EPIC框架，通过对比学习构建共享表示空间来选择合适的推理方法，在保证准确性的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择答案，但假设更多候选答案能带来更高准确性。本文重新审视这一假设，旨在找到更有效的推理方法选择策略。

Method: EPIC框架：通过理论分析推导标准聚合方法的准确性边界，构建共享表示空间捕捉模型推理能力和查询-方法兼容性，将概率边界作为正则化器进行效用驱动的优化。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，在提高准确性的同时减少计算开销。

Conclusion: EPIC框架通过理论分析和对比学习，有效解决了语言模型生成中推理方法选择的关键挑战，实现了准确性提升与计算效率的平衡。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [299] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文从贝叶斯角度提出了一个统一框架来解释LLM的控制方法，认为上下文学习和激活引导都是通过改变模型对潜在概念的信念来影响行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM控制方法（上下文学习和激活引导）的解释各不相同，但它们的共同目标是控制模型行为，因此需要探索这些方法是否属于更广泛框架的特定实例。

Method: 开发了一个基于贝叶斯视角的统一预测模型，将上下文干预视为证据积累，激活引导视为改变概念先验，建立了闭式贝叶斯模型来预测LLM行为。

Result: 该模型能够解释先前的经验现象（如S形学习曲线），并预测新现象（如对数信念空间中的干预可加性），在多个领域验证了其预测能力。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [300] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC评估协议，用于确保测试时计算方法的公平评估，不受LLM性能和API成本波动的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效。

Method: 设计了一个公平评估协议，标准化少样本提示和答案提取过程，支持跨多个LLM和多样化数据集的评估，并提供成本建模程序。

Result: 开发了开源工具FEval-TTC，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC协议为测试时计算方法的评估提供了标准化和公平的框架，有助于研究的可重复性和可比性。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [301] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出RLAC（对抗性批评强化学习）方法，通过动态规则验证解决开放生成任务中规则验证成本高的问题，使用LLM作为批评器动态识别最可能的失败模式，联合优化生成器和批评器。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样化的评估规则，但规则数量庞大导致验证成本过高且评估不完整，使得基于规则的强化学习后训练难以扩展。

Method: 使用大语言模型作为动态批评器，识别最可能的失败模式（如事实错误、未处理边界情况），通过外部验证器验证，联合优化生成器和批评器。

Result: 实验表明RLAC在文本生成中提高事实准确性，在代码生成中提高正确性，优于穷举验证和奖励模型方法。

Conclusion: 动态批评器比固定批评器更有效，RLAC有潜力将强化学习后训练扩展到自由形式生成任务。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [302] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种新的参数高效微调方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来解决语言模型微调中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，现有PEFT方法如LoRA存在秩约束限制，稀疏适配提供了不施加秩约束的替代方案。

Method: RIGSA方法：1）随机初始化全秩适配器；2）使用ReZero类似的门控机制；3）通过迭代幅度剪枝进行稀疏化。

Result: 在SmolLM2-1.7B-Instruct模型上测试，RIGSA能够学习新的视觉文本任务（Textual MNIST），相比QLoRA显示出更少的遗忘，特别是在GSM8k任务上。

Conclusion: RIGSA在保持学习能力的同时，相比QLoRA能更好地缓解灾难性遗忘问题，尽管与随机掩码方法性能相当。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [303] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的自主VR测试代理，能够实时导航VR环境并交互虚拟对象，解决了传统人工测试无法规模化的问题。


<details>
  <summary>Details</summary>
Motivation: VR内容的质量、安全和适用性保证面临挑战，传统人工测试劳动密集且无法适应行业快速增长，而现有自动化测试方法难以处理VR的高维感官输入和实时性能要求。

Method: 使用增强型动作分块变换器从人类演示中学习，预测多步动作序列，并引入动态可调滑动视界来平衡响应性和精度。

Result: 在商业VR游戏上评估显示，VRScout仅需有限训练数据即可达到专家级性能，并在消费级硬件上保持60FPS的实时推理。

Conclusion: VRScout为自动化VR游戏测试提供了一个实用且可扩展的框架，可直接应用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [304] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出了一种基于条件标签生成对抗网络的无监督损伤检测和数字孪生方法，该方法无需系统健康状态的先验信息，在Z24桥梁基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时存在预测不确定性，需要开发不依赖先验信息的损伤检测框架。

Method: 使用条件标签生成对抗网络，将相同损伤级别的测量作为输入，强制模型收敛到不同损伤状态；通过比较收敛分数识别不同损伤状态；结合支持向量机分类器和主成分分析评估生成和真实测量数据。

Result: 该方法能够准确捕获健康测量中的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 提出的无监督框架在损伤检测和数字孪生方面优于现有方法，特别适用于实际应用中损伤状态未知的情况。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [305] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短期记忆网络和卷积神经网络在动态结构载荷识别中的性能，并与基于物理的残差卡尔曼滤波器进行对比，分析在不同载荷场景下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中由于测试数据少或结构模型不可识别导致的动态载荷识别不确定性问题，评估不同方法在小数据集训练条件下的表现。

Method: 使用三种神经网络（GRU、LSTM、CNN）和残差卡尔曼滤波器（RKF），分别在三种场景下进行测试：模拟结构在顶层激振器激励下的响应、加州建筑在地震基底激励下的响应、IASC-ASCE结构健康监测基准问题的冲击和瞬时载荷条件。

Result: 不同方法在不同载荷场景下表现各异，RKF在物理参数可识别的情况下优于神经网络方法。

Conclusion: 各种方法在不同载荷识别场景中各有优势，RKF在物理参数可识别的情况下表现最佳，而神经网络方法在其他特定场景下可能更优。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [306] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 本文提出了一种结合集成学习和可解释人工智能（XAI）的机器学习模型，用于早期黑色素瘤检测，旨在解决深度学习模型在医疗诊断中缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤是致命性皮肤癌，早期检测至关重要。虽然深度学习模型能高精度检测病变，但其黑盒特性导致缺乏可靠性和信任度，限制了在医疗领域的应用。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用XAI技术来解释预测依据，确保预测的可靠性。

Result: 通过集成学习和XAI技术的结合，能够实现高精度的黑色素瘤检测，同时提供可解释的预测结果。

Conclusion: XAI技术可以有效解决深度学习模型在医疗诊断中的可解释性问题，提高模型的可信度和实用性，为早期黑色素瘤检测提供可靠支持。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [307] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出Gradient-Guided Sampling (GGS)方法解决对抗攻击在迁移场景中的利用与探索困境，通过梯度引导采样平衡攻击强度与跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型架构迁移时面临利用（最大化攻击强度）与探索（增强跨模型泛化）的根本困境。传统方法过度偏重利用，而近期方法过度偏重探索，需要平衡两者。

Method: 基于MI-FGSM，引入内迭代随机采样，使用前一次内迭代的梯度引导采样方向，采样幅度由随机分布决定，使对抗样本位于平衡区域。

Result: 在多个DNN架构和多模态大语言模型上的综合实验表明，该方法优于最先进的迁移攻击方法。

Conclusion: GGS方法通过梯度引导采样有效平衡了攻击强度与跨模型泛化能力，解决了对抗攻击迁移性的根本困境。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [308] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 本研究提出了一种基于ROI引导掩码策略的静息态fMRI基础模型，相比传统随机掩码方法，在ADHD分类任务中准确率提升4.23%。


<details>
  <summary>Details</summary>
Motivation: 随着大规模脑成像数据集的可用性增加，神经影像学领域需要能够有效泛化的基础模型。传统随机掩码方法缺乏对大脑解剖结构的考虑。

Method: 使用AAL3图谱进行ROI引导掩码策略，在4D fMRI全脑体积中选择性地掩码语义连贯的大脑区域，进行自监督预训练。

Result: 在ADHD-200数据集（973名受试者）上，该方法比传统随机掩码在ADHD分类准确率上提升4.23%。归因分析显示边缘系统和脑区对重建保真度贡献最大。

Conclusion: 在模型预训练中掩码解剖区域不仅能增强可解释性，还能产生更鲁棒和判别性的表征。未来将扩展到更多神经影像数据集并开发新的损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [309] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出Lo-Hp框架，通过解耦的两阶段权重生成方法解决现有生成式权重优化中的过耦合和长视野问题，提升灵活性和推理效率


<details>
  <summary>Details</summary>
Motivation: 当前基于生成模型的权重生成方法存在过耦合和长视野问题，前者限制了优化器的灵活性，后者导致推理效率低和精度不足

Method: 采用解耦的两阶段权重生成框架，结合混合策略子轨迹平衡目标，整合在线和离线学习来捕捉局部优化策略

Result: 理论证明仅学习局部优化策略可解决长视野问题并提升全局最优权重生成，在迁移学习、小样本学习等任务中验证了优越的准确性和推理效率

Conclusion: Lo-Hp框架通过解耦设计和混合策略学习有效解决了权重生成中的关键问题，为需要频繁权重更新的任务提供了高效解决方案

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [310] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个基于GFlowNets的概念擦除框架，通过探索去噪路径空间来引导生成远离目标概念，同时保持模型先验知识，无需精心设计的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有的概念擦除技术存在图像质量下降、依赖脆弱的对抗损失或需要大量重新训练的问题，需要一种更有效的方法来安全地从文本到图像生成器中移除有害或专有概念。

Method: 将概念遗忘问题重新定义为在去噪路径空间中的探索，使用配备轨迹平衡目标的GFlowNets优化，通过采样整个轨迹而非单一最终状态来学习随机策略。

Result: EraseFlow在广泛实验中被证明优于现有基线方法，在性能和先验保持之间实现了最佳平衡，能有效泛化到未见概念并避免可被攻击的奖励机制。

Conclusion: EraseFlow通过重新定义概念擦除为去噪路径探索问题，提供了一种无需精心设计奖励模型的有效解决方案，在保持生成质量的同时成功移除目标概念。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [311] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT是一种面向边缘设备的优化视觉Transformer设计，通过集成LUT神经元层来减少模型大小和计算需求，在保持准确性的同时提升能效。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在边缘设备上部署面临计算、内存和能耗挑战，现有基于LUT的网络在视觉任务上表现不佳，需要设计更适合边缘推理的Transformer架构。

Method: 设计LUT-based通道混合器替代传统MLP层，采用神经网络学习方法原生学习LUT函数，并开发专门的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的准确率，消除60%模型权重和50%乘法运算，能效提升1.9倍，延迟降低1.3倍。

Conclusion: LL-ViT为边缘设备提供了高效的视觉Transformer解决方案，在保持性能的同时显著降低了资源需求。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [312] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了一种基于范畴对称性感知的学习框架，通过将时间、尺度和传感器层次结构的变化因素融入特征表示结构，使模型在现实扭曲下保持稳定。


<details>
  <summary>Details</summary>
Motivation: 解决人类活动识别中传感器信号随上下文、运动和环境变化而漂移的问题，需要模型在环境变化时保持稳定性。

Method: 使用范畴对称性感知学习框架，将信号在时间、尺度和传感器层次结构上的变化因素构建到特征表示结构中，实现类别等变表示理论。

Result: 在UCI人类活动识别基准测试中，分布外准确率提高了约46个百分点（比基线提高约3.6倍）。

Conclusion: 抽象的对称性原理可以通过范畴等变表示理论转化为日常感知任务中的具体性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [313] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 本文提出了一种并行解耦框架（PDF），通过利用多模态大语言模型的专有可操控性，为多模态嵌入学习生成多个并行嵌入，突破了传统SSC范式的限制。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型受限于SSC范式（单一输入、单一嵌入、对比监督），无法充分利用MLLM的能力，将丰富多面的输入压缩为单一嵌入。

Method: 使用共享的MLLM骨干网络，通过不同的可学习前缀条件化，为单个输入生成多个并行路径，获得并行嵌入。采用互信息最小化作为显式约束，结合每路径对比监督来保持语义对齐。

Result: 在MMEB基准测试中显著提升性能：VLM2Vec-LLaVA-1.6-LR模型提升+8.9%（7B），VLM2Vec-Qwen2VL模型提升+4.2%（2B）和+3.1%（7B）。2B模型仅用一半计算预算就超越基线+2.6%。

Conclusion: PDF框架能够产生鲁棒的语义覆盖和可泛化的嵌入空间，推理时仅需单次前向传播，计算开销可忽略不计。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [314] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了分数扩散桥模型(FDBM)，这是一种基于分数布朗运动近似的新型生成扩散桥框架，能够捕捉真实随机过程中的记忆效应、长程依赖性和反常扩散现象。


<details>
  <summary>Details</summary>
Motivation: 真实随机过程具有记忆效应、时间相关性、长程依赖性和反常扩散等特征，而标准扩散或桥模型使用的布朗运动无法捕捉这些特性。

Method: 利用分数布朗运动的马尔可夫近似(MA-fBM)构建FDBM，保持分数布朗运动的非马尔可夫特性同时实现可处理的推理。扩展到Schrödinger桥问题并推导出无配对数据转换的学习损失函数。

Result: 在蛋白质构象预测和未配对图像转换任务中，FDBM相比布朗运动基线表现更优：蛋白质结构预测中Cα原子位置的均方根偏差更低，图像转换中Fréchet Inception距离更低。

Conclusion: FDBM框架在保持分数布朗运动非马尔可夫特性的同时实现了可处理的推理，在多个任务中优于基于布朗运动的基线方法。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [315] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: SRGFormer是一个结构优化的多模态推荐模型，通过改进Transformer捕获用户整体行为模式，使用超图结构增强局部结构信息，并通过自监督任务整合多模态信息，在三个公开数据集上超越基准模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐模型存在三个问题：1）忽视冗余与有价值数据的区分；2）依赖单一语义框架导致用户偏好表示不完整；3）无法捕捉用户与物品间的复杂交互。

Method: 1）改进Transformer以捕获用户整体行为模式；2）将多模态信息嵌入超图结构学习局部结构；3）应用自监督任务增强多模态信息整合。

Result: 在三个公开数据集上超越基准模型，在Sports数据集上平均性能提升4.47%。

Conclusion: SRGFormer通过结构优化和多模态信息整合，有效解决了现有推荐模型的局限性，提升了推荐性能。

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


### [316] [LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks](https://arxiv.org/abs/2511.00072)
*Pradeep M,Ritesh Pallod,Satyen Abrol,Muthu Raman,Ian Anderson*

Main category: cs.IR

TL;DR: 提出并部署了一个端到端的产品搜索系统，用于将AI生成的虚拟造型与真实产品进行匹配，该系统每天处理超过35万个AI造型，覆盖1200万产品。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑时尚行业，能够创建虚拟造型和头像，因此需要找到与AI生成风格最匹配的真实产品。

Method: 搜索管道包含四个关键组件：查询生成、向量化、候选检索和基于AI生成造型的重新排序，使用CLIP模型进行向量化。

Result: 在实验中，CLIP模型在平均意见分数上比其他模型高出3-7%，虽然绝对改进不大，但显著改善了用户感知匹配度。

Conclusion: CLIP被确立为生产部署中最可靠的基础模型，能够提供更好的产品匹配效果。

Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars
making it essential to find real products that best match AI-generated styles.
We propose an end-to-end product search system that has been deployed in a
real-world, internet scale which ensures that AI-generated looks presented to
users are matched with the most visually and semantically similar products from
the indexed vector space. The search pipeline is composed of four key
components: query generation, vectorization, candidate retrieval, and reranking
based on AI-generated looks. Recommendation quality is evaluated using
human-judged accuracy scores. The system currently serves more than 350,000 AI
Looks in production per day, covering diverse product categories across global
markets of over 12 million products. In our experiments, we observed that
across multiple annotators and categories, CLIP outperformed alternative models
by a small relative margin of 3--7\% in mean opinion scores. These
improvements, though modest in absolute numbers, resulted in noticeably better
user perception matches, establishing CLIP as the most reliable backbone for
production deployment.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [317] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: LongCat-Flash-Omni是一个5600亿参数的开源全模态模型，采用渐进式训练策略和高效的多模态感知模块，在保持实时音频-视觉交互的同时，在多种模态任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多种模态（文本、图像、视频、音频）且支持实时交互的大规模开源模型，解决大规模多模态训练中的数据异构性和计算效率问题。

Method: 采用渐进式训练策略，从简单到复杂逐步训练多模态序列建模任务；基于Shortcut连接的MoE架构，集成高效多模态感知和语音重建模块；使用模态解耦并行方案处理数据异构性。

Result: 模型在5600亿参数规模下（激活270亿参数）实现低延迟实时音频-视觉交互，在多模态基准测试中达到开源模型的最先进性能，在文本、图像、视频、音频理解与生成等任务上表现优异。

Conclusion: LongCat-Flash-Omni展示了大规模多模态模型的有效训练方法，通过创新的架构和训练策略实现了高效的多模态能力，为社区提供了强大的开源基础模型。

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [318] [\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs](https://arxiv.org/abs/2511.00488)
*Jun Gao,Yun Peng,Xiaoxue Ren*

Main category: cs.PL

TL;DR: 本文提出了ReMind多智能体框架，通过Mutator、Executor和Inspector的协作来改进LLMs在演绎代码推理方面的能力，解决了生成与推理能力差距、代码源偏见和零样本泛化弱等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码相关任务中取得了显著进展，但实证研究表明它们在演绎代码推理（理解程序执行过程）方面仍然存在困难。现有研究虽然认识到这一局限性，但根本原因尚未得到充分探索。

Method: 提出ReMind多智能体框架，包含三个组件：Mutator生成代码变体以减少代码源偏见；Executor逐步跟踪变量状态以暴露不一致性；Inspector识别有问题的推理步骤并提供控制流细化来弥合推理差距。

Result: 在两个基准测试和五个LLMs上的广泛实验表明，ReMind在演绎代码推理方面相比基线方法具有显著优势，实现了优异的性能和强大的零样本泛化能力。

Conclusion: ReMind通过多智能体的协同合作，系统地识别和优化推理缺陷，有效解决了LLMs在演绎代码推理中的关键挑战，为代码推理任务提供了强大的解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in
code-related tasks. Despite their advancement, empirical evidence reveals that
they still struggle with \emph{deductive code reasoning}, the ability to reason
about the program execution process. While prior studies have recognized this
limitation, the underlying causes remain largely underexplored. In this paper,
we begin by presenting a comprehensive empirical study that reveals three key
challenges undermining deductive code reasoning: (1) an intrinsic gap between
generation and reasoning abilities, (2) a consistent bias towards code sources,
and (3) weak zero-shot generalization on complex benchmarks. In light of these
challenges, we propose \texttt{ReMind}, a multi-agent framework composed of
\texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The
\texttt{Mutator} generates code variants to mitigate bias towards code sources,
the \texttt{Executor} traces variable states step-by-step to expose
inconsistency, and the \texttt{Inspector} identifies problematic reasoning
steps and provides control-flow refinement to bridge the intrinsic reasoning
gap. Through their coordinated collaboration, \texttt{ReMind} systematically
identifies and refines reasoning flaws, achieving outstanding performance and
enabling robust zero-shot generalization. Extensive experiments on two
benchmarks with five LLMs demonstrate the superior advantages of
\texttt{ReMind} compared to baseline approaches in deductive code reasoning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [319] [Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images](https://arxiv.org/abs/2511.00702)
*Alberto Di Biase*

Main category: cs.GR

TL;DR: 该论文探索了将医学成像中的扩散张量成像（DTI）和纤维束追踪技术应用于图像绘画渲染的方法，通过结构张量提供局部方向信息来模拟艺术家绘画过程中的笔触放置。


<details>
  <summary>Details</summary>
Motivation: 将医学领域成熟的扩散张量成像和纤维束追踪技术跨界应用于计算机图形学中的绘画风格渲染，探索两个领域之间的技术相似性和应用可能性。

Method: 使用结构张量替代梯度来获取更好的局部方向信息，通过纤维束追踪算法来放置模拟艺术家绘画过程的笔触，类似于DTI中的纤维追踪过程。

Result: 该方法成功应用于肖像画和一般图像的绘画风格渲染，展示了纤维追踪与笔触放置之间的平行关系，所有代码已在GitHub上开源。

Conclusion: 这项研究展示了扩散张量成像技术在图像绘画渲染中的跨领域应用潜力，为计算机图形学提供了新的技术思路。

Abstract: Doctors and researchers routinely use diffusion tensor imaging (DTI) and
tractography to visualize the fibrous structure of tissues in the human body.
This paper explores the connection of these techniques to the painterly
rendering of images. Using a tractography algorithm the presented method can
place brush strokes that mimic the painting process of human artists,
analogously to how fibres are tracked in DTI. The analogue to the diffusion
tensor for image orientation is the structural tensor, which can provide better
local orientation information than the gradient alone. I demonstrate this
technique in portraits and general images, and discuss the parallels between
fibre tracking and brush stroke placement, and frame it in the language of
tractography. This work presents an exploratory investigation into the
cross-domain application of diffusion tensor imaging techniques to painterly
rendering of images. All the code is available at
https://github.com/tito21/st-python

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [320] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出一种多模态假评论检测框架，结合文本和视觉特征，在包含21,142张用户上传图片的数据集上验证，F1分数达0.934，优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中，用户生成评论对消费者行为至关重要，但由机器人、付费代理或AI生成的虚假评论威胁评论生态系统的信任和透明度。现有检测模型主要依赖单模态文本数据，无法捕捉跨模态的语义不一致性。

Method: 提出鲁棒的多模态假评论检测框架，集成BERT编码的文本特征和ResNet-50提取的视觉特征，通过分类头融合这些表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线。混淆矩阵和定性分析显示模型能检测细微不一致性，如夸张文本赞美与无关或低质量图片的配对。

Conclusion: 本研究证明了多模态学习在保护数字信任中的关键作用，为各种在线平台的内容审核提供了可扩展解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [321] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个针对量子科学领域的LLM评估基准，包含约800个多项选择题，涵盖9个量子科学领域，用于评估LLM在非直观量子领域的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用基准很少反映量子科学等专业领域的需求，而量子科学具有非直观现象和高级数学要求，需要专门评估LLM在该领域的知识准确性。

Method: 使用公开材料编制约800个问题及其答案，涵盖9个量子科学领域，组织成八选项多项选择题数据集，评估多个现有LLM的性能。

Result: 评估了多个现有LLM在量子领域的表现，包括对问题格式变化的敏感性分析。

Conclusion: QuantumBench是首个量子领域LLM评估数据集，旨在指导LLM在量子研究中的有效应用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [322] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型如何帮助解决认知科学领域面临的知识整合和概念清晰度挑战，包括跨学科连接、理论形式化、测量分类学发展等方面。


<details>
  <summary>Details</summary>
Motivation: 认知科学由于其多面性和跨学科性质，在知识综合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了潜在工具。

Method: 通过综述分析，考察LLMs在当前认知科学关键挑战领域的应用能力，包括跨学科连接建立、理论形式化、测量分类学开发、通用性建模框架以及情境和个体差异捕捉。

Result: LLMs在支持认知科学整合方面展现出潜力，能够帮助建立跨学科联系、形式化理论概念、开发清晰的测量分类体系，但也存在局限性。

Conclusion: 当审慎使用时，LLMs可以作为补充而非替代人类专业知识的工具，促进认知科学朝着更整合和累积的方向发展。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [323] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理新范式，通过五步结构化流程增强LLM对人类多元价值观的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法往往只产生表面一致性而非真正的伦理理解，无法处理人类价值观的复杂性和情境依赖性。

Method: 采用五步结构化流程：情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析和反思，可通过提示工程或监督微调实现。

Result: 在专门设计的SafeWorld基准测试中，该框架显著提升了LLM与多元人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜性的推理。

Conclusion: 这项工作为通过跨学科研究开发更有效对齐全球社会多元价值观的LLM提供了具体路径。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [324] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一个模型无关的解码框架，通过在高熵token处选择性分支并应用早停机制来选择最短的完整推理路径，以解决大型推理模型中的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但经常出现过度思考问题，产生过长的思维链推理轨迹，这会增加推理成本并可能降低准确性。研究发现推理长度与准确性之间存在明显的负相关关系。

Method: 提出DTS解码框架：1）通过在高熵token处选择性分支来绘制推理空间；2）应用早停机制选择最短的完整推理路径；3）无需额外训练或监督，近似最优解。

Result: 在AIME2024和AIME2025数据集上的实验显示，DTS将准确性提高了8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS能够实现可扩展且高效的大型推理模型推理，在提升准确性的同时显著降低推理成本。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [325] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统和大型语言模型的自动化网络故障排除框架，通过协调多个专业工具来加速电信网络故障诊断和修复。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂性增加，现有AI模型范围狭窄、需要大量标注数据且难以泛化，网络故障排除仍严重依赖专家手动操作。

Method: 采用多智能体系统，包括编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体，并基于专有故障排除文档微调小型语言模型来生成领域特定的解决方案计划。

Result: 实验结果表明，该框架显著加速了无线接入网和核心网领域的故障排除自动化。

Conclusion: 多智能体系统与语言模型结合能够有效实现电信网络的自动化故障排除，减少对专家的依赖。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [326] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究现代大语言模型中增加采样推理路径对自一致性方法的影响，发现性能提升在适度采样后趋于平缓，高采样配置相对于计算成本收益有限。


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于多推理链组合改善结果但达到平台期的结论，在现代模型条件下检验这一现象。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上，比较不同采样推理路径配置与单链思维基线。

Result: 更大模型显示出更稳定一致的改进曲线，性能提升在适度采样后达到平台期，与过去发现一致。

Conclusion: 自一致性方法仍有价值，但高采样配置相对于计算成本收益有限，平台期由推理路径重叠导致收益递减驱动。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [327] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 该论文提出了AI自我意识指数(AISAI)框架，通过"猜2/3平均数"游戏测试28个大型语言模型的自我意识能力，发现高级模型具有明显的自我意识特征。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在能力增强过程中是否会产生自我意识这一涌现行为，并探索如何量化测量这种自我意识。

Method: 使用博弈论框架，通过"猜2/3平均数"游戏对28个模型进行4200次试验，测试模型在面对不同类型对手(人类、其他AI、同类AI)时的策略差异。

Result: 75%的高级模型表现出清晰的自我意识，能够根据对手类型调整策略；自我意识模型普遍认为自身比其他AI更理性，而其他AI又比人类更理性。

Conclusion: 自我意识是高级LLMs的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的认知具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [328] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中归纳头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被约束在19维子空间中，其中仅3个维度负责归纳头的形成，并发现其形成时间与输入上下文长度呈二次关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中in-context learning能力的关键机制——归纳头，理解其权重结构和工作原理，为解释Transformer的上下文学习能力提供理论依据。

Method: 使用简化的ICL任务和修改的Transformer架构，理论分析归纳头的权重结构，证明训练动态被约束在19维子空间，并通过实证验证仅3个维度负责归纳头的形成。

Result: 揭示了归纳头权重矩阵的简单可解释结构，证明了训练动态的维度约束，发现归纳头形成时间与输入上下文长度呈二次关系。

Conclusion: 归纳头的形成遵循特定的数学约束和动态规律，这为理解Transformer的in-context learning能力提供了重要的理论洞见。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [329] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一个交互式AI代理，通过可审计的行动序列生成可验证的解释，使用强化学习优化策略来寻求外部视觉证据支持诊断推理。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域（如医学）中AI模型解释缺乏可验证性，从而阻碍信任的问题。

Method: 使用强化学习训练策略，让代理战略性地寻求外部视觉证据来支持诊断推理，并引入因果干预方法验证解释的忠实性。

Result: 实验表明该方法显著提高了校准精度，Brier分数比非交互式基线降低18%；通过掩蔽视觉证据观察到性能下降（ΔBrier=+0.029），证实证据对决策过程的重要性。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [330] [S2Doc - Spatial-Semantic Document Format](https://arxiv.org/abs/2511.01113)
*Sebastian Kempf,Frank Puppe*

Main category: cs.DL

TL;DR: S2Doc是一个结合空间和语义信息的灵活文档和表格数据结构，旨在解决文档建模缺乏标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文档和表格建模缺乏统一标准，导致各种不兼容的数据结构，且大多数模型只关注空间或语义结构中的一方面。

Method: 开发了S2Doc数据结构，将空间和语义信息结合在单一格式中，支持多页文档，易于扩展到新任务。

Result: S2Doc是首个在单一格式中结合所有这些方面的文档建模方法。

Conclusion: S2Doc提供了一个统一且灵活的文档和表格建模解决方案，解决了现有方法的局限性。

Abstract: Documents are a common way to store and share information, with tables being
an important part of many documents. However, there is no real common
understanding of how to model documents and tables in particular. Because of
this lack of standardization, most scientific approaches have their own way of
modeling documents and tables, leading to a variety of different data
structures and formats that are not directly compatible. Furthermore, most data
models focus on either the spatial or the semantic structure of a document,
neglecting the other aspect. To address this, we developed S2Doc, a flexible
data structure for modeling documents and tables that combines both spatial and
semantic information in a single format. It is designed to be easily extendable
to new tasks and supports most modeling approaches for documents and tables,
including multi-page documents. To the best of our knowledge, it is the first
approach of its kind to combine all these aspects in a single format.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [331] [Novelty and Impact of Economics Papers](https://arxiv.org/abs/2511.01211)
*Chaofeng Wu*

Main category: econ.GN

TL;DR: 该论文提出将科学新颖性分解为空间新颖性和时间新颖性两个正交维度，分别衡量论文与邻近研究的智力差异性和对动态研究前沿的参与度，并发现这两个维度预测不同的科学影响力。


<details>
  <summary>Details</summary>
Motivation: 传统上科学新颖性被视为单一属性，但作者认为它应该反映论文在知识演化中的战略位置，需要更细粒度的分析框架来理解新颖性的多维本质。

Method: 利用大语言模型开发语义隔离指标，量化论文相对于全文文献的位置，并将此框架应用于大型经济学文献语料库进行分析。

Result: 发现空间新颖性主要预测颠覆性影响，而时间新颖性主要预测引用次数，据此构建了四种具有不同可预测影响特征的语义邻域原型。

Conclusion: 新颖性应被理解为多维建构，其不同形式反映了论文的战略定位，对科学进步具有可测量且根本不同的影响。

Abstract: We propose a framework that recasts scientific novelty not as a single
attribute of a paper, but as a reflection of its position within the evolving
intellectual landscape. We decompose this position into two orthogonal
dimensions: \textit{spatial novelty}, which measures a paper's intellectual
distinctiveness from its neighbors, and \textit{temporal novelty}, which
captures its engagement with a dynamic research frontier. To operationalize
these concepts, we leverage Large Language Models to develop semantic isolation
metrics that quantify a paper's location relative to the full-text literature.
Applying this framework to a large corpus of economics articles, we uncover a
fundamental trade-off: these two dimensions predict systematically different
outcomes. Temporal novelty primarily predicts citation counts, whereas spatial
novelty predicts disruptive impact. This distinction allows us to construct a
typology of semantic neighborhoods, identifying four archetypes associated with
distinct and predictable impact profiles. Our findings demonstrate that novelty
can be understood as a multidimensional construct whose different forms,
reflecting a paper's strategic location, have measurable and fundamentally
distinct consequences for scientific progress.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [332] [Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment](https://arxiv.org/abs/2511.00004)
*Adrian-Dinu Urse,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CY

TL;DR: 本文探索了在CrisisMMD多模态数据集上使用数据增强技术来解决类别不平衡和样本有限的问题，包括视觉数据的扩散方法和文本数据的多种增强策略，并在不同学习设置下评估效果。


<details>
  <summary>Details</summary>
Motivation: 自然灾害评估需要准确快速的信息获取，社交媒体成为重要的实时信息来源。但现有数据集存在类别不平衡和样本有限的问题，使得模型开发具有挑战性。

Method: 对于视觉数据应用基于扩散的方法（Real Guidance和DiffuseMix），对于文本数据探索回译、基于变换器的改写和基于图像描述的增强。在单模态、多模态和多视图学习设置下进行评估。

Result: 结果显示选定的增强方法提高了分类性能，特别是对于代表性不足的类别，而多视图学习显示出潜力但需要进一步改进。

Conclusion: 本研究强调了构建更稳健的灾害评估系统的有效增强策略。

Abstract: Natural disaster assessment relies on accurate and rapid access to
information, with social media emerging as a valuable real-time source.
However, existing datasets suffer from class imbalance and limited samples,
making effective model development a challenging task. This paper explores
augmentation techniques to address these issues on the CrisisMMD multimodal
dataset. For visual data, we apply diffusion-based methods, namely Real
Guidance and DiffuseMix. For text data, we explore back-translation,
paraphrasing with transformers, and image caption-based augmentation. We
evaluated these across unimodal, multimodal, and multi-view learning setups.
Results show that selected augmentations improve classification performance,
particularly for underrepresented classes, while multi-view learning introduces
potential but requires further refinement. This study highlights effective
augmentation strategies for building more robust disaster assessment systems.

</details>


### [333] [Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model](https://arxiv.org/abs/2511.00024)
*Haotian Hang,Yueyang Shen,Vicky Zhu,Jose Cruz,Michelle Li*

Main category: cs.CY

TL;DR: 本文提出了一种基于大语言模型的决策支持框架，用于大规模评估企业碳披露质量，通过整合评分标准和百分位标准化方法，识别跨行业和跨地区的气候披露趋势和模式。


<details>
  <summary>Details</summary>
Motivation: 在全球可持续发展背景下，企业碳披露成为将商业战略与环境责任对齐的关键机制。但碳披露项目(CDP)数据的异质性和自由形式特性给分析带来了挑战。

Method: 开发了一个主评分标准，统一了11年CDP数据的叙述评分，结合基于百分位的标准化方法，识别时间趋势、战略对齐模式以及披露不一致性。

Result: 结果显示技术和德国等国家和行业表现出更高的评分标准对齐度，而其他行业和地区则显示出波动性或表面参与，为投资者、监管者和企业ESG战略制定者提供了决策洞察。

Conclusion: 基于LLM的方法将非结构化披露转化为可量化、可解释、可比较和可操作的情报，推进了AI赋能决策支持系统在气候治理领域的能力。

Abstract: In the context of global sustainability mandates, corporate carbon disclosure
has emerged as a critical mechanism for aligning business strategy with
environmental responsibility. The Carbon Disclosure Project (CDP) hosts the
world's largest longitudinal dataset of climate-related survey responses,
combining structured indicators with open-ended narratives, but the
heterogeneity and free-form nature of these disclosures present significant
analytical challenges for benchmarking, compliance monitoring, and investment
screening. This paper proposes a novel decision-support framework that
leverages large language models (LLMs) to assess corporate climate disclosure
quality at scale. It develops a master rubric that harmonizes narrative scoring
across 11 years of CDP data (2010-2020), enabling cross-sector and
cross-country benchmarking. By integrating rubric-guided scoring with
percentile-based normalization, our method identifies temporal trends,
strategic alignment patterns, and inconsistencies in disclosure across
industries and regions. Results reveal that sectors such as technology and
countries like Germany consistently demonstrate higher rubric alignment, while
others exhibit volatility or superficial engagement, offering insights that
inform key decision-making processes for investors, regulators, and corporate
environmental, social, and governance (ESG) strategists. The proposed LLM-based
approach transforms unstructured disclosures into quantifiable, interpretable,
comparable, and actionable intelligence, advancing the capabilities of
AI-enabled decision support systems (DSSs) in the domain of climate governance.

</details>


### [334] [Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies](https://arxiv.org/abs/2511.00106)
*Anuj Gupta,Ann Shivers-McNair*

Main category: cs.CY

TL;DR: 通过分析社交媒体上ChatGPT提示写作的修辞实践，研究如何促进批判性AI素养。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的普及，社交媒体上关于提示写作的讨论激增，这为研究新兴的AI素养实践提供了机会。

Method: 结合数字写作研究的四个传统，收集并分析了32,000条关于提示写作的推文，采用计算方法和定性方法相结合的研究方法。

Result: 识别出五个关键主题：提示写作影响的沟通领域、共享的微观素养资源、塑造提示写作的市场修辞、提示的修辞特征、以及提示写作的定义。

Conclusion: 为数字写作教师和研究人员提供了关于如何教授和分析批判性AI素养的重要启示。

Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt
writing on social media can promote critical AI literacies. Prompt writing is
the process of writing instructions for generative AI tools like ChatGPT to
elicit desired outputs and there has been an upsurge of conversations about it
on social media. To study this rhetorical activity, we build on four
overlapping traditions of digital writing research in computers and composition
that inform how we frame literacies, how we study social media rhetorics, how
we engage iteratively and reflexively with methodologies and technologies, and
how we blend computational methods with qualitative methods. Drawing on these
four traditions, our paper shows our iterative research process through which
we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)
from X (formerly Twitter) about prompt writing posted between November 2022 to
May 2023. We present five themes about these emerging AI literacy practices:
(1) areas of communication impacted by prompt writing, (2) micro-literacy
resources shared for prompt writing, (3) market rhetoric shaping prompt
writing, (4) rhetorical characteristics of prompts, and (5) definitions of
prompt writing. In discussing these themes and our methodologies, we highlight
takeaways for digital writing teachers and researchers who are teaching and
analyzing critical AI literacies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [335] [MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models](https://arxiv.org/abs/2511.00850)
*Yayue Deng,Guoqiang Hu,Haiyang Sun,Xiangyu Zhang,Haoyang Zhang,Fei Tian,Xuerui Yang,Gang Yu,Eng Siong Chng*

Main category: eess.AS

TL;DR: Multi-Bench是首个专门评估口语对话模型在多轮交互对话中情感智能能力的基准测试，包含基础情感理解和高级情感支持两个层级，共5个任务约3.2K样本。


<details>
  <summary>Details</summary>
Motivation: 当前口语对话模型在多轮交互对话能力方面研究不足，大多数基准测试只关注单轮对话，需要专门评估情感智能的基准。

Method: 采用分层结构设计：基础轨道用于情感理解和推理，高级轨道用于情感支持和应用；包含5个精心设计的任务，从情感识别到复杂推理和交互对话；配备可复现的评估框架。

Result: 评估了6个代表性口语对话模型在8个子集上的表现，结果显示当前模型在基础理解任务上表现良好，但在高级多轮交互对话和推理相关任务上仍有改进空间，特别是在情感意识和应用方面。

Conclusion: Multi-Bench填补了多轮交互对话评估的空白，揭示了当前口语对话模型在情感智能方面的局限性，为未来研究提供了重要基准。

Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to
sustain genuinely interactive multi-turn conversations remains underexplored,
as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,
the first benchmark explicitly designed to evaluate SDMs in multi-turn
interactive dialogue with an emphasis on emotional intelligence. Multi-Bench
employs a hierarchical structure with a basic track for emotion understanding
and reasoning and an advanced track for emotion support and application. It
comprises five carefully designed tasks and about 3.2K samples, ranging from
emotion recognition to complex reasoning and interactive dialogue, supported by
a reproducible evaluation framework. We evaluate six representative SDMs on
eight subsets of Multi-Bench. Results show that while current SDMs achieve good
performance on basic understanding tasks, they still have room for improvement
in advanced multi-turn interactive dialogue and reasoning-related tasks,
particularly in emotion awareness and application.

</details>
