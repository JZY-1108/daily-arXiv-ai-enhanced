<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CV](#cs.CV) [Total: 83]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 7]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.LG](#cs.LG) [Total: 3]
- [q-bio.PE](#q-bio.PE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文首次系统评估了大型语言模型在意大利语性别中性重写任务中的表现，提出了衡量中立性和语义保真度的二维框架，发现开源模型优于现有专用模型，微调后的小模型能达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 意大利语等语法性别语言的性别中性重写具有挑战性，需要消除不必要的性别指定同时保持语义，但目前缺乏系统评估。

Method: 采用少样本提示比较多个LLM，对选定模型进行微调，应用针对性清洗提升任务相关性，使用二维框架评估中立性和语义保真度。

Result: 开源权重LLM优于现有意大利语GNR专用模型，微调后的小模型以更小规模达到或超过最佳开源LLM性能。

Conclusion: 研究揭示了在中立性优化和意义保持之间的权衡关系，为语法性别语言的性别中性重写提供了有效的评估框架和解决方案。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: Op-Fed数据集：包含1044个FOMC会议记录的人工标注句子，用于货币政策立场分析，解决了类别不平衡和跨句子依赖的技术挑战


<details>
  <summary>Details</summary>
Motivation: 美联储FOMC的货币政策决策影响数百万人，但现有数据缺乏对货币政策立场的细粒度标注，需要构建专门的数据集来支持相关研究

Method: 采用五阶段分层标注框架分离观点、货币政策和立场方面，使用主动学习选择标注实例，将正例数量翻倍

Result: 最佳闭源LLM在观点分类上达到0.80零样本准确率，但在货币政策立场分类上仅0.61，低于人类基准0.89

Conclusion: Op-Fed数据集可用于未来模型训练、置信度校准和作为后续标注工作的种子数据集，显示LLM在复杂金融政策分析任务上仍有改进空间

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: DSTC12 Track 1提出两个子任务：多维度对话自动评估和多语言文化安全检测，基准模型在文化安全检测方面表现不佳，显示需要改进文化感知安全能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展需要更强大的对话系统评估方法，传统指标不足且安全考虑常存在文化偏见

Method: 设置两个子任务：1）对话级多维度自动评估指标（10个维度），2）多语言多文化安全检测，使用Llama-3-8B和Llama-Guard-3-1B作为基线模型

Result: 任务1中Llama-3-8B基线获得最高平均Spearman相关系数0.1681；任务2中参赛团队在多语言安全子集上显著优于基线（最佳ROC-AUC 0.9648），但基线在文化子集上表现更好（0.5126 ROC-AUC）

Conclusion: 对话系统评估仍需大幅改进，特别是在文化感知安全方面存在关键需求，多维度评估和多文化安全检测是重要研究方向

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 论文提出了一个分析框架来研究LLM在不同任务间的迁移学习效果，发现性能提升往往与表面数据集相似性无关，而是由隐藏的统计因素和语言特征决定


<details>
  <summary>Details</summary>
Motivation: 由于无法为所有任务获取高质量训练数据，需要依赖迁移学习来处理分布外请求，因此需要理解跨任务交互的复杂动态

Method: 构建迁移学习矩阵和降维分析框架，训练10个模型来识别潜在能力（推理、情感分类、自然语言理解、算术等）并分析迁移学习的副作用

Result: 发现性能改进往往无法用表面数据集相似性或源数据质量来解释，而是由源数据集的隐藏统计因素（如类别分布、生成长度倾向）和特定语言特征更影响

Conclusion: 这项工作揭示了迁移学习的复杂动态，为更可预测和有效的LLM适应铺平了道路

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 研究发现LLMs在内部表征中线性编码问题歧义性，通过识别歧义编码神经元(AENs)可以实现歧义检测和行为控制


<details>
  <summary>Details</summary>
Motivation: 现实问题普遍存在歧义性，但LLMs往往自信回答而非寻求澄清，需要理解模型如何内部处理歧义信息

Method: 在模型预填充阶段识别歧义编码神经元，训练探针进行歧义检测，并通过神经元操控控制模型行为

Result: 仅需少量神经元(少至1个)即可编码歧义信息，AENs探针在歧义检测上表现优异且具有跨数据集泛化能力，浅层即出现AENs

Conclusion: LLMs形成紧凑的内部歧义表征，使得模型行为具有可解释性和可控性，可从直接回答转向弃权

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 提出了首个中文文献语法纠错持续学习基准CL²GEC，包含10个学科的1万条标注句子，评估模型在多学科学术写作中的适应能力和抗遗忘性能。


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏多学科学术写作的专用基准，忽视了持续学习在处理领域特异性语言变异和防止灾难性遗忘方面的潜力。

Method: 构建包含10个学科10,000句的标注数据集，在持续学习设置下评估大语言模型，包括顺序调优、参数高效适应和四种代表性CL算法，使用标准GEC指标和适应任务级变异的持续学习指标。

Result: 实验结果表明，基于正则化的方法比基于回放或简单顺序方法更能有效缓解遗忘问题。

Conclusion: 该基准为跨多学科学术领域的自适应语法纠错研究提供了严谨的基础，推动了中文语法纠错系统在真实编辑场景中的实际应用。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: AgentCTG是一个新颖的可扩展框架，通过模拟多智能体工作流中的控制和调节机制，实现对文本生成的精确复杂控制，在多个公开数据集上达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域在许多任务上取得显著进展，但受控文本生成(CTG)仍面临诸多挑战，特别是在实现细粒度条件控制方面。实际应用场景中还需要考虑成本、可扩展性、领域知识学习和更精确控制等要求。

Method: 提出AgentCTG框架，通过多智能体协作机制模拟控制和调节过程，探索不同智能体间的协作方法，并引入自动提示模块来进一步提升生成效果。

Result: 在多个公开数据集上达到state-of-the-art效果。提出的新任务Character-Driven Rewriting能够将原始文本转换为符合特定角色配置文件的新文本，同时保留领域知识。在线导航角色扮演应用中显著提升了驾驶体验。

Conclusion: AgentCTG通过优化上下文相关文本的生成，实现了更沉浸式的在线社区交互，促进了更高的个性化和用户参与度，为受控文本生成提供了有效的解决方案。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: CARE是一个新的检索增强推理框架，通过教导LLM在推理过程中显式整合上下文证据，显著提高了检索准确性和答案生成性能，无需昂贵的监督微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在上下文保真度方面存在困难，对基于提供信息的问题产生不一致的回答。现有方法要么依赖昂贵的监督微调，要么训练模型进行网络搜索但未必改善给定上下文的利用。

Method: 提出CARE框架，教导LLM在推理过程中显式整合上下文证据，利用模型自身的检索能力，只需要有限的标记证据数据，通过策略性检索的上下文标记来增强推理链。

Result: 在多个真实世界和反事实QA基准测试中的广泛实验表明，该方法显著优于监督微调、传统检索增强生成方法和外部检索解决方案。

Conclusion: 这项工作代表了在使LLM更准确、可靠和高效地处理知识密集型任务方面的根本性进步。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 该研究构建了一个专注于比较级的日语NLI数据集，评估了各种LLM在零样本和少样本设置下的表现，发现模型性能受提示格式和示例标签影响，且在处理日语特有语言现象时存在困难，但包含逻辑语义表示的提示能帮助模型解决困难推理问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现优异，但涉及数值和逻辑表达式的推理仍然具有挑战性。比较级是与此类推理相关的关键语言现象，但LLM在处理比较级方面的鲁棒性，特别是在训练数据中不占主导地位的语言（如日语）中，尚未得到充分探索。

Method: 构建了一个专注于比较级的日语NLI数据集，在零样本和少样本设置下评估各种LLM，分析提示格式和示例标签对模型性能的影响，并测试包含逻辑语义表示的提示效果。

Result: 模型性能在零样本设置中对提示格式敏感，在少样本设置中受黄金标签影响；LLM在处理日语特有语言现象时存在困难；包含逻辑语义表示的提示能帮助模型预测那些即使在少样本示例下也难以解决的推理问题的正确标签。

Conclusion: 该研究揭示了LLM在处理日语比较级推理时的局限性，强调了提示工程的重要性，并证明逻辑语义表示可以增强模型在复杂推理任务中的表现，为改进多语言NLI系统提供了重要见解。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 使用DSPy提示优化技术让指令调优的大语言模型能够同时处理临床文本和结构化电子健康记录数据，在临床分类任务上达到与专业多模态系统相当的性能，但更简单且适应性更强


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本生成方面表现出色，但在处理包含时间序列等结构化数据的临床分类任务方面仍有待探索

Method: 采用基于DSPy的提示优化技术来适配指令调优的大语言模型，使其能够联合处理临床笔记和结构化EHR输入

Result: 该方法在性能上与专业的多模态系统相当，同时需要更少的复杂性，并在不同任务间具有更好的适应性

Conclusion: 通过提示优化技术，大语言模型可以有效地处理临床分类任务，为医疗AI应用提供了更简单灵活的解决方案

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: DSCC-HS是一个主动式框架，通过在自回归解码过程中注入实时转向向量来抑制LLM幻觉，无需修改目标模型，在TruthfulQA和BioGEN基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法如RAG通常是反应式的，LLM幻觉问题严重阻碍了其可靠部署，需要一种主动干预的解决方案。

Method: 基于双过程认知理论，使用紧凑代理模型分别作为事实对齐代理(FAP)和幻觉检测代理(HDP)，在推理时动态注入FAP和HDP对数概率差异作为转向向量。

Result: 在TruthfulQA上达到99.2%的事实一致性率，在BioGEN基准上获得最高FActScore 46.50。

Conclusion: DSCC-HS是一个原理清晰且高效的解决方案，能够显著提升LLM的事实性。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [12] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 开发了一个基于自然语言处理的筛查工具，用于自动检测放射肿瘤学中的高严重性事件报告，通过跨机构迁移学习实现了与人类专家相当的分类性能


<details>
  <summary>Details</summary>
Motivation: 医疗事件报告的手动审查耗时且需要专业知识，需要自动化工具来提高安全性和质量改进的效率

Method: 使用支持向量机(SVM)和预训练语言模型BlueBERT，在两个机构的放射肿瘤学事件报告数据集上进行训练和评估，采用跨机构迁移学习策略

Result: 在机构内部测试中AUROC达到0.82(SVM)和0.81(BlueBERT)，通过迁移学习后跨机构性能提升至AUROC 0.78，在人工编辑数据集上达到与人类专家相当的AUROC 0.81

Conclusion: 成功开发了跨机构的NLP模型，能够有效检测高严重性事件报告，性能与人类专家相当，为医疗安全质量改进提供了自动化解决方案

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [13] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的双阶段渐进式提示压缩方法DSPC，通过粗粒度句子过滤和细粒度token剪枝来减少LLM提示长度，在保持语义的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的提示越来越长，导致计算成本增加。现有提示压缩方法需要训练辅助模型，带来额外计算开销。

Method: 双阶段渐进压缩：1）粗粒度阶段基于TF-IDF过滤低语义价值句子；2）细粒度阶段使用注意力贡献、跨模型损失差异和位置重要性评估token重要性，剪枝低效用token。

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo上验证，在受限token预算下表现一致提升。在Longbench数据集的FewShot任务中，仅用1/3的token就达到49.17性能，比最佳基线LongLLMLingua提升7.76。

Conclusion: DSPC提供了一种无需训练的高效提示压缩方案，能显著减少计算成本同时保持模型性能，为解决提示膨胀问题提供了有效途径。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [14] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本研究提出了ccg-jcomp系统，这是一个基于组合语义学的日语比较级逻辑推理系统，专门针对日语比较级在形态和语义上与英语的差异而设计。


<details>
  <summary>Details</summary>
Motivation: 日语和英语比较级在形态和语义上存在显著差异，使得现有的英语比较级逻辑推理系统无法直接应用于日语。需要开发专门针对日语比较级的逻辑推理系统。

Method: 基于组合语义学构建逻辑推理系统ccg-jcomp，专门处理日语比较级的数值和逻辑表达式。

Result: 在包含比较表达的日语NLI数据集上评估系统性能，并与现有大型语言模型的准确率进行比较。

Conclusion: ccg-jcomp系统在日语比较级自然语言推理任务中表现出有效性，证明了基于组合语义学的逻辑方法在处理日语比较级推理方面的优势。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [15] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探索了阿拉伯语方言识别(ADI)的数据高效和参数高效方法，包括软提示策略、LoRA重参数化以及零样本/少样本推理，发现LLM在少样本设置下难以区分方言细微差别，而LoRA微调模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 研究如何通过数据高效和参数高效的方法来解决阿拉伯语方言识别问题，探索不同提示策略和微调方法在有限数据和计算资源下的效果

Method: 采用多种软提示策略(prefix-tuning、prompt-tuning、P-tuning、P-tuning V2)和LoRA重参数化；使用阿拉伯语专用编码器模型和开源解码器模型进行实验；分析零样本和少样本推理能力

Result: LLM在少样本或零样本设置下难以区分方言细微差别；软提示编码器变体表现更好；基于LoRA的微调模型表现最佳，甚至超过完全微调

Conclusion: LoRA微调是阿拉伯语方言识别中最有效的参数高效方法，而LLM在少样本设置下的方言识别能力有限，需要更专门的微调策略

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [16] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: CAMPUS框架通过动态选择和能力感知调整，解决了现有课程学习方法在指令调优中的刚性限制，实现了更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖静态启发式难度指标，无法适应模型在训练过程中的能力演变，导致固定的、可能次优的学习轨迹。

Method: 提出CAMPUS框架，包含动态子课程选择、能力感知的课程进度调整和基于多难度的调度策略。

Result: 大量实验证明CAMPUS在高效指令调优方面优于其他最先进的基线方法。

Conclusion: CAMPUS通过动态适应模型能力变化的课程学习方法，显著提升了指令调优的效果。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [17] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 本文研究了语法性别对自动职位排名系统的影响，提出了基于RBO的性别偏见评估方法，创建了四种语法性别语言的测试集，并发现现有多语言模型都存在不同程度的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 研究语法性别在职位名称中的显式分配如何影响自动职位排名系统的结果，旨在评估和量化这些系统中的性别偏见。

Method: 提出使用RBO（Rank-Biased Overlap）指标来比较控制性别因素的排名结果，生成并共享了四种语法性别语言的测试集，包含男性和女性形式的职业名称，并标注了性别和匹配相关性。

Result: 使用新测试集和方法评估了多个现成的多语言模型，结果显示所有模型都表现出不同程度的性别偏见。

Conclusion: 该研究为评估职位排名系统中的性别偏见奠定了基础，证明了现有模型普遍存在性别偏见问题，需要进一步改进。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [18] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于几何框架的黑盒方法，能同时估计全局和局部不确定性，用于检测大语言模型的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒方法只能提供全局不确定性估计，而能提供局部不确定性的方法通常需要白盒访问模型内部状态。需要一种新的黑盒斶策来同时解决这两个问题。

Method: 基于几何框架，通过对批量响应的原型分析来量化不确定性。全局层面使用Geometric Volume（测量原型嵌入的凸包体积），局部层面使用Geometric Suspicion（根据可靠性对响应进行排序）。

Result: 在短形式问答数据集上表现与现有方法相当或更好，在医疗数据集上表现更优，尤其在幻觉风险特别高的场景下。

Conclusion: 该几何框架提供了一种有效的黑盒方法，能同时处理全局和局部不确定性估计，并且通过理论证明了凸包体积与信息载之间的联系。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [19] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: AutoMin 2025共享任务包括会议纪要生成和问答两个任务，涵盖英语和捷克语，参与团队较少但包含多个基线系统评估


<details>
  <summary>Details</summary>
Motivation: 组织自动会议纪要生成共享任务，评估当前大语言模型在结构化会议纪要生成和基于会议记录的问答任务上的性能

Method: 设置两个主要任务：1）多语言会议纪要生成（英语和捷克语，项目会议和欧洲议会）；2）问答任务（单语英语问答和跨语言捷克语问答）。包含基线系统进行综合评估

Result: 2025年参与度较低，只有1个团队参加纪要生成任务，2个团队参加问答任务。组织方提供了多个基线系统来评估当前大语言模型的表现

Conclusion: 尽管参与团队有限，但通过基线系统的综合评估，为自动会议纪要生成和问答任务提供了有价值的基准测试结果

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [20] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 这篇论文研究大语言模型对德国方言语者的偏见，发现LLMs在联想和决策任务中都显示出显著的方言命名和使用偏见，而明确标注语言人口统计特征反而会增强偏见。


<details>
  <summary>Details</summary>
Motivation: 方言作为人类文化的重要组成部分，虽然在德国有40%以上人口使用，但方言语者常遇到负面社会定览。研究考察大语言模型是否也存在类似偏见。

Method: 采用社会语言学方法，通过联想任务和决策任务评估LLMs的方言命名偏见和方言使用偏见，并构建包含七种德国地区方言的新领域评估语料库。

Result: 所有评估的LLMs都显示出显著的方言偏见（负面形容词联想），所有模型都在决策中复现这些偏见，而明确标注语言人口统计特征比隐含线索更能增强偏见。

Conclusion: 大语言模型存在对德国方言语者的系统性偏见，这些偏见与社会定览相互映射，需要重视AI模型中的语言平等问题。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [21] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型在不同类型偏见场景下与人类价值观的对齐情况，发现模型规模不一定带来更好的对齐效果，模型对特定场景类型有偏好，且同一模型家族的判断一致性更高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能在与人类价值观不一致时产生不良后果，特别是在涉及复杂敏感社会偏见的场景中。之前的研究主要使用专家设计或基于代理的偏见场景，但不同类型场景下的对齐差异尚不清楚。

Method: 研究分析了来自4个模型家族的12个LLM和4个数据集，评估模型在不同类型偏见场景下的对齐情况，包括负面与非负面问题场景，并研究了模型对HVSB的解释能力。

Result: 发现大规模参数模型不一定具有更低的不对齐率和攻击成功率；模型对特定类型场景有对齐偏好；同一模型家族判断一致性更高；不同LLM对HVSB的理解无显著差异；模型偏好自身生成的解释；小模型经过微调后可生成更易读但模型认同度较低的解释。

Conclusion: LLM与人类价值观的对齐效果受场景类型影响，模型规模不是决定性因素，同一模型家族表现更一致，小模型可以通过微调获得解释能力但需要进一步提升模型认同度。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [22] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER框架结合科学证据检索、大语言模型推理和监督真实性预测，用于生物医学事实核查，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域中的错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成威胁。生物医学事实核查面临术语复杂、需要领域专业知识以及必须基于科学证据的独特挑战。

Method: 提出CER（Combining Evidence and Reasoning）框架，整合科学证据检索、大语言模型推理和监督真实性预测。通过将大语言模型的文本生成能力与高质量生物医学科学证据检索技术相结合，有效减少幻觉风险。

Result: 在专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上评估显示达到最先进性能，并展现出良好的跨数据集泛化能力。

Conclusion: CER框架为生物医学事实核查提供了有效的解决方案，通过证据检索和推理的结合确保输出基于可验证的证据来源，代码和数据已开源以促进透明性和可重复性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [23] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER是一个新颖的生物医学事实核查框架，结合科学证据检索、大语言模型推理和监督真实性预测，在多个专家标注数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域中的错误信息（如疫苗犹豫和未经证实的治疗方法）对公共卫生和医疗系统信任构成风险。生物医学声明验证具有独特挑战性，包括复杂术语、需要领域专业知识以及必须基于科学证据。

Method: CER框架整合了科学证据检索、通过大语言模型进行推理，以及监督真实性预测。通过将大语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术相结合，有效减少幻觉风险。

Result: 在专家标注数据集（HealthFC、BioASQ-7b、SciFact）上的评估显示达到了最先进的性能，并展现出有前景的跨数据集泛化能力。

Conclusion: CER框架通过结合证据检索和推理，为生物医学事实核查提供了有效的解决方案，代码和数据已开源以确保透明度和可重复性。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文评估了大语言模型在词义消歧任务中的表现，发现GPT-4o和DeepSeek-V3等领先模型与专门WSD系统性能相当，且在生成任务中能达到98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量评估工作，但LLMs是否真正理解词义仍缺乏深入探索。本文旨在填补这一空白，系统评估LLMs的词义理解能力。

Method: 评估指令调优LLMs的词义消歧能力，并与专门WSD系统比较；测试两种顶级开源和闭源LLMs在三种生成任务中的表现：定义生成、自由解释和示例生成。

Result: 在WSD任务中，GPT-4o和DeepSeek-V3等模型与专门WSD系统性能相当，且在不同领域和难度级别上表现更稳健；在生成任务中，LLMs解释词义的准确率高达98%，自由解释任务表现最佳。

Conclusion: LLMs在词义理解和生成方面表现出色，能够达到与专门系统相当的性能，特别是在自由形式的解释任务中展现了强大的生成能力。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 研究发现多语言检索增强生成系统存在语言偏好偏见，模型倾向于引用英文来源，特别是在查询为英文时，这种偏见在低资源语言和中间位置文档中更加明显，有时甚至会牺牲文档相关性来选择语言偏好。


<details>
  <summary>Details</summary>
Motivation: 研究多语言检索增强生成系统中不同文档语言的混合是否会对生成和引用产生意外影响，特别是语言偏好是否会影响引用行为。

Method: 采用受控方法，利用模型内部机制来衡量语言偏好，同时保持文档相关性等其他因素不变，在八种语言和六个开源模型上进行测试。

Result: 发现模型在英文查询时优先引用英文来源，这种偏见在低资源语言和中间位置文档中被放大，模型有时会牺牲文档相关性来选择语言偏好。

Conclusion: 研究揭示了语言模型如何利用多语言上下文并影响引用行为，表明引用选择并非总是由信息量驱动，语言偏好是一个重要因素。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 基于COMET框架构建的翻译质量评估系统，通过长上下文数据增强训练来预测错误跨度标注(ESA)分数，整合多种人工标注数据集，实验显示长上下文信息能提升与人工评估的相关性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统基于短片段训练的翻译质量评估模型在长上下文信息利用方面的不足，提升评估结果与人工判断的相关性。

Method: 使用COMET框架，通过拼接领域内人工标注句子构建长上下文训练数据，计算加权平均分数，整合MQM、SQM和DA多种人工判断数据集，训练多语言回归模型从源文本、假设翻译和参考翻译预测质量分数。

Result: 实验结果表明，相比仅基于短片段训练的模型，引入长上下文信息能够显著提高与人工评估的相关性。

Conclusion: 长上下文信息的有效利用是提升自动化翻译质量评估性能的关键因素，该方法在WMT25共享任务中展示了良好的效果。

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC是一种基于思维层面链间相似性的逐步剪枝策略，通过识别和移除冗余推理链来加速Self-Consistency方法，在保持或提高准确性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: Self-Consistency(SC)方法虽然能提升LLM推理性能，但其数量级的计算开销限制了广泛应用。现有加速方法主要依赖模型置信度分数或缺乏实证支持的启发式方法。

Method: 提出Slim-SC方法：1）理论和实证分析SC的低效性；2）基于思维层面的链间相似性识别冗余链；3）采用逐步剪枝策略移除冗余推理链

Result: 在三个STEM推理数据集和两种LLM架构上的实验表明：推理延迟降低45%，KVC使用减少26%，同时保持或提高准确性

Conclusion: Slim-SC为SC提供了一个简单而高效的测试时扩展替代方案，有效解决了计算开销问题

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT是一种推理时方法，通过检测答案收敛性来提前停止长思维链生成，平均减少41%推理token，同时保持与标准CoT相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过生成长思维链解决复杂问题，但长CoT带来高推理成本。需要一种方法能在保持性能的同时缩短推理过程。

Method: 在每个推理步骤结束时提示LLM输出当前最终答案（步骤答案），跟踪连续相同步骤答案的运行长度作为收敛度量。当运行长度出现急剧增加并超过最小阈值时终止生成。

Result: 在5个推理数据集和3个LLM上的实验表明，ES-CoT平均减少约41%的推理token，同时保持与标准CoT相当的准确性。该方法还能与自一致性提示无缝集成，且对超参数选择具有鲁棒性。

Conclusion: ES-CoT是一种实用有效的推理效率提升方法，通过检测答案收敛性实现早期停止，在显著减少计算成本的同时保持推理性能。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala是一个阿拉伯语为中心的指令和翻译模型家族，通过翻译调优流程构建，在阿拉伯语基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯语NLP研究的需求，开发专门针对阿拉伯语的高质量指令跟随模型，解决阿拉伯语指令数据稀缺的问题。

Method: 采用翻译调优流程：先压缩AR-EN教师模型到FP8精度，生成高质量双语监督数据，然后用轻量级模型LFM2-1.2B微调并翻译英文指令集到阿拉伯语，最后训练不同参数规模的Hala模型并应用slerp合并技术。

Result: 在阿拉伯语中心基准测试中，Hala在"nano"(≤2B)和"small"(7-9B)类别中都取得了最先进的成果，性能超越了其基础模型。

Conclusion: Hala模型系列为阿拉伯语NLP研究提供了有效的解决方案，通过创新的翻译调优流程和模型合并技术，显著提升了阿拉伯语指令跟随能力，并公开了模型、数据和配方以加速相关研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 本研究比较了文本和音频两种方式对机器翻译质量的评估效果，发现基于音频的众包评估与文本评估结果基本一致，但能识别出更多系统间的显著差异，建议将语音评估纳入未来MT评估框架


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量评估主要依赖文本方式，但许多实际应用涉及语音翻译，需要更自然的语音评估方式来替代纯文本评估

Method: 使用Amazon Mechanical Turk收集众包评估，比较10个WMT通用MT共享任务系统的文本和音频评估结果，进行统计显著性检验和自我复制实验

Result: 基于音频的评估结果与文本评估基本一致，但在某些情况下能识别出翻译系统间的显著差异，语音提供了更丰富自然的模态

Conclusion: 建议将基于语音的评估纳入未来机器翻译评估框架，以更好地反映实际应用场景

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文通过构建不同比例上下文相关示例的训练数据集，系统验证了训练数据稀疏性是上下文利用困难的关键瓶颈，并提出两种训练策略显著提升翻译质量


<details>
  <summary>Details</summary>
Motivation: 标准训练数据中上下文丰富示例的稀疏性被认为是机器翻译难以有效利用上下文的主要原因，需要系统验证这一假设

Method: 构建具有受控比例上下文相关示例的训练数据集，在单语和多语设置下进行实验，并提出两种数据利用训练策略

Result: 证实训练数据稀疏性与模型性能强相关，改进一个上下文现象不能泛化到其他现象，跨语言迁移有限，提出的训练策略在ctxPro评估中分别获得6%和8%的准确率提升

Conclusion: 数据稀疏性是上下文利用的关键瓶颈，需要针对性的训练策略来改善翻译中的上下文处理能力

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 本文提出了ConfMAD框架，通过在多智能体辩论系统中引入置信度表达机制，让LLM能够明确表达其置信水平，从而提升辩论效果和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论系统中，虽然某些LLM在特定任务上具有更优越的知识或推理能力，但由于缺乏置信度表达，它们难以在辩论中清晰传达这种优势。不恰当的置信度表达会导致智能体要么固执地坚持错误信念，要么过早收敛到次优答案，从而降低辩论效果。

Method: 提出了ConfMAD框架，将置信度表达整合到多智能体辩论过程中，让LLM能够在整个辩论过程中明确表达其置信水平。

Result: 实验结果表明该方法有效，并进一步分析了置信度如何影响辩论动态，为设计置信度感知的多智能体辩论系统提供了见解。

Conclusion: 通过引入置信度表达机制，ConfMAD框架能够显著提升多智能体辩论系统的性能和辩论效果，为解决LLM在辩论中沟通优势的难题提供了有效解决方案。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 本文提出了基于问题的手语翻译(QB-SLT)新任务，通过对话上下文提升翻译效果，开发了SSL-SSAW跨模态自监督学习方法，在新建数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 手语翻译需要对话上下文线索来提升翻译质量，而对话注释比传统gloss注释更自然且易于标注，因此探索如何有效整合对话信息来改进手语翻译。

Method: 提出跨模态自监督学习结合Sigmoid自注意力加权(SSL-SSAW)的融合方法：使用对比学习对齐多模态特征，SSAW模块自适应提取问题和手语序列特征，利用自监督学习增强表示能力。

Result: 在新建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上达到state-of-the-art性能，问题辅助可以达到甚至超过gloss辅助的效果，可视化结果证明了对话整合的有效性。

Conclusion: 基于问题的对话整合为手语翻译提供了有效的上下文信息，提出的SSL-SSAW方法成功实现了多模态特征对齐和自适应特征提取，证明了问题辅助在手语翻译中的实用价值。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一个快速、鲁棒的多语言语音识别和语音翻译模型，支持25种欧洲语言，在英语ASR上比Whisper-large-v3表现更好且快10倍


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的多语言语音处理模型，在保持高性能的同时显著提升处理速度，并减少语音识别和翻译中的幻觉问题

Method: 采用FastConformer编码器和Transformer解码器架构，使用17M小时数据进行两阶段预训练和微调，加入非语音音频数据减少幻觉，使用NeMo强制对齐器提供时间戳

Result: 在英语ASR上超越Whisper-large-v3且速度快10倍，在多语言ASR和AST任务上与Seamless-M4T-v2-large等更大模型竞争，同时发布了更轻量的Parakeet-TDT-0.6B-v3模型

Conclusion: Canary-1B-v2展示了在语音处理任务中实现高性能和高效率的可行性，nGPT编码器在大规模数据下表现良好，而FastConformer在微调后表现优异，为多语言语音处理提供了有效的解决方案

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS是一个新的代码切换语音识别和翻译数据集，包含4个测试集，覆盖113种语言对的52种语言，以及128小时的训练数据，旨在扩展代码切换语音研究的范围。


<details>
  <summary>Details</summary>
Motivation: 现有的代码切换语音识别和翻译系统主要针对高资源语言，缺乏对低资源语言的支持。为了推动代码切换语音研究在更广泛语言中的应用，需要开发覆盖更多语言的数据集。

Method: 构建包含4个测试集的CS-FLEURS数据集：1）14个X-英语语言对，真实语音朗读合成代码切换句子；2）16个X-英语语言对，使用生成式文本转语音；3）60个{阿拉伯语、普通话、印地语、西班牙语}-X语言对，使用生成式文本转语音；4）45个X-英语低资源语言对，使用拼接式文本转语音。同时提供128小时的生成式文本转语音训练数据。

Result: CS-FLEURS数据集覆盖了113种独特的代码切换语言对，涉及52种语言，为代码切换语音识别和翻译系统的开发和评估提供了全面的资源。

Conclusion: CS-FLEURS数据集有助于扩大代码切换语音研究的范围，为开发更广泛的代码切换语音系统提供了重要资源，特别是在低资源语言方面。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 提出了AssoCiAm基准，用于评估多模态大语言模型的联想能力，通过混合计算方法解决联想任务中的模糊性问题，发现认知与联想之间存在强正相关关系。


<details>
  <summary>Details</summary>
Motivation: 现有的联想能力评估框架往往忽视联想任务中固有的模糊性，这种模糊性源于联想的发散性，会降低评估的可靠性。

Method: 将模糊性分解为内部模糊性和外部模糊性，引入AssoCiAm基准，采用混合计算方法来规避模糊性，并进行大量实验验证。

Result: 发现认知与联想之间存在强正相关关系，评估过程中的模糊性会导致MLLMs行为更加随机化，验证了方法的有效性。

Conclusion: AssoCiAm基准能够提供更准确可靠的联想能力评估，为MLLMs的创造力评估提供了有效工具。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本文提出了一个整合金融背景和行为金融学的新框架，用于构建端到端财务顾问的监督数据，通过精心策划的数据集训练8B参数模型，在保持性能的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的财务规划系统维护成本高且收益不佳，需要一种能够综合考虑用户目标、约束、风险承受能力和管辖区域的个性化财务建议方法。

Method: 创建了一个包含19k样本的推理数据集，整合相关金融背景和行为金融学研究，对Qwen-3-8B模型进行全面的微调。

Result: 8B模型在事实准确性、流畅性和个性化指标上达到了与更大基线模型（14-32B参数）相当的性能，同时成本降低了80%。

Conclusion: 通过精心策划的数据整合和行为金融学方法，较小的模型也能在财务顾问任务中实现与大模型相当的性能，同时大幅降低成本。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 使用大语言模型分析英美议会75年移民讨论，发现美国日益极化而英国党派态度相对一致，保守党与工党意识形态差距在2025年达到最负面水平，英国讨论转向边境安全叙事而长期整合议题减少


<details>
  <summary>Details</summary>
Motivation: 通过大规模计算分析比较英美议会移民话语，探索LLM在政治历史语境中进行细粒度话语分析的可扩展性

Method: 使用开源大语言模型标注移民立场态度，建立半自动化框架提取细粒度叙事框架，追踪跨时间和政党的移民话语趋势

Result: 美国话语日益极化，英国各党派态度相对一致但保守党与工党意识形态差距在2025年达最负面；英国叙事转向边境控制和非法移民等安全化框架，社会整合等长期整合框架减少；移民讨论从国内法转向国际法和人权

Conclusion: 大语言模型能够支持政治历史语境中可扩展的细粒度话语分析，揭示了移民话语的演变趋势和跨国差异

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是一个完全开源的大型语言模型套件，专注于数据合规性和多语言表示，使用开放可用数据训练，支持1800多种语言，在8B和70B规模上达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源模型生态系统中数据合规性和多语言代表性不足的问题，避免使用未经许可的数据和有害内容。

Method: 使用完全开放可用数据进行预训练，遵循robots.txt排除规则，过滤非许可、有毒和个人身份信息内容，采用Goldfish目标抑制数据记忆，同时保持下游任务性能。

Result: Apertus模型在多语言基准测试中达到或超越同类开源模型的最先进水平，支持超过1800种语言，非英语数据占比约40%。

Conclusion: Apertus提供了一个完全透明、合规的开源LLM解决方案，释放了包括数据准备脚本、检查点、评估套件和训练代码在内的所有科学成果，支持审计和扩展。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 本文提出了一种基于证据检索的不确定性感知决策机制，通过检索近邻样本并融合其预测分布来替代全局固定阈值，实现更可靠和可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 传统的基于预测熵的全局阈值方法存在置信度错误预测较多的问题，需要一种更可靠、透明且可审计的不确定性感知决策方法。

Method: 为每个测试实例在嵌入空间中检索近邻样本，使用Dempster-Shafer理论融合这些样本的预测分布，生成每个实例自适应的阈值标准。

Result: 在CIFAR-10/100数据集上，使用BiT和ViT骨干网络，该方法显示出更高或相当的不确定性感知性能，显著减少了置信度错误预测，同时保持可持续的审核负载。

Conclusion: 证据条件标记为操作不确定性感知决策提供了比固定预测熵阈值更可靠和可解释的替代方案，仅需少量证据即可实现显著增益。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [41] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 混合量子-经典神经网络在三个基准数据集上相比纯经典模型展现出更高的准确率、更快的训练速度和更好的资源效率，特别是在复杂数据集上优势更明显


<details>
  <summary>Details</summary>
Motivation: 系统比较混合量子-经典神经网络与纯经典模型在性能、效率和鲁棒性方面的差异，评估量子计算在深度学习中的实际价值

Method: 在MNIST、CIFAR100和STL10数据集上对比混合模型（参数化量子电路+经典深度学习架构）和经典CNN模型，进行50个训练周期的实验，评估验证准确率、测试准确率、训练时间、计算资源使用和对抗鲁棒性

Result: 混合模型在所有数据集上准确率更高（MNIST 99.38% vs 98.21%，CIFAR100 41.69% vs 32.25%，STL10 74.05% vs 63.76%），训练速度快5-12倍，参数减少6-32%，内存使用更少（4-5GB vs 5-6GB），CPU利用率更低（9.5% vs 23.2%），在简单数据集上对抗鲁棒性更好

Conclusion: 混合量子-经典架构在准确率、训练效率和参数可扩展性方面具有显著优势，特别适用于复杂视觉任务，为量子计算在深度学习中的应用提供了有力证据

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [42] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: 本研究提出一个集成技术框架来解决高速公路交通拥堵问题，通过优化车辆感知算法和改进拥堵预测模型，显著提高了遮挡条件下的车辆检测精度和长序列依赖的拥堵预警能力。


<details>
  <summary>Details</summary>
Motivation: 高速公路交通拥堵严重降低出行效率并阻碍区域连通性。现有的"检测-预测"系统存在关键缺陷：遮挡条件下车辆感知精度低，以及拥堵预测中长序列依赖关系的丢失。

Method: 1) 车辆感知：优化YOLOv11为YOLOv11-DIoU（用DIoU Loss替换GIoU Loss），改进DeepSort算法融合马氏距离和余弦距离；2) 拥堵预警：构建GRU-Attention模型捕捉拥堵前兆，使用流量、密度和速度数据进行训练

Result: YOLOv11-DIoU达到95.7% mAP（比基线高6.5个百分点），遮挡漏检率5.3%；DeepSort达到93.8% MOTA（比SORT高11.3个百分点）；GRU-Attention模型测试准确率99.7%，10分钟提前预警30分钟拥堵的时间误差≤1分钟

Conclusion: 该框架为高速公路拥堵控制提供了量化支持，在智能交通应用中具有良好前景，能够有效解决现有系统的关键缺陷，实现高精度的车辆感知和准确的拥堵预警。

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [43] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: 本研究利用卷积神经网络和图像模式识别技术，自动化路边停车服务的测试分析过程，将人工资源时间减少99.58%


<details>
  <summary>Details</summary>
Motivation: 优化基于众包车辆数据的实时路边停车服务质量，通过自动化现有测试流程来替代人工工程工作

Method: 应用机器学习特别是图像模式识别方法，使用卷积神经网络实现高水平的自动化分析

Result: 实现了99.58%的人工资源时间减少，显著提高了停车服务测试的效率

Conclusion: 自动化分析工具在停车服务测试中表现出色，为未来发展和潜在应用提供了良好基础

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [44] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文系统分析了基于视觉语言模型(VLM)的零样本分布外检测机制，揭示了VLM在语义新颖性利用方面的优势，同时发现其对提示词表述高度敏感的关键脆弱性


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP等视觉语言模型在零样本分布外检测方面表现出色，但研究界对其工作机制、相对于单模态方法的优势以及行为鲁棒性仍缺乏全面理解

Method: 使用分布内和分布外提示词进行系统性实证分析，包括：1)形式化VLM嵌入空间的关键操作特性；2)量化比较VLM与单模态方法；3)评估模型对图像噪声和提示词表述的敏感性

Result: VLM在零样本分布外检测中优于单模态方法，主要优势在于能够利用丰富的语义新颖性；但发现显著的鲁棒性不对称性：对常见图像噪声具有韧性，但对提示词表述高度敏感

Conclusion: 研究提供了对VLM基分布外检测方法优势和关键脆弱性的结构化理解，为开发更鲁棒可靠的未来设计提供了基于实证的重要指导

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [45] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 本文提出了一种基于截面曲率概念的离散度量空间几何分析方法，通过曲率剖面评估数据表示效果和估计数据集本征维度


<details>
  <summary>Details</summary>
Motivation: 开发一种基于几何曲率的方法来分析离散度量空间，特别是为了评估降维技术产生的数据表示效果和估计数据集的本征维度

Method: 利用新发展的截面曲率抽象概念，构建离散度量空间的曲率几何剖面，该方法捕捉点三元组与其他点之间的度量关系

Result: 实验证明该曲率分析方法可用于估计数据集的本征维度，探索经验网络的大规模几何结构，并评估降维技术的有效性

Conclusion: 基于曲率的几何剖面为离散度量空间分析提供了有效工具，特别在评估数据表示质量和维度估计方面具有实用价值

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [46] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 使用机器学习和遥感技术分析斐济Nadi地区2013-2024年土地利用变化，重点关注城市化发展


<details>
  <summary>Details</summary>
Motivation: 斐济作为发展中国家正经历快速城市化，需要技术手段来监测土地利用变化，为土地规划提供支持

Method: 使用Landsat-8卫星影像，结合Google Earth Engine平台，采用k-means聚类无监督学习和卷积神经网络监督学习进行土地覆盖分类

Result: 生成了土地覆盖变化可视化结果，突出了城市区域随时间的变化情况

Conclusion: 该研究为土地利用建模和变化检测提供了有效的技术框架，能够监测城市化进程中的土地覆盖变化

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [47] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: 一种基于YOLOv7分割和ConvNeXt特征提取的三阶段框架，用于电力传输系统的实时异物入侵检测与跟踪，支持边缘设备部署和增量更新


<details>
  <summary>Details</summary>
Motivation: 解决电力传输系统中异物入侵检测的实时性、准确性和逆光处理挑战，适配边缘设备的资源限制

Method: 三阶段框架：1)YOLOv7分割模型定位对象；2)ConvNeXt特征提取器使用三元组损失训练；3)特征辅助IoU跟踪器处理遮挡和运动

Result: 在真实监控和无人机视频数据集上高准确性和稳健性，NVIDIA Jetson边缘设备上实现实时性能

Conclusion: 该框架为电力传输安全提供了高效、可扩展的实时监控解决方案，适合大规模场景部署

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [48] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: EdiVal-Agent是一个自动化、可扩展的细粒度评估框架，用于多轮指令式图像编辑，通过对象中心视角和专家工具套件来解决当前评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前指令式图像编辑评估方法存在两个问题：(i)依赖配对参考图像导致覆盖范围有限且继承先前生成模型的偏见；(ii)仅依赖零样本视觉语言模型(VLMs)，其基于提示的评估在指令遵循、内容一致性和视觉质量方面往往不精确。

Method: EdiVal-Agent首先将图像分解为语义上有意义的对象，然后合成多样化、上下文感知的编辑指令。评估时结合VLMs与开放词汇对象检测器评估指令遵循，使用语义级特征提取器评估内容一致性，利用人类偏好模型判断视觉质量。

Result: 研究表明，将VLMs与对象检测器结合使用在指令遵循评估方面比单独使用VLMs和基于CLIP的指标与人类判断有更强的一致性。

Conclusion: EdiVal-Agent的模块化设计允许未来工具无缝集成，随时间提高评估准确性。该框架可用于识别现有失败模式，为下一代编辑模型的开发提供信息。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [49] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything是一个统一的基于transformer的前馈模型，能够处理单张或多张图像以及可选的几何输入（如相机内参、位姿、深度或部分重建），直接回归出度量3D场景几何和相机参数。


<details>
  <summary>Details</summary>
Motivation: 为了解决多种3D视觉任务需要不同专门模型的问题，研究者希望开发一个统一的模型来处理包括未标定运动恢复结构、标定多视角立体、单目深度估计、相机定位、深度补全等广泛任务。

Method: 采用基于transformer的前馈架构，使用分解的多视角场景几何表示（深度图、局部射线图、相机位姿和度量尺度因子），通过标准化监督和训练以及灵活的输入增强来实现统一处理。

Result: 实验表明MapAnything在性能上优于或匹配专门的专家模型，同时提供更高效的联合训练行为，展现出作为通用3D重建骨干网络的潜力。

Conclusion: MapAnything为构建通用3D重建骨干网络铺平了道路，通过单一前馈传递就能处理广泛的3D视觉任务，实现了性能与效率的平衡。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [50] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: 提出SCM-PR框架，通过融合RGB图像的语义信息与LiDAR地图进行跨模态地点识别，解决了传统RGB方法对光照、天气变化的敏感性问题，在复杂场景和视角变化下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决GPS不可用环境下机器人精确定位问题，传统RGB视觉地点识别方法对光照、天气等环境变化敏感，现有跨模态方法在复杂场景、细粒度匹配和视角变化下表现不佳。

Method: 提出SCM-PR框架：使用VMamba骨干网络提取RGB特征；语义感知特征融合模块结合地点描述符和分割掩码；LiDAR描述符融合语义和几何信息；在NetVLAD中引入跨模态语义注意力机制；设计多视角语义-几何匹配和语义一致性损失函数。

Result: 在KITTI和KITTI-360数据集上的实验表明，SCM-PR相比其他跨模态地点识别方法达到了最先进的性能。

Conclusion: 通过有效融合语义信息，SCM-PR框架显著提升了跨模态地点识别的鲁棒性和准确性，特别是在复杂环境和视角变化条件下。

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [51] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于场景自适应格点向量量化(SALVQ)的3D高斯泼溅压缩方法，替代传统的均匀标量量化，在保持低复杂度的同时显著提升率失真性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然渲染质量高且实时性好，但数据量巨大需要压缩。现有基于锚点的神经压缩方法都使用简单的均匀标量量化，作者希望探索更先进的量化器是否能以最小开销提升压缩性能。

Method: 采用格点向量量化(LVQ)替代均匀标量量化，并为每个场景优化格点基向量，实现场景自适应的LVQ(SALVQ)。通过缩放格点基向量可动态调整格点密度，支持多码率目标。

Result: SALVQ在保持低复杂度的同时，显著提升了率失真效率，可以无缝集成到现有3DGS压缩架构中，仅需最小修改和计算开销。

Conclusion: 场景自适应的格点向量量化为3D高斯泼溅压缩提供了高效的解决方案，在率失真性能和复杂度之间取得了良好平衡，且支持灵活的码率调整。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [52] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: 提出了MINGLE方法，通过三阶段管道检测图像中的社交群体区域，并发布了包含10万张街景图像的新数据集


<details>
  <summary>Details</summary>
Motivation: 理解公共场所的群体社交互动对城市规划至关重要，需要从图像中解读超越传统物体检测的复杂视觉线索

Method: 三阶段模块化管道：1）现成的人体检测和深度估计；2）基于VLM的成对社交关系分类；3）轻量级空间聚合算法定位社交连接群体

Result: 开发了MINGLE方法并创建了包含10万张标注图像的新数据集，结合人工标注和模型输出确保语义丰富性和场景覆盖

Conclusion: 提出的社交群体区域检测任务和方法为理解城市环境中的人际互动提供了有效工具，支持未来研究

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [53] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap是一个模型无关的框架，用于发现稳定扩散模型中的潜在概念级表征偏见，通过交叉注意力归因图揭示人口统计特征与语义概念的结构性纠缠，并提出基于能量引导扩散采样的偏见缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏见发现方法主要关注输出层面的人口统计分布，无法保证缓解后概念表征的解耦。需要更深入地揭示生成过程中的表征偏见。

Method: 利用交叉注意力归因图量化人口统计特征与语义概念的空间纠缠（通过IoU指标），并通过能量引导扩散采样在去噪过程中直接修改潜在噪声空间来最小化SoftIoU期望值。

Result: 研究发现现有公平性干预措施可能减少输出分布差距，但往往无法解耦概念级耦合，而BiasMap的缓解方法能够在图像生成中减轻概念纠缠，同时补充分布偏见缓解。

Conclusion: BiasMap提供了一个新的视角来发现和缓解TTI模型中的表征偏见，超越了传统的输出级分布分析方法，能够揭示和解决更深层次的概念纠缠问题。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [54] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: LivePyxel是一个基于Python的实时图像标注工具，可直接连接成像设备（如显微镜、摄像头）进行实时标注，解决了传统标注工具需要先上传数据集的问题


<details>
  <summary>Details</summary>
Motivation: 现有图像标注软件需要用户先上传预收集的数据集，这限制了按需流水线的支持，在实验室实时数据采集环境中特别不便

Method: 开发Python图形用户界面，集成Bézier样条、二值掩码等专业标注工具，支持非破坏性图层编辑，使用OpenCV和Numpy进行高性能矩阵运算

Result: 实现了与各种视频设备的广泛兼容性，优化了目标检测操作，支持实时图像采集和标注

Conclusion: LivePyxel简化了数据收集和标注流程，加速了实验工作流中AI模型的开发，该软件已在GitHub开源

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [55] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 该论文提出DEFT-VTON方法，通过Doob's h-transform高效微调技术，仅训练1.42%的参数实现虚拟试穿，并引入自适应一致性损失来减少推理时间至15步，在保持性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现实应用中虚拟试穿(VTO)需要有限的训练和推理预算，而现有方法需要大量端到端训练，计算成本高昂。

Method: 采用DEFT高效微调技术冻结预训练模型参数，训练小型h-transform网络学习条件变换；结合自适应一致性损失和去噪得分匹配损失进行微调。

Result: DEFT-VTON在VTO任务上达到最先进性能，仅需15个去噪步骤，参数量仅为传统PEFT方法的1.42%（对比5.52%）。

Conclusion: 该方法以极低的计算成本实现了高质量的虚拟试穿，为实际部署提供了高效的解决方案。

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [56] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: 本文提出了一种数据增强管道，通过生成虚拟行人和对抗学习光照条件来改善自动驾驶中合成数据与真实数据之间的域差距，从而提高行人识别性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域需要合成数据来覆盖特定交通场景，但合成数据与真实数据之间存在域差距问题，特别是在行人识别方面。为了解决这个问题，需要提高合成数据的真实感。

Method: 开发了一个数据增强管道，用于在Cityscapes数据集中添加虚拟行人。提出了一种新颖的生成对抗网络架构，用于学习数据集的光照条件，以提高增强的真实感。

Result: 该方法在语义分割和实例分割任务上进行了评估，证明了其有效性。

Conclusion: 通过对抗学习光照条件和生成虚拟行人的数据增强方法，可以有效减少合成数据与真实数据之间的域差距，提高自动驾驶系统中行人识别的性能。

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [57] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 提出了FunKAN和U-FunKAN模型，用于医学图像增强和分割，在多个数据集上表现优于其他KAN方法，提供了可解释的解决方案


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法架构复杂且可解释性有限，而Kolmogorov-Arnold网络会破坏图像的空间结构特征，需要专门针对图像处理设计的可解释神经网络框架

Method: 提出Functional Kolmogorov-Arnold Network (FunKAN)，将Kolmogorov-Arnold表示定理推广到函数空间，使用Hermite函数的傅里叶分解学习内部函数

Result: 在IXI数据集上抑制MRI吉布斯伪影，在BUSI、GlaS、CVC-ClinicDB三个医学数据集上进行分割任务，在PSNR、TV、IoU、F1等指标上优于其他KAN方法

Conclusion: 该工作填补了理论函数逼近与医学图像分析之间的空白，为临床应用提供了鲁棒且可解释的解决方案

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [58] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态双流图神经网络模型，通过构建实例图和互补权重图来突出仇恨内容，在仇恨视频分类任务上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法通常忽视仇恨内容的决定性作用，对所有内容一视同仁，且无法系统捕获视频中的结构化信息，限制了多模态融合效果

Method: 将视频分割为多个实例构建实例图提取特征，通过互补权重图为特征分配重要性权重以突出仇恨实例，结合权重和特征生成视频标签

Result: 在公开数据集上的广泛实验表明，该模型在仇恨视频分类方面达到最先进水平，并具有很强的可解释性

Conclusion: 提出的多模态双流图神经网络模型有效解决了现有方法的局限性，在仇恨视频检测任务上表现出色且具有解释性

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [59] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter是一个基于扩散模型的深度估计方法，专门用于结肠镜检查视频，能够生成时间一致性的深度图，在C3VD数据集上实现了零样本最先进性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜三维场景理解面临挑战，现有深度估计模型在视频序列中缺乏时间一致性，限制了3D重建的应用。

Method: 使用基于扩散的深度估计模型，从合成结肠镜序列学习几何先验，并引入风格迁移技术将真实临床视频适配到合成训练域。

Result: 在C3VD数据集上实现了零样本最先进性能，超越了通用和结肠镜专用方法，支持3D点云生成和表面覆盖评估。

Conclusion: 虽然完整轨迹3D重建仍有挑战，但ColonCrafter展示了临床相关应用潜力，为结肠镜3D场景理解提供了有效解决方案。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [60] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 该论文针对嵌入式平台资源受限的问题，提出了基于体素空间几何相似性的3D高斯基元合并方法和Patch-Grid点采样初始化技术，在降低GPU内存使用的同时提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅技术主要关注高性能桌面GPU，忽视了嵌入式平台（如微型飞行器）的计算资源限制。这些设备需要在系统性能和重建质量之间进行权衡。

Method: 1. 在SLAM中基于几何相似性在体素空间合并冗余的3D高斯基元，降低GPU内存使用；2. 通过Patch-Grid点采样初始化3D高斯基元，实现更精确的场景建模。

Result: 在公开数据集上的定量和定性评估证明了该方法在降低GPU内存使用的同时提升了渲染质量，且不影响系统运行时性能。

Conclusion: 该方法有效解决了嵌入式平台资源受限的问题，在保持实时性能的同时实现了更好的渲染质量和内存效率，为3DGS在资源受限设备上的应用提供了可行方案。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [61] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 本文提出了一种自适应轨迹级OOD检测框架，通过显式建模预测误差模式，在复杂驾驶环境中实现了检测延迟和误报率的显著改善。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在部署中面临训练数据与真实世界条件之间的分布偏移问题，现有OOD检测研究主要集中在计算机视觉任务，轨迹级OOD检测研究相对不足。

Method: 基于快速变化检测(QCD)任务框架，引入自适应机制，显式建模预测误差的模式依赖性分布及其随时间演变的特性。

Result: 在多个真实世界数据集上的实验表明，该方法在检测延迟和误报率方面显著优于先前的UQ和基于视觉的OOD方法，同时在计算效率上也有优势。

Conclusion: 该框架为实现可靠、驾驶感知的自主性提供了一条实用路径，在复杂驾驶环境中实现了鲁棒的OOD检测性能。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [62] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的方法，利用地球观测卫星图像对检测亚马逊雨林砍伐，并通过视觉语义模型自动为检测到的变化生成相关标注。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林是重要的生态系统，对调节地球气候和维护生物多样性至关重要。砍伐森林对全球碳排放和生物多样性有重大影响，需要有效的监测方法。

Method: 使用深度学习技术比较同一区域不同日期的卫星图像对，识别森林覆盖变化。提出视觉语义模型，从亚马逊相关科学文献中提取关键词自动标注检测到的变化。

Result: 在亚马逊图像对数据集上评估，证明该方法能有效检测砍伐并生成相关标注。

Conclusion: 该方法为监测和研究亚马逊砍伐影响提供了有用工具，虽然专注于环境应用，但具有通用性可应用于其他领域。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [63] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型的多模态医疗图像分析框架，整合Google Gemini 2.5 Flash进行肿瘤检测和临床报告生成，支持CT、MRI、X-ray和超声等多种成像模式。


<details>
  <summary>Details</summary>
Motivation: 医疗影像AI的快速发展正在变革诊断医学和临床决策过程，需要开发能够整合多模态影像并自动生成临床报告的智能诊断支持系统。

Method: 采用视觉语言模型(VLMs)，结合视觉特征提取和自然语言处理，包含坐标验证机制和高斯概率建模，使用多层可视化技术和精确的提示工程进行结构化信息提取。

Result: 实验评估显示在多模态异常检测方面表现优异，位置测量达到80像素平均偏差，具备零样本学习能力，减少了对大型数据集的依赖。

Conclusion: 该框架在自动化诊断支持和放射工作流程效率方面取得显著进展，但需要进行临床验证和多中心评估才能广泛应用。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [64] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: CLAP是一种基于聚类的2D定位算法，具有出色的抗异常值能力，本文将其扩展到3D定位和图像拼接领域，并揭示了CLAP、RANSAC和Hough变换之间的关系。


<details>
  <summary>Details</summary>
Motivation: CLAP算法在RoboCup 2024比赛中表现出色，但仅限于2D定位。为了将其优势应用到更广泛的领域，需要将其扩展到3D定位和图像拼接等更通用的框架。

Method: 通过聚类方法来抑制噪声和处理错误的特征匹配，提供了一种替代传统RANSAC等异常值剔除方案的新方法。将CLAP算法从2D扩展到3D定位和图像拼接应用。

Result: 成功将CLAP算法推广到3D定位和图像拼接领域，建立了CLAP、RANSAC和Hough变换之间的理论联系。

Conclusion: CLAP的通用化框架可广泛应用于多个领域，成为处理噪声和不确定性的有效工具，为计算机视觉和机器人定位提供了新的解决方案。

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [65] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: SAMIR是一个基于Segment Anything Model的医学图像配准框架，通过SAM提取结构感知特征嵌入，结合轻量级3D头部和分层特征一致性损失，在心脏和腹部CT图像配准任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督医学图像配准方法需要解剖学先验知识（如分割掩码或地标），但这些标签往往难以获取，限制了实际应用。受视觉基础模型强大表示学习能力的启发，本文利用SAM来增强特征提取。

Method: 设计了任务特定的适配管道，使用SAM的图像编码器提取结构感知特征嵌入；构建轻量级3D头部来细化嵌入空间中的特征；引入分层特征一致性损失来指导从粗到细的特征匹配。

Result: 在基准数据集上，SAMIR在心脏图像配准任务上性能提升2.68%（ACDC数据集），在腹部CT图像配准任务上提升6.44%，显著优于最先进方法。

Conclusion: SAMIR框架有效利用了预训练视觉基础模型的表示学习能力，无需额外的弱监督标签就能实现优异的医学图像配准性能，具有很好的实用价值。

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [66] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 本文提出了一种基于联邦学习的分布式方法，用于从卫星图像中识别和定位森林砍伐，在保护数据隐私的同时实现多客户端协作训练。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练方法需要合并数据，会损害客户端的数据安全和隐私。卫星图像处理需要分布式协作，同时保护各地卫星中心的数据安全。

Method: 使用FLOWER框架结合RAY框架实现分布式联邦学习，采用YOLOS-small、Faster R-CNN（ResNet50骨干）和Faster R-CNN（MobileNetV3骨干）三种模型在公开数据集上进行训练和测试。

Result: 开发了一个能够有效识别和定位森林砍伐的联邦学习框架，实现了分布式网络客户端间的协作训练，同时确保了数据隐私和安全。

Conclusion: 该方法为卫星图像分割任务提供了新的分布式解决方案，在保护数据隐私的前提下实现了准确的森林砍伐识别，具有实际应用价值。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [67] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: GARPS是一个无需训练的两视图相机位姿估计框架，通过直接对齐两个独立重建的3D高斯混合模型来实现度量相对位姿估计，在Real-Estate10K数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的两视图位姿估计方法无法提供度量尺度信息（相机平移只有尺度未知），且在宽基线和纹理贫乏区域表现不佳。需要一种能够提供度量尺度信息且对纹理贫乏区域鲁棒的方法。

Method: 使用度量单目深度估计器和高斯场景重建器为每张图像构建度量3D高斯混合模型(GMM)，然后通过优化可微分的GMM对齐目标来精化初始位姿估计。对齐目标综合考虑几何结构、视角无关颜色、各向异性协方差和语义特征一致性。

Result: 在Real-Estate10K数据集上的大量实验表明，GARPS超越了传统方法和最先进的学习方法（包括MASt3R），实现了更好的性能。

Conclusion: 该方法展示了将单视图感知与多视图几何相结合来实现鲁棒度量相对位姿估计的潜力，无需显式2D对应关系就能处理遮挡和纹理贫乏区域。

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [68] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 该论文提出用查找表操作替代传统卷积神经网络中的乘法运算，以降低计算复杂度和能耗，提高移动设备上的推理效率。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络中的乘法运算计算复杂度高、能耗大，阻碍了在移动设备上的部署。受资源受限边缘设备使用查找表简化复杂运算的启发，作者希望用查找操作替代乘法运算。

Method: 提出可微分的查找表操作作为神经网络基础构建块，替代权重和激活值的乘法计算。设计了多种训练策略来促进查找表的端到端优化收敛。

Result: 在图像分类、图像超分辨率和点云分类任务中，查找网络在能耗和推理速度方面效率更高，同时保持了与原始卷积网络相当的性能。

Conclusion: 查找网络在不同任务（分类和回归）和不同数据类型（图像和点云）上都取得了最先进的性能，证明了查找操作在提高神经网络效率方面的有效性。

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [69] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 通过利用SAM生成语义超像素来压缩视觉令片，减少93%的视觉令片数量同时保持性能，加快MLLM训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在指引图像分割中视觉令片冗余问题，传统补丁式视觉投影器无法合理平衡令片数量减少和语义保持。

Method: 提出语义视觉投影器，利用SAM生成的语义超像素识别"视觉单词"，通过压缩超像素来减少视觉令片。使用语义超像素位置嵌入和语义超像素聚合器来保留细细节和全局上下文。

Result: 方法减少93%的视觉令片，性能无损失，训练和推理速度显著提升，在RIS任务上超过现有压缩视觉投影器。

Conclusion: 该方法通过语义超像素压缩有效解决了视觉令片冗余问题，在保持语义信息的同时大幅提升效率，为MLLM在分割任务中的应用提供了高效解决方案。

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [70] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: FishBEV是一个专门针对鱼眼相机设计的BEV分割框架，通过三个创新模块解决鱼眼相机的几何畸变、多视角对应模糊和时间动态不稳定问题，在Synwoodscapes数据集上表现优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有BEV分割方法主要针对针孔相机，但鱼眼相机存在严重的几何畸变、多视角对应关系模糊和时间动态不稳定等问题，这些因素显著降低了BEV分割性能，因此需要专门针对鱼眼相机设计的解决方案。

Method: 提出了FishBEV框架，包含三个核心模块：1) 抗畸变多尺度提取(DRME)主干网络，在畸变下学习鲁棒特征并保持尺度一致性；2) 不确定性感知空间交叉注意力(U-SCA)机制，利用不确定性估计实现可靠的跨视角对齐；3) 距离感知时间自注意力(D-TSA)模块，自适应平衡近场细节和远场上下文以确保时间一致性。

Result: 在Synwoodscapes数据集上的大量实验表明，FishBEV在环视鱼眼BEV分割任务中持续优于现有的最先进基线方法。

Conclusion: FishBEV成功解决了鱼眼相机特有的挑战，通过三个互补的创新模块有效提升了BEV分割性能，为自动驾驶中鱼眼相机的应用提供了有效的解决方案。

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [71] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: 该研究提出了三种基于样条的KAN网络(SBTAYLOR-KAN、SBRBF-KAN、SBWAVELET-KAN)用于医学图像分类，在有限数据集上实现了高精度分类，参数量远少于传统CNN，且具有良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决资源有限临床环境中医学图像分类的挑战，特别是在数据集有限且多样化的情况下，需要开发轻量级、可解释且泛化能力强的分类模型。

Method: 提出三种基于样条的KAN网络：SBTAYLOR-KAN(结合B样条和泰勒级数)、SBRBF-KAN(结合B样条和径向基函数)、SBWAVELET-KAN(在Morlet小波变换中嵌入B样条)，利用样条基函数逼近来捕捉局部和全局非线性特征。

Result: SBTAYLOR-KAN达到98.93%准确率，仅用30%训练数据仍保持86%以上准确率，在皮肤癌数据集上达到68.22%准确率。相比ResNet50需要2418万参数，SBTAYLOR-KAN仅需2872个可训练参数。

Conclusion: 该框架为医学图像分类提供了轻量级、可解释且泛化性强的解决方案，特别适合数据稀缺的临床AI应用场景。

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [72] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: 本文提出StyleProtect方法，通过选择性更新交叉注意力层来有效防御针对艺术风格的恶意扩散模型模仿，保护艺术家的独特创作风格。


<details>
  <summary>Details</summary>
Motivation: 生成模型特别是扩散模型的快速发展使其可能被恶意用于复制艺术家的独特风格，侵犯艺术家的创作劳动和个人视觉表达，因此需要开发有效的风格保护方法。

Method: 基于发现某些交叉注意力层对艺术风格特别敏感的研究，提出StyleProtect方法，仅更新选定的交叉注意力层来实现轻量级但有效的风格防御。

Result: 实验使用基于WikiArt的30位艺术家作品数据集和Anita动画数据集，证明该方法在保护艺术风格免受恶意扩散定制方面表现优异，同时保持较好的不可感知性。

Conclusion: StyleProtect提供了一种高效轻量的解决方案，能够有效防御微调扩散模型对艺术风格的模仿，为艺术作品和动画的风格保护提供了有前景的方法。

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [73] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: UM-Depth是一个自监督单目深度估计框架，通过运动感知和不确定性感知的细化方法，在动态物体边界和纹理缺失区域提升深度估计精度，无需额外标签且在推理时无额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督单目深度估计方法在低纹理区域和动态区域存在不确定性，导致深度精度下降。需要一种方法能够处理这些不确定性，同时避免推理时的额外计算成本和标签需求。

Method: 提出UM-Depth框架，采用师生训练策略，在训练过程中将不确定性估计嵌入到训练流程和网络架构中。使用光流仅在教师网络训练时使用，无需额外标签且推理时无开销。

Result: 在KITTI和Cityscapes数据集上的广泛实验表明，该方法在动态物体边界和纹理缺失区域显著提升了深度估计精度，在KITTI数据集上实现了自监督深度和姿态估计的最先进结果。

Conclusion: UM-Depth通过结合运动感知和不确定性感知的细化方法，有效解决了自监督单目深度估计中的不确定性问题，在保持实时性能的同时达到了state-of-the-art的精度。

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [74] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: TQF方法将引用查询分解为三个专门组件：外观查询、帧内交互查询和帧间运动查询，通过动态构建查询和运动感知聚合模块来解决查询选择偏差问题，在多个RVOS基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于查询的RVOS方法使用静态文本查询进行跨模态对齐，但容易被外观或运动相似的干扰物误导，导致查询选择偏差问题。

Method: 提出三重查询变换器(TQF)，将引用查询分解为三个专门组件：外观查询处理静态属性，帧内交互查询处理空间关系，帧间运动查询处理时序关联。查询动态构建，结合语言线索和视觉指导。引入两个运动感知聚合模块：帧内交互聚合和帧间运动聚合。

Result: 在多个RVOS基准测试上进行了广泛实验，证明了TQF方法的优势和结构化查询设计及运动感知聚合模块的有效性。

Conclusion: TQF通过分解查询为三个专门组件和引入运动感知聚合，有效解决了查询选择偏差问题，在RVOS任务中取得了优异性能。

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [75] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: InstanceVG是一个多任务通用视觉定位框架，首次同时处理广义指称表达式理解(GREC)和分割(GRES)任务，通过实例查询统一实例级边界框和掩码的联合一致性预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立处理GREC和GRES任务，忽视了联合训练的优势，且将GRES视为语义分割任务而忽略了实例感知能力的重要性。

Method: 提出InstanceVG框架，为每个实例查询分配先验参考点，统一预测点、边界框和掩码，确保同一实例的多粒度预测一致性。

Result: 在10个数据集上的4个任务中实现了最先进性能，在各种评估指标上显著超越现有方法。

Conclusion: InstanceVG是第一个同时处理GREC和GRES并融入实例感知能力的框架，通过统一的多粒度一致性预测取得了优异效果。

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [76] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: FMFA是一个跨模态全模式细粒度对齐框架，通过显式细粒度对齐和隐式关系推理来解决文本-图像人物检索中的模态对齐问题，在三个公开数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本-图像人物检索中跨模态对齐的挑战，现有方法缺乏验证局部特征是否正确对齐的能力，并且主要关注困难负样本而忽略了错误匹配的正样本对。

Method: 提出FMFA框架，包含自适应相似度分布匹配（A-SDM）模块来修正未匹配的正样本对，以及显式细粒度对齐（EFA）模块来加强显式跨模态细粒度交互和局部对齐验证。

Result: 在三个公开数据集上实现了最先进的性能，在所有全局匹配方法中表现最佳。

Conclusion: FMFA通过全模式（显式+隐式）的细粒度对齐方法，有效解决了跨模态匹配中的对齐验证和正样本对修正问题，显著提升了文本-图像人物检索的性能。

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [77] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出颜色映射模块解决文本驱动图像编辑中颜色控制的精度和连续性不足问题，通过建立文本嵌入空间与RGB值的对应关系实现精确连续的颜色控制。


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性和离散性导致文本驱动的图像颜色编辑存在精度不足和难以连续控制的问题，现有线性插值方法缺乏对颜色变化范围的精确控制。

Method: 引入颜色映射模块，显式建模文本嵌入空间与图像RGB值之间的对应关系，根据给定RGB值预测对应的嵌入向量，实现精确的颜色控制。

Result: 实验结果表明该方法在颜色连续性和可控性方面表现良好，用户可指定目标RGB范围生成具有连续颜色变化的图像。

Conclusion: 该方法实现了更细粒度、连续且可控的颜色编辑，在保持语义一致性的同时解决了文本驱动图像颜色编辑的精度和连续性挑战。

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [78] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 这篇论文提出了一种迭代提示精炼算法，通过视觉语言模型分析生成的图像来提高文本到图像生成的安全性，而不仅仅依靠文本提示精炼。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I模型安全方法主要依靠LLMs精炼提示，但忽略了生成图像的实际内容，导致安全问题或对安全提示的不必要修改。

Method: 提出迭代提示精炼算法，利用视觉语言模型(VLMs)同时分析输入提示和生成图像，通过视觉反馈来更有效地精炼提示。还构建了包含文本和视觉安全信号的新数据集用于监督微调。

Result: 实验结果显示，该方法能够生成更安全的输出，同时保持与用户意图的对齐性，性能可与现有LLM基础方法相比。

Conclusion: 该研究提供了一种通过视觉反馈来提高T2I内容安全性的实用解决方案，在保持用户意图的同时有效减少有害内容的生成。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [79] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: TA-ISP是一个紧凑的RAW-to-RGB框架，通过预测轻量级多尺度调制算子来生成面向任务的图像表示，显著降低计算开销同时提升下游视觉任务性能


<details>
  <summary>Details</summary>
Motivation: 现有RAW数据处理方法面临两个主要问题：大规模ISP网络计算开销大，传统ISP调优方法表示能力有限。需要一种既能保持丰富RAW信息又计算高效的解决方案

Method: 提出Task-Aware ISP框架，使用轻量级多尺度调制算子（全局、区域、像素尺度）来重塑图像统计特征，替代传统的密集卷积管道

Result: 在多个RAW域检测和分割基准测试中，TA-ISP在白天和夜间条件下均能持续提升下游任务准确率，同时显著减少参数数量和推理时间

Conclusion: TA-ISP适合在资源受限设备上部署，通过因子化控制扩展了空间变化变换的表示范围，同时严格控制内存使用、计算和延迟

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [80] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 提出NDLPNet网络用于夜间图像去雨，通过位置感知模块捕获雨条纹空间位置信息，在低光环境下有效去除雨条纹并保留背景信息，构建了真实夜间雨景数据集NSR，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去雨技术主要针对白天条件，在夜间光照下性能不佳，因为雨分布的空间异质性和光依赖的条纹可见性影响夜间监控和自动驾驶导航性能。

Method: 提出NDLPNet网络，包含位置感知模块(PPM)来捕获空间上下文信息，增强特征通道重要性识别和校准能力，有效处理夜间雨条纹去除。

Result: 在现有数据集和新建的NSR数据集上进行大量实验，定性和定量评估均表明该方法在夜间去雨任务中优于最先进方法。

Conclusion: NDLPNet能有效解决夜间图像去雨问题，提出的位置感知模块和真实夜间数据集为夜间去雨研究提供了新基准和解决方案。

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [81] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: VocSegMRI是一个多模态框架，通过跨注意力融合视频、音频和语音学输入来实时分割MRI中的发音结构，在USC-75数据集上达到Dice分数0.95的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的发音结构分割方法主要依赖视觉线索，而同步的声学和语音学信号能提供互补的上下文信息，丰富视觉信息并提高分割精度。

Method: 提出VocSegMRI多模态框架，通过跨注意力融合实现动态特征对齐，并引入对比学习目标来增强跨模态表示，即使在推理时音频模态不可用也能保持性能。

Result: 在USC-75 rtMRI数据集子集上达到Dice分数0.95和HD_95距离4.20mm的SOTA性能，优于单模态和多模态基线方法。消融研究证实了跨注意力和对比学习对分割精度和鲁棒性的贡献。

Conclusion: 研究结果突显了集成多模态建模在准确声道分析中的价值，证明了音频和语音学信号对视觉分割任务的补充作用。

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [82] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: 提出基于扩散先验的生成式编码框架，在低码率下通过预训练扩散模型提升压缩性能，在视觉保真度和压缩效率方面优于传统方法


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的发展，视觉内容混合了自然和AI生成图像，需要更高效的编码技术来保持感知质量。传统编解码器和学习方法在高压缩比下难以维持主观质量，现有生成方法面临视觉保真度和泛化性挑战

Method: 使用预优化编码器生成广义压缩域表示，通过轻量级适配器和注意力融合模块与预训练模型内部特征集成。引入分布重归一化方法增强重建保真度，可高效适配不同预训练模型

Result: 方法在低码率下视觉保真度优于现有方法，压缩性能比H.266/VVC提升高达79%，为AI生成内容提供高效解决方案且可适配更广泛内容类型

Conclusion: 提出的生成式编码框架有效利用预训练扩散模型，在低码率压缩中实现了优异的视觉质量和压缩效率，具有很好的适应性和实用性

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [83] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: AdaThinkDrive是一个新颖的视觉语言动作框架，采用双模式推理机制（快速回答和慢速思考），通过自适应选择是否使用思维链推理来平衡自动驾驶决策的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理技术在简单场景中表现不佳，引入不必要的计算开销却无法提升决策质量，需要一种能够自适应选择推理模式的方法。

Method: 1) 在大规模自动驾驶场景上进行预训练，获取世界知识和驾驶常识；2) 监督微调时引入双模式数据集（有/无思维链）；3) 提出自适应思维奖励策略与GRPO结合，通过比较不同推理模式的轨迹质量来奖励模型选择性应用思维链。

Result: 在Navsim基准测试中达到90.3 PDMS，比最佳纯视觉基线高1.7分；相比始终使用思维链的基线，PDMS提升1.4分，推理时间减少14%。

Conclusion: AdaThinkDrive通过自适应推理机制，在保持高精度的同时显著提升效率，证明了在自动驾驶中平衡准确性和效率的有效性。

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [84] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: 提出了一种新的深度伪造定位方法，通过独立使用局部和全局视角预测篡改区域，并采用形态学操作融合输出，有效抑制噪声并增强空间一致性


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测虽然准确率不断提高，但精确定位篡改区域仍面临挑战。现有方法通常忽略局部细节和全局语义上下文的互补性，且融合策略简单导致噪声放大和性能下降

Method: 独立使用局部和全局视角预测篡改区域，采用形态学操作融合两个分支的输出，有效抑制噪声并增强空间一致性

Result: 大量实验证明每个模块都能有效提高伪造定位的准确性和鲁棒性

Conclusion: 该方法通过创新的局部-全局独立预测和形态学融合策略，显著提升了深度伪造区域定位的性能

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [85] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 提出Variable-Rate Spatial Event Mamba架构，直接处理原始事件流，无需中间表示，通过自适应速率控制实现低延迟和高效率


<details>
  <summary>Details</summary>
Motivation: 现有方法需要将事件流转换为帧、体素网格或点云等中间表示，这引入了窗口延迟；而逐点检测方法计算成本高，难以实现实时效率

Method: 使用轻量级因果空间邻域编码器捕获局部几何关系，然后采用基于Mamba的状态空间模型进行线性复杂度的可扩展时序建模，推理时控制器根据事件率自适应调整处理速度

Result: 该方法避免了中间表示带来的窗口延迟，同时通过线性复杂度实现了计算效率，能够在事件率变化时自适应调整处理速度

Conclusion: 提出的Variable-Rate Spatial Event Mamba架构成功解决了事件相机处理中的延迟和计算效率问题，为高速视觉任务提供了有效的解决方案

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [86] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: BWCache是一种无需训练的加速方法，通过动态缓存和重用DiT块特征来减少扩散变换器视频生成的计算冗余，实现2.24倍加速且保持视觉质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)在视频生成中表现出色，但其顺序去噪过程导致不可避免的延迟，限制了实际应用。现有加速方法要么因架构修改而损害视觉质量，要么无法在适当粒度上重用中间特征

Method: 提出Block-Wise Caching (BWCache)方法：1) 分析发现DiT块是推理延迟的主要贡献者；2) 特征变化呈现U形模式，中间时间步具有高度相似性；3) 动态缓存和重用DiT块特征；4) 引入相似性指示器，仅在相邻时间步块特征差异低于阈值时触发特征重用

Result: 在多个视频扩散模型上的广泛实验表明，BWCache实现了高达2.24倍的加速，同时保持可比的视觉质量

Conclusion: BWCache是一种有效的训练免费加速方法，通过智能特征重用显著减少了DiT基视频生成模型的计算冗余，在保持视觉保真度的同时大幅提升推理速度

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [87] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 本文针对大型视觉语言模型中的目标幻觉问题，提出了VHBench-10基准测试和VisionWeaver解决方案，通过多专家特征聚合有效减少幻觉现象


<details>
  <summary>Details</summary>
Motivation: 不同视觉编码器的训练范式导致不同的归纳偏置，从而产生多样化的幻觉表现，现有基准测试无法捕捉这种细粒度的幻觉差异

Method: 提出VHBench-10基准测试（约10,000样本，10个细粒度幻觉类别），并设计VisionWeaver - 基于上下文感知路由网络，使用全局视觉特征生成路由信号，动态聚合多个专业专家的视觉特征

Result: 评估确认不同编码器具有独特的幻觉特征，VisionWeaver能显著减少幻觉并提升整体模型性能

Conclusion: 视觉编码器的选择对减少幻觉至关重要，VisionWeaver通过智能特征融合机制有效解决了多编码器特征聚合的挑战

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [88] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出了首个针对航天器姿态估计关键点回归的监督域适应框架，通过联合优化域不变表示和任务特定风险，显著减小域偏移下的泛化误差，在SPEED+基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决航天器姿态估计中合成数据到真实数据的域适应问题，现有无监督域适应方法在有少量标注目标数据时表现不佳，需要更有效的监督域适应方案。

Method: 基于学习不变表示和风险(LIRR)范式，联合使用标注的合成数据和有限的标注真实数据，优化域不变表示和任务特定风险。

Result: 在SPEED+基准测试中 consistently优于仅源域训练、微调和oracle基线，仅用5%标注目标数据就能达到或超过使用更多标注数据的oracle性能。

Conclusion: 该框架轻量级、骨干网络无关且计算高效，为现实空间环境中鲁棒可部署的航天器姿态估计提供了实用途径。

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [89] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: 提出SWA-PF方法解决无人机在GNSS拒止环境中的定位问题，通过语义加权粒子滤波实现高效定位，定位误差低于10米，计算效率提升10倍


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的无人机定位方法存在数据集不足、实时性能差、环境敏感性和泛化能力有限等问题，特别是在动态或时变环境中表现不佳

Method: 提出大规模多高度飞行段数据集(MAFS)和语义加权自适应粒子滤波(SWA-PF)方法，整合无人机图像和卫星图像的语义特征，包含语义加权机制和优化的粒子滤波架构

Result: 在自建数据集上评估，计算效率比特征提取方法提升10倍，全局定位误差低于10米，能在数秒内使用低分辨率卫星地图完成4自由度位姿估计

Conclusion: SWA-PF方法有效解决了无人机在GNSS拒止环境中的定位挑战，具有高计算效率和精确定位能力，代码和数据集将开源

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [90] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: 提出Masked Feature Modeling (MFM)方法，通过在特征空间进行掩码重建来增强无监督域自适应语义分割性能，无需修改推理架构且零计算开销


<details>
  <summary>Details</summary>
Motivation: 现有对比学习等自监督方法在语义分割域自适应中已取得进展，但掩码建模方法由于架构不兼容和优化目标不一致而未被充分探索

Method: 在特征空间进行特征掩码和重建，引入轻量级Rebuilder模块进行联合训练（推理时丢弃），利用分割解码器对重建特征进行分类，使辅助目标与像素级预测任务紧密耦合

Result: 在各种架构和UDA基准测试中一致提升分割性能

Conclusion: MFM为无监督域自适应语义分割提供了一种简单、高效且可推广的策略

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [91] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: 该论文研究了在训练数据有限的情况下，使用MiniROCKET和HDC-MiniROCKET进行高光谱图像光谱分类，相比当前最优的1D-Justo-LiuNet模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前最优的1D-Justo-LiuNet模型在训练数据有限时性能下降严重，需要找到对有限训练数据更鲁棒的分类方法。

Method: 采用MiniROCKET和HDC-MiniROCKET模型进行光谱分类，这些模型在特征提取部分没有可训练参数，通过精心设计的特征工程来提取特征。

Result: MiniROCKET在有限数据场景下优于1D-Justo-LiuNet，在一般情况下性能相当，尽管参数量更多但对有限训练数据更鲁棒。

Conclusion: MiniROCKET系列模型是解决光谱分类中有限训练数据问题的有效方法，为未来改进空间-光谱方法提供了有价值的补充。

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [92] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: Semi-MOE是首个用于半监督组织病理学图像分割的多任务混合专家框架，通过三个专家网络和自适应多目标损失，在低标签设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习方法在处理组织病理学图像分割时，由于腺体边界模糊和形态学误分类，难以处理噪声伪标签问题。

Method: 提出Semi-MOE框架，包含三个专家网络：主分割专家、符号距离场回归专家和边界预测专家，通过多门控伪标签模块动态聚合专家特征，并采用自适应多目标损失来平衡多个学习目标。

Result: 在GlaS和CRAG基准测试上的广泛实验表明，该方法在低标签设置下优于最先进的方法。

Conclusion: 该方法展示了基于MoE的架构在推进半监督分割方面的潜力，代码已开源。

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [93] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文挑战了表示学习中视图不相关假设，提出显式对齐不同视图表示的方法，在自监督学习中取得了优异的下游任务性能


<details>
  <summary>Details</summary>
Motivation: 当前表示学习方法隐含假设数据点的不相关视图足以学习有意义的表示，但作者发现潜在空间的有意义结构不会自然涌现，需要显式诱导

Method: 提出Consistent View Alignment方法，对齐不同数据视图的表示以整合互补信息，同时避免产生假阳性

Result: 在MICCAI 2025 SSL3D挑战赛中，使用Primus vision transformer和ResEnc卷积神经网络分别获得第一和第二名

Conclusion: 结构化视图对齐在学习有效表示中起着关键作用，显式对齐方法能显著提升下游任务性能

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [94] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecDiff是一种训练免费的多级特征缓存策略，通过自推测引入未来信息，结合历史信息实现动态特征选择和分类，在保持质量的同时显著加速扩散模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅依赖历史信息，导致准确性和速度性能受限。需要引入未来信息来突破速度-准确性的权衡瓶颈。

Method: 提出自推测范式，利用不同迭代次数相同时间步的信息相似性引入未来信息。包括基于自推测信息的缓存特征选择算法和基于特征重要性分数的多级特征分类算法。

Result: 在Stable Diffusion 3、3.5和FLUX上分别实现平均2.80×、2.74×和3.17×的加速，质量损失可忽略不计。

Conclusion: 通过融合推测和历史信息，SpecDiff突破了速度-准确性权衡瓶颈，推动了高效扩散模型推理的Pareto前沿。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [95] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出了Dense Video Understanding (DVU)框架和Gated Residual Tokenization (GRT)方法，通过运动补偿和语义场景融合技术，实现高帧率视频理解的同时减少计算开销和token数量增长。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型主要依赖低帧率采样，丢弃了密集时间信息，无法处理需要精确时间对齐的任务（如讲座理解）。高帧率处理会导致token数量线性增长和冗余计算。

Method: 提出GRT两阶段框架：1) 运动补偿门控tokenization - 使用像素级运动估计跳过静态区域；2) 语义场景内部tokenization合并 - 融合静态区域内的token。实现亚线性token增长和计算量减少。

Result: 在DIVE基准测试中，GRT优于更大的VLLM基线模型，且性能随帧率增加而提升，证明了密集时间信息的重要性。

Conclusion: GRT方法能够实现高效、可扩展的高帧率视频理解，解决了当前视频理解模型在密集时间推理方面的局限性。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [96] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS是一个新的数据集蒸馏框架，利用图像中的隐式文本语义信息，通过视觉语言模型和大型语言模型生成文本原型，结合扩散模型合成紧凑数据集，显著提升蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏方法主要捕获低级视觉特征，忽略了图像中的高级语义和结构信息，导致蒸馏效果有限。

Method: 使用视觉语言模型生成外部文本并与图像特征融合，构建先验聚类缓冲区；通过局部语义感知选择代表性样本生成图像和文本原型；最后采用双原型引导策略通过扩散模型生成最终合成数据集。

Result: 大量实验证实该方法在保持竞争性模型性能的同时，实现了高效学习，显著提升了数据集蒸馏的效果。

Conclusion: EDITS框架通过利用图像中的文本语义信息，成功解决了传统方法忽略高级语义的问题，为数据集蒸馏提供了新的有效解决方案。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [97] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss是一种基于高斯溅射辐射光栅化的X射线层析成像重建算法，专门针对稀疏视角采集条件，通过创新的初始化策略过滤层析伪影，仅需3%的完整视角数据即可实现优于传统方法的重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统CT在平板结构（如微芯片、复合电池材料）的无损检测中存在几何约束问题，而层析成像在稀疏视角条件下的高质量体积重建仍然具有挑战性。

Method: 结合高斯溅射辐射光栅化和专用的探测器到世界坐标变换模型（包含层析倾斜角），采用初始化策略显式过滤层析伪影，防止高斯分配到虚假结构，集中模型容量表示真实物体。

Result: 在合成和真实数据集上的广泛实验表明，该方法仅使用3%的完整视角数据就能实现优于在全数据集上优化的迭代方法的性能。

Conclusion: LamiGauss能够直接从稀疏投影中有效优化，实现有限数据下的准确高效重建，在层析成像重建领域表现出卓越的有效性和优越性。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [98] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: DAM4SAM是一个针对SAM2的干扰物感知内存模块，通过改进内存管理和自省机制，有效减少跟踪漂移并提升遮挡后重检测能力，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于内存的视频分割方法（如SAM2）在分割任务中表现优异，但在视觉目标跟踪中面临干扰物（与目标视觉相似的物体）的关键挑战，需要专门针对跟踪任务进行优化。

Method: 提出了干扰物感知的即插即用内存模块和基于自省的内存管理方法，构建了DiDi干扰物蒸馏数据集用于分析，并将该模块集成到实时跟踪器中。

Result: 在13个基准测试中超越SAM2.1，在10个测试中达到新的SOTA；集成到EfficientTAM中提升11%性能，与EdgeTAM集成提升4%，展现出良好的架构泛化能力。

Conclusion: DAM4SAM通过专门设计的干扰物感知内存机制，显著提升了目标跟踪在干扰物存在情况下的性能，同时保持了良好的实时性和架构兼容性。

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [99] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: PelFANet是一个双流注意力网络，融合原始骨盆X光片和分割骨图像，通过Fused Attention Blocks迭代交换和精炼特征，显著提升骨盆骨折分类性能，特别是在细微骨折检测方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 骨盆骨折在标准X光片中常常难以诊断，特别是当骨折迹象细微或不可见时，需要开发更有效的诊断方法来提高检测准确性。

Method: 提出PelFANet双流注意力网络，融合原始X光片和分割骨图像，使用Fused Attention Blocks迭代交换和精炼特征，采用两阶段分割引导训练流程。

Result: 在AMERI数据集上，可见骨折准确率88.68%、AUC 0.9334；不可见骨折准确率82.29%、AUC 0.8688，显著优于传统方法。

Conclusion: 解剖感知的双输入架构在骨盆骨折检测方面具有重要临床潜力，特别是在放射学表现细微的情况下表现出色。

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [100] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: EvHand-FPV是一个轻量级单事件相机第一人称视角3D手部追踪框架，通过手腕ROI定位、端到端映射和多任务学习策略，在保持高精度的同时大幅降低计算量和参数数量，适合XR设备应用。


<details>
  <summary>Details</summary>
Motivation: 传统帧式方法在精度、延迟和能效方面难以满足XR设备的需求，事件相机具有微秒级时间分辨率和毫瓦级功耗优势，但缺乏第一人称视角的基准数据集。

Method: 构建合成训练数据与真实评估数据相结合的事件数据集；引入手腕ROI几何定位；端到端映射嵌入ROI偏移减少计算；多任务学习辅助几何特征头提升表征能力。

Result: 在真实测试集上2D-AUCp从0.77提升到0.85，参数量减少89%至1.2M，计算量减少89%至0.185G FLOPs；合成数据上保持0.84的3D-AUCp。

Conclusion: 该方法实现了准确高效的ego-centric事件手部追踪，适合设备端XR应用，数据集和代码已开源。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [101] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出WARM模块，通过白化和着色变换解决少样本3D点云分割中可学习原型标记与支持特征之间的分布差距问题，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本3D点云分割方法使用传统算法（如最远点采样）构建原型，但初始随机性严重影响性能，且原型生成过程研究不足。注意力机制虽具潜力，但存在分布差距问题。

Method: 提出White Aggregation and Restoration Module (WARM)，在白化和着色变换之间夹入交叉注意力：白化将支持特征与原型标记对齐，注意力处理后着色恢复原始分布，从而生成更具代表性的原型。

Result: 在多个少样本3D点云分割基准测试中取得了显著领先的最先进性能，通过大量实验证明了方法的有效性。

Conclusion: WARM模块通过简单的设计实现了鲁棒的注意力机制，能够捕捉支持特征间的语义关系，生成高质量原型，有效解决了少样本3D点云分割中的关键挑战。

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [102] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 提出SRC框架，通过迭代校准视觉语言模型中答案与推理过程的对齐，包括轻量级推理微调、候选答案搜索、R-Scorer评分模型和偏好微调，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视觉问答中存在推理过程与生成答案不一致的问题，导致错误响应，需要校准推理与答案的对齐关系

Method: 1) 轻量级推理微调改变响应格式；2) 搜索多样化候选响应；3) 使用R-Scorer进行成对评分；4) 置信度加权的偏好微调

Result: 在多个基准测试中显著提升了视觉语言模型的感知、推理和泛化能力

Conclusion: 强调推理导向的对齐方法在挖掘大型视觉语言模型潜力方面的重要性

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [103] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: AntiPure是一种新型保护性扰动方法，通过补丁频率引导和错误时间步引导来对抗净化攻击，在保持图像感知质量的同时实现有效的后定制失真。


<details>
  <summary>Details</summary>
Motivation: 扩散模型如Stable Diffusion的强大定制能力带来了严重的安全风险（如深度伪造和版权侵权），现有的保护性扰动方法容易被净化技术移除，需要开发抗净化的保护方案。

Method: 提出AntiPure方法，采用两种引导机制：1）补丁频率引导减少模型对高频分量的影响；2）错误时间步引导破坏模型在不同时间步的去噪策略，嵌入难以被净化的不可感知扰动。

Result: 实验表明AntiPure在净化-定制工作流中实现了最小的感知差异和最大的失真效果，优于其他保护性扰动方法，作为净化技术的压力测试表现优异。

Conclusion: AntiPure成功暴露了净化技术的脆弱性，为保护图像免受恶意伪造提供了有效的抗净化解决方案，在安全性和感知质量之间取得了良好平衡。

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [104] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: 提出了Noise Level Guidance (NLG)方法，通过优化初始噪声来提高扩散模型生成质量和提示遵循度，无需额外数据、网络或反向传播


<details>
  <summary>Details</summary>
Motivation: 扩散模型初始的随机高斯噪声会影响最终输出质量和提示遵循度，现有噪声优化方法需要额外数据集、网络或反向传播，实用性受限

Method: NLG方法通过增加初始噪声与通用指导对齐的可能性来优化噪声，提供统一框架适用于条件和无条件扩散模型，支持多种扩散级指导形式

Result: 在五个标准基准测试上的实验表明，该方法提高了输出生成质量和输入条件遵循度，计算效率高

Conclusion: NLG作为扩散模型的实用且可扩展的增强方法，能够无缝集成现有指导方法并保持计算效率

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [105] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: PairTally是一个专门评估细粒度视觉计数能力的基准数据集，包含681张高分辨率图像，每张图像包含两个物体类别，要求模型根据形状、大小、颜色或语义的细微差异进行区分和计数。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉计数模型（包括类别无关计数模型和大规模视觉语言模型）在细粒度、意图驱动的计数任务上的能力尚不明确，需要专门的评估基准。

Method: 构建PairTally数据集，包含681张高分辨率图像，每张图像有两个物体类别，分为类别间（不同类别）和类别内（密切相关的子类别）两种设置，对多种最先进模型进行基准测试。

Result: 尽管最近有所进展，但当前模型在可靠地计数用户意图方面仍然困难，特别是在细粒度和视觉模糊的情况下。

Conclusion: PairTally为诊断和改进细粒度视觉计数系统提供了新的基础，揭示了当前模型在精细计数任务上的局限性。

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [106] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA是一种知识蒸馏方法，通过对象级别的跨架构对齐，将大型视觉语言教师模型的多模态语义知识转移到轻量级视觉目标检测学生模型中，在少样本个性化检测任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注密集或全局对齐，缺乏对象级别的语义知识转移。需要一种无需修改教师模型、推理时无需文本输入的高效方法，将多模态语义知识迁移到轻量级视觉检测器中。

Method: 使用翻译模块将学生特征映射到联合空间，通过双目标损失函数（局部对齐和全局关系一致性）指导学生和翻译模块的训练，在对象级别进行跨架构对齐。

Result: 在四个少样本个性化检测基准测试中，平均得分提升+10.1，性能达到与大型多模态模型相当的水平，证明了其实际部署的适用性。

Conclusion: MOCHA通过对象级别的知识蒸馏，成功实现了多模态语义向视觉检测器的高效迁移，在保持紧凑架构的同时达到优异性能，适合实际应用部署。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [107] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: 提出改进版YOLO-FEDER FusionNet无人机检测框架，通过融合通用目标检测和伪装目标检测技术，在复杂视觉环境中显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 解决无人机在复杂视觉环境中检测困难的问题，包括背景杂乱、目标尺度小和伪装效应等挑战，传统YOLO检测器在低目标-背景分离度的杂乱环境中性能下降

Method: 在原始架构基础上改进训练数据组成、特征融合策略和骨干网络设计：1) 使用大规模照片级合成数据+少量真实样本增强鲁棒性 2) 系统评估多尺度FEDER特征贡献 3) 全面测试多种YOLO骨干网络配置

Result: 最佳配置(YOLOv8l骨干+DWD模块FEDER特征)相比基线：FNR降低39.1个百分点，mAP@0.5提升62.8个百分点

Conclusion: 集成中间FEDER特征与骨干网络升级能显著提升复杂环境中无人机检测性能，证明了融合通用检测和伪装检测技术的有效性

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [108] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-VL2是一个开源的2B和8B参数规模视觉语言基础模型，在106个数据集上实现最先进性能，特别是在MMMU和MathVista等复杂推理基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 作为SAIL-VL的继任者，旨在开发一个全面的多模态理解和推理基础模型，通过大规模数据策展、渐进式训练框架和架构创新来提升模型能力。

Method: 采用三核心创新：1)大规模数据策展管道，通过评分和过滤策略提升数据质量；2)渐进式训练框架，从预训练视觉编码器到多模态预训练，再到思维融合SFT-RL混合范式；3)架构创新，扩展到稀疏混合专家(MoE)设计。

Result: 在106个数据集上展现竞争优势，在MMMU和MathVista等挑战性推理基准上达到最先进性能。SAIL-VL2-2B在OpenCompass排行榜上位居4B参数规模以下开源模型首位。

Conclusion: SAIL-VL2为开源多模态社区提供了一个高效且可扩展的基础模型，在多个基准测试中实现了最先进的性能表现。

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [109] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: PROFUSEme模型通过融合临床、影像和病理多模态数据，使用Cox比例风险回归器预测前列腺癌根治术后生化复发风险，在内部验证和外部挑战中均表现优异


<details>
  <summary>Details</summary>
Motivation: 约30%前列腺癌患者在根治性前列腺切除术后会出现生化复发，导致死亡率增加。早期准确预测BCR有助于临床决策和改善患者预后

Method: 提出PROFUSEme方法，采用中间融合策略学习临床、影像和病理数据的跨模态交互，结合Cox比例风险回归器进行预测

Result: 在内部5折嵌套交叉验证中平均C-index为0.861(σ=0.112)，在CHIMERA 2025挑战验证排行榜的保留数据上C-index为0.7103，优于后期融合配置

Conclusion: 多模态数据融合方法能够有效预测前列腺癌术后生化复发，为临床早期干预提供有力工具

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [110] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate是一个统一的角色动画和替换框架，能够根据参考视频精确复制角色的表情和动作来生成高质量角色视频，或将动画角色无缝集成到参考视频中替换原始角色。


<details>
  <summary>Details</summary>
Motivation: 现有的角色动画方法往往难以同时实现高保真度的动作复制和环境光照的完美融合，需要开发一个统一的框架来解决角色动画和替换的多任务需求。

Method: 基于Wan模型构建，采用改进的输入范式区分参考条件和生成区域，使用空间对齐的骨架信号复制身体运动，从源图像提取隐式面部特征重现表情，并开发了辅助的Relighting LoRA模块来增强环境光照融合。

Result: 实验结果表明Wan-Animate达到了最先进的性能水平，能够生成具有高度可控性和表现力的角色视频，并实现无缝的环境集成效果。

Conclusion: Wan-Animate成功统一了角色动画和替换任务，通过创新的技术方法实现了高质量的角色视频生成和环境融合，作者承诺将开源模型权重和源代码。

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [111] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: 本文提出VSE-MOT框架，通过视觉语义增强技术解决低质量视频中的多目标跟踪问题，在真实低质量场景中性能提升8%-20%


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪算法在低质量视频中性能显著下降，需要提升在真实世界低质量视频场景中的应用能力

Method: 设计三分支架构利用视觉语言模型提取全局视觉语义信息，并引入MOT-Adapter和VSFM模块来适配跟踪任务和增强特征融合

Result: 在真实低质量视频场景中跟踪性能指标比现有方法提升约8%到20%，同时在常规场景中保持稳健性能

Conclusion: VSE-MOT框架有效解决了低质量视频中的多目标跟踪挑战，为实际应用提供了有力解决方案

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [112] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文提出了AD-DINOv3，首个基于DINOv3的零样本异常检测框架，通过多模态对比学习和异常感知校准模块解决特征对齐和局部异常检测问题，在多个基准测试中达到或超越SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统ZSAD方法主要基于CLIP模型，而新兴的视觉基础模型DINOv3具有强大的可迁移表示能力。但直接应用DINOv3面临两个挑战：大规模预训练数据与异常检测任务之间的领域偏差导致特征错位，以及预训练表示对全局语义的固有偏向导致细微异常被误判为正常前景。

Method: 提出AD-DINOv3多模态框架：1) 将异常检测构建为多模态对比学习问题，使用DINOv3提取视觉特征，CLIP文本编码器提供正常/异常提示嵌入；2) 引入轻量级适配器桥接领域差距；3) 设计异常感知校准模块(AACM)，显式引导CLS token关注异常区域而非通用前景语义。

Result: 在8个工业和医学基准测试上的广泛实验表明，AD-DINOv3始终匹配或超越最先进方法，验证了其作为通用零样本异常检测框架的优越性。

Conclusion: AD-DINOv3成功将DINOv3适配到ZSAD任务，通过多模态对比学习和异常感知机制有效解决了特征对齐和局部异常检测问题，为零样本异常检测提供了新的强大解决方案。

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [113] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出EMA引导的伪监督框架和类别感知跨模态一致性损失，用于弱监督音频-视觉视频解析，在LLP和UnAV-100数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决现有弱监督音频-视觉视频解析方法缺乏稳定的片段级监督和类别感知跨模态对齐的问题

Method: 1) EMA引导的伪监督框架，通过自适应阈值或top-k选择生成可靠的片段级掩码；2) 类别感知跨模态一致性(CMA)损失，在可靠的片段-类别对上对齐音频和视觉嵌入

Result: 在LLP和UnAV-100数据集上实现了最先进的性能表现

Conclusion: 所提出的方法通过提供稳定的片段级监督和有效的跨模态对齐，显著提升了弱监督音频-视觉视频解析的性能

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [114] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 该论文提出了一种通过集成软混合专家(Soft MoE)机制来提升遥感基础模型效率的方法，在保持或提高表征性能的同时显著降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基础模型要么计算复杂度高，要么表征能力有限，限制了实际应用。需要开发计算效率更高且性能优越的模型。

Method: 将软混合专家机制集成到Cross-Sensor Masked Autoencoder模型中，形成CSMoE模型，并采用主题-气候描述符驱动的采样策略构建训练集。

Result: CSMoE在场景分类、语义分割和图像检索任务中实现了超过两倍的计算效率提升，同时保持竞争性性能表现。

Conclusion: 提出的软混合专家集成方法有效解决了遥感基础模型的计算效率问题，在表征能力、准确性和计算效率之间实现了优越的平衡。

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [115] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: 提出了一种具有级联配准机制的鲁棒虚拟染色框架，解决了生成输出与真实染色图像之间的空间不匹配问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统组织病理学诊断需要多种化学染色，过程耗时、劳动密集且对环境有害。现有虚拟染色方法依赖对齐良好的配对数据，但化学染色过程会导致组织变形，单个组织切片无法进行多次染色，导致数据集多为非配对或粗略配对，难以实现准确的像素级监督。

Method: 提出了一个包含级联配准机制的鲁棒虚拟染色框架，通过配准机制解决生成输出与真实染色图像之间的空间不匹配问题。

Result: 在五个数据集上显著优于最先进模型，内部数据集平均提升3.2%，外部数据集平均提升10.1%。在严重不对齐的数据集上，峰值信噪比相比基线模型提升了23.8%。

Conclusion: 该方法在不同数据集上表现出卓越的鲁棒性，简化了虚拟染色的数据采集过程，为推进虚拟染色技术的发展提供了新思路。

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [116] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 美颜滤镜会降低深度伪造和换脸攻击检测器的性能，导致检测准确性下降


<details>
  <summary>Details</summary>
Motivation: 社交媒体美颜滤镜的流行引发了对面部图像视频可靠性和自动化人脸分析有效性的担忧，特别是对数字篡改检测器的影响

Method: 在基准数据集上评估多个最先进的检测器，在应用各种平滑滤镜前后进行性能对比分析

Result: 研究发现美颜滤镜会导致检测器性能下降，揭示了面部美化带来的脆弱性

Conclusion: 需要开发能够抵抗此类美颜修饰的鲁棒检测模型

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [117] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025挑战赛聚焦多模态推理，发布了Lens和AdsQA两个数据集，涵盖12个日常场景和广告视频特定领域，评估了40+基线模型，吸引了76个团队参与。


<details>
  <summary>Details</summary>
Motivation: 整合多模态机器学习和LLM的不同方法，通过大型基准测试跟踪这一快速发展领域的最新进展，并专注于现实世界和专业化场景以拓宽多模态推理应用。

Method: 组织团队发布两个定制数据集(Lens和AdsQA)，评估40+基线模型(包括通用MLLM和任务特定模型)，开设三个竞赛赛道(VG-RS、VQA-SA、VR-Ads)。

Result: 76个知名学术和工业机构团队注册，40+有效提交(从1200+中筛选)进入排名，数据集、代码集和排名结果公开可用。

Conclusion: MARS2 2025挑战赛成功建立了多模态推理的基准测试平台，为研究人员提供了跟踪最新技术和评估模型性能的宝贵资源。

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [118] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: 该论文研究了原始形状构成的抽象图像能否有效传递视觉语义信息，发现当前抽象图像表示效果不如传统光栅图像，为此构建了分层抽象图像数据集HAID进行多任务评估。


<details>
  <summary>Details</summary>
Motivation: 探索抽象图像（由原始形状构成）能否像传统光栅图像一样有效传递视觉语义信息，并分析不同抽象层次下高级语义内容的捕获能力。

Method: 引入分层抽象图像数据集HAID，包含从正常光栅图像生成的多层次抽象图像，在分类、分割和目标检测等任务上训练和评估传统视觉系统。

Result: 研究发现抽象图像确实能够传递视觉语义信息，但其表示效果通常不如传统光栅图像，性能存在差距。

Conclusion: 论文探讨了抽象图像作为传递视觉语义信息格式的潜在有效性，为视觉任务提供了新的数据表示形式研究。

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [119] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: BEVUDA++：首个针对BEV感知中多视角3D目标检测的领域自适应方法，通过几何感知的师生框架解决跨域性能下降问题，在四个跨域场景中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: BEV感知在自动驾驶中具有重要价值，但现有研究忽视了领域偏移问题，导致跨域传输时性能显著下降。研究发现多几何空间（2D、3D体素、BEV）中的域偏移累积是主要挑战

Method: 提出几何感知师生框架BEVUDA++，包含可靠深度教师(RDT)和几何一致学生(GCS)模型。RDT通过不确定性估计融合目标LiDAR和深度预测生成深度感知信息；GCS将多空间特征映射到统一几何嵌入空间；引入不确定性引导指数移动平均(UEMA)减少误差累积

Result: 在四个跨域场景的全面实验中取得最先进性能，特别是在昼夜适应任务上实现12.9% NDS和9.5% mAP的提升

Conclusion: 该方法有效解决了BEV感知中的领域自适应挑战，通过几何一致性约束和不确定性引导显著减少了跨域性能下降，为自动驾驶BEV感知的实用化提供了重要解决方案

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [120] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: STEP是一个结合动态patch合并和token剪枝的混合框架，通过dCTS策略网络和早期退出机制，在保持精度的同时显著降低Vision Transformer的计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现出色但计算和内存成本高昂，需要提高效率而不显著牺牲准确性。

Method: 提出STEP框架：1) dCTS轻量级CNN策略网络实现灵活的超patch合并 2) 编码器集成早期退出机制移除高置信度supertokens 3) 动态token减少策略

Result: dCTS单独应用时token数量减少2.5倍，计算成本降低2.6倍，吞吐量提升3.4倍。完整STEP框架计算复杂度降低4倍，推理速度提升1.7倍，精度损失不超过2.0%，40%的token可在最终层前提前终止。

Conclusion: STEP框架有效解决了ViT在语义分割中的效率问题，通过混合token减少策略实现了计算效率和精度的良好平衡。

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [121] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: Cineaste是一个用于评估长视频电影理解能力的综合基准数据集，包含3119个多选题，涵盖200部电影的1805个场景，测试5种细粒度推理能力。现有模型在该基准上表现不佳，最高准确率仅63.15%，表明长时序推理是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解基准主要测试短片段识别或使用模板化问题，缺乏对长叙事内容的细粒度推理能力评估。需要一个新的基准来填补这一空白，评估模型对长电影内容的深度叙事理解能力。

Method: 使用GPT-4o生成多样化、上下文丰富的问题，整合视觉描述、字幕、场景标题和摘要。采用两阶段过滤流程：上下文独立性过滤确保问题需要视频上下文，上下文真实性过滤验证与电影内容的事实一致性，减少幻觉。

Result: 现有多模态大语言模型在Cineaste基准上表现不佳，最佳开源模型准确率仅为63.15%。分析表明长时序推理是主要性能瓶颈，凸显了在细粒度上下文理解方面的重大挑战。

Conclusion: Cineaste基准揭示了当前视频理解模型在长电影叙事理解方面的局限性，特别是在长时序推理方面存在显著挑战，需要在该领域进行进一步的技术创新和模型改进。

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [122] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam是首个多学科文本到图像考试基准，包含10个学科的1000个样本，采用四级分类法组织考试式提示，用于评估语义正确性和视觉合理性。


<details>
  <summary>Details</summary>
Motivation: 现有考试式基准主要关注理解和推理任务，而生成基准强调世界知识和视觉概念的展示，忽视了严格绘图考试的评估。

Method: 构建包含1000个样本的多学科文本到图像考试基准，每个问题配备真实图像和细粒度评分点，采用四级分类法组织考试式提示。

Result: 实验显示即使是GPT-Image-1和Gemini-2.5-Flash-Image等最先进模型也仅获得不到15%的严格分数，大多数模型得分接近0%。

Conclusion: GenExam通过将图像生成框架化为考试，提供了对模型整合知识、推理和生成能力的严格评估，为通往通用AGI的道路提供了见解。

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [123] [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
*Shun Huang,Zhihua Fang,Liang He*

Main category: cs.SD

TL;DR: 本文提出了一种名为OS-SCL的单阶段监督对比学习方法，通过特征扰动和噪声监督对比学习解决无监督异常声音检测中的误报问题，并提出了TFgram时频特征，在DCASE 2020挑战赛上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 无监督异常声音检测在处理来自不同机器的同类样本时经常出现误报问题，现有的自监督方法未能有效解决这一挑战。

Method: 提出OS-SCL训练技术：在嵌入空间进行特征扰动，采用单阶段噪声监督对比学习方法；同时提出从原始音频提取的TFgram时频特征。

Result: 仅使用Log-Mel特征时达到94.64% AUC、88.42% pAUC和89.24% mAUC；使用TFgram特征时性能进一步提升至95.71% AUC、90.23% pAUC和91.23% mAUC。

Conclusion: OS-SCL方法有效解决了异常声音检测中的误报问题，TFgram特征能够更好地捕捉关键信息，该方法在DCASE 2020挑战赛上表现出色。

Abstract: Unsupervised anomalous sound detection aims to detect unknown anomalous
sounds by training a model using only normal audio data. Despite advancements
in self-supervised methods, the issue of frequent false alarms when handling
samples of the same type from different machines remains unresolved. This paper
introduces a novel training technique called one-stage supervised contrastive
learning (OS-SCL), which significantly addresses this problem by perturbing
features in the embedding space and employing a one-stage noisy supervised
contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved
94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.
Additionally, a time-frequency feature named TFgram is proposed, which is
extracted from raw audio. This feature effectively captures critical
information for anomalous sound detection, ultimately achieving 95.71\% AUC,
90.23\% pAUC, and 91.23\% mAUC. The source code is available at:
\underline{www.github.com/huangswt/OS-SCL}.

</details>


### [124] [A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds](https://arxiv.org/abs/2509.13390)
*Deepti Kunte,Bram Cornelis,Claudio Colangeli,Karl Janssens,Brecht Van Baelen,Konstantinos Gryllias*

Main category: cs.SD

TL;DR: 本文提出了一种基于领域知识的模型选择方法，通过使用健康样本的扰动生成代理异常来支持无监督异常检测中的模型选择，在汽车舱内声音异常检测任务上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 汽车舱内声音异常检测通常是无监督学习问题，由于缺乏标注的故障数据，传统基于验证重建误差的模型选择方法可靠性有限，需要一种有效的模型选择策略。

Method: 提出领域知识指导的方法，通过对健康样本的频谱图进行结构化扰动来生成代理异常，将这些代理异常用于验证集来支持模型选择。

Result: 在包含5种典型故障类型（不平衡、调制、啸叫、风噪、脉宽调制）的高保真电动汽车数据集上实验表明，使用代理异常选择的模型显著优于传统模型选择策略。

Conclusion: 该方法为无监督异常检测中的模型选择提供了有效解决方案，公开的数据集将促进相关研究的进一步发展。

Abstract: The detection of anomalies in automotive cabin sounds is critical for
ensuring vehicle quality and maintaining passenger comfort. In many real-world
settings, this task is more appropriately framed as an unsupervised learning
problem rather than the supervised case due to the scarcity or complete absence
of labeled faulty data. In such an unsupervised setting, the model is trained
exclusively on healthy samples and detects anomalies as deviations from normal
behavior. However, in the absence of labeled faulty samples for validation and
the limited reliability of commonly used metrics, such as validation
reconstruction error, effective model selection remains a significant
challenge. To overcome these limitations, a domain-knowledge-informed approach
for model selection is proposed, in which proxy-anomalies engineered through
structured perturbations of healthy spectrograms are used in the validation set
to support model selection. The proposed methodology is evaluated on a
high-fidelity electric vehicle dataset comprising healthy and faulty cabin
sounds across five representative fault types viz., Imbalance, Modulation,
Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced
sound synthesis techniques, and validated via expert jury assessments, has been
made publicly available to facilitate further research. Experimental
evaluations on the five fault cases demonstrate the selection of optimal models
using proxy-anomalies, significantly outperform conventional model selection
strategies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [125] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: TICL是一种使用文本嵌入KNN选择语义相关上下文示例的方法，无需微调即可显著提升多模态模型的语音识别性能，在多种挑战性任务上实现最高84.7%的相对WER降低。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型已展示出上下文学习能力，但有效的上下文示例选择方法仍未被充分探索，需要一种简单有效的方法来提升现有多模态模型的语音识别性能。

Method: 提出Text-Embedding KNN for SICL (TICL)管道，利用语义上下文通过K近邻算法选择相关示例，无需对现成的大型多模态模型进行微调。

Result: 在带口音英语、多语言语音和儿童语音等挑战性ASR任务上，该方法使模型超越零样本性能，实现最高84.7%的相对词错误率降低。消融研究证明了方法的鲁棒性和效率。

Conclusion: TICL提供了一种简单有效的上下文示例选择方法，能够显著提升语音识别性能，证明了语义上下文在语音上下文学习中的重要性。

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [126] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本文分析了自动化问题解决中LLM代理工具的失败模式，提出了一个包含3个主要阶段、9个类别和25个子类别的失败分类法，并设计了一个专家-执行者协作框架来解决推理缺陷和认知僵局问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化问题解决评估主要报告总体成功率，但掩盖了成功和失败的根本原因，难以诊断模型弱点或指导针对性改进。需要从高层次性能指标转向根本原因分析。

Method: 首先分析三种最先进工具在SWE-Bench-Verified中的表现和效率，然后对150个失败实例进行系统手动分析，建立失败模式分类法，最后提出专家-执行者协作框架来纠正推理缺陷和打破认知僵局。

Result: 研究发现两种架构范式具有不同的失败特征，大多数代理失败源于有缺陷的推理和认知僵局。提出的专家-执行者框架解决了领先单代理22.2%之前无法解决的问题。

Conclusion: 这些发现为通过诊断评估和协作设计构建更强大的代理铺平了道路，展示了协作框架在解决自动化问题解决中关键挑战的有效性。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [127] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: 思维链(CoT)推理虽然能提升大语言模型性能，但会带来高计算成本。研究发现过长的CoT反而会导致截断、准确率下降和延迟增加。为此提出了SEER自适应框架，通过压缩CoT在保持准确性的同时减少42.1%的长度。


<details>
  <summary>Details</summary>
Motivation: CoT推理虽然能提高LLM在算术、逻辑和常识任务中的准确性和鲁棒性，但会产生高计算成本（延迟、内存使用和KV缓存需求），特别是在需要简洁确定性输出的软件工程任务中。需要研究这种权衡并找到优化方法。

Method: 提出了SEER（Self-Enhancing Efficient Reasoning）自适应框架，结合Best-of-N采样和任务感知自适应过滤，通过动态调整阈值来压缩CoT推理过程，减少冗余和计算开销。

Result: 在三个软件工程任务和一个数学任务上的评估显示：SEER平均缩短CoT 42.1%，通过减少截断提高了准确性，并消除了大多数无限循环问题。

Conclusion: SEER是一个实用方法，能在资源约束下使CoT增强的LLM更加高效和鲁棒，挑战了"推理越长越好"的假设，强调了自适应CoT控制的必要性。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [128] [An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies](https://arxiv.org/abs/2509.12577)
*Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CY

TL;DR: 本文开发了基于LLM的方法来分析审议大会的转录文本，追踪观点演变和投票动态，为理解民主审议过程提供新工具


<details>
  <summary>Details</summary>
Motivation: 在政治极化和制度信任下降的背景下，代表审议大会成为解决复杂政策问题的重要民主论坛，但缺乏系统追踪观点演变过程的实证研究

Method: 开发基于大型语言模型(LLM)的方法论，分析技术增强型面对面审议大会的转录文本，识别和可视化表达建议的空间，重建代表观点演变过程

Result: 该方法能够揭示传统大会输出中不可见的高分辨率动态，为审议过程提供新的实证见解

Conclusion: LLM方法能够有效追踪审议过程中观点的演变和优先排序，为民主审议研究提供了创新的分析工具

Abstract: In an era of increasing societal fragmentation, political polarization, and
erosion of public trust in institutions, representative deliberative assemblies
are emerging as a promising democratic forum for developing effective policy
outcomes on complex global issues. Despite theoretical attention, there remains
limited empirical work that systematically traces how specific ideas evolve,
are prioritized, or are discarded during deliberation to form policy
recommendations. Addressing these gaps, this work poses two central questions:
(1) How might we trace the evolution and distillation of ideas into concrete
recommendations within deliberative assemblies? (2) How does the deliberative
process shape delegate perspectives and influence voting dynamics over the
course of the assembly? To address these questions, we develop LLM-based
methodologies for empirically analyzing transcripts from a tech-enhanced
in-person deliberative assembly. The framework identifies and visualizes the
space of expressed suggestions. We also empirically reconstruct each delegate's
evolving perspective throughout the assembly. Our methods contribute novel
empirical insights into deliberative processes and demonstrate how LLMs can
surface high-resolution dynamics otherwise invisible in traditional assembly
outputs.

</details>


### [129] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: 本文提出"准确性悖论"概念，认为过度依赖准确性作为评估LLM幻觉的主要标准反而会产生反效果，掩盖了更深层的认知和社会风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常决策中的普及，其认知风险和社会风险需要紧急审视。当前监管、学术和技术讨论将准确性作为缓解幻觉危害的主要基准，但作者认为这种过度依赖会误诊问题并产生反效果。

Method: 通过跨学科文献分析，构建了幻觉类型的分类法，并从三个相互关联的维度（输出、个体和社会）展示准确性悖论。同时分析了欧盟AI法案、GDPR和DSA等现行法规的局限性。

Result: 研究发现：1）准确性只是可靠性的表面代理，鼓励优化修辞流畅性和表面正确性而非认知可信度；2）准确性无法检测非事实错误但具有误导性、价值负载或社会扭曲的伤害；3）监管对准确性的过度强调掩盖了幻觉的更广泛社会后果。

Conclusion: 当前法规在结构上尚未准备好应对这些认知、关系和系统性伤害。文章呼吁向多元化、情境感知和抗操纵的AI可信治理方法进行根本性转变。

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [130] [CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI](https://arxiv.org/abs/2509.13356)
*Hasin Jawad Ali,Ilhamul Azam,Ajwad Abrar,Md. Kamrul Hasan,Hasan Mahmud*

Main category: cs.CY

TL;DR: CogniAlign是一个基于自然主义道德现实主义的多智能体审议框架，通过跨学科专家代理的辩论来提升AI的道德推理能力，在60多个道德问题上显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 解决AI与人类价值观对齐的挑战，现有方法存在道德原则抽象冲突和黑箱推理的问题，需要更透明和实证基础的道德推理方法。

Method: 基于自然主义道德现实主义，将道德推理建立在个体和集体生存能力基础上，通过神经科学、心理学、社会学和进化生物学等学科专家代理进行结构化辩论，由仲裁者综合判断。

Result: 在60多个道德问题上，CogniAlign平均在分析质量上提升16.2分，广度提升14.3分，解释深度提升28.4分。在海因茨困境中得分89.2，显著优于GPT-4o的69.2分。

Conclusion: CogniAlign通过减少黑箱推理和避免欺骗性对齐，展示了跨学科审议作为可扩展的安全透明AI对齐路径的潜力。

Abstract: The challenge of aligning artificial intelligence (AI) with human values
persists due to the abstract and often conflicting nature of moral principles
and the opacity of existing approaches. This paper introduces CogniAlign, a
multi-agent deliberation framework based on naturalistic moral realism, that
grounds moral reasoning in survivability, defined across individual and
collective dimensions, and operationalizes it through structured deliberations
among discipline-specific scientist agents. Each agent, representing
neuroscience, psychology, sociology, and evolutionary biology, provides
arguments and rebuttals that are synthesized by an arbiter into transparent and
empirically anchored judgments. We evaluate CogniAlign on classic and novel
moral questions and compare its outputs against GPT-4o using a five-part
ethical audit framework. Results show that CogniAlign consistently outperforms
the baseline across more than sixty moral questions, with average performance
gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4
points in depth of explanation. In the Heinz dilemma, for example, CogniAlign
achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a
decisive advantage in handling moral reasoning. By reducing black-box reasoning
and avoiding deceptive alignment, CogniAlign highlights the potential of
interdisciplinary deliberation as a scalable pathway for safe and transparent
AI alignment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [131] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 本文系统比较了思考型和非思考型LLM在作为评判者时的表现，发现思考型模型在准确率、计算效率和鲁棒性方面均优于非思考型模型，即使经过多种增强策略改进后，非思考型模型仍存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作自动评判工具，确保其可靠性、效率和鲁棒性变得至关重要。本研究旨在系统比较思考型和非思考型LLM在评判任务中的表现差异。

Method: 使用开源Qwen 3模型（0.6B、1.7B和4B参数）在RewardBench任务上评估准确性和计算效率（FLOPs），并测试了非思考型模型的多种增强策略，包括上下文学习、规则引导评判、基于参考的评估和n-best聚合。

Result: 思考型模型比非思考型模型准确率高出约10个百分点，计算开销仅增加不到2倍，而少样本学习等增强策略虽然带来适度提升但成本更高（>8倍）。在多种偏见条件下，思考型模型的一致性显著更高（平均高6%）。多语言实验也证实显式推理的优势不仅限于英语。

Conclusion: 显式推理在LLM-as-a-judge范式中具有明显优势，不仅在准确性和效率方面，而且在鲁棒性方面都表现出色，为LLM作为评判者的可靠性提供了系统证据。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [132] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct指令微调框架，通过逻辑思维链推理增强大语言模型在符号规划任务中的能力，在标准基准测试中达到94%的规划准确率，相比基线模型提升66%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样化任务中表现出色，但在需要形式化表示（如PDDL）的结构化符号规划方面能力有限，需要弥合通用推理能力与自动规划所需逻辑精度之间的差距。

Method: 开发指令提示引导模型通过精确的逻辑推理步骤来确定动作适用性、状态转换和计划有效性，将规划过程分解为关于前提条件满足、效果应用和不变性保持的显式推理链。

Result: 在多个规划领域的实验结果显示，基于思维链推理的指令微调模型显著提升了规划能力，在标准基准测试中达到94%的规划准确率。

Conclusion: 该工作为大语言模型与自动规划之间的能力差距提供了有效解决方案，为开发更好的AI规划系统指明了有前景的方向。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [133] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为（如奉承和常识道德）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作主要关注真实性或推理能力，但表示引导方法的副作用和权衡关系尚未得到系统研究，特别是在安全相关行为方面的效果和纠缠效应。

Method: 构建了一个模块化的引导框架，收集了安全相关的主要和次要行为数据集，评估了五种流行引导方法在Qwen-2.5-7B和Llama-3.1-8B模型上的表现。

Result: 发现强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠。

Conclusion: 表示引导方法的效果具有高度依赖性，需要仔细选择方法、模型和目标的组合，以避免负面副作用和概念纠缠。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [134] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文提出了State-aware Reasoning (StaR)训练方法，解决多模态代理在GUI切换控制指令执行不可靠的问题，特别是在当前状态与期望状态一致时的执行瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前多模态代理在图形用户界面(GUI)控制中，特别是在执行切换(toggle)控制指令时存在可靠性问题，当当前切换状态已经与期望状态匹配时表现尤其不可靠。

Method: 提出了State-aware Reasoning (StaR)训练方法，教导代理感知当前切换状态，从指令中分析期望状态，并相应采取行动。构建了包含二进制切换指令的状态控制基准。

Result: 在三个多模态代理上的实验表明，StaR可以将切换指令执行准确率提高30%以上。在三个公共基准测试上的进一步评估显示，StaR还能提升一般任务性能。动态环境评估突显了StaR在现实应用中的潜力。

Conclusion: StaR方法有效解决了多模态代理在GUI切换控制中的可靠性问题，显著提升了执行准确率，并展现出在现实世界应用中的良好潜力。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [135] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习实现工具集成层次优化的方法，用于提升LLM在数学推理中的性能，包括数据生成、细粒度优化和推理增强


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得显著进展，但在高精度任务如数值计算和符号操作方面仍有困难，需要集成外部工具来弥补这一差距

Method: 提出THOR方法：1) TIRGen多智能体actor-critic流水线构建高质量工具集成推理数据集；2) 分层RL策略联合优化轨迹级问题解决和步骤级代码生成；3) 自校正机制利用工具反馈动态修正推理路径

Result: 方法在多个数学基准测试中达到同类规模模型的最先进性能，在代码基准测试中也获得一致改进，展现出良好的跨模型泛化能力

Conclusion: THOR通过工具集成和分层优化有效提升了LLM在数学推理任务中的性能，为解决高精度计算问题提供了有前景的解决方案

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [136] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究使用人工神经网络模型探讨了信息流结构变化如何导致认知性能的过渡性变化，发现循环网络相比前馈网络在处理复杂语法时具有质的性能提升，并观察到训练难度造成的过渡障碍。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证认知进化是否通过一系列主要过渡来实现，这些过渡通过操纵生物神经网络结构来根本改变信息流，从而产生认知性能的质的飞跃。

Method: 使用理想化的信息流模型和人工神经网络，比较前馈、循环和分层拓扑结构的网络性能，控制网络大小和资源，测试它们学习不同复杂度人工语法的能力。

Result: 发现循环网络相比前馈网络能够处理更多类型的输入，在复杂语法学习上表现出质的性能提升。循环网络的训练难度形成了过渡障碍和偶然不可逆性。分层网络在语法学习任务中并未表现出优势。

Conclusion: 研究结果表明某些信息流结构的变化确实能够产生认知性能的过渡性变化，这为认知进化通过主要过渡的理论提供了计算模型支持。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [137] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 本文对16个最先进的视觉语言模型在6个多模态数据集上进行全面的不确定性基准测试，发现大模型具有更好的不确定性量化能力，数学和推理任务的不确定性表现较差。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂视觉理解方面取得显著进展，但不确定性量化这一关键维度未得到足够关注，需要超越以往有限设置进行全面的不确定性基准研究。

Method: 评估16个开源和闭源的最先进VLMs，在6个多模态数据集上使用3种不同的评分函数进行不确定性基准测试。

Result: 发现大模型始终表现出更好的不确定性量化能力；确定性更高的模型获得更高准确率；数学和推理任务在所有模型中相比其他领域表现出更差的不确定性性能。

Conclusion: 这项工作为多模态系统中可靠的不确定性评估奠定了基础，表明知道更多的模型也更好地知道它们不知道什么。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [138] [3D Reconstruction of Coronary Vessel Trees from Biplanar X-Ray Images Using a Geometric Approach](https://arxiv.org/abs/2509.13358)
*Ethan Koland,Lin Xi,Nadeev Wijesuriya,YingLiang Ma*

Main category: eess.IV

TL;DR: 这篇论文提出了一种从双平面X光血管影像重建3D血管树的框架，包括自动分割、运动时相匹配和几何重建三个主要组件，通过追踪静止物体来减少影响，并使用新的几何算法提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: X光血管影像在心血管干预中广泛使用，但传统的3D重建方法存在流程复杂和误差较大的问题，需要一种更简化且更准确的方法来重建3D血管树以支持临床诊断和治疗。

Method: 框架包括三为主要组件：1）自动视频分割方法进行语义分割；2）通过追踪静止物体（如导管）来匹配相同呼吸和心致运动时相的影像对；3）使用新的几何重建算法，通过计算两个3D表面的交线来生成3D血管中心线，替代传统的对极约束方法。

Result: 在62个X光血管影像视频序列上训练和验证，分割准确度达到0.703；3D重建框架的重投影误差为0.62mm ± 0.38mm，显示了良好的重建精度。

Conclusion: 该框架通过简化重建流程和提高准确性，为从双平面X光影像重建3D血管树提供了一种有效的方法，在心血管干预中具有重要的应用价值。

Abstract: X-ray angiography is widely used in cardiac interventions to visualize
coronary vessels, assess integrity, detect stenoses and guide treatment. We
propose a framework for reconstructing 3D vessel trees from biplanar X-ray
images which are extracted from two X-ray videos captured at different C-arm
angles. The proposed framework consists of three main components: image
segmentation, motion phase matching, and 3D reconstruction. An automatic video
segmentation method for X-ray angiography to enable semantic segmentation for
image segmentation and motion phase matching. The goal of the motion phase
matching is to identify a pair of X-ray images that correspond to a similar
respiratory and cardiac motion phase to reduce errors in 3D reconstruction.
This is achieved by tracking a stationary object such as a catheter or lead
within the X-ray video. The semantic segmentation approach assigns different
labels to different object classes enabling accurate differentiation between
blood vessels, balloons, and catheters. Once a suitable image pair is selected,
key anatomical landmarks (vessel branching points and endpoints) are matched
between the two views using a heuristic method that minimizes reconstruction
errors. This is followed by a novel geometric reconstruction algorithm to
generate the 3D vessel tree. The algorithm computes the 3D vessel centrelines
by determining the intersection of two 3D surfaces. Compared to traditional
methods based on epipolar constraints, the proposed approach simplifies there
construction workflow and improves overall accuracy. We trained and validated
our segmentation method on 62 X-ray angiography video sequences. On the test
set, our method achieved a segmentation accuracy of 0.703. The 3D
reconstruction framework was validated by measuring the reconstruction error of
key anatomical landmarks, achieving a reprojection errors of 0.62mm +/- 0.38mm.

</details>


### [139] [PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma](https://arxiv.org/abs/2509.13360)
*L. Zimmer,J. Weidner,M. Balcerak,F. Kofler,I. Ezhov,B. Menze,B. Wiestler*

Main category: eess.IV

TL;DR: PREDICT-GBM是一个用于胶质母细胞瘤生长建模和评估的综合平台，包含255例患者的临床数据集，通过个性化放疗计划相比传统均匀边界方法能更好地覆盖肿瘤复发区域。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤具有高度侵袭性和高复发率，传统放疗采用均匀治疗边界，无法考虑患者特异性解剖和生物学因素。现有计算模型临床转化有限，需要建立系统评估平台。

Method: 开发PREDICT-GBM集成管道和数据集，包含专家策划的255例患者完整肿瘤分割和组织特征图，用于系统评估最先进的肿瘤生长模型。

Result: 分析显示，基于肿瘤生长预测的个性化放疗计划在两种模型中相比传统均匀边界方法实现了更好的复发覆盖。

Conclusion: 该工作建立了强大的平台，用于推进和系统评估前沿肿瘤生长建模方法，最终目标是促进临床转化和改善患者预后。

Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by
its highly invasive behavior and exceptionally high rates of recurrence.
Conventional radiation therapy, which employs uniform treatment margins, fails
to account for patient-specific anatomical and biological factors that
critically influence tumor cell migration. To address this limitation, numerous
computational models of glioblastoma growth have been developed, enabling
generation of tumor cell distribution maps extending beyond radiographically
visible regions and thus informing more precise treatment strategies. However,
despite encouraging preliminary findings, the clinical adoption of these growth
models remains limited. To bridge this translational gap and accelerate both
model development and clinical validation, we introduce PREDICT-GBM, a
comprehensive integrated pipeline and dataset for modeling and evaluation. This
platform enables systematic benchmarking of state-of-the-art tumor growth
models using an expert-curated clinical dataset comprising 255 subjects with
complete tumor segmentations and tissue characterization maps. Our analysis
demonstrates that personalized radiation treatment plans derived from tumor
growth predictions achieved superior recurrence coverage compared to
conventional uniform margin approaches for two of the evaluated models. This
work establishes a robust platform for advancing and systematically evaluating
cutting-edge tumor growth modeling approaches, with the ultimate goal of
facilitating clinical translation and improving patient outcomes.

</details>


### [140] [Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging](https://arxiv.org/abs/2509.13372)
*Prahlad G Menon*

Main category: eess.IV

TL;DR: 使用Google Gemini 2.5 Flash和Tencent Hunyuan3D-2mini AI流水线，从单视角血管影像生成减少血流动力学分析的3D模型，对Fontan手术后处理进行流体动力学可视化


<details>
  <summary>Details</summary>
Motivation: 传统2D影像无法充分描述Fontan手术后复杂的血流模式，而现有的血管影像技术提供的3D几何信息有限，影响计算流体动力学分析和手术规划

Method: 开发多步高AI流水线，利用Google Gemini 2.5 Flash进行医学影像预处理、血管分割、对比度增强、伪影移除等迭代处理，最终通过Tencent Hunyuan3D-2mini生成光水剖文件

Result: 成功从单视角影像生成几何优化的2D投影，准确保持Fontan复杂几何结构，AI生成的虚拟流体可视化识别了中央连接处的滞流区域和分支动脉流动模式，整体处理时间小于15分钟

Conclusion: 该方法证明了从常规血管影像数据生成CFD适用几何模型的临床可行性，能够在完整CFD模拟前进行快速的虚拟流体可视化分析，为利用常规影像数据进行高级几何和血流动力学分析奠定了基础

Abstract: Fontan palliation for univentricular congenital heart disease progresses to
hemodynamic failure with complex flow patterns poorly characterized by
conventional 2D imaging. Current assessment relies on fluoroscopic angiography,
providing limited 3D geometric information essential for computational fluid
dynamics (CFD) analysis and surgical planning.
  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash
(2.5B parameters) for systematic, iterative processing of fluoroscopic
angiograms through transformer-based neural architecture. The pipeline
encompasses medical image preprocessing, vascular segmentation, contrast
enhancement, artifact removal, and virtual hemodynamic flow visualization
within 2D projections. Final views were processed through Tencent's
Hunyuan3D-2mini (384M parameters) for stereolithography file generation.
  The pipeline successfully generated geometrically optimized 2D projections
from single-view angiograms after 16 processing steps using a custom web
interface. Initial iterations contained hallucinated vascular features
requiring iterative refinement to achieve anatomically faithful
representations. Final projections demonstrated accurate preservation of
complex Fontan geometry with enhanced contrast suitable for 3D conversion.
AI-generated virtual flow visualization identified stagnation zones in central
connections and flow patterns in branch arteries. Complete processing required
under 15 minutes with second-level API response times.
  This approach demonstrates clinical feasibility of generating CFD-suitable
geometries from routine angiographic data, enabling 3D generation and rapid
virtual flow visualization for cursory insights prior to full CFD simulation.
While requiring refinement cycles for accuracy, this establishes foundation for
democratizing advanced geometric and hemodynamic analysis using readily
available imaging data.

</details>


### [141] [Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT](https://arxiv.org/abs/2509.13576)
*Haodong Li,Shuo Han,Haiyang Mao,Yu Shi,Changsheng Fang,Jianjia Zhang,Weiwen Wu,Hengyong Yu*

Main category: eess.IV

TL;DR: 提出CDPIR框架解决稀疏视图CT重建中的分布外问题，通过跨分布扩散先验和可扩展插值变换器，在多个数据集上实现最先进的性能


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建虽然提高时间分辨率和降低辐射剂量，但由于视图减少和扫描仪、协议或解剖变异导致的域偏移，在分布外场景中会产生伪影和性能下降

Method: 提出CDPIR框架，集成跨分布扩散先验（基于可扩展插值变换器SiT）和基于模型的迭代重建方法。使用无分类器引导在多个数据集上训练，学习域特定和域不变先验

Result: 在稀疏视图CT重建中实现最先进的性能，特别是在分布外条件下显著优于现有方法，具有优越的细节保留能力

Conclusion: CDPIR框架通过跨分布扩散先验和统一随机插值框架，有效解决了稀疏视图CT重建中的分布外问题，展示了在挑战性成像场景中的鲁棒性和临床潜力

Abstract: Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces
radiation dose, yet its clinical use is hindered by artifacts due to view
reduction and domain shifts from scanner, protocol, or anatomical variations,
leading to performance degradation in out-of-distribution (OOD) scenarios. In
this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative
Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR
integrates cross-distribution diffusion priors, derived from a Scalable
Interpolant Transformer (SiT), with model-based iterative reconstruction
methods. Specifically, we train a SiT backbone, an extension of the Diffusion
Transformer (DiT) architecture, to establish a unified stochastic interpolant
framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets.
By randomly dropping the conditioning with a null embedding during training,
the model learns both domain-specific and domain-invariant priors, enhancing
generalizability. During sampling, the globally sensitive transformer-based
diffusion model exploits the cross-distribution prior within the unified
stochastic interpolant framework, enabling flexible and stable control over
multi-distribution-to-noise interpolation paths and decoupled sampling
strategies, thereby improving adaptation to OOD reconstruction. By alternating
between data fidelity and sampling updates, our model achieves state-of-the-art
performance with superior detail preservation in SVCT reconstructions.
Extensive experiments demonstrate that CDPIR significantly outperforms existing
approaches, particularly under OOD conditions, highlighting its robustness and
potential clinical value in challenging imaging scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [142] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP是一种软硬件协同设计方法，通过编译器分析代码温度（冷热）并指导硬件优化指令缓存替换策略，在移动CPU上减少热代码的驱逐率，实现性能提升


<details>
  <summary>Details</summary>
Motivation: 移动CPU软件具有复杂的运行时行为，导致指令重用距离大，传统硬件中心的指令缓存替换策略不足。移动代码常出现大量前端停顿，且代码复杂度和占用空间增长速度快于片上内存容量

Method: 提出TRRIP软硬件协同设计：编译器分析代码温度（热/冷）并进行代码转换，通过操作系统接口提供代码温度信息；硬件扩展利用温度属性优化指令缓存替换策略

Result: 在已使用PGO优化的移动代码基础上，TRRIP可将L2指令MPKI降低26.5%，实现3.9%的几何平均加速比

Conclusion: TRRIP是一种实用且可在真实移动系统中采用的方法，通过软硬件协同优化有效解决了移动CPU指令缓存管理问题

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [143] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRUT模型通过时间感知提示和趋势感知推理，有效捕捉用户偏好演化，在推荐任务中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法主要关注物品序列顺序，但忽略了物品间的时间动态性，而时间动态性能够反映用户偏好的演化

Method: 提出GRUT模型，包含时间感知提示（用户级时间上下文和物品级转移上下文）和训练免费的趋势感知推理方法

Result: 在四个基准数据集上，Recall@5和NDCG@5指标分别提升15.4%和14.3%

Conclusion: GRUT通过有效利用时间信号捕捉用户偏好演化，显著提升了生成式推荐性能

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [144] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: GEM-Bench是首个针对生成引擎营销(GEM)中广告注入响应生成的综合基准测试，包含三个数据集、多维度评估指标和基线方法，发现基于提示的方法能提高参与度但降低用户满意度


<details>
  <summary>Details</summary>
Motivation: 现有基准测试并非专门为生成引擎营销(GEM)中的广告注入响应生成和评估而设计，这限制了该领域的研究发展

Method: 提出GEM-Bench基准测试，包含三个精心策划的数据集(覆盖聊天机器人和搜索场景)、捕捉用户满意度和参与度的多维度指标本体，以及在可扩展多智能体框架中实现的多个基线解决方案

Result: 初步结果显示，简单的基于提示的方法能实现合理的参与度(如点击率)，但通常会降低用户满意度；而基于预先生成无广告响应的广告插入方法有助于缓解此问题，但会引入额外开销

Conclusion: 研究结果强调了未来需要设计更有效和高效的解决方案来生成GEM中的广告注入响应

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [145] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)
*Julia S. Dollis,Iago A. Brito,Fernanda B. Färber,Pedro S. F. B. Ribeiro,Rafael T. Sousa,Arlindo R. Galvão Filho*

Main category: cs.HC

TL;DR: 本文提出了一个将大型语言模型集成到VR中的框架，用于创建具有医学一致性和个性特征的虚拟患者，通过医师评估验证了该方法的可行性和训练效果。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实在模拟物理环境方面表现出色，但在训练复杂人际技能方面效果有限，主要原因是缺乏心理上合理的虚拟人类。这在医疗教育等高风险领域尤其关键，因为沟通是核心能力。

Method: 开发了一个模块化架构框架，将大型语言模型集成到沉浸式VR中，创建具有医学一致性和独特、一致个性的虚拟患者，将个性特征与临床数据解耦。采用混合方法的受试者内研究设计，让执业医师参与模拟咨询。

Result: 该方法不仅可行，而且被医师认为是一种高度有益和有效的训练增强手段。分析发现了关键设计原则，包括"真实感-冗长悖论"（沟通较少的智能体可能显得更人工）以及挑战需要被感知为真实才能具有指导性。

Conclusion: 这项工作为开发下一代社会智能VR训练环境提供了一个经过验证的框架和关键见解，解决了医疗教育中虚拟患者训练的重要挑战。

Abstract: While virtual reality (VR) excels at simulating physical environments, its
effectiveness for training complex interpersonal skills is limited by a lack of
psychologically plausible virtual humans. This is a critical gap in high-stakes
domains like medical education, where communication is a core competency. This
paper introduces a framework that integrates large language models (LLMs) into
immersive VR to create medically coherent virtual patients with distinct,
consistent personalities, built on a modular architecture that decouples
personality from clinical data. We evaluated our system in a mixed-method,
within-subjects study with licensed physicians who engaged in simulated
consultations. Results demonstrate that the approach is not only feasible but
is also perceived by physicians as a highly rewarding and effective training
enhancement. Furthermore, our analysis uncovers critical design principles,
including a ``realism-verbosity paradox" where less communicative agents can
seem more artificial, and the need for challenges to be perceived as authentic
to be instructive. This work provides a validated framework and key insights
for developing the next generation of socially intelligent VR training
environments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [146] [Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans](https://arxiv.org/abs/2509.13612)
*Chuyang Zhou,Ziao Ji,Daochang Liu,Dongang Wang,Chenyu Wang,Chang Xu*

Main category: q-bio.NC

TL;DR: Rest2Visual是一个条件生成模型，能够从静息态fMRI和2D视觉刺激预测视觉诱发fMRI激活，实现了静息态脑活动到刺激对齐表征的转换。


<details>
  <summary>Details</summary>
Motivation: 解决任务态fMRI采集成本高、难以规模化的问题，同时提升静息态fMRI的直接可解释性，探索自发脑活动与刺激驱动神经响应之间的关系。

Method: 采用体积编码器-解码器设计，通过多尺度3D特征提取和图像嵌入的适应性归一化调制，构建大规模三元组数据集进行模型训练。

Result: 预测的激活图在相似性和表征指标上与真实值高度匹配，支持下游图像重建，并保留了个体特异性功能结构。

Conclusion: 证明了个体化自发神经活动可以转化为刺激对齐表征，为可扩展、无任务的功能性脑建模开辟了新途径。

Abstract: Understanding how spontaneous brain activity relates to stimulus-driven
neural responses is a fundamental challenge in cognitive neuroscience. While
task-based functional magnetic resonance imaging (fMRI) captures localized
stimulus-evoked brain activation, its acquisition is costly, time-consuming,
and difficult to scale across populations. In contrast, resting-state fMRI
(rs-fMRI) is task-free and abundant, but lacks direct interpretability. We
introduce Rest2Visual, a conditional generative model that predicts visually
evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It
follows a volumetric encoder--decoder design, where multiscale 3D features from
rs-fMRI are modulated by image embeddings via adaptive normalization, enabling
spatially accurate, stimulus-specific activation synthesis. To enable model
training, we construct a large-scale triplet dataset from the Natural Scenes
Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their
corresponding ve-fMRI activation maps. Quantitative evaluation shows that the
predicted activations closely match ground truth across standard similarity and
representational metrics, and support successful image reconstruction in
downstream decoding. Notably, the predicted maps preserve subject-specific
structure, demonstrating the model's capacity to generate individualized
functional surrogates. Our results provide compelling evidence that
individualized spontaneous neural activity can be transformed into
stimulus-aligned representations, opening new avenues for scalable, task-free
functional brain modeling.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [147] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新的相机内镜视频相关三维重建管道，结合语义分割技术实时标注阻塞组织，为自动化机器人治疗提供支持。


<details>
  <summary>Details</summary>
Motivation: 中央气道阻塞(CAO)是一种危及生命的病患，传统治疗方法风险高，需要更安全有效的治疗手段。机器人干预与场景理解结合为自动化治疗提供了可能性。

Method: 结合DROID-SLAM算法和训练好的分割模型，通过单目内镜视频进行实时三维重建，分割掩码指导在重建点云中标注阻塞区域。

Result: 在ex vivo模型上验证，重建结果与真实CT扫描呈现高度相似性(0.62mm Chamfer距离)，实时生成标注了临床相关区域的3D地图。

Conclusion: 这是首个将语义分割与实时单目SLAM结合用于内镜CAO场景的研究，模块化设计可扩展到其他解剖结构和手术，为自主机器人干预提供了有前景的基础。

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [148] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: 使用强化学习控制机器人双手进行触觉探索，通过收集的3D点云数据迭代优化物体形状和姿态估计，无需先验几何知识


<details>
  <summary>Details</summary>
Motivation: 在视觉数据有限或受光照、遮挡、外观影响的情况下，触觉传感器提供的局部接触信息难以准确重建物体姿态，需要主动探索方法

Method: 采用强化学习训练机器人主动控制手部与物体交互，一只手固定物体，另一只手进行主动触觉探索，收集3D点云数据迭代优化形状和姿态

Result: 该方法能够主动探索物体表面以识别关键姿态特征，无需物体几何形状的先验知识

Conclusion: 提出的双手机器人触觉姿态估计方法通过主动传感器运动探索有效解决了局部触觉数据下的物体姿态估计问题

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [149] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey是一个利用道路交叉口作为地标的跨模态全局定位框架，通过联合编码点云和OpenStreetMap的道路建筑特征生成二进制描述符，在GNSS失效环境下实现高精度车辆定位。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在GNSS信号退化环境（如城市峡谷、隧道）中的可靠全局定位问题，利用免费且全球可用的OpenStreetMap替代昂贵的高清地图，克服OSM粗糙抽象带来的传感器数据匹配挑战。

Method: 提出跨模态框架，以道路交叉口为特征地标，通过联合编码点云和OSM的道路建筑印记构建紧凑二进制描述符，采用差异缓解、方向确定和区域均衡采样策略来弥合模态差距。

Result: 在KITTI数据集上的实验表明，InterKey实现了最先进的精度，大幅优于现有基线方法，能够推广到任何能产生密集结构点云的传感器。

Conclusion: 该框架为车辆鲁棒定位提供了可扩展且成本效益高的解决方案，特别适用于GNSS不可靠的环境。

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [150] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: MAP是一个新颖的端到端轨迹规划框架，通过显式整合语义地图特征和当前ego状态，显著提升了自动驾驶轨迹规划性能，在多个指标上大幅超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法未能充分利用在线地图模块的潜力，地图特征对轨迹规划的增强作用尚未得到充分挖掘。

Method: 提出MAP框架，包含三个核心模块：Plan-enhancing Online Mapping模块、Ego-status-guided Planning模块和基于当前ego状态的Weight Adapter，显式整合分割式地图特征和当前ego状态。

Result: 在DAIR-V2X-seq-SPD数据集上，L2位移误差降低16.6%，脱轨率降低56.2%，综合评分提升44.5%；在CVPR2025 MEIS Workshop挑战赛中排名第一，综合评分比第二名高39.5%。

Conclusion: 显式利用语义地图特征能有效提升规划性能，为端到端自动驾驶系统的结构设计提供了新的改进方向。

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [151] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: 该论文提出了MetricNet和MetricNav方法，通过预测路径点之间的度量距离来解决生成式导航策略中的尺度问题和短视行为，显著提升了导航和探索性能。


<details>
  <summary>Details</summary>
Motivation: 生成式导航策略存在两个结构性问题：1）采样轨迹存在于抽象的无尺度空间中，缺乏度量基础；2）控制策略丢弃完整路径，只朝向单个路径点移动，导致短视和不安全的动作。

Method: 提出MetricNet作为生成式导航的附加模块，预测路径点之间的度量距离，将策略输出锚定到真实世界坐标中。进一步提出MetricNav，将MetricNet集成到导航策略中，引导机器人避开障碍物同时朝向目标移动。

Result: 在仿真环境中使用新的基准框架评估，执行MetricNet缩放的路径点显著提高了导航和探索性能。在真实世界实验中也验证了方法的有效性。

Conclusion: MetricNet能够有效解决生成式导航的尺度问题和短视行为，MetricNav进一步整合了这一能力，实现了更安全和有效的导航性能。

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [152] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM是首个基于纯RGB输入的多相机3D高斯溅射SLAM系统，通过多视角融合和尺度一致性模块实现实时高精度建图与轨迹估计


<details>
  <summary>Details</summary>
Motivation: 现有的密集SLAM方法主要针对单目设置，在鲁棒性和几何覆盖方面存在不足，需要开发能够利用多相机输入提升性能的SLAM系统

Method: 使用多相机束调整(MCBA)通过密集光度学和几何残差联合优化位姿和深度，并采用尺度一致性模块利用低秩先验确保多视图间的度量对齐

Result: 在合成和真实数据集上的实验表明，MCGS-SLAM能够产生准确的轨迹和逼真的重建效果，通常优于单目基线方法，多相机广视角能够重建单目系统遗漏的侧视区域

Conclusion: 多相机高斯溅射SLAM在机器人和自动驾驶领域的高保真建图中展现出巨大潜力，特别是对于需要安全自主操作的应用场景

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [153] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私的私有预测框架，用于生成高质量合成文本，在保证强隐私保护的同时维持高实用性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在隐私泄露风险，攻击者可能从提示中提取敏感信息，需要开发既能保护隐私又能保持文本质量的解决方案

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token的输出分布，结合私有和公共推理的混合操作来增强实用性

Result: 在上下文学习任务上优于现有最先进方法，能够生成长且连贯的合成文本

Conclusion: 该方法为隐私保护文本生成提供了有前景的方向，在保持高实用性的同时提供强隐私保证

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [154] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 语言模型的激活值线性编码了训练期间信息学习的时间顺序，通过顺序微调实验发现模型能够区分不同时间学习的信息


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否能够编码信息在训练过程中被学习的时间顺序，以及这种时间信息如何影响模型对冲突数据的处理和知识修改

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的命名实体数据集上，分析激活值的线性编码模式，使用线性探测和微调方法来验证时间信息的编码

Result: 发现测试样本的平均激活值能够准确编码训练顺序（在2D子空间中呈直线排列），线性探测能90%准确区分"早期"和"晚期"实体，微调后模型能80%准确报告未见实体的训练阶段

Conclusion: 语言模型确实能够编码信息获取的时间顺序，这一发现对理解模型如何处理冲突数据和响应知识修改具有重要意义

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [155] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved (LLM-I)是一个灵活的框架，将交错图像-文本生成重新定义为工具使用问题，通过强化学习训练LLM智能协调多种视觉工具，在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前统一模型只能使用单一工具生成合成图像，在需要事实基础或编程精度的任务上表现不佳的瓶颈问题。

Method: 设计一个中央LLM/MLLM代理，通过强化学习框架协调多种专业视觉工具（在线图像搜索、扩散生成、代码执行、图像编辑），使用混合奖励系统结合规则逻辑和LLM评估器判断。

Result: 在四个基准测试中表现出最先进的性能，大幅超越现有方法，并引入了新的测试时扩展策略获得进一步性能提升。

Conclusion: LLM-I框架成功突破了单一工具限制，通过智能工具协调实现了更灵活、准确和多样化的图像-文本生成能力。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [156] [Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence in the United Kingdom. Can We Take the Human Out of the Loop?](https://arxiv.org/abs/2509.13428)
*Katrina Nash,James Vaz,Ahmed Maiter,Christopher Johns,Nicholas Woznitza,Aditya Kale,Abdala Espinosa Morgado,Rhidian Bramley,Mark Hall,David Lowe,Alex Novak,Sarim Ather*

Main category: q-bio.PE

TL;DR: 本文探讨AI自主报告正常胸部X光片的可行性，分析技术、法律和监管挑战，强调需要谨慎采用。


<details>
  <summary>Details</summary>
Motivation: 解决放射科医生短缺导致的报告延迟问题，通过AI识别正常X光片来减少放射科工作量。

Method: 分析AI工具区分正常与异常CXRs的能力，探讨定义正常、泛化性、敏感度-特异性权衡等技术问题，以及法律合规性和监管框架。

Result: AI自主报告正常CXRs具有明显益处，但面临技术可靠性、法律问责和监管合规等多重挑战。

Conclusion: 虽然AI自主报告正常胸部X光片前景广阔，但需要谨慎采用，建立完善的监管框架和问责机制，并考虑对放射科实践的影响。

Abstract: Chest X-rays (CXRs) are the most commonly performed imaging investigation. In
the UK, many centres experience reporting delays due to radiologist workforce
shortages. Artificial intelligence (AI) tools capable of distinguishing normal
from abnormal CXRs have emerged as a potential solution. If normal CXRs could
be safely identified and reported without human input, a substantial portion of
radiology workload could be reduced.
  This article examines the feasibility and implications of autonomous AI
reporting of normal CXRs. Key issues include defining normal, ensuring
generalisability across populations, and managing the sensitivity-specificity
trade-off. It also addresses legal and regulatory challenges, such as
compliance with IR(ME)R and GDPR, and the lack accountability frameworks for
errors. Further considerations include the impact on radiologists practice, the
need for robust post-market surveillance, and incorporation of patient
perspectives. While the benefits are clear, adoption must be cautious.

</details>
