<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.CV](#cs.CV) [Total: 181]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.MA](#cs.MA) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 24]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究量子自然语言处理在自然语言推理任务中的应用，比较量子、混合和经典模型在少样本设置下的表现，提出信息增益参数比指标评估学习效率，并设计基于词簇的参数共享架构。


<details>
  <summary>Details</summary>
Motivation: 探索量子自然语言处理在语义建模中的潜力，特别是在低资源环境下利用量子电路的结构敏感性优势，解决传统模型参数效率低的问题。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路，训练句子对的语义相关性和推理分类；提出信息增益参数比指标；设计基于词簇的参数共享架构。

Result: 量子模型在参数数量大幅减少的情况下达到与经典基线相当的性能，在推理任务上优于随机初始化的transformer，在相关任务上测试误差更低，学习效率比经典模型高5个数量级。

Conclusion: 量子自然语言处理在低资源、结构敏感场景中具有显著优势，量子模型展现出更高的参数效率和学习能力，为未来NLP发展提供了新方向。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的多模型融合框架，结合ChatGPT和Claude两种先进大语言模型，在CheXpert数据集上提升胸部X光片解读的可靠性。通过相似性共识方法，在单模态和多模态输入条件下均显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多模型融合和输出共识方法，提高AI辅助放射学诊断的可靠性和临床实用性，减少诊断错误。

Method: 使用ChatGPT和Claude两种LLM模型，在CheXpert数据集上评估单模态（仅图像）和多模态（图像+合成临床笔记）性能。采用基于95%输出相似性阈值的共识方法进行模型融合。

Result: 单模态条件下：ChatGPT准确率62.8%，Claude 76.9%，共识方法77.6%。多模态条件下：ChatGPT 84%，Claude 76%，共识方法达到91.3%。共识融合始终优于单个模型。

Conclusion: 多模态输入和输出级共识融合能显著提升AI辅助放射学诊断的准确性和可信度，为减少诊断错误提供了实用路径，且计算开销最小。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 本文介绍了CorrectBench基准测试，用于评估大语言模型的自校正策略效果，发现在复杂推理任务中自校正能提升准确性，但会降低效率，而简单的思维链基线表现具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种自校正方法被提出，但缺乏对这些方法的全面评估，且LLMs是否真正能够自我校正仍是一个重要问题。

Method: 开发了CorrectBench基准测试，评估内在、外部和微调三种自校正策略在常识推理、数学推理和代码生成三个任务上的效果。

Result: 1) 自校正方法能提高准确性，特别是复杂推理任务；2) 混合不同策略能进一步改进但降低效率；3) 推理LLMs在额外自校正方法下优化有限且时间成本高；4) 简单思维链基线表现出竞争性准确性和效率。

Conclusion: 自校正有潜力提升LLM推理性能，但需平衡推理能力和操作效率，建议进一步研究优化这一平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR是一个让LLM智能体通过闭环经验生命周期实现自我改进的框架，包含离线自我蒸馏和在线交互两个阶段，在复杂多跳问答任务中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在工具使用方面表现良好，但缺乏从自身经验中系统学习的能力，无法迭代优化问题解决策略。

Method: 采用闭环经验生命周期：1）离线自我蒸馏：将交互轨迹合成结构化、可重用的战略原则库；2）在线交互：与任务交互并检索蒸馏原则指导决策，积累行为轨迹；使用策略强化机制迭代更新智能体。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于现有强智能体基线的性能表现。

Conclusion: 该工作为智能体不仅从外部数据学习，还能从自身行为后果中学习提供了全面蓝图，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大语言模型在系统文献综述筛选阶段的交互作用，评估了6个LLM在5种提示类型下的表现，发现CoT-few-shot提供最可靠的精确度-召回率平衡，并推荐了分阶段工作流程。


<details>
  <summary>Details</summary>
Motivation: 系统分析提示策略与LLM在文献筛选自动化中的交互作用，为任务适应性部署提供实用指导。

Method: 评估6个LLM（GPT-4o、GPT-4o-mini、DeepSeek-Chat-V3、Gemini-2.5-Flash、Claude-3.5-Haiku、Llama-4-Maverick）在5种提示类型（零样本、少样本、思维链、思维链-少样本、自我反思）下的表现，使用准确率、精确度、召回率和F1分数指标。

Result: CoT-few-shot提供最可靠的精确度-召回率平衡；零样本在高灵敏度筛选时召回率最高；自我反思表现不佳；GPT-4o和DeepSeek整体表现稳健；GPT-4o-mini在显著较低成本下表现具竞争力。

Conclusion: 推荐分阶段工作流程：先用低成本模型配合结构化提示进行初筛，仅将边界案例升级到高容量模型，突显LLM在文献筛选自动化中的不均衡但有前景的潜力。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一个合成测试平台，用于系统分析语言模型中统计规律与事实关联的交互对泛化能力的影响。研究发现上下文多样性和结构对事实回忆的泛化能力有复杂影响，并识别了不同的优化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对语言模型中统计规律与事实关联交互影响的系统分析，特别是这种交互如何影响泛化能力。需要可控的实验环境来研究这些因素。

Method: 设计了一个灵活的合成测试平台，结合通用标记的统计流和源-目标标记对的事实流，可以独立控制上下文结构和多样性水平。通过受控实验分析不同条件下的表现。

Result: 研究发现：1) 高上下文多样性延迟ID事实准确性；2) OOD事实泛化受上下文结构影响；3) 在某些情况下多样性对事实回忆至关重要；4) 识别了统计泛化独立失败的情况；5) 发现嵌入层和解嵌入层是OOD失败的主要瓶颈。

Conclusion: 上下文设计和多样性水平的交互对不同泛化方面有显著影响。合成框架为未来研究提供了可控测试平台，能够分离在大规模研究中可能混淆的效应。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文首次对生成式AI的信任与不信任进行大规模计算研究，使用2022-2025年Reddit数据，发现信任与不信任基本平衡，技术性能和可用性是主要维度，个人经验是态度形成的最常见原因。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI融入日常生活，理解公众对其的信任对负责任采用和治理至关重要。现有研究缺乏计算性、大规模和纵向的信任测量方法。

Method: 使用多年度Reddit数据集（39个子版块，197,618个帖子），结合众包标注和分类模型进行扩展分析。

Result: 信任与不信任随时间基本平衡，主要模型发布时出现波动。技术性能和可用性是最主要维度，个人经验是态度形成的最常见原因，不同用户群体呈现不同模式。

Conclusion: 研究提供了大规模信任分析的方法框架，并揭示了公众对生成式AI认知的演变趋势。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了EgMM-Corpus，一个专门针对埃及文化的多模态数据集，包含3000多张图像，涵盖313个概念。该数据集旨在解决中东和非洲地区文化多样性数据不足的问题，并为评估和训练具有文化意识的视觉语言模型提供可靠资源。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏中东和非洲地区的多模态文化多样性数据集，特别是埃及文化背景的数据。这限制了视觉语言模型在这些文化背景下的表现和评估。

Method: 设计并运行新的数据收集流程，收集了涵盖地标、食物和民间传说等313个概念的3000多张图像。每个条目都经过人工验证，确保文化真实性和多模态一致性。

Result: 在EgMM-Corpus上评估CLIP模型的零样本性能，结果显示Top-1准确率为21.2%，Top-5准确率为36.4%。这些结果揭示了大规模视觉语言模型中存在的文化偏见。

Conclusion: EgMM-Corpus作为开发文化感知模型的重要基准数据集，突显了解决视觉语言模型中文化偏见的重要性，并为未来研究提供了有价值的资源。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 该论文通过理论分析和实证研究探讨了语言模型是否真正掌握了语法知识，提出了三个关于语法、意义和字符串概率关系的预测，并在英中双语数据上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否真正学习到了语法知识，以及如何通过字符串概率来揭示模型的语法结构知识，这对语言学理论有重要意义。

Method: 基于语料库数据生成过程的简单假设，建立语法、意义和字符串概率关系的理论框架，并在280K英中句子对上进行实证验证。

Result: 验证了三个预测：(1)最小对字符串间的概率相关性；(2)模型与人类在最小对中差异的相关性；(3)语法和不合语法字符串在概率空间中分离度差。

Conclusion: 为使用概率来了解语言模型的结构知识提供了理论基础，并为未来语言模型语法评估工作指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 提出两种方法增强语言模型的多元对齐能力：多元解码和模型引导，在低资源设置下仅用50个标注样本就能提升模型性能，减少高风险任务中的误报。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练范式假设每个查询只有一个最优答案，导致响应泛化且对齐效果差。需要确保模型能反映人类价值观的多样性和细微差别。

Method: 使用多元解码和模型引导两种方法，在低资源设置下仅需50个标注样本进行训练。

Result: 模型引导方法在零样本和少样本基线上表现一致提升，在仇恨言论检测和错误信息检测等高风险任务中减少误报，在GlobalOpinionQA中提升与人类价值观的分布对齐。

Conclusion: 强调了多样性的重要性，展示了语言模型如何适应考虑细微视角，为增强模型多元对齐提供了有效方法。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出了Profile-to-PEFT框架，使用超网络将用户配置文件直接映射到适配器参数，无需为每个用户单独训练，实现实时个性化LLM。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法需要为每个用户训练单独的适配器，计算成本高且无法实时更新，限制了大规模应用。

Method: 使用端到端训练的超网络，将编码后的用户配置文件直接映射到完整的适配器参数集（如LoRA），部署时无需每个用户的训练。

Result: 实验表明该方法在部署时使用更少计算资源的情况下，优于基于提示的个性化和OPPU方法，对分布外用户有强泛化能力。

Conclusion: Profile-to-PEFT框架实现了高效、可扩展和自适应的LLM个性化，适合大规模应用。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 本文探讨了后训练技术增强的LLM是否意识到自己的学习和推理过程，定义了三个核心能力：对学习策略的认知、策略跨领域泛化能力、推理过程与输出的对齐性。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术让LLM能够通过生成规划令牌来处理复杂逻辑任务，需要探究这些模型是否真正意识到自己的学习和推理过程。

Method: 在多个需要学习不同策略的任务上实证评估三种能力，对比SFT、DPO和GRPO三种后训练方法的模型表现。

Result: RL训练模型比SFT模型表现出更强的学习行为认知和泛化能力，但推理过程与最终输出的对齐性较弱，GRPO训练模型这一现象最明显。

Conclusion: 后训练方法显著影响LLM的元认知能力，RL方法在策略认知和泛化方面表现更好，但在推理-输出对齐方面存在挑战。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本研究探索使用LLMs生成针对疫苗错误信息的实时反驳论点，通过多种提示策略和微调方法优化反驳生成，并训练分类器对反疫苗推文进行多标签分类以实现上下文感知的反驳。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体影响公共卫生的时代，打击疫苗怀疑论和错误信息成为关键社会目标。虽然错误信息检测取得进展，但生成针对此类声明的实时定制反驳论点仍是一个未充分探索的领域。

Method: 基于先前的错误信息辟谣研究，实验多种提示策略和微调方法优化反驳论点生成，训练分类器将反疫苗推文分类为疫苗效力、副作用和政治影响等多标签类别，实现上下文感知的反驳。

Result: 通过人工判断、LLM评估和自动指标的评估显示这些方法之间具有强一致性。整合标签描述和结构化微调能增强反驳论点的有效性。

Conclusion: 研究表明，整合标签描述和结构化微调能有效增强反驳论点的有效性，为大规模缓解疫苗错误信息提供了有前景的方法。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 该研究提出了自回归论证结构预测（AASP）框架，用于联合建模论证挖掘中的组件和关系，在三个标准基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的论证挖掘方法通常通过生成式范式扁平化处理论证结构，难以有效建模论证组件与关系之间的复杂依赖关系。

Method: 基于自回归结构预测框架，将论证结构建模为预定义的约束动作集，使用条件预训练语言模型逐步构建论证结构。

Result: 在三个标准AM基准测试中，AASP在两个基准上实现了所有任务的最先进结果，在一个基准上表现强劲。

Conclusion: AASP框架能够有效捕捉论证推理流程，为论证挖掘任务提供了一种高效的端到端解决方案。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级方法，通过线性变换特定层的激活向量来提升LLM在心理健康评估任务中的表现，无需计算密集型技术。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs快速发展，但小规模模型在特定领域应用中仍表现不佳，特别是在敏感的心理健康领域需要更有效的适应方法。

Method: 采用轻量级的线性变换方法，对特定层的激活向量应用转向向量来引导模型输出。

Result: 该方法在两个任务上取得改进：识别Reddit帖子是否有助于检测抑郁症状的相关性预测任务，以及基于用户Reddit历史完成标准化抑郁筛查问卷的任务。

Conclusion: 转向机制作为计算效率高的工具，在LLMs心理健康领域适应方面具有未开发的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景的基准测试，用于评估AI的道德推理过程，包括识别道德考量、权衡利弊和提供可行建议。研究显示现有模型在数学、代码等任务上的表现无法预测其道德推理能力，且模型偏向特定道德框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与决策，需要理解AI如何做出决策而不仅仅是决策结果。道德困境是评估AI推理过程的理想测试平台，因为允许多种合理结论。

Method: 构建MoReBench基准测试，包含1000个道德场景和23000多个专家制定的评分标准，涵盖道德考量识别、权衡分析和行动建议。另外创建MoReBench-Theory测试AI在五种规范伦理学框架下的推理能力。

Result: 研究发现扩展定律和现有数学、代码等基准测试无法预测模型的道德推理能力。模型显示出对特定道德框架（如边沁功利主义和康德义务论）的偏好，这可能是流行训练范式的副作用。

Conclusion: MoReBench基准测试推动了以过程为重点的推理评估，有助于开发更安全、更透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出ATA方法，将LLM任务分解为离线知识提取和在线任务处理两个阶段，通过神经符号方法提升可信度，在保持竞争力的同时实现完美确定性、增强稳定性并免疫提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在关键领域部署时存在的幻觉、不稳定性和缺乏透明度等可信度问题，构建透明、可审计、可靠的自主代理。

Method: 神经符号方法：1) 离线知识提取 - LLM将非正式问题规范转换为形式化知识库，经专家验证；2) 在线任务处理 - 输入编码为形式语言，符号决策引擎结合知识库生成可靠结果。

Result: 在复杂推理任务中，ATA与最先进的端到端推理模型竞争力相当，在人工验证知识库下显著优于更大模型，具有完美确定性、增强稳定性和免疫提示注入攻击。

Conclusion: ATA通过符号推理生成决策，为构建下一代透明、可审计、可靠的自主代理提供了实用可控的架构。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本研究探索了Whisper语音识别模型在第二语言口语评估中的潜力，通过提取隐藏表示中的声学和语言特征，仅需训练轻量级分类器即可超越现有先进基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要外部分析Whisper生成的转录文本，本研究旨在挖掘其潜在能力，探索其在第二语言口语评估中的应用价值。

Method: 从Whisper的中间和最终输出中提取声学和语言特征，仅训练轻量级分类器，并融入图像和文本提示作为辅助相关性线索。

Result: 在GEPT图片描述数据集上表现优异，超越包括多模态方法在内的现有先进基线，通过分析嵌入发现模型内在编码了熟练度模式和语义信息。

Conclusion: 即使没有任务特定的微调，Whisper也能作为口语评估和其他口语理解任务的强大基础模型，展示了其作为基础模型的潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt是一个LLM提示压缩框架，通过保留最具语义重要性的token来减少输入长度，在保持性能的同时降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: LLM的长输入上下文导致高昂的货币成本、碳足迹和推理延迟，而大多数token是冗余的，只有少数token承载主要语义权重。

Method: 使用GlobEnc和DecompX两种token归因方法为输入序列中的每个token分配显著度分数，按原始顺序保留前k%的token，获得稀疏的压缩提示。

Result: 在情感分析、常识问答和摘要任务中，20%的提示压缩仅导致轻微性能损失；但在数学推理任务中性能急剧下降，表明对完整token连续性的依赖更强。

Conclusion: 该研究有助于更细致地理解LLM在性能-效率权衡中的行为，并界定了容忍上下文稀疏性的任务与需要详尽上下文的任务之间的边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器评估推理轨迹质量，在保持较低推理成本的同时实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有外部TTS方法存在计算开销大和未充分利用LLM内在潜表示的问题，需要更高效的轨迹选择方法。

Method: 使用轻量级验证器（仅0.6B参数）评估步骤级轨迹质量，聚合得分选择最优推理轨迹，采用端到端数据驱动训练方法。

Result: 在五个基准测试中，TrajSelector在Best-of-32设置下比多数投票准确率高4.61%，比现有过程奖励模型高4.31%到12.21%。

Conclusion: TrajSelector框架能够有效利用LLM的隐藏状态，在降低推理成本的同时显著提升推理任务的性能。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个集成课程强化学习和多模态大语言模型的广告视频违规检测框架，通过渐进式训练策略和GRPO优化实现精确时间定位和违规分类，在工业数据集和在线服务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确时间定位、噪声标注和泛化能力方面存在不足，需要更智能的检测框架来解决这些问题。

Method: 结合课程强化学习和多模态大语言模型，采用渐进式训练策略整合精确和粗略标注数据，使用GRPO优化开发推理能力，设计多层次复杂奖励机制确保时间定位和分类一致性。

Result: 在工业数据集和公共基准测试中，RAVEN在违规类别准确性和时间间隔定位方面表现优异，在线A/B测试验证了其实际应用价值，显著提升了精确率和召回率。

Conclusion: RAVEN框架有效解决了广告视频违规检测的关键挑战，展示了强大的泛化能力，缓解了监督微调带来的灾难性遗忘问题，具有实际部署价值。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文扩展了NLI标注变异的分析范围，不仅关注标注者推理类型的差异，还考察了标注步骤的分歧。研究发现表面标签不一致可能掩盖深层的解释一致性，且推理类型一致性比标签一致性更能反映语义相似性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注标注者标签一致但解释不同的情况，本文希望更全面地分析NLI标注变异，包括标注者可能在推理类型和标注步骤上都存在分歧的情况。

Method: 将LiTEx分类法应用于两个英文NLI数据集，从NLI标签一致性、解释相似性和分类法一致性三个维度对齐标注变异，并考虑标注者选择偏见的复合因素。

Result: 发现标注者标签不一致但解释高度相似的案例，表明表面分歧可能掩盖深层理解一致性。分析还揭示了标注者在解释策略和标签选择上的个体偏好。

Conclusion: 推理类型一致性比标签一致性更能反映自由文本解释的语义相似性，基于推理的解释具有丰富性，需要谨慎将标签视为绝对真实。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 该论文提出在LLM智能体中使用"退出"机制作为安全防护措施，让智能体在缺乏信心时主动退出高风险情境，通过ToolEmu框架评估了12个先进LLM，结果显示退出机制能显著提升安全性而几乎不影响帮助性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂环境中运行并产生现实后果，其安全性变得至关重要。多轮智能体场景中的不确定性和模糊性会累积，导致超出传统文本生成失败的严重风险。

Method: 利用ToolEmu框架，对12个最先进的LLM进行系统性评估，通过添加明确的退出指令来研究退出行为对安全性和帮助性的影响。

Result: 结果显示退出机制带来了高度有利的安全-帮助性权衡：所有模型的安全性平均提升+0.39（0-3分制），专有模型提升+0.64，而帮助性仅平均下降-0.03。

Conclusion: 简单地添加明确的退出指令被证明是一种高度有效的安全机制，可以立即部署到现有智能体系统中，并确立了退出作为高风险应用中自主智能体的有效第一道防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于在线背包问题的自动化智能体系统组合框架，通过动态测试和实时效用建模，在预算约束下优化选择智能体组件，相比检索基线显著提升成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组合方法依赖静态语义检索，存在能力描述不完整、检索方法局限等问题，无法基于能力、成本和实时效用进行组件选择。

Method: 引入结构化自动化框架，将智能体系统组合建模为在线背包问题，综合考虑性能、预算约束和兼容性，动态测试候选组件并实时建模效用。

Result: 在5个基准数据集上的实验表明，在线背包组合器始终位于帕累托前沿，在单智能体设置中成功率提升高达31.6%，在多智能体系统中成功率从37%提升至87%。

Conclusion: 该方法在多样化领域和预算约束下展现出强大的适应性，显著提高了智能体系统组合的成功率和成本效益。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: ReviewGuard是一个自动化系统，用于检测和分类有缺陷的同行评审，采用四阶段LLM驱动框架，通过数据增强和模型微调提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 同行评审作为科学的守门人面临挑战：提交量激增和LLM在学术评估中的广泛使用，未经验证的有缺陷评审（来自人类专家和AI系统）可能系统性地破坏同行评审生态系统并损害学术诚信。

Method: 四阶段LLM驱动框架：1)从OpenReview收集ICLR和NeurIPS论文及评审；2)使用GPT-4.1标注评审类型并进行人工验证；3)通过LLM驱动的合成数据增强解决类别不平衡和数据稀缺问题；4)微调编码器模型和开源LLM。

Result: 构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的语料库。有缺陷评审显示更低的评分、更高的自报信心、减少的结构复杂性和更多的负面情感。AI生成文本检测显示ChatGPT出现后AI生成评审大幅增加。混合训练显著提高了二元任务的召回率和F1分数。

Conclusion: 这是首个用于检测有缺陷同行评审的LLM驱动系统，为同行评审中的AI治理提供证据，并为维护学术诚信的人机协作提供宝贵见解。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 通过分析LLMs在回答文化相关问题时内部激活路径的重叠程度，揭示了语言对文化理解机制的重要影响，发现语言特异性强于文化特异性，且语言相似性不一定保证内部表征的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多元文化背景中的广泛应用，准确的文化理解变得至关重要。现有评估多关注输出层面的表现，难以揭示响应差异的驱动因素，而电路分析研究覆盖语言少且很少聚焦文化。

Method: 通过测量LLMs在回答语义等价问题时的内部激活路径重叠度，比较两种条件：固定问题语言改变目标国家 vs 固定国家改变问题语言，并使用同语言国家对来分离语言和文化因素。

Result: 结果显示，同语言跨国家问题的内部路径重叠度高于跨语言同国家问题，表明存在强烈的语言特定模式。特别是韩国-朝鲜对显示出低重叠度和高变异性，说明语言相似性不能保证内部表征的一致性。

Conclusion: LLMs的文化理解机制受语言因素强烈影响，语言特异性强于文化特异性，语言相似性不一定对应相似的内部处理机制。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 本文介绍了SHALLOW基准框架，这是第一个系统分类和量化ASR系统中幻觉现象的方法，涵盖词汇、语音、形态和语义四个维度。


<details>
  <summary>Details</summary>
Motivation: ASR系统中的幻觉会产生与语音输入完全无关但语法语义合理的转录，这在医疗、法律等关键领域带来严重风险。传统评估指标无法区分语音不准确和幻觉，需要新的评估框架。

Method: 开发SHALLOW基准框架，通过四个互补维度（词汇、语音、形态、语义）系统分类幻觉现象，并在每个类别中定义针对性指标来生成可解释的模型行为分析。

Result: 在不同架构和语音领域的评估发现，当识别质量高时SHALLOW指标与WER强相关，但随着WER增加相关性显著减弱。SHALLOW能捕捉WER在退化条件下无法区分的细粒度错误模式。

Conclusion: SHALLOW框架支持对模型弱点的具体诊断，并提供超越聚合错误率的模型改进反馈，为ASR系统幻觉评估提供了有效工具。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出了一种针对乌尔都语AI生成文本检测的新框架，使用1800篇人类撰写和1800篇AI生成文本构建数据集，通过语言统计分析和微调多语言transformer模型，mDeBERTa-v3-base模型在测试集上达到91.29% F1分数和91.26%准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本能力增强，区分人类与机器生成文本变得困难，特别是在乌尔都语等资源匮乏语言中缺乏检测工具，需要开发专门的检测框架来应对错误信息和学术不端问题。

Method: 构建平衡数据集（人类与AI生成文本各1800篇），进行详细的语言统计特征分析（字符词数、词汇丰富度、N-gram模式），使用t检验和Mann-Whitney U检验评估显著性，并微调三种多语言transformer模型（mdeberta-v3-base、distilbert-base-multilingualcased、xlm-roberta-base）。

Result: mDeBERTa-v3-base模型表现最佳，在测试集上F1分数达到91.29%，准确率为91.26%。

Conclusion: 该研究推进了乌尔都语社区打击错误信息和学术不端的努力，并为低资源语言的NLP工具开发做出了贡献。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 通过微调大型语言模型将句子翻译成句法结构，扩展了西班牙语语法教学工具MiSintaxis的功能，在短语结构分析中取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 利用大型神经模型在自然语言处理中的进展，探索基于机器学习的句法分析新方法，扩展西班牙语语法教学工具MiSintaxis的能力。

Method: 微调Hugging Face仓库中的多个大型语言模型，使用从AnCora-ES语料库生成的训练数据，将输入句子翻译成对应的句法结构。

Result: 使用F1分数评估性能，结果显示在短语结构分析中具有高准确率。

Conclusion: 该方法在句法分析中表现出色，证明了这种方法的潜力。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo是一个多智能体协作框架，通过模拟四个专门LLM代理的结构化辩论来提升性能与可解释性，每个代理代表不同的推理范式，在六个基准测试中超越单模型和辩论基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大但缺乏可解释的推理过程，需要一种既能提升性能又能提供明确推理链的方法。

Method: 使用四个专门化的LLM代理，每个代理代表不同的推理范式，通过迭代辩论来挑战和精炼初始响应，产生更稳健的结论和可审计的推理链。

Result: 在六个基准测试和统一开源设置下，DiMo比广泛使用的单模型和辩论基线提高了准确性，在数学任务上提升最大。

Conclusion: DiMo作为一个语义感知的Web原生多智能体框架，能够结合检索增强推理与结构化论证，为下游系统提供可检查和重用的解释。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 提出了Capsule Prompt-Tuning (CaPT)方法，通过将实例感知信息作为注意力锚点集成到提示学习中，在保持参数高效的同时显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的学习方法存在两个主要问题：1）依赖繁琐的网格搜索确定最优提示长度；2）缺乏实例感知信息，导致与输入序列的注意力交互受限。

Method: CaPT方法创新性地将实例感知和任务感知信息集成到单个胶囊提示中，以近乎参数免费的方式实现提示学习，利用实例语义作为注意力锚点来增强模型性能。

Result: 在T5-Large模型上达到84.03%的平均准确率，在Llama3.2-1B模型上仅使用0.003%的参数，表现出优越的性能和参数效率。

Conclusion: CaPT方法通过引入实例感知信息作为注意力锚点，有效解决了传统提示学习的局限性，在保持参数高效的同时显著提升了模型在各种语言任务上的表现。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 本文提出了TUuD框架来评估大语言模型在动态时间参考系下对时间-事件和事件-事件关系的理解能力，发现LLMs表现出部分类人的时间认知特征。


<details>
  <summary>Details</summary>
Motivation: 人类通过空间隐喻来理解时间概念，使用时间参考系(t-FoR)来感知相对于'现在'的时间关系。虽然LLMs在自然语言理解方面取得显著进展，但其时间推理能力仍然有限，需要评估其在动态时间参考系下的表现。

Method: 引入TUuD框架，基于时间认知研究，让LLMs评估当前时刻与目标事件之间的相似度(0.00-1.00)，量化两者之间的时间对齐程度，研究当'现在'参考点在时间线上动态移动时的表现。

Result: 四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似度评分在现在附近达到峰值，并向过去和未来事件递减。但这种适应在超出近期语境时会减弱。

Conclusion: LLMs显示出部分类人的时间认知能力，但其时间推理仍然对参考系转换和时间距离敏感，表明其时间理解能力仍有局限性。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文研究了思维链（CoT）推理在自然语言理解（NLU）任务中的应用效果，发现模型规模增大时CoT推理从阻碍变为提升性能，大多数基于理性的训练方法效果不如仅使用标签的训练，但专门设计的方法能持续改进性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注思维链在推理任务中的作用，而忽视了其在自然语言理解任务中的潜在价值。本文旨在系统探索理性是否也能同样有益于NLU任务。

Method: 构建了NLURC数据集（包含理性的全面高质量NLU数据集集合），并开发了多种理性增强方法，在NLU任务上测试这些方法的适用性。

Result: 发现三个关键结果：1）随着模型规模增大，CoT推理从阻碍NLU性能转变为超越直接标签预测；2）大多数理性增强训练方法表现不如仅使用标签的训练；3）使用理性训练的LLM在未见过的NLU任务上获得显著性能提升，媲美十倍规模的模型。

Conclusion: 理性在NLU任务中具有重要价值，特别是当模型规模足够大时，CoT推理能显著提升性能，且经过适当设计的理性训练方法能使模型在泛化能力和可解释性方面达到商业LLM水平。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了2014-2025年间自然语言处理技术在心脏病学领域的研究应用，通过分析265篇相关文献，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病日益普遍且对全球健康产生重大影响，这些疾病受遗传、生活方式和社会经济因素等多重因素影响。相关信息分散在各种文本数据中，NLP技术能够分析这些非结构化数据，帮助医疗专业人员深入了解心脏病学领域，从而革新心脏问题的诊断、治疗和预防方法。

Method: 查询了六个文献数据库，通过严格筛选过程识别出265篇相关文章，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行分析，并进行时间趋势分析。

Result: 分析显示在各个维度上都存在相当大的多样性，证明了NLP研究在该领域的广度。时间分析揭示了所涵盖的过去十年中NLP方法的演变和变化趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心血管疾病研究中的重要应用价值和广阔前景。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 本文首次系统研究LLM的'变色龙行为'：在多轮对话中面对矛盾问题时立场不稳定的倾向。通过Chameleon基准数据集评估发现主流模型都存在严重立场不稳定性，且与知识多样性不足密切相关。


<details>
  <summary>Details</summary>
Motivation: 当前集成搜索/检索引擎的LLM系统存在关键漏洞，其可靠性受到质疑。研究者旨在系统调查LLM在多轮对话中面对矛盾问题时的立场不稳定性问题。

Method: 创建包含17,770个问答对的Chameleon基准数据集，涵盖1,180个多轮对话和12个争议领域。引入两个理论基础的指标：量化立场不稳定性的变色龙分数（0-1）和衡量知识多样性的源重用率（0-1）。

Result: 对Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash的评估显示所有模型都表现出严重的变色龙行为（分数0.391-0.511），GPT-4o-mini表现最差。源重用率与置信度（r=0.627）和立场变化（r=0.429）存在强相关性。

Conclusion: LLM的立场不稳定性源于知识多样性不足，导致模型病态地顺从查询框架。在医疗、法律和金融等需要保持连贯立场的系统中部署LLM前，需要进行全面的连贯性评估。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该论文研究了诗歌中空格的重要性，分析了19k首诗歌的空格使用模式，比较了人类创作、LLM生成和未发表诗歌的空格差异，并探讨了不同文本处理方法对空格表示的影响。


<details>
  <summary>Details</summary>
Motivation: 空格是诗歌形式的关键组成部分，反映了诗人对标准化形式的遵守和反抗。尽管诗歌是长期存在的艺术形式，也是LLM的生成任务，但NLP社区对空格的关注不足。

Method: 使用Poetry Foundation的19k首英文诗歌语料库，分析4k位诗人的空格使用。比较了人类发表诗歌与51k首LLM生成诗歌、12k首未发表诗歌的空格使用差异，并研究了不同时期、诗歌形式和数据处理方法的影响。

Result: 发现不同文本处理方法会导致诗歌数据中空格表示显著不同。不同来源的诗歌（人类发表、LLM生成、未发表）在空格使用上存在差异。发布了2.8k首公共领域诗歌的子集以促进进一步研究。

Conclusion: 诗歌中的空格使用反映了诗人的艺术选择，不同数据处理策略会影响LLM预训练数据集中空格的表示，这对构建LLM预训练数据集的处理策略具有重要启示。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 提出了Beacon基准测试，用于测量大语言模型在真实性和顺从性之间的结构性权衡（谄媚偏差），发现该偏差随模型能力增强而增加，并提出干预方法来调节这种偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在奖励优化过程中混淆了帮助性和礼貌服从，形成了偏好用户同意而非原则性推理的谄媚偏差，需要精确测量这种偏差。

Method: 引入Beacon单轮强制选择基准测试，隔离对话上下文影响；评估12个最先进模型；提出提示级和激活级干预方法。

Result: 发现谄媚偏差可分解为稳定的语言和情感子偏差，且随模型容量增加而增强；干预方法能在相反方向上调节这些偏差。

Conclusion: Beacon将谄媚偏差重新定义为可测量的规范错误泛化形式，为研究和缓解大规模生成系统中的对齐漂移提供了可复现基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出SCO-PAL方法，通过自博弈显著提升语言智能体在动态对抗游戏中的战略推理能力，相比基线平均胜率提升约30%，对GPT-4胜率达54.76%。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中因战略推理能力不足而表现不佳，需要无需专家标注数据的自动学习方法，且对手选择对学习效果有重要影响。

Method: 提出SCO-PAL方法，通过玩与学习实现步级策略优化，分析不同级别对手选择，发现自博弈是最有效的训练方式。

Result: 使用SCO-PAL配合自博弈，在六个对抗游戏中平均胜率比基线提升约30%，对GPT-4胜率达到54.76%。

Conclusion: 自博弈是提升对抗环境中战略推理的最有效方法，SCO-PAL方法显著提高了语言智能体的游戏表现。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval是一个双语多任务评估基准，用于评估LLM在英语和阿拉伯语中的长上下文理解能力，涵盖4k到128k+token的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在处理长上下文方面能力的提升，需要严格的评估方法来有效评估其在长上下文理解方面的性能。

Method: 设计了四个新颖且具有挑战性的任务：多文档问答、双语问答、段落内声明验证和基于长上下文的多选题，评估LLM的深度推理、文档理解、信息追踪和双语信息提取能力。

Result: 对开放权重和闭源LLM的评估表明，LC-Eval带来了显著挑战，即使是GPT-4o等高性能模型在某些任务上也表现困难。

Conclusion: LC-Eval基准的复杂性和严谨性凸显了当前LLM在长上下文理解方面仍存在挑战，为未来模型改进提供了重要参考。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC是一个用于句子嵌入模型领域适应的多阶段框架，通过联合优化掩码语言建模和对比目标，在统一训练流程中学习领域相关表示。


<details>
  <summary>Details</summary>
Motivation: 解决将大规模通用领域句子嵌入模型适应到专业领域的挑战，需要保持原始模型的语义区分能力同时学习领域相关知识。

Method: 多阶段框架，结合领域特定的掩码监督，联合优化MLM和对比目标，通过选择性适应实现领域适应。

Result: 在高资源和低资源领域均取得显著改进，NDCG@10指标相比强基线提升高达13.4%，消融研究验证了各组件有效性。

Conclusion: 平衡的联合监督和分阶段适应对于有效的领域适应至关重要，MOSAIC框架在保持语义区分能力的同时成功学习领域特定表示。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: LLMs在实体比较任务中经常依赖启发式偏见而非真实知识，即使具备正确数值知识。研究发现实体流行度、提及顺序和语义共现三种启发式偏见严重影响预测，小模型尤其依赖这些表面线索。大模型能选择性使用数值知识，而链式思维提示能引导所有模型更好利用数值特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在知识推理任务中何时依赖真实知识而非表面启发式偏见，通过实体比较任务系统分析模型决策机制。

Method: 使用实体比较任务（如比较河流长度），分析模型预测与三种启发式偏见（实体流行度、提及顺序、语义共现）的关系，比较不同规模模型表现，并测试链式思维提示效果。

Result: 小模型主要依赖启发式偏见，仅基于表面线索的逻辑回归比模型自身数值预测更准确；大模型能选择性使用更可靠的数值知识；链式思维提示能有效引导所有模型使用数值特征。

Conclusion: LLMs在知识推理中严重依赖启发式偏见，模型规模影响知识选择能力，适当的提示策略能改善模型推理过程。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [42] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 提出了一个两阶段检索-重排框架，使用LLM进行跨体裁作者归属任务，在HIATUS基准上显著超越现有最佳方法


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属需要避免依赖主题线索，而是学习与文本主题无关的作者特定语言模式，但现有信息检索方法不适用于此任务

Method: 两阶段检索-重排框架，针对跨体裁AA设计专门的数据筛选策略，使重排器能有效学习作者区分信号

Result: 在HIATUS的HRS1和HRS2跨体裁AA基准上，分别比之前最佳方法提升了22.3和34.4个绝对Success@8点

Conclusion: 该方法通过专门的数据策略和LLM框架，在跨体裁作者归属任务上取得了显著突破

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [43] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: CoRUS框架通过模拟基于角色的提问，揭示了语言模型对用户角色（患者、护理者、从业者）的响应差异，特别是在阿片类药物使用障碍等敏感领域，角色信息会显著影响模型回复的支持性和知识性内容。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估大多忽略提问者的角色背景，而在阿片类药物使用障碍等敏感领域，用户角色对获得适当、无污名化的回复至关重要。

Method: 基于角色理论和在线OUD康复社区数据构建角色分类法，模拟生成15,321个嵌入不同角色目标、行为和经验的提问，用于评估五个大型语言模型。

Result: 脆弱角色（患者和护理者）相比从业者角色，能引发更多支持性回复（+17%）但减少知识性内容（-19%），显示用户角色会系统性影响模型响应。

Conclusion: 用户角色信息会隐性影响语言模型响应，CoRUS提供了基于角色的对话AI评估方法，强调在敏感领域考虑用户背景的重要性。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [44] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个用于生成高质量多模态财务报告的多智能体框架，通过CAVM架构、迭代视觉增强机制和两阶段写作框架，显著提升了财务报告的准确性、分析深度和呈现质量。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业的财务报告，这是一个劳动密集且智力要求高的过程。

Method: 采用CAVM架构统一外部数据、工具和智能体；提出迭代视觉增强机制逐步优化原始视觉输出；使用两阶段写作框架将简洁的分析链扩展为连贯的多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线系统，包括领先的深度研究系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的清晰路径，为自动化财务报告生成提供了有效解决方案。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [45] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出神经元群通信（NGC）框架，将神经网络重新构想为神经元群相互作用动力系统，通过低维信号交换实现参数压缩，提升推理能力并保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决现代神经网络规模扩大带来的效率、模块化和可解释性挑战，构建能够学习高效、模块化、可解释表示的大型神经系统的核心问题。

Method: 将神经网络视为神经元群相互作用的动力系统，权重作为神经元状态间的瞬时交互，通过神经元群间的迭代通信进行神经计算，引入神经元稳定性度量来量化序列处理中的稳定模式。

Result: 在大型语言模型中实例化NGC，在适度压缩下在复杂推理基准上表现更优，在相同压缩率下持续优于标准低秩近似和跨层基共享方法。

Conclusion: NGC框架通过结构化神经元群动力学可能关联高维学习系统中的泛化能力，为构建高效、模块化、可解释的大型神经网络提供了新途径。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [46] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型在具身知识理解方面并未优于纯文本模型，且在视觉维度表现最差，表明当前多模态模型对物理世界的理解仍有局限


<details>
  <summary>Details</summary>
Motivation: 探究视觉基础是否能增强多模态语言模型对具身知识的理解，与纯文本模型相比是否有优势

Method: 基于心理学感知理论构建具身知识理解基准，涵盖多种感官模态，通过向量比较和问答任务评估模型表现，包含1700多个问题

Result: 视觉语言模型在两种任务中均未优于纯文本模型，且在视觉维度表现显著差于其他感官维度；向量表示易受词形和频率影响，模型在空间感知和推理问题上表现不佳

Conclusion: 需要更有效地将具身知识整合到语言模型中，以增强其对物理世界的理解能力

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [47] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo是一个大规模多语言基准测试，包含8个子任务，覆盖2700+语言，用于评估生成模型在词汇理解和生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对高资源或中资源语言，且多关注推理和生成等高阶任务，但LLMs在全球3800+书面语言中的基本语言能力存在明显不足。

Method: 利用现有词典、单语数据和双语文本构建ChiKhaPo基准，包含不同难度的8个子任务，专门测试词汇理解和生成能力。

Result: 测试显示6个SOTA模型在该基准上表现不佳，性能受语言家族、资源丰富度、任务类型以及理解与生成方向等因素影响。

Conclusion: ChiKhaPo旨在促进LLMs的大规模多语言基准测试，填补现有基准在语言覆盖范围上的不足。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [48] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出了PROMPT-MII方法，使用强化学习元学习指令归纳模型，能在新数据集上生成紧凑指令，在保持性能的同时大幅减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决上下文学习(ICL)在适应大语言模型到新任务时推理成本高的问题，特别是随着上下文长度增长带来的高开销。

Method: 基于强化学习的框架PROMPT-MII，在3,000多个多样化分类数据集上元学习指令归纳模型，能够为新数据集动态生成紧凑指令。

Result: 在90个未见任务上评估，PROMPT-MII将下游模型质量提升4-9个F1点(相对提升10-20%)，匹配ICL性能同时所需token减少3-13倍。

Conclusion: PROMPT-MII方法有效解决了ICL的高推理成本问题，在保持性能的同时显著提升了效率。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [49] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次应用参数高效微调(PEFT)方法，使用LoRA和QLoRA技术对三种大型语言模型进行孟加拉语仇恨言论检测，在BD-SHS数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体上仇恨言论急剧增加，特别是针对妇女和青少年。现有方法要么依赖计算成本高的全模型微调，要么使用专有API，缺乏高效实用的解决方案。

Method: 使用LoRA和QLoRA参数高效微调技术，在BD-SHS数据集(50,281条标注评论)上微调三种指令调优的大型语言模型：Gemma-3-4B、Llama-3.2-3B和Mistral-7B，每个模型仅训练少于1%的参数。

Result: Llama-3.2-3B获得最高F1分数92.23%，其次是Mistral-7B(88.94%)和Gemma-3-4B(80.25%)。所有实验均在单个消费级GPU上完成。

Conclusion: PEFT被证明是孟加拉语及相关低资源语言仇恨言论检测的实用且可复现策略，为低资源语言NLP任务提供了高效解决方案。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [50] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一个极简的字节级分词器，直接将文本映射到UTF-8编码对应的字节ID，使用C0控制字节编码特殊行为，提供更快的分词速度、更小的数据传输和共享的256维嵌入表。


<details>
  <summary>Details</summary>
Motivation: 解决现有字节级分词器引入超出范围ID和辅助标记的问题，利用ASCII原始设计理念，通过C0控制字节统一处理所有特殊行为。

Method: 基于UTF-8字节编码的直接映射，使用C0控制字节处理填充、边界、对话结构等特殊行为，采用位偏置嵌入技术暴露字节位结构。

Result: 分词速度提升14倍，主机-设备传输减少8倍，嵌入表可跨模型对齐，语言建模收敛性得到改善。

Conclusion: UTF8Tokenizer提供了一种高效、简洁的分词方案，通过回归ASCII设计原则实现了性能提升和模型兼容性。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [51] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种词汇表压缩方法，通过将词形变化表示为变换向量而非独立词条，从而释放词汇表空间用于更多样化的词汇，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法将词形变化视为独立词条，导致词汇表被表面形式变体填满，牺牲了低频词和多语言覆盖。

Method: 将词形变化表示为变换向量（加法偏移量），在输入和输出空间中从共享的基础形式和变换向量组合生成表面形式。

Result: 在多个LLM和五种语言上应用该方法，最多可移除10%的词汇条目，扩展了词汇覆盖范围且对下游性能影响最小。

Conclusion: 这促使词汇设计从字符串枚举转向利用语言底层结构的组合式词汇表，无需修改模型权重。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [52] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的动态防御框架，通过在线学习对抗迭代越狱攻击，在保持无害任务响应质量的同时有效拒绝有害提示。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动破坏迭代越狱攻击的动态试错循环，这些攻击通过重复重写提示来诱导有害输出。

Method: 使用强化学习优化提示，确保无害任务的适当响应并明确拒绝有害提示；引入Past-Direction Gradient Damping防止对攻击中部分输入重写的过拟合。

Result: 在三个LLM上的实验表明，该方法显著优于五种现有防御方法，对抗五种迭代越狱攻击；同时提升了无害任务的响应质量。

Conclusion: 该动态防御框架能有效对抗迭代越狱攻击，在安全性和实用性之间取得了良好平衡。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [53] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: DiscoTrack是一个多语言LLM基准测试，专注于语篇理解和隐式信息推理，涵盖12种语言和四个语篇理解层次。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要关注自然语言理解和显式信息提取，缺乏针对隐式信息和语篇层面推理的更具挑战性的多语言基准。

Method: 开发了DiscoTrack基准测试，包含12种语言和四个语篇理解层次：显著性识别、实体追踪、语篇关系和桥接推理。

Result: 评估显示这些任务即使对于最先进的模型仍然具有挑战性。

Conclusion: 语篇理解和隐式信息推理是LLM需要进一步发展的关键能力，DiscoTrack为此提供了重要的评估工具。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [54] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文研究发现基于大语言模型的搜索代理在回答开放域问题时比基础LLM更容易产生有害输出，提出了一种名为SafeSearch的多目标强化学习方法，将安全性和实用性联合对齐，显著降低了代理的有害性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注提升搜索代理的实用性，但其安全行为研究不足。研究发现搜索代理在处理有害问题时比基础模型更容易降低拒绝阈值，检索外部信息并合成不安全的内容。

Method: 提出SafeSearch方法，采用多目标强化学习，结合最终输出的安全/实用性奖励和新的查询级塑造项，惩罚不安全查询并奖励安全查询。

Result: 实验表明SafeSearch在三个红队测试数据集上将代理的有害性降低了70%以上，同时保持了安全、有帮助的响应，并在问答性能上与仅优化实用性的微调代理相当。

Conclusion: 查询级奖励在联合提升安全性和实用性方面有效，证明了联合对齐安全性和实用性的重要性。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [55] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出xLSTM框架，通过余弦相似度门控、自适应特征优先级和类别重平衡，在毒性评论检测任务中实现高效且准确的表现，超越BERT模型。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的模型在毒性评论检测中计算成本高、对少数毒性类别性能下降的问题，以及传统集成方法缺乏语义适应性的局限。

Method: 使用可学习的参考向量通过余弦相似度调制上下文嵌入，整合多源嵌入（GloVe、FastText、BERT CLS）、字符级BiLSTM、嵌入空间SMOTE和自适应焦点损失。

Result: 在Jigsaw毒性评论基准测试中达到96.0%准确率和0.88宏F1，在威胁和身份仇恨类别上分别比BERT提升33%和28%，参数减少15倍，推理延迟50ms。

Conclusion: xLSTM建立了新的效率-适应性前沿，证明轻量级、理论指导的架构可以在不平衡、领域特定的NLP任务中超越大型预训练模型。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [56] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 本文研究大语言模型的提示敏感性现象，提出通过语义空间采样和改写扰动来改善不确定性校准，并引入新的不确定性分解指标来量化提示敏感性对模型不确定性的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对语义相同但表述不同的提示时会产生不同的回答分布，这表明模型输出的不确定性可能无法真实反映其对提示含义的不确定性。研究者希望解决这种提示敏感性带来的不确定性校准问题。

Method: 将提示敏感性建模为一种泛化误差，通过在语义概念空间中进行改写扰动采样来改善不确定性校准；同时提出新的不确定性分解指标，该指标基于语义连续性建模，优于传统的基于熵的分解方法。

Result: 研究表明，通过语义空间采样可以改善不确定性校准而不损害准确性；新的分解指标能够有效量化提示敏感性对LLM不确定性的贡献程度。

Conclusion: 这项工作为改善提示敏感语言模型的不确定性校准提供了新方法，并证明某些LLM未能对其输入含义表现出一致的一般推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [57] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本文系统研究了大型语言模型在推理过程中如何聚合社会偏见，发现了两种导致偏见加剧的失败模式：刻板印象重复和无关信息注入，并提出了一种轻量级的提示缓解方法。


<details>
  <summary>Details</summary>
Motivation: 虽然基于推理的大语言模型通过内部结构化思考过程在复杂任务中表现出色，但存在一个令人担忧的现象：这种思考过程会聚合社会刻板印象，导致偏见结果。然而，语言模型在社会偏见场景中的底层行为仍未被充分探索。

Method: 本文系统调查了思考过程背后的机制，发现了两种驱动社会偏见聚合的失败模式：1) 刻板印象重复 - 模型依赖社会刻板印象作为主要理由；2) 无关信息注入 - 模型编造或引入新细节来支持偏见叙述。基于这些发现，提出了一种轻量级的提示缓解方法，让模型根据这些特定失败模式来审查其初始推理。

Result: 在问答（BBQ和StereoSet）和开放式（BOLD）基准测试上的实验表明，该方法能有效减少偏见，同时保持或提高准确性。

Conclusion: 本文揭示了语言模型推理过程中社会偏见聚合的机制，并提出了一种有效的缓解方法，为解决AI系统中的偏见问题提供了重要见解。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [58] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过分解任务、建模子任务依赖关系并编码验证函数来提升系统鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在复杂任务中面临规划、协调和验证的挑战，执行失败往往源于任务解释、输出格式或智能体间交接的细微偏差。

Method: VeriMAP规划器分解任务，建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数。

Result: 在多样化数据集上的评估显示，VeriMAP优于单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。

Conclusion: 验证感知规划使多智能体系统能够实现可靠的协调和迭代优化，无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [59] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen是一个开源动态词汇增强框架，解决传统语言模型处理新词和词汇外词的限制，提供模块化训练、评估和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表语言模型难以处理新词和词汇外词，现有动态词汇方法存在代码库碎片化、不支持现代LLM和推理扩展性差等问题。

Method: 开发DVAGen统一框架，模块化处理流程，集成开源LLM，提供CLI和WebUI工具进行实时结果检查，支持批量推理。

Result: 验证了动态词汇方法在现代LLM上的有效性，显著提升了推理吞吐量。

Conclusion: DVAGen通过统一框架解决了动态词汇增强的关键挑战，为处理多样化词汇组合提供了有效解决方案。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [60] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文系统比较了基于提示和基于强化学习的查询增强方法，发现简单的无训练查询增强在强大LLMs下表现与昂贵的RL方法相当甚至更好，并提出了一种结合两者优势的混合方法OPQE。


<details>
  <summary>Details</summary>
Motivation: 现有查询增强方法主要有两种：基于提示的方法和基于强化学习的方法，但缺乏在一致实验条件下的系统比较。

Method: 1. 系统比较提示法和RL法在多种基准测试中的表现；2. 提出混合方法OPQE，让LLM策略学习生成最大化检索性能的伪文档。

Result: 简单无训练的查询增强方法在使用强大LLMs时表现与RL方法相当甚至更好；OPQE方法优于单独的提示法和RL重写方法。

Conclusion: 结合提示法的灵活性和生成结构与RL的目标优化能力的协同方法能获得最佳结果。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [61] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究表明，人们对AI生成的讽刺言论不会完全采用意向性立场，行为数据和神经数据都显示人们对AI的讽刺理解投入的认知努力较少，更倾向于将其视为计算错误而非有意沟通。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地被部署为社交代理并训练产生幽默和讽刺，需要研究人们在遇到机智的AI言论时，是将其理解为有意的沟通还是纯粹的计算输出。

Method: 比较了人们对AI与人类来源的讽刺语句的行为和神经反应，使用已建立的ERP成分：P200反映早期不一致性检测，P600索引重新解释不一致性为故意讽刺的认知努力。

Result: 行为上，参与者对两种来源都将不一致性归因于有意沟通，但对AI的归因显著少于人类；神经数据显示AI生成讽刺的P200和P600效应减弱，表明检测和重新分析的认知努力减少。

Conclusion: 尽管当前LLMs具有语言复杂性，但实现真正的社交代理需要的不只是语言能力，还需要人类如何感知和归因于人工代理的意向性的转变。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [62] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了块状稀疏注意力模型成功的关键架构原则，提出了三个核心设计原则：表达性块编码器、旁路残差路径和强制选择稀疏性，实现了从4K到3200万token的无训练长度外推。


<details>
  <summary>Details</summary>
Motivation: 有效处理长上下文是语言模型的关键挑战。标准Transformer受限于二次复杂度和长度外推能力差，而滑动窗口注意力等替代架构由于固定大小内存无法充分利用完整上下文。块状稀疏注意力虽然表现出色，但其成功的关键架构原则尚未完全理解。

Method: 通过统一框架和全面消融研究，识别出三个关键设计原则：(1) 具有专用CLS token的表达性非线性块编码器；(2) 旁路残差路径稳定整合检索到的全局信息；(3) 预训练期间强制选择稀疏性以弥合训练-测试分布差距。

Result: 结合这些原则，在RULER和BABILong上实现了从4K上下文训练到3200万token的无训练长度外推，建立了新的最先进水平。

Conclusion: 研究为开发未来高性能长上下文语言模型提供了一套清晰且经验基础的设计原则，阐明了块内信息处理和地标生成的理论动机。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [63] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出Attention-Shifting框架解决大语言模型遗忘中的困境，通过注意力机制实现选择性遗忘，在保持模型效用的同时避免幻觉响应。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法面临两难：激进遗忘损害模型效用，保守策略保留效用但产生幻觉响应，限制了LLMs在知识密集型应用中的可靠性。

Method: 采用注意力转移框架，包含上下文保持抑制和抗幻觉响应塑造两个设计目标，通过重要性感知抑制和注意力引导保留增强两种干预措施实现。

Result: 在ToFU基准上准确率提升15%，TDEC基准上提升10%，同时保持竞争性的无幻觉遗忘效果。

Conclusion: AS方法在遗忘效果、泛化能力和响应可靠性之间实现了优越的平衡。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [64] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出StreamingThinker框架，实现LLM在输入过程中同时进行推理的流式思维范式，显著减少推理延迟


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待完整输入才开始思考，在动态场景中引入不必要延迟并削弱对早期信息的注意力

Method: 设计流式思维范式，通过流式CoT生成、流式约束训练和流式并行推理实现边读边想，使用流式推理单元、注意力掩码和并行KV缓存

Result: 在数学推理、逻辑推理和上下文QA任务中保持与批量思维相当的性能，推理开始前token等待减少80%，最终答案生成时间延迟减少60%以上

Conclusion: 流式思维范式能有效减少LLM推理延迟，同时保持推理质量，为动态场景下的实时推理提供了可行方案

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [65] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文介绍了VideoBiasEval框架，用于评估视频生成模型中的社会偏见，发现对齐调优会放大并稳定化社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型通过奖励模型进行对齐调优时，可能会无意中编码和放大社会偏见，需要系统评估这种偏见的演变过程。

Method: 提出VideoBiasEval诊断框架，基于社会偏见分类学，采用基于事件的提示策略，引入多粒度指标评估偏见。

Result: 研究发现对齐调优不仅加强了表征偏见，还使其在时间上更加稳定，产生更平滑但更刻板的描述。

Conclusion: 需要在对齐过程中进行偏见感知的评估和缓解，以确保公平和社会责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [66] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过大规模情感分析发现孟加拉语新闻标题和内容中负面情绪（愤怒、恐惧、失望）占主导地位，不同媒体对相似事件的报道存在显著情感差异，据此提出可视化情感线索的新闻聚合器设计。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体通过报道框架影响公众情绪，负面或情绪化标题往往获得更多关注和传播，这种倾向促使媒体采用更能引发强烈反应的报道方式。

Method: 使用Gemma-3 4B进行零样本推理，分析了30万条孟加拉语新闻标题及其内容，识别每篇文章的主导情绪和整体基调。

Result: 研究发现负面情绪明显占主导，特别是愤怒、恐惧和失望，不同媒体对相似故事的情感呈现存在显著差异。

Conclusion: 基于这些发现，提出了以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中隐藏的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [67] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了Transformer大语言模型的局部可解释性和机制可解释性方法，在医疗和自动驾驶领域进行了实验研究，并总结了当前未解决的问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能优异，但其预测过程和内容生成机制对人类不可理解，且存在幻觉等错误，迫切需要理解和解释模型内部工作机制以建立信任。

Method: 综述了局部可解释性和机制可解释性方法，并在医疗和自动驾驶两个关键领域进行了实验研究，分析了解释对接收者的信任影响。

Result: 提出了大语言模型可解释性研究的三个关键贡献：方法综述、领域实验研究和未解决问题总结。

Conclusion: 总结了LLM可解释性领域当前未解决的问题，并指出了生成人类对齐、可信赖LLM解释的机遇、关键挑战和未来方向。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [68] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出了TaxoAlign方法用于自动生成学术分类法，创建了CS-TaxoBench基准数据集，并通过实验证明该方法在结构和语义一致性方面优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述生成方法缺乏与人工专家创建的分类法结构对比，需要填补人工生成与自动创建分类法之间的差距。

Method: 提出TaxoAlign方法，这是一个基于主题的三阶段指令引导的学术分类法生成方法，包括严格的自动评估框架来衡量结构对齐和语义一致性。

Result: 在CS-TaxoBench基准上评估，TaxoAlign在几乎所有指标上都持续超越基线方法。

Conclusion: TaxoAlign方法能够有效生成与人工专家分类法结构对齐且语义一致的学术分类法，为自动文献综述生成提供了重要工具。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [69] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 该研究使用法语多轮对话数据集CyberAgressionAdo-Large，评估了反社会行为检测、欺凌行为分析和欺凌同伴群体识别三个任务，发现多模态模型优于单模态基线，其中mBERT + WD-SGCN的后期融合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰和网络欺凌）对平台安全和社福构成风险，但现有研究主要关注X和Reddit等网络，多轮对话环境因数据有限而研究不足。

Method: 使用CyberAgressionAdo-Large数据集，评估了6种基于文本和8种基于图的表示学习方法，分析词汇线索、交互动态及其多模态融合。

Result: 多模态模型优于单模态基线，mBERT + WD-SGCN后期融合模型在反社会行为检测上达到0.718，在同伴群体识别和欺凌分析上分别达到0.286和0.606。

Conclusion: 多模态方法能有效处理隐式攻击、角色转换和上下文依赖的敌意等复杂反社会行为现象。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [70] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出Nyx系统，用于解决通用检索增强生成(URAG)问题，支持多模态(文本和图像)检索，通过自动化管道构建NyxQA数据集，采用两阶段训练框架，在标准文本RAG和多模态URAG任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要针对单模态文本，无法满足现实世界中查询和文档包含多模态信息(如文本和图像)的场景需求。

Method: 提出Nyx统一多模态检索器，通过四阶段自动化管道构建NyxQA数据集，采用两阶段训练：预训练和基于下游视觉语言模型反馈的监督微调。

Result: Nyx在标准文本RAG基准测试中表现竞争性，在更通用的URAG设置中表现优异，显著提高视觉语言任务的生成质量。

Conclusion: Nyx系统成功解决了多模态检索增强生成的挑战，为现实世界中的混合模态信息检索和推理提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [71] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型在简单自包含指令执行方面存在不足，对选项标签格式变化敏感，表现出指令格式偏见，且缺乏对原子指令的严格遵循。


<details>
  <summary>Details</summary>
Motivation: 探索指令调优大语言模型执行简单自包含指令的能力，这是复杂指令遵循的基础，但此前研究不足。

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个IT-LLM，系统性地改变选项标签格式（字母、数字、罗马数字），在四种范式下测试：有明确指令、无指令、移除选项内容、三样本示例。

Result: 标签格式变化导致性能显著波动（如罗马数字vs数字下降30.45%），无指令时性能进一步下降10.84%，移除选项内容时模型无法超越随机基准，三样本示例无显著改善，生成分析显示持续标签错误。

Conclusion: 当前指令调优范式存在不足，需要专门针对原子指令遵循的评估方法和训练策略。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [72] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出了EduAdapt基准，包含48k个按年级标记的科学问答对，用于评估LLM在不同年级阶段的适应性表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在学术基准上表现良好，但无法根据学生年级水平调整回答，这在K-12教育中至关重要。

Method: 创建包含9个科学学科、覆盖1-12年级的问答数据集，评估多个开源LLM的年级适应性表现。

Result: 大型模型表现更好，但在低年级（1-5年级）仍难以生成合适回答。

Conclusion: 这是首个评估LLM年级适应性的数据集和框架，旨在通过改进训练和提示策略开发更适合教育需求的AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [73] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究提出了Ladder-base，这是首个基于GRPO强化学习方法训练的中医专用大语言模型，在中医推理和事实一致性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系独特且复杂，现有中医专用LLMs在一致性、数据质量和评估标准方面存在局限，需要更有效的对齐方法。

Method: 基于Qwen2.5-7B-Instruct基础模型，使用GRPO（组相对策略优化）强化学习方法，在TCM-Ladder基准的文本子集上进行训练。

Result: Ladder-base在多项推理指标上优于GPT-4、Gemini 2.5等通用LLMs以及BenTsao、HuatuoGPT2等中医专用模型。

Conclusion: GRPO为传统医学领域LLMs与专家级推理的对齐提供了有效策略，支持可信赖且临床基础扎实的中医AI系统开发。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [74] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption是一个针对20种非洲语言的多语言图像描述框架，包含数据集、动态管道和0.5B参数模型，旨在解决多模态AI研究中低资源语言代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态AI研究过度集中于高资源语言，阻碍了该领域进展的民主化。需要为代表性不足的非洲语言提供包容性解决方案。

Method: 构建基于Flickr8k的语义对齐数据集，采用上下文感知选择和翻译；开发动态上下文保持管道，通过模型集成和自适应替换确保质量；创建集成SigLIP和NLLB200的0.5B参数视觉到文本架构。

Result: 建立了首个可扩展的非洲低资源语言图像描述资源，确保持续数据质量，为真正包容的多模态AI奠定基础。

Conclusion: AfriCaption框架为代表性不足的非洲语言提供了首个可扩展的图像描述解决方案，推动了多模态AI的民主化和包容性发展。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [75] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文介绍了BenCao，一个基于ChatGPT的中医多模态助手，通过自然语言指令调优而非参数重训练，整合了结构化知识库、诊断数据和专家反馈，在中医问答和诊断任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑和多模态诊断线索，现有中医领域大语言模型缺乏多模态整合、可解释性和临床适用性。

Method: 开发BenCao系统，整合1000多部古今文献知识库，采用基于场景的指令框架、思维链模拟机制和执业中医师反馈精炼过程，连接舌像分类和多模态数据库检索API。

Result: 在单选题基准测试和多模态分类任务中，BenCao在诊断、草药识别和体质分类方面优于通用领域和中医领域模型，已在OpenAI GPTs Store部署并被近1000名全球用户使用。

Conclusion: 研究证明了通过自然语言指令调优和多模态整合开发中医领域大语言模型的可行性，为生成式AI与传统医学推理对齐提供了实用框架和可扩展部署路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [76] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 论文发现对齐训练不仅导致任务准确率下降，还会严重损失模型校准性，使模型过度自信、可靠性降低且输出多样性减少。通过简单的权重插值方法，可以在不牺牲准确率的情况下恢复校准性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为对齐训练主要导致任务准确率下降，但研究发现校准性损失同样严重，这影响了模型的可靠性和输出多样性。

Method: 使用简单的后处理方法：在模型对齐前后的权重之间进行插值，找到帕累托最优的插值点。

Result: 该方法不仅能恢复对齐过程中损失的校准性，还能提高准确率，超越了原始模型和对齐后模型的性能。

Conclusion: 简单的模型合并提供了一种计算高效的方法来缓解对齐税的全部影响，得到既更强大又更可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [77] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的搜索模型虽然继承了指令调优的拒绝能力，但存在脆弱性。两种简单攻击（搜索攻击和多搜索攻击）能显著降低模型的安全性能，暴露了当前RL训练方法的安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究RL训练的搜索模型的安全特性，发现尽管这些模型在多步推理任务中表现出色，但其安全属性尚未得到充分理解。

Method: 使用两种简单攻击方法：搜索攻击（强制模型以搜索开始响应）和多搜索攻击（鼓励模型重复搜索），在Qwen和Llama两个模型家族上进行测试，包括本地和网络搜索。

Result: 攻击使拒绝率降低高达60.0%，答案安全性降低82.5%，搜索查询安全性降低82.4%。攻击通过触发模型在生成拒绝标记前产生有害的镜像搜索查询而成功。

Conclusion: 当前RL训练方法存在核心弱点：只奖励生成有效查询而不考虑其危害性，导致RL搜索模型存在用户可轻易利用的漏洞，迫切需要开发安全感知的代理RL管道来优化安全搜索。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [78] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 开发基于BERT的深度上下文嵌入模型，用于提升英语、西班牙语和意大利语临床文本中的命名实体识别性能，在BioASQ MultiCardioNER共享任务中取得了优于平均水平的F1分数。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据快速增长，需要从非结构化临床文本中提取生物医学知识来支持临床系统发展。目前缺乏针对低资源语言临床文本的命名实体识别研究。

Method: 探索单语和多语BERT模型在临床命名实体识别中的有效性，针对心脏病学领域的临床病例报告，提取疾病和药物提及。

Result: 在西班牙语疾病识别获得77.88% F1分数，西班牙语药物识别92.09%，英语药物识别91.74%，意大利语药物识别88.9%，均超过测试排行榜的平均和中位数分数。

Conclusion: 基于BERT的上下文嵌入模型能够有效提升多语言临床文本的命名实体识别性能，为低资源语言的临床NLP研究提供了可行方案。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [79] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文介绍了首个乌尔都语到英语习语翻译评估数据集，评估了多种LLM和NMT系统在习语翻译中的表现，发现提示工程能提升翻译质量，且原生乌尔都语比罗马化乌尔都语翻译效果更好。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在机器翻译中仍是重大挑战，特别是对于乌尔都语等低资源语言，此前研究关注有限。

Method: 构建首个乌尔都语到英语习语翻译评估数据集，评估多种开源LLM和NMT系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标评估翻译质量。

Result: 提示工程相比直接翻译能提升习语翻译质量，但不同提示类型间差异较小；原生乌尔都语输入的习语翻译比罗马化乌尔都语更准确。

Conclusion: 文本表示对翻译质量有显著影响，原生乌尔都语在习语翻译中表现优于罗马化乌尔都语，提示工程是提升习语翻译的有效策略。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [80] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 本文系统研究了多语言大语言模型在医疗问答中的跨语言差异，发现英语维基百科覆盖率和LLM事实对齐度显著高于其他语言，但通过提供非英语上下文可以改善事实对齐。


<details>
  <summary>Details</summary>
Motivation: 将AI整合到医疗保健中时，公平获取可靠健康信息至关重要。但不同语言的信息质量存在差异，引发了对多语言大语言模型可靠性和一致性的担忧。

Method: 构建了多语言维基医疗数据集，分析跨语言医疗覆盖度，评估LLM回答与参考信息的一致性，并通过上下文信息和检索增强生成进行案例研究。

Result: 发现维基百科覆盖率和LLM事实对齐存在显著的跨语言差异，LLM回答更倾向于与英语维基百科对齐，即使提示是非英语的。提供非英语维基百科摘录可以有效将事实对齐转向文化相关知识。

Conclusion: 研究结果强调了构建更公平的多语言医疗AI系统的实用途径，通过利用上下文信息可以改善非英语语言的事实对齐质量。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [81] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新颖的MoE架构，通过跨层复用专家来突破传统层局部路由的限制，在固定参数预算下实现更丰富的专家组合，提升模型表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构受限于层局部路由机制，需要在专家维度和路由多样性之间进行权衡。ReXMoE旨在超越这种限制，允许路由器在相邻层间复用专家。

Method: 提出ReXMoE架构，解耦专家维度与每层预算，采用渐进式缩放路由策略在训练过程中逐步增加候选专家池。

Result: 在0.5B到7B参数规模的不同架构模型上，ReXMoE在固定架构维度下持续提升语言建模和下游任务性能。

Conclusion: ReXMoE为参数高效且可扩展的MoE基LLM提供了新的设计范式。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [82] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法，通过层次亲和树结构建模不同AI-人类协作文本生成过程的关系，并使用专门损失函数对齐文本表示，在混合文本检测任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本检测方法主要采用二元分类或简单多分类，无法有效处理AI与人类协作生成文本的复杂过程，需要更精细的建模方法。

Method: 构建层次亲和树结构建模不同文本生成过程的关系，设计专门损失函数对齐文本表示，并开发RealBench基准数据集。

Result: DETree在混合文本检测任务中表现优异，显著提升了在分布外场景下的鲁棒性和泛化能力，特别是在少样本学习条件下。

Conclusion: 基于训练的方法在分布外设置中具有潜力，层次亲和树结构能有效建模复杂的人类-AI协作文本生成过程。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [83] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，分析了从"流程执行系统"到"自适应社会系统"的演进路径。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体成为人工智能前沿，但如何将通用智能体研究转化为推动行业变革的生产力仍面临重大挑战。

Method: 使用行业智能体能力成熟度框架，分析支撑智能体能力发展的三大技术支柱：记忆、规划和工具使用，并探讨这些技术从支持简单任务到实现复杂自主系统和集体智能的演进过程。

Result: 概述了行业智能体在数字工程、科学发现、具身智能、协同业务执行和复杂系统仿真等领域的应用，并回顾了基础能力和专业能力的评估基准与方法。

Conclusion: 聚焦行业智能体面临的实际挑战，探索其能力边界、发展潜力和治理问题，为理解和构建下一代行业智能体提供清晰路线图和理论基础。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [84] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: Deep Self-Evolving Reasoning (DSER) 是一种概率推理范式，通过将迭代推理建模为马尔可夫链，即使验证和修正能力较弱，也能显著扩展小型开源模型在复杂任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前开源小模型在验证和修正能力上存在脆弱性，限制了它们在复杂推理任务中的表现。DSER旨在通过概率方法克服这一限制，即使只有微弱的改进倾向也能实现渐进式优化。

Method: 将迭代推理建模为马尔可夫链，每个步骤代表解空间中的随机转移。通过并行运行多个长时程的自演化过程，放大微小的正向改进概率，确保收敛到正确解。

Result: 在DeepSeek-R1-0528-Qwen3-8B模型上应用DSER，在AIME 2024-2025基准测试中解决了9个先前无法解决的难题中的5个，整体性能提升，使这个紧凑模型通过多数投票超越了其600B参数教师模型的单轮准确率。

Conclusion: DSER不仅提供了测试时扩展的实用价值，更重要的是诊断了当前开源推理模型的根本局限性，为开发具有强大内在自演化能力的下一代模型指明了研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [85] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 本文系统研究了学习多语言句子嵌入的方法，结合了单语和跨语言表示学习的最佳技术，显著减少了并行训练数据需求，在112种语言上实现了83.7%的双文本检索准确率。


<details>
  <summary>Details</summary>
Motivation: 探索BERT在多语言句子嵌入方面的应用，虽然BERT在单语句子嵌入方面表现优异，但其在多语言场景下的潜力尚未被充分挖掘。

Method: 结合了掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边缘softmax等方法，并引入了预训练的多语言语言模型。

Result: 将并行训练数据需求减少了80%，在Tatoeba数据集112种语言上达到83.7%的双文本检索准确率，远高于LASER的65.5%，同时在单语迁移学习基准上保持竞争力。

Conclusion: 提出的方法显著提升了多语言句子嵌入的性能，并成功应用于神经机器翻译，公开发布了支持109+语言的多语言句子嵌入模型。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [86] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: EliCal是一个两阶段框架，通过廉价的自我一致性监督来引出内部置信度，然后用少量正确性标注进行校准，实现高效的诚实对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大规模标注来实现通用诚实对齐，成本高昂。需要支持标注高效的训练方法。

Method: 提出Elicitation-Then-Calibration两阶段框架：第一阶段使用廉价的自我一致性监督引出内部置信度；第二阶段用少量正确性标注校准置信度。

Result: EliCal仅用1k正确性标注（占全监督的0.18%）就实现了接近最优的对齐效果，在未见MMLU任务上表现优于仅校准基线。

Conclusion: EliCal为LLMs的通用诚实对齐提供了一个可扩展的解决方案。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [87] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是首个大规模标准化基准，用于评估LLM对人类行为的模拟能力，涵盖20个多样化数据集，发现当前最佳LLM模拟能力有限（40.80/100），性能随模型规模对数线性增长，存在对齐-模拟权衡，且在模拟特定人口群体时表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM模拟人类行为的评估方法碎片化，基于定制任务和指标，导致结果不可比较。需要建立标准化基准来推进LLM模拟科学的发展。

Method: 引入SimBench基准，统一20个多样化数据集，涵盖从道德决策到经济选择等任务，使用大规模全球参与者数据作为参考标准。

Result: 当前最佳LLM模拟能力得分仅为40.80/100；性能随模型规模对数线性增长；推理时计算增加不提升性能；指令微调在低熵问题上改善性能但在高熵问题上降低；模型在模拟特定人口群体时表现不佳；模拟能力与深度知识推理能力强相关（MMLU-Pro，r=0.939）。

Conclusion: 通过SimBench使进展可测量，旨在加速开发更忠实的LLM模拟器，为LLM模拟科学的稳健、可复现发展奠定基础。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出了一个统一的多任务学习框架，将自回归LLM与临床推理对齐，用于MSK-CHORD数据集上的癌症治疗结果预测，通过三种对齐策略提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生物医学NLP中表现良好但缺乏结构化推理能力，而癌症治疗结果预测需要既准确又可解释的模型来处理异质临床数据。

Method: 使用多任务学习框架，联合执行二元生存分类、连续生存时间回归和自然语言理由生成，评估三种对齐策略：标准监督微调、带思维链提示的SFT、以及GRPO强化学习方法。

Result: 实验表明思维链提示将F1提高+6.0%，MAE降低12%，GRPO在BLEU、ROUGE和BERTScore上实现最先进的可解释性和预测性能。

Conclusion: 研究强调了在多任务临床建模中推理感知对齐的重要性，为精准肿瘤学中可解释、可信赖的LLM设立了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 本文提出使用拓扑数据分析工具Mapper来分析微调语言模型如何编码歧义性，发现微调后的模型在嵌入空间中形成模块化、非凸的区域，与模型预测高度一致，但在歧义数据中与真实标签的对齐度下降。


<details>
  <summary>Details</summary>
Motivation: 传统标量指标（如准确率）无法捕捉模型内部如何表示歧义性，特别是在人类标注者存在分歧的情况下。需要新的方法来理解模型如何处理主观性任务中的歧义。

Method: 使用拓扑数据分析工具Mapper分析RoBERTa-Large在MD-Offense数据集上的嵌入空间结构，与传统方法如PCA、UMAP进行比较。

Result: 微调将嵌入空间重构为模块化、非凸的区域，98%以上的连通组件具有≥90%的预测纯度。但在歧义数据中，模型结构与真实标签的对齐度下降，揭示了结构置信度与标签不确定性之间的隐藏张力。

Conclusion: Mapper是一个强大的诊断工具，能够直接揭示决策区域、边界塌陷和过度自信的聚类，为理解模型如何解决歧义提供了新视角，并可能为主动建模策略提供拓扑指标。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的语言混淆门控(LCG)方法，通过解码时过滤令牌来解决LLM中的语言混淆问题，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前解决语言混淆的方法要么需要模型重新训练，要么无法区分有害混淆和可接受的语码转换，因此需要一种更轻量且精确的解决方案。

Method: 使用语言混淆门控(LCG)，这是一种基于规范调整自蒸馏训练的插件方法，在解码时预测适当的语言家族并仅在需要时应用掩码。

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1等多种模型上的评估显示，LCG能显著减少语言混淆，通常降低一个数量级，且不影响任务性能。

Conclusion: LCG提供了一种有效且轻量级的解决方案，能够显著减少LLM的语言混淆问题，同时保持模型性能。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: HGAdapter：一种基于超图适配器的方法，通过捕获代码中的高阶数据相关性来增强预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在代码任务中表现良好，但未能充分利用代码中潜在的高阶数据相关性，限制了性能进一步提升。

Method: 提出三种代码高阶相关性（抽象语法树家族相关性、词汇相关性、行相关性），设计token和超边生成器捕获这些相关性，改进超图神经网络架构并结合适配器调优，提出HGAdapter方法。

Result: 在多个公共数据集上的实验表明，该方法在六种语言的代码摘要和代码克隆检测任务中，不同程度地提升了PLMs的性能。

Conclusion: 引入高阶数据相关性有助于提高代码相关任务的效果，HGAdapter能够有效编码这些相关性并增强各种PLMs的性能。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出了LawChain框架来显式建模中国侵权民事案件中的法律推理过程，构建了评估基准LawChain$_{eval}$，并通过实验验证了基于LawChain的基线方法在法律推理任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法主要依赖通用推理框架，未能全面考察法律推理的细致过程，且研究多集中于刑事案件，对民事案件的建模不足。

Method: 开发了LawChain三模块推理框架，将侵权分析中的法律推理过程操作化；构建评估基准LawChain$_{eval}$；提出基于提示或后训练的基线方法。

Result: 当前大语言模型在侵权法律推理的关键要素处理上仍有不足；提出的基线方法在侵权相关法律推理中取得显著改进，并能很好地泛化到相关法律分析任务。

Conclusion: 显式建模法律推理链能有效增强语言模型的推理能力，LawChain框架在法律推理任务中具有重要价值。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文提出了一种改进的遗忘学习方法，在保持原有遗忘效果的同时，恢复了模型在提示中包含被遗忘信息时的上下文利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法评估只关注目标知识的遗忘程度和保留集性能，但忽视了模型在被遗忘信息重新出现在提示中时的上下文利用能力，这影响了模型的实用性。

Method: 在现有遗忘学习目标基础上增加一个插件项，以保持模型在上下文包含被遗忘知识时的利用能力。

Result: 实验表明该方法在保持有效遗忘和保留集性能的同时，将上下文效用恢复到接近原始水平。

Conclusion: 提出的方法解决了现有遗忘学习方法损害上下文效用的问题，实现了更全面的模型实用性。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: Qomhr'a是一个在低资源条件下开发的爱尔兰语-英语双语大语言模型，通过双语持续预训练、指令微调和人类偏好对齐的完整流程构建。


<details>
  <summary>Details</summary>
Motivation: 在爱尔兰语资源有限的情况下，开发能够同时保持英语能力并提升爱尔兰语性能的双语模型。

Method: 混合和整理新获取的爱尔兰语语料库与英语文本；使用Gemini-2.5-Pro合成指令微调和人类偏好数据集；进行双语持续预训练、指令微调和人类偏好对齐。

Result: 在翻译、性别理解、主题识别和世界知识等基准测试中，爱尔兰语性能提升达29%，英语性能提升达44%；指令跟随能力明显进步。

Conclusion: Qomhr'a模型在低资源条件下成功开发，显著提升了爱尔兰语性能同时保持英语能力，为双语模型开发提供了有效方法。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种对话分析方法来评估教育应用中LLM的有效性，重点关注学习者与LLM的互动策略而非单纯的技术性能。


<details>
  <summary>Details</summary>
Motivation: 现有教育应用评估方法主要关注技术性能或学习成果，忽视了学习者与LLM之间的互动过程，需要填补这一研究空白。

Method: 采用对话分析方法，包括对话数据收集、对话行为标注、对话模式挖掘和预测模型构建四个步骤。

Result: 初步研究结果展示了该方法在识别有效教学策略方面的潜力，为未来研究奠定了基础。

Conclusion: 评估基于LLM的教育应用需要重点关注对话动态和教学策略，对话分析方法为此提供了有效途径。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: QueST框架通过难度感知图采样和拒绝微调生成具有挑战性的编程问题，显著提升语言模型在竞争性编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限（仅数千到数万问题），且依赖人工标注数据，限制了语言模型的可扩展性。需要大规模、具有挑战性的编程问题训练数据来提升模型推理能力。

Method: 提出QueST框架，结合难度感知图采样和难度感知拒绝微调，直接优化专用生成器来创建具有挑战性的编程问题。使用生成的问题进行知识蒸馏（从强教师模型）或强化学习（对小模型）。

Result: QueST生成器在创建挑战性问题方面优于GPT-4o。使用100K QueST生成的问题微调Qwen3-8B-base后，在LiveCodeBench上超越原始Qwen3-8B性能。添加28K人工问题与合成解决方案后，8B模型性能匹配671B的DeepSeek-R1。

Conclusion: 通过QueST生成复杂问题为推进大型语言模型在竞争性编程和推理方面的前沿提供了有效且可扩展的方法。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出轻量级少样本NER框架，通过改进指令调优模板和数据增强技术，在低资源场景下实现与最先进模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源场景下NER任务标注数据不足的问题，现有零样本和指令调优方法在领域特定实体上泛化能力不足，且无法有效利用有限数据。

Method: 1) 新的指令调优模板，简化输出格式以利用大语言模型的大上下文窗口；2) 战略性数据增强技术，在保持实体信息的同时对上下文进行改写，扩展训练数据而不破坏语义关系。

Result: 在基准数据集上，少样本方法在CrossNER数据集上平均F1分数达80.1，使用改写方法训练的模型比基线版本F1分数提升最高17分。

Conclusion: 为拥有有限NER训练数据和计算资源的群体提供了有前景的解决方案。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了AcademicEval，一个基于arXiv论文的实时长文本生成评估基准，包含标题、摘要、引言和相关工作四个学术写作任务，无需人工标注且能避免标签泄露问题。


<details>
  <summary>Details</summary>
Motivation: 当前长文本LLM基准存在上下文长度固定、标注成本高以及在LLM训练中标签泄露的问题，需要一种更有效的评估方法。

Method: 使用arXiv论文构建学术写作任务，集成高质量专家策划的少样本演示，支持灵活上下文长度，并采用实时评估机制。

Result: LLMs在具有层次抽象级别的任务上表现不佳，且难以处理长少样本演示，凸显了该基准的挑战性。

Conclusion: AcademicEval为评估LLMs的长文本建模能力提供了有效基准，并通过实验分析揭示了增强LLMs长文本建模能力的见解。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出一种基于二元检索增强奖励的在线强化学习方法，有效减少语言模型的外在幻觉，在保持其他任务性能的同时显著提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缓解语言模型外在幻觉的方法往往会在开放式生成和下游任务上造成性能下降，限制了实际应用价值。

Method: 使用二元检索增强奖励的在线强化学习方法，只有当模型输出完全正确时才给予奖励1，否则为0。

Result: 在开放式生成中幻觉率降低39.3%；在问答任务中学会策略性弃权，在PopQA和GPQA上分别减少44.4%和21.7%的错误答案；且不损害指令遵循、数学和代码能力。

Conclusion: 二元奖励方案在提升事实准确性的同时避免了连续奖励强化学习带来的质量回归问题，具有更好的实用价值。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该调查通过自主性级别框架(L0-L3)重新构建医学大语言模型评估，将现有基准与各层级允许的操作和风险对齐，为临床应用的可靠评估提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在标准基准测试中表现良好，但这些结果向临床工作流程中安全可靠性能的转移仍面临挑战，需要更贴近实际临床使用的评估方法。

Method: 采用自主性级别框架(L0-L3)：信息工具、信息转换与聚合、决策支持、监督代理，将现有基准和指标与各层级的允许操作和风险进行对齐。

Result: 建立了基于自主性级别的评估蓝图，包括指标选择、证据收集和声明报告的方法，将评估与监管联系起来。

Conclusion: 通过以自主性为中心，该调查推动领域超越基于分数的声明，转向为真实临床使用提供可信、风险感知的证据。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文提出了FARE（基础自动推理评估器），通过大规模数据驱动的方法训练了8B和20B参数的评估器，在多个推理评估任务中超越了更大的专业评估器。


<details>
  <summary>Details</summary>
Motivation: 当前生成式评估器的研究主要关注新方法如强化学习，而忽视了大规模数据驱动的发展。本文专注于数据扩展，构建了包含250万样本的数据集来训练更有效的评估器。

Method: 使用简单的迭代拒绝采样监督微调方法，在包含5种评估任务和多个推理领域的250万样本数据集上训练8B和20B参数的FARE评估器。

Result: FARE-8B挑战了更大的专业RL训练评估器，FARE-20B为开源评估器设立了新标准，超越了专门的70B+评估器。在实际应用中，FARE-20B在MATH上达到接近oracle的性能，在RL训练中比字符串匹配验证器提升14.1%的下游模型性能。

Conclusion: 大规模数据驱动的简单SFT方法可以训练出超越更大专业评估器的模型，FARE在多个实际应用场景中表现出色，为评估器的发展提供了新的方向。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了可执行知识图谱（xKG），一种模块化知识库，通过整合技术洞察、代码片段和领域知识来提升LLM代理的AI研究复现能力，在PaperBench上实现了10.9%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码方面存在困难，主要由于背景知识不足和RAG方法无法捕捉隐藏技术细节，且缺乏结构化知识表示来支持多粒度检索和重用。

Method: 开发了可执行知识图谱（xKG），这是一个模块化可插拔的知识库，自动从科学文献中提取技术洞察、代码片段和领域特定知识。

Result: 在三个代理框架和两种不同LLM上集成xKG后，在PaperBench上实现了显著的性能提升（使用o3-mini时提升10.9%）。

Conclusion: xKG被证明是自动化AI研究复现的通用且可扩展解决方案，代码将在GitHub上发布。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: EDR是一个多智能体系统，通过主规划代理和四个专业搜索代理，结合MCP工具生态系统，实现企业级深度研究，在开放基准测试中优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 企业面临将非结构化数据转化为可操作洞察的压力，现有自主代理在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: 采用多智能体架构：主规划代理进行自适应查询分解，四个专业搜索代理（通用、学术、GitHub、LinkedIn），可扩展的MCP工具生态系统支持NL2SQL、文件分析和企业工作流，可视化代理生成数据驱动洞察，以及检测知识差距的反思机制。

Result: 在DeepResearch Bench和DeepConsult等开放基准测试中，EDR无需人工干预即优于最先进的智能体系统。

Conclusion: EDR框架和基准轨迹的发布将推动多智能体推理应用的研究发展，实现自动化报告生成、实时流式处理和无缝企业部署。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [104] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: ESCAA框架通过结构化时空理解来情境化具身智能体，核心是SGClip模型，这是一种基于CLIP的开放域可提示场景图生成模型，无需人工标注即可训练，显著提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的训练主要依赖高层级的视觉-声音-文本对，缺乏像素级视觉内容与文本语义之间的细粒度结构化对齐，这限制了具身智能体的发展。

Method: 提出ESCAA框架和SGClip模型，通过神经符号学习管道在87K+开放域视频上训练，利用视频-字幕对进行模型驱动的自监督学习和结构化推理，支持基于提示的推理和任务特定微调。

Result: SGClip在场景图生成和动作定位基准测试中表现出色，ESCAA框架持续改进开源和商业MLLMs，在两个具身环境中实现最先进性能，显著减少智能体感知错误并使开源模型超越专有基线。

Conclusion: ESCAA框架通过结构化空间-时间理解有效情境化具身智能体，SGClip模型无需人工标注即可实现高质量的开放域场景图生成，为多模态大语言模型提供了重要的细粒度对齐能力。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [105] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 提出CrossRay3D稀疏多模态检测器，通过Ray-Aware Supervision和Class-Balanced Supervision提升token表示质量，在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，且运行速度快1.84倍


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器忽视token表示质量，导致前景质量次优和性能受限。研究发现几何结构保持和类别分布是提升稀疏检测器性能的关键

Method: 提出Sparse Selector(SS)，核心模块包括：Ray-Aware Supervision(RAS)在训练阶段保持丰富几何信息；Class-Balanced Supervision自适应重加权类别语义显著性；Ray Positional Encoding解决LiDAR与图像模态分布差异

Result: 在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，运行速度比其他领先方法快1.84倍，在LiDAR或相机数据部分或完全缺失场景下表现出强鲁棒性

Conclusion: CrossRay3D通过改进token表示质量，在保持稀疏检测器计算效率优势的同时，显著提升了检测性能，证明了几何结构保持和类别平衡监督的重要性

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [106] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 提出一个基于CCTV监控的智能城市基础设施缺陷检测系统，结合YOLO目标检测和视觉语言模型，自动生成结构化的维修行动计划。


<details>
  <summary>Details</summary>
Motivation: 智能城市基础设施监控需求增长，人工检测成本高且危险，现有自动系统通常只处理单一缺陷类型或输出非结构化结果，无法直接指导维修工作。

Method: 使用YOLO系列目标检测器进行多缺陷检测和分割，然后将检测结果传递给视觉语言模型(VLM)进行场景感知总结，生成包含事件描述、推荐工具、尺寸、维修计划和紧急警报的JSON格式结构化行动计划。

Result: 在公共数据集和捕获的CCTV片段上的实验评估表明，系统能准确识别多种缺陷并生成连贯的总结。

Conclusion: 讨论了将系统扩展到城市范围部署所面临的挑战和未来方向。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [107] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 提出IAD-GPT，一种基于多模态大语言模型的工业异常检测新范式，结合文本语义与图像信息，通过异常提示生成器和文本引导增强器提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和详细描述能力，而基于大模型的方法尚未充分激发其在异常检测任务中的潜力。

Method: 使用异常提示生成器生成详细异常提示，通过文本引导增强器增强视觉定位能力，并设计多掩码融合模块整合掩码作为专家知识。

Result: 在MVTec-AD和VisA数据集上的实验表明，该方法在自监督和少样本异常检测与分割任务中达到最先进性能。

Conclusion: IAD-GPT成功结合了多模态大语言模型的因果推理能力与工业异常检测需求，提供了一种有效的异常检测新方法。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [108] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 本研究评估了三种放射学报告模式（自由文本、结构化报告、AI辅助结构化报告）对图像分析行为、诊断准确性、效率和用户体验的影响。AI辅助结构化报告在诊断准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 结构化报告和人工智能可能改变放射科医生与影像研究的交互方式，但需要实证研究来评估不同报告模式的实际效果。

Method: 前瞻性研究，8名读者（4名新手和4名非新手）使用定制化查看器和眼动追踪系统分析35张床边胸部X光片，比较三种报告模式的表现。

Result: AI-SR的诊断准确性最高（κ=0.71），报告时间最短（25±9秒），眼动指标显示视觉注意力更集中于图像区域，且是用户首选模式。

Conclusion: 结构化报告通过引导视觉注意力改善效率，而AI预填充的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [109] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出了一个数据驱动的框架IFEF来分析和减轻图像分类中的交叉偏见，并提出了基于子组统计的BWA数据增强方法，在Open Images V7数据集上显著提升了弱势群体的准确率。


<details>
  <summary>Details</summary>
Motivation: 在非平衡数据集上训练的机器学习模型经常表现出交叉偏见——由多个属性（如对象类别和环境条件）相互作用产生的系统性错误。

Method: 引入交叉公平性评估框架(IFEF)结合定量公平性指标和可解释性工具来识别偏见模式；提出Bias-Weighted Augmentation(BWA)数据增强策略，基于子组分布统计自适应调整变换强度。

Result: 在Open Images V7数据集上，BWA将弱势类-环境交叉组的准确率提升了24个百分点，公平性指标差异减少了35%，统计显著性p<0.05。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏见提供了可复现的途径。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [110] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 本文提出了一种可微分的神经网络量化方法，支持n位量化，在ImageNet数据集上使用ResNet18进行权重量化时，仅需15个训练周期就能达到与全精度模型相差不到1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有量化方法的两大问题：1）大多数方法不可微分，在反向传播中需要手动设置导数；2）移位/对数量化方法要么避免激活量化，要么准确率较低。

Method: 提出可微分的量化方法，支持n位量化，特别是移位位量化，无需高精度乘法运算。

Result: 在ImageNet数据集上，仅权重量化时准确率与全精度模型相差不到1%；权重和激活同时量化时，准确率与SOTA方法相当，仅需15个训练周期。

Conclusion: 该方法不仅可微分且收敛性有理论保证，在保持高准确率的同时显著降低了计算和内存需求。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [111] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出StripRFNet深度学习网络，通过三个模块解决道路损伤检测的挑战：形状感知模块增强形状识别，条状感受野模块捕捉细长裂缝特征，小尺度增强模块提升小目标检测。在RDD2022基准测试中表现优异，达到最先进精度和实时效率。


<details>
  <summary>Details</summary>
Motivation: 道路网络维护对实现可持续发展目标至关重要，但道路表面损伤检测面临挑战：损伤形状多样、细长裂缝难以捕捉、小尺度损伤识别错误率高。

Method: StripRFNet包含三个核心模块：1) 形状感知模块(SPM)使用大分离核注意力增强形状判别；2) 条状感受野模块(SRFM)采用大条状卷积和池化捕捉细长裂缝特征；3) 小尺度增强模块(SSEM)利用高分辨率P2特征图、专用检测头和动态上采样提升小目标检测。

Result: 在RDD2022基准测试中，中国子集的F1分数、mAP50和mAP50:95分别比基线提高4.4、2.9和3.4个百分点；完整数据集上达到80.33%的最高F1分数，优于CRDDC'2022参与者和ORDDC'2024第二阶段结果，同时保持有竞争力的推理速度。

Conclusion: StripRFNet在道路损伤检测中实现了最先进的精度和实时效率，为智能道路维护和可持续基础设施管理提供了有前景的工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [112] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms是一种通过对象特定变换来量化和减少基于视觉的目标检测中不确定性的技术，在训练和推理时分别使用颜色空间扰动和扩散模型来提升鲁棒性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中基于视觉的目标检测神经网络容易受到数据偏差和分布偏移等不确定性因素的影响，需要可靠的不确定性量化方法来确保安全决策。

Method: 在训练时对单个对象进行颜色空间扰动以增强对光照和颜色变化的鲁棒性，并使用扩散模型生成多样化的行人实例；在推理时对检测到的对象应用扰动，利用检测得分的方差来实时量化预测不确定性。

Result: 在NuImages 10K数据集上的YOLOv8实验显示，该方法在所有对象类别上都带来了显著的准确率提升和不确定性减少，在推理时对假阳性预测了比真阳性更高的不确定性值。

Conclusion: ObjectTransforms作为一种轻量级但有效的机制，在训练时减少不确定性，在推理时量化不确定性，在基于视觉的感知系统中具有重要潜力。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [113] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是一个使用Aria Gen 2眼镜采集的自我中心多模态开放数据集，包含五个主要场景的原始传感器数据和机器感知算法输出。


<details>
  <summary>Details</summary>
Motivation: 提供及时可访问的自我中心多模态数据集，展示设备感知佩戴者、环境和交互的能力。

Method: 使用Aria Gen 2眼镜采集主要对象Dia'ane及其朋友的日常活动数据，涵盖清洁、烹饪、进食、玩耍和户外步行五个场景。

Result: 数据集包含全面的原始传感器数据和各种机器感知算法输出，展示了设备在不同用户和条件下的稳健性能。

Conclusion: A2PD数据集已在projectaria.com公开提供，并附带开源工具和使用示例。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [114] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出了一种基于通用引导的训练无关方法，用于将外观（图像或文本）转移到3D资产上，通过周期性添加可微分损失函数引导来改进采样过程，在几何差异大的情况下仍能成功转移纹理和几何细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入和外观对象的几何形状差异较大时效果不佳，直接应用3D生成模型无法产生令人满意的结果，需要一种更有效的外观转移方法。

Method: 使用预训练的整流流模型，在采样过程中周期性添加引导，引导可建模为可微分损失函数，包括部分感知损失和自相似性损失。

Result: 方法成功将纹理和几何细节转移到输入3D资产，在定性和定量评估中均优于基线方法，并通过GPT-based系统和用户研究验证了效果。

Conclusion: 该方法具有通用性，可扩展到不同类型的扩散模型和引导函数，为外观转移任务提供了有效的解决方案。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [115] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的自监督框架，通过回归任务自动分类骨骼标志点，旨在提高缺血性卒中血栓切除术的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是治疗缺血性卒中的有效方法，但资源密集且对人员要求高。希望通过深度学习自动化关键环节来提升效率和安全性。

Method: 采用自监督学习框架，使用基于回归的前置任务来分类各种骨骼标志点。

Result: 实验表明该模型在回归和分类任务上均优于现有方法，位置前置任务显著提升了下游分类性能。

Conclusion: 该方法有效提升了血栓切除术的自动化水平，未来工作将扩展至完全自主的C臂控制，优化从骨盆到头部的轨迹规划。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [116] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch是一种用于医学图像分割的双分支半监督框架，通过异步优化编码器和解码器，结合解耦dropout扰动、成对CutMix交叉引导和一致性匹配来提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据有限，半监督学习具有吸引力。教师-学生框架虽流行但联合优化可能导致收敛和稳定性问题，特别是在挑战性场景中。

Method: 提出DuetMatch双分支框架，异步优化编码器或解码器；引入解耦dropout扰动增强正则化；设计成对CutMix交叉引导提升模型多样性；提出一致性匹配减少噪声伪标签的确认偏差。

Result: 在ISLES2022和BraTS等脑MRI分割基准数据集上的广泛实验表明，DuetMatch持续优于最先进方法。

Conclusion: DuetMatch在多样化半监督分割场景中展现出有效性和鲁棒性。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [117] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出了一种自主导航C臂到预定义解剖标志的管道，利用X射线图像预测3D位移向量，结合不确定性估计和保形预测确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床工作流程依赖手动对齐C臂，这会增加辐射暴露和手术延迟，需要自动化解决方案来提高效率和安全性。

Method: 使用X射线图像预测3D位移向量，结合概率损失和骨骼姿态正则化，采用保形预测校准不确定性，生成3D置信区域。

Result: 在DeepDRR生成的合成X射线数据集上验证，显示出强大的定位准确性和良好校准的预测边界。

Conclusion: 该管道有潜力成为安全可靠自主C臂系统的组成部分，能够减少辐射暴露并提高手术效率。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [118] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出一个成本节约公式，用于评估图像质量自动预筛选在生成图像生产流程中的经济效益，并在背景修复用例中验证了51.61%的成本节约。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型生成的图像质量仍无法完全达到传统摄影标准，需要人工图像质量评估，但这个过程成本高、效率低。通过自动预筛选可以降低人工评估的工作量。

Method: 开发了一个成本节约计算公式，该公式基于自动图像质量评估引擎的精度和通过率来估计成本节约。在背景修复用例中使用AutoML解决方案进行验证。

Result: 在背景修复用例中，使用简单的AutoML解决方案实现了51.61%的成本节约，显著降低了获得高质量图像的平均成本。

Conclusion: 自动图像质量预筛选可以有效降低生成图像生产流程的成本，提出的成本节约公式为评估此类系统的经济效益提供了实用工具。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [119] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 本文提出PRISM模型，将fMRI信号投射到结构化文本空间作为中间表示，用于视觉刺激重建。研究发现fMRI信号更接近语言模型的文本空间而非视觉空间，并强调需要捕捉视觉刺激的组合性特征。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习的重要挑战。从fMRI信号重建视觉刺激需要找到最佳的潜在空间表示，但目前不清楚哪种潜在空间最适合这种转换以及如何有效组织。

Method: 提出PRISM模型：1）将fMRI信号投射到结构化文本空间；2）包含以对象为中心的扩散模块，通过组合单个对象生成图像以减少对象检测错误；3）属性关系搜索模块自动识别与神经活动最匹配的关键属性和关系。

Result: 在真实世界数据集上的广泛实验表明，该框架优于现有方法，感知损失降低高达8%。

Conclusion: 使用结构化文本作为中间空间来桥接fMRI信号和图像重建具有重要意义，fMRI信号更接近文本空间而非视觉空间，需要捕捉视觉刺激的组合性特征。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [120] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提出数据为中心的人工智能方法来解决热带农业遥感制图的挑战，包括数据质量差、标注成本高、数据变异性和区域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感面临高云量、多样化作物日历和有限数据集等独特挑战，传统模型中心方法效果有限，需要转向数据中心的解决方案。

Method: 采用数据为中心的人工智能管道，重点应用自信学习、核心集选择、数据增强和主动学习等技术，并推荐了25种策略和9种最成熟的方法。

Result: 开发了适合热带农业动态现实的数据中心方法管道，能够提高模型鲁棒性和可扩展性。

Conclusion: 数据为中心的方法为热带农业大规模制图项目提供了实用解决方案，通过数据质量和整理来驱动模型性能提升。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [121] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种名为StretchySnake的灵活训练方法，通过动态采样不同时空分辨率的视频并插值模型权重，使状态空间模型能够无缝处理从短片段到长活动的各种视频，在动作识别任务中优于transformer和SSM基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解训练方法主要针对transformer设计，未能充分利用状态空间模型的独特属性。固定分辨率和视频长度的训练导致模型在未见时空分辨率上性能下降，限制了模型处理短时和长时视频的能力。

Method: 提出灵活训练方法：在训练期间采样不同时空分辨率的视频，并动态插值模型权重以适应任意时空尺度。比较了五种灵活训练变体，确定了最适合视频SSM的策略。

Result: 在短动作（UCF-101、HMDB-51）和长动作（COIN、Breakfast）基准测试中，StretchySnake比transformer和SSM基线模型性能提升高达28%，在细粒度动作数据集（SSV2、Diving-48）上表现出强大的适应性。

Conclusion: 该方法提供了一个简单的即插即用训练方案，使视频SSM在各种动作识别场景中更加鲁棒、分辨率无关且高效。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [122] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出VM-BeautyNet，一种融合Vision Transformer和Mamba视觉模型的异构集成架构，用于面部美颜预测任务，在SCUT-FBP5500数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型难以捕捉全局面部特征，而ViT虽然能建模长距离空间关系但存在二次复杂度问题，需要一种能同时兼顾全局特征建模和计算效率的方法。

Method: 采用异构集成架构，将ViT主干（擅长捕捉全局面部结构和对称性）与基于Mamba的视觉模型主干（线性复杂度下高效建模长距离依赖关系）进行协同融合。

Result: 在SCUT-FBP5500数据集上，VM-BeautyNet取得了皮尔逊相关系数0.9212、平均绝对误差0.2085和均方根误差0.2698的优异性能。

Conclusion: 该工作为计算美学提供了一个强大的新架构范式，通过Grad-CAM可视化分析证实了两个主干网络的互补特征提取能力。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [123] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 使用卷积神经网络和图像采集硬件系统来早期检测口腔鳞状细胞癌，通过分析不同分辨率图像的预测准确性，发现图像分辨率越高预测精度越高但存在边际效应递减。


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌由于早期症状不明显、生长位置隐蔽且生长缓慢，往往难以早期发现，导致可预防的死亡。需要开发有效的早期检测方法。

Method: 训练了一个包含4293张图像的CNN模型，涵盖良性肿瘤、恶性肿瘤和阴性样本。设计了图像采集处理硬件系统，测试了5种常见分辨率下的预测准确性。

Result: 图像分辨率增加时预测准确性按对数比例提高，但高像素数存在边际效应递减。开发的硬件系统能够捕获详细图像，应用程序实现了CNN的开放访问。

Conclusion: CNN结合图像采集硬件系统能够有效检测口腔鳞状细胞癌，图像分辨率对预测准确性有重要影响，但超过一定阈值后改善效果有限。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [124] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个包含500小时3D运动数据的多模态数据集，来自439名参与者，包含超过5400万帧的3D运动追踪数据。


<details>
  <summary>Details</summary>
Motivation: 创建大规模、多模态的3D人体运动数据集，支持人体行为分析和交互研究。

Method: 在多摄像头采集环境中收集数据，包括单人运动、手势、移动，以及多人的行为对话数据。

Result: 提供了包含手部追踪、身体形态、文本标注和独立音频轨道的完整数据集。

Conclusion: Embody 3D数据集为人体运动和行为研究提供了丰富的多模态资源。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [125] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出了一种主动场景分解与重建方法，利用人-物交互线索在线迭代分解和重建动态环境，解决静态物体级重建的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 人类行为是场景动态的主要来源，包含丰富的动态线索。静态物体级重建存在固有模糊性，需要利用人类与物体的交互意图来动态优化分解重建过程。

Method: 基于高斯泼溅技术，通过观察人-物交互来迭代分解和重建环境，整合相机位姿估计、物体位姿估计、实例分解和在线地图更新等任务。

Result: 在多个真实场景中验证了有效性，实现了准确一致的动态场景建模，具有逼真高效的渲染效果。

Conclusion: 该方法为传统物体级重建方法提供了灵活、渐进式的替代方案，能够利用第一人称视角直播中的人-物交互线索进行动态环境建模。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [126] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus是一个用于实时视频异常检测的两级级联系统，通过离线学习正常行为规则，结合轻量级过滤和细粒度视觉语言模型推理，实现高效准确的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的视频异常检测方法虽然具有优越的零样本检测能力，但计算成本巨大且视觉定位性能不稳定，阻碍了实时部署。

Method: 采用两级级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理。关键创新包括运动掩码提示和基于规则的偏差检测。

Result: 在四个数据集上的评估显示，Cerberus在NVIDIA L40S GPU上平均达到57.68 fps，速度提升151.79倍，准确率达到97.2%，与最先进的VLM-based VAD方法相当。

Conclusion: Cerberus为实时视频分析提供了一个实用的解决方案，在保持高准确性的同时显著提升了检测速度。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [127] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了评估大型视觉语言模型成员推理攻击时存在的根本挑战。研究发现先前工作的高攻击成功率主要源于检测数据集构建中的分布偏差，而非真正的成员身份识别。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击评估存在偏差问题，高攻击成功率往往是由于数据集构建中的分布偏差而非真正的隐私泄露检测。需要建立一个无偏基准来准确评估攻击方法的有效性。

Method: 构建包含6000张图像的受控基准，精心平衡成员和非成员样本的分布，提供三个不同训练阶段的真实成员标签，在无偏条件下测试最先进的MIA方法。

Result: 在无偏条件下，最先进的MIA方法性能收敛到随机猜测水平，表明先前报告的高攻击成功率主要源于数据集偏差而非真正的攻击能力。

Conclusion: OpenLVLM-MIA通过提供透明无偏的基准，阐明了LVLM上MIA研究的当前局限性，为开发更强的隐私保护技术奠定了坚实基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [128] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一个无需训练的框架，通过跨图像笔画注意力机制实现参考风格的笔画属性精确迁移，同时保持语义结构和内容保真度。


<details>
  <summary>Details</summary>
Motivation: 生成参考风格引导的草图需要精确传输笔画属性（如线宽、变形、纹理稀疏度），同时保持语义结构和内容保真度。

Method: 提出跨图像笔画注意力机制嵌入自注意力层，建立细粒度语义对应关系；开发自适应对比度增强和语义聚焦注意力来强化内容保留和前景强调。

Result: Stroke2Sketch能有效合成风格忠实的草图，与手工制作结果高度相似，在表达性笔画控制和语义连贯性方面优于现有方法。

Conclusion: 该方法无需训练即可实现准确的笔画属性迁移，在保持结构完整性的同时自适应整合参考笔画特征。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [129] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文系统研究了深度伪造检测的缩放定律，发现检测误差随真实图像域数量和深度伪造方法数量的增加呈幂律衰减，类似于大语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 由于现有数据集无法满足研究规模需求，需要构建大规模数据集来系统分析深度伪造检测任务中的缩放规律。

Method: 构建了ScaleDF数据集（包含580万真实图像来自51个域，880万伪造图像来自102种方法），分析模型性能与真实域数量、伪造方法数量和训练图像数量的关系。

Result: 观察到检测误差随真实域数量或伪造方法数量的增加呈幂律衰减，可以预测达到目标性能所需的额外资源。

Conclusion: 缩放定律为对抗不断进化的深度伪造技术提供了数据中心的解决方案，同时探讨了预训练和数据增强在缩放中的作用及其局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [130] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT是一个新的扩散框架，通过分层局部注意力和低分辨率全局引导，实现了超高清图像的高效生成，支持4K分辨率而无需额外高分辨率训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受限于注意力机制的二次复杂性和缺乏原生4K训练数据，无法生成超高清图像，需要解决细粒度纹理合成和全局结构一致性的挑战。

Method: 采用分层局部注意力机制，将高分辨率潜在空间划分为固定大小的局部窗口以降低计算复杂度，同时使用低分辨率潜在空间注入全局语义，并通过轻量级LoRA适配器连接全局和局部路径。

Result: Scale-DiT相比密集注意力基线实现了2倍以上的推理速度提升和更低的内存使用，能够可靠扩展到4K×4K分辨率，在定量指标和定性比较中均表现出优越性能。

Conclusion: 分层局部注意力配合引导性低分辨率锚点是推进超高清图像生成的有效方法，在保持全局一致性和局部细节方面表现出色。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [131] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX是一个云边协同框架，通过轻量级设备端扩散模型快速生成预览图像，高容量云模型进行最终精修，动态平衡计算负载，在保持图像质量的同时显著减少生成时间。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成过程计算密集，用户需要多次迭代优化提示词，增加了延迟和云资源负担。

Method: 提出云边协同框架，设备端轻量模型快速预览，云端高容量模型最终精修，引入噪声水平预测器动态平衡计算负载。

Result: 相比Stable Diffusion v1.5平均生成时间减少15.8%，图像质量相当；相比Tiny-SD仅慢0.9%但图像质量显著提升。

Conclusion: DiffusionX在最小开销下展示了效率和可扩展性，有效解决了扩散模型生成延迟和资源消耗问题。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [132] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: TokenAR框架通过token级增强机制解决多参考图像生成中的身份混淆问题，包含token索引嵌入、指导token注入和身份token解缠策略，显著提升身份一致性和背景重建质量。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在条件图像生成中表现出色，但在多参考生成中难以解耦不同参考身份，存在身份混淆问题。

Method: 提出TokenAR框架，包含三个token级增强组件：token索引嵌入聚类相同参考图像的token；指导token注入作为额外视觉特征容器；身份token解缠策略指导token独立表示每个身份特征。

Result: 实验验证该方法在多个参考图像生成任务中超越当前最先进模型，实现良好的身份一致性和高质量背景重建。

Conclusion: TokenAR框架有效解决了多参考图像生成中的身份混淆问题，并发布了首个开源的大规模多参考输入数据集InstructAR，包含28K训练对。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [133] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 该研究发现多模态语言模型（MLLM）的训练策略（特别是强化学习RL vs 监督微调SFT）不仅影响下游任务性能，还从根本上重塑了视觉编码器的表示能力。RL能产生更强且更精确定位的视觉表示，作者据此提出了PIVOT方法，能以极低计算成本构建更强的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究过度关注LLM主干而忽视了视觉编码器的作用，特别是在训练范式从SFT转向RL的背景下，缺乏对视觉编码器如何被重塑的分析。

Method: 通过多种深度实验分析视觉编码器，包括ImageNet分类、分割和梯度可视化，比较SFT和RL训练策略对视觉表示的影响，并提出了PIVOT优化方法。

Result: RL相比SFT在强视觉相关的VQA基准上表现更优，能产生更强且更精确定位的视觉表示。PIVOT训练的视觉编码器性能超越更大规模训练的对手，计算成本不到标准视觉预训练的1%。

Conclusion: 训练策略对MLLM的视觉表示有根本性影响，RL优于SFT。PIVOT方法为MLLM视觉骨干的发展提供了高效路径，能以极低成本获得显著性能提升。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [134] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [135] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 提出了首个专门针对社交制造场景的异常检测基准数据集MIRAD，该数据集捕捉了个性化产品的多样性、地理分布性和成像异质性三大关键维度，并通过SOTA方法评估揭示了真实世界个性化生产中缺陷检测的未解挑战。


<details>
  <summary>Details</summary>
Motivation: 社交制造模式带来了质量控制的重大挑战，主要困难包括：产品高度定制化配置、生产涉及碎片化小批量订单、分布式站点成像环境差异大。现有数据集和算法难以应对这些真实世界的复杂性。

Method: 构建MIRAD数据集，包含三个关键维度：(1) 多样化个性化产品，具有大的类内差异；(2) 来自六个地理分散制造节点的数据；(3) 显著的成像异质性，包括光照、背景和运动条件变化。然后对SOTA异常检测方法进行广泛评估。

Result: 所有模型在MIRAD上的性能相比传统基准都显著下降，突显了真实世界个性化生产中缺陷检测的未解决复杂性。

Conclusion: MIRAD通过连接工业需求和学术研究，为开发工业5.0所需的稳健质量控制解决方案提供了现实基础，数据集已公开可用。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [136] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出了一个包含3000个白内障手术视频的数据集，具有四个标注层：手术阶段、实例分割、器械-组织交互追踪和技能评分，并进行了基准实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏多样性和标注深度，无法训练泛化性强的深度学习模型。

Method: 收集来自两个手术中心的3000个白内障手术视频，包含四个标注层：时间手术阶段、器械和解剖结构实例分割、器械-组织交互追踪、基于ICO-OSCAR标准的技能评分。

Result: 通过基准实验验证了数据集在手术工作流识别、场景分割和自动技能评估等关键任务上的技术质量，并建立了领域适应基线。

Conclusion: 该数据集为白内障手术AI系统开发提供了高质量、多样化的训练资源，支持多种手术AI任务的研究。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [137] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个用于印度道路坑洼实时检测、GPS地理标记和道路健康可视化的端到端自动化平台，基于YOLO模型和OpenStreetMap，支持智能治理和自动问责。


<details>
  <summary>Details</summary>
Motivation: 印度道路网络多样且维护不足，道路坑洼带来严重安全隐患和维护挑战，需要自动化解决方案来改善道路基础设施维护。

Method: 使用自标注的7000多张印度道路仪表盘摄像头图像数据集微调Ultralytics YOLO模型进行坑洼检测，结合OCR提取的时间戳和外部GPS日志进行精确定位，通过优化后端数据库管理道路段和承包商信息。

Result: 开发了完整的自动化平台，能够实时检测坑洼、精确地理标记、自动发送警报给承包商和官员，并提供可操作的网络界面分析。

Conclusion: iWatchRoadv2通过自动化完整的坑洼监测生命周期，实现了数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [138] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: 提出了Demeter，一种数据驱动的参数化植物形态模型，能够编码植物拓扑结构、形状、关节运动和变形，支持不同物种的拓扑变化，并推进作物植物建模。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型主要针对人类和动物，缺乏对植物形态的同等表达能力，需要开发能够处理植物特有形态变化的参数化模型。

Method: 构建Demeter参数化模型，编码植物拓扑、形状、关节运动和变形，收集大规模地面实况大豆数据集作为测试平台。

Result: 实验表明Demeter能够有效合成形状、重建结构并模拟生物物理过程。

Conclusion: Demeter为植物建模提供了强大的参数化表示，在作物植物建模方面具有重要应用价值。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [139] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 设计了一个轻量级框架，采用编码器-解码器架构，通过稀疏卷积、SPLite解码器和量化感知训练，在保持精度的同时显著提升AR/VR设备上的推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备的普及，边缘设备需要实时推理、低功耗和最小延迟，但现有框架难以平衡效率与性能。

Method: 在ResNet-18骨干网络上应用稀疏卷积，利用手部姿态图像的固有稀疏性；提出SPLite解码器架构；应用量化感知训练。

Result: 端到端效率提升42%；在树莓派5上解码帧率提升3.1倍；内存使用减少，精度基本保持（PA-MPJPE仅从9.0mm增加到9.1mm）；整体在树莓派5 CPU上实现2.98倍加速。

Conclusion: 该方法在保持与最先进方法相当精度的同时，显著提升了计算效率，适用于AR/VR边缘设备部署。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [140] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM是一个创新的MLLM-agent框架，通过3D高斯泼溅表示实现开放世界的基于推理的分割，无需大量3D特定后训练。


<details>
  <summary>Details</summary>
Motivation: 弥合复杂人类指令与精确3D对象定位之间的差距是视觉和机器人领域的重大挑战。现有3D分割方法难以解释模糊的基于推理的指令，而擅长此类推理的2D视觉语言模型缺乏内在的3D空间理解。

Method: 直接在3D高斯泼溅表示上执行分割，利用其渲染逼真新视图的能力。提出全局到局部空间定位策略：首先并行输入多个全局视图到MLLM进行粗粒度定位，然后合成多个对象特写视图进行细粒度局部分割。

Result: 在LERF、3D-OVS和新引入的REALM3D基准测试中，REALM在解释显式和隐式指令方面表现出色。框架还无缝支持对象移除、替换和风格转换等3D交互任务。

Conclusion: REALM展示了在3D场景中基于推理进行分割的实用性和多功能性，为复杂指令与3D对象定位的桥梁提供了有效解决方案。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [141] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL是一个利用自监督学习任务作为可验证奖励信号的新框架，用于视觉语言模型的强化学习微调，解决了传统RL方法缺乏可扩展可靠奖励机制的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视觉中心任务中往往过度依赖语言先验或在推理中使用文本捷径，缺乏对视觉证据的充分利用。传统强化学习方法由于缺乏可扩展可靠的奖励机制而难以应用于VLMs。

Method: 提出SSL4RL框架，将自监督学习目标（如图像旋转预测、掩码补丁重建）重新表述为密集、自动的奖励信号，无需人工偏好数据或不可靠的AI评估器。

Result: SSL4RL在视觉中心和视觉语言推理基准测试中显著提升性能，并通过系统消融实验确定了影响SSL4RL任务有效性的关键因素。

Conclusion: SSL4RL建立了一个使用可验证自监督目标来对齐多模态模型的通用有效范式，并在图学习中也展示了良好的泛化能力。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [142] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一种轻量级的点和线段匹配器，通过注意力线消息传递机制实现高效匹配，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统的点和线匹配被视为独立任务，虽然GlueStick提出了联合匹配方法，但其复杂架构无法满足实时应用或边缘设备部署需求。

Method: 提出注意力线消息传递机制，显式地将线段连通性暴露给网络，实现节点间高效通信。

Result: 在多个基准测试中建立了新的最先进性能，代码已开源。

Conclusion: LightGlueStick通过轻量化设计实现了高效的点和线段联合匹配，适合实时应用和边缘设备部署。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [143] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了可解释深度伪造视频检测(EDVD)任务，并设计了EDVD-LLaMA多模态大语言模型推理框架，通过时空细微信息标记化和细粒度多模态思维链机制，提供可追溯的推理过程和可信解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法存在原理不透明、泛化能力不足的问题，迫切需要能够识别伪造内容并提供可验证推理解释的检测器。

Method: 1. 时空细微信息标记化(ST-SIT)提取融合全局和局部跨帧深度伪造特征；2. 细粒度多模态思维链(Fg-MCoT)引入面部特征数据作为硬约束，实现像素级时空视频定位；3. 构建可解释推理FF++基准数据集(ER-FF++set)。

Result: 大量实验表明EDVD-LLaMA在检测精度、可解释性、跨伪造方法和跨数据集场景处理能力方面表现出色且稳健。

Conclusion: 相比之前的深度伪造视频检测方法，EDVD-LLaMA提供了更可解释且优越的解决方案。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [144] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: RAVAR任务旨在通过自然语言描述识别特定人物的细粒度原子级动作。本文扩展了RefAVA数据集为RefAVA++，并提出RefAtomNet++框架，通过多层级语义对齐交叉注意力和多轨迹Mamba建模，在跨模态信息对齐和检索方面取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决传统动作识别在复杂多人场景中精确语言引导动作理解的不足，特别是针对特定人物的细粒度原子级动作识别。

Method: 提出RefAtomNet++框架，采用多层级语义对齐交叉注意力机制（包括部分关键词、场景属性和整体句子层级）和多轨迹Mamba建模，动态选择最近视觉空间token构建扫描轨迹。

Result: RefAtomNet++在RefAVA++数据集上建立了新的最先进结果，显著提升了目标人物定位和细粒度动作预测的性能。

Conclusion: RefAtomNet++通过改进的跨模态token聚合机制，在语言引导的原子视频动作识别任务中取得了优越性能，为复杂多人场景下的交互式人类动作分析提供了有效解决方案。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [145] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 本文提出了一种改进的损失函数，通过高斯边界框表示和Bhattacharyya距离来提升旋转目标检测的准确性和鲁棒性，解决了传统检测框架在旋转目标上的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉应用中，如航空影像、遥感和自动驾驶，准确高效地检测旋转目标是一个重要挑战。传统目标检测框架对轴对齐目标有效，但在处理旋转目标时由于无法有效捕捉方向变化而表现不佳。

Method: 提出使用高斯边界框表示和Bhattacharyya距离的改进损失函数，采用各向异性高斯表示来解决类正方形物体的各向同性方差问题，并集成到最先进的深度学习旋转目标检测器中。

Result: 大量实验表明，与现有方法相比，该方法在平均精度均值指标上取得了显著提升，证明了其在旋转目标检测中的有效性。

Conclusion: 该方法有潜力为旋转目标检测建立新的基准，对需要精确可靠目标定位的各种应用具有广泛意义。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [146] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种视觉提示初始化策略，通过将提示与嵌入空间中的语义信息区域对齐，并注入超越预训练子空间的新表示方向，来增强自监督模型的适应能力。


<details>
  <summary>Details</summary>
Motivation: 在大规模基础模型时代，完全微调预训练网络对每个下游任务来说资源消耗巨大。现有视觉提示调优方法在专门化提示或丰富表示空间方面存在不足，特别是在挑战性任务和数据稀缺设置中。

Method: VIPAMIN通过单次前向传播和轻量级操作，将提示与嵌入空间中的语义信息区域对齐，并注入新的表示方向。

Result: VIPAMIN在不同任务和数据集大小上持续提升性能，在视觉提示调优中达到了新的最先进水平。

Conclusion: VIPAMIN提供了一种简单有效的视觉提示初始化策略，能够显著增强自监督模型在下游任务中的适应能力。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [147] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种弱监督域自适应方法，利用稀疏点标注来改进电子显微镜图像中线粒体分割，通过多任务学习和实例感知伪标签选择策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中线粒体分割需要大量标注，无监督域自适应方法在实际应用中性能较低。为减少标注成本同时提升性能，研究利用稀疏点标注的弱监督域自适应方法。

Method: 提出多任务学习框架，联合进行分割和中心检测，采用交叉教学机制和类聚焦跨域对比学习。引入具有实例感知伪标签选择策略的分割自训练方法。

Result: 在多个挑战性数据集上的验证表明，该方法优于现有的无监督和弱监督域自适应方法，显著缩小了与全监督方法的性能差距。

Conclusion: 该方法在弱监督和无监督设置下均取得显著改进，为生物医学图像分析提供了一种高效的标注策略。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [148] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 本文提出了一种面向目标视觉语言导航的前瞻性智能体，通过Q学习从大规模无标签轨迹数据中学习场景布局和物体关系知识，结合未来信息与历史信息进行A*式搜索。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于历史信息做决策，忽略了行动的长期后果和未来影响，需要开发能够预见未来结果的前瞻性智能体。

Method: 使用Q学习训练Q模型生成描述行动潜在未来信息的Q特征，通过跨模态未来编码器将任务无关的Q特征与导航指令结合，产生反映未来前景的行动分数，结合历史分数进行A*式搜索。

Result: 在广泛使用的目标导向VLN数据集上的大量实验验证了该方法的有效性。

Conclusion: 提出的前瞻性方法能够有效探索更可能到达目的地的区域，在目标导向视觉语言导航任务中表现出色。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [149] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar是一个用于动态3D虚拟人高效传输和高质量渲染的分层高斯压缩框架，通过结构层和运动层的解耦设计，在低比特率下实现更好的压缩效率和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D高斯泼溅的压缩方法缺乏人体先验知识，导致解码端的比特率效率和重建质量不理想，阻碍了其在可流式3D虚拟人系统中的应用。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器将姿态映射到高斯）和运动层（利用SMPL-X模型紧凑地表示时间姿态变化），支持分层压缩、渐进解码和可控渲染。

Result: 实验结果表明HGC-Avatar为快速3D虚拟人渲染提供了可流式解决方案，在视觉质量和压缩效率方面显著优于现有方法。

Conclusion: HGC-Avatar通过分层设计和面部注意力机制，成功解决了动态虚拟人传输中的压缩效率和质量问题，为沉浸式通信提供了可行的技术方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [150] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实科学论文中审稿人标记不一致性的多模态基准测试，包含262个不一致案例，评估模型检测、纠正和跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么孤立单一模态，要么依赖合成错误，无法捕捉真实世界的复杂性。需要评估大模型能否可靠理解和推理科学论文的多模态复杂性。

Method: 通过多阶段流程（审稿挖掘、LLM辅助过滤和人工验证）收集真实不一致案例，设计三个任务：不一致识别、修复和配对匹配，并引入结构化JSON答案表示以减少语言偏见。

Result: 评估21个领先LMM模型，结果显示性能极低（26.1-54.2%），包括大型开源模型和专有模型都表现不佳。

Conclusion: 多模态科学推理具有挑战性，当前模型在检测和解决科学论文中的跨模态不一致性方面能力有限，需要进一步开发可信赖的科学助手。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [151] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 提出OOS-DSD方法，通过辅助学习改进缺货检测，在YOLOv8基础上增加卷积分支同时进行缺货检测、产品分割和深度估计，性能超越现有最佳方法1.8% mAP。


<details>
  <summary>Details</summary>
Motivation: 缺货检测是重要的零售验证过程，需要准确推断货架上产品的不可用性。现有方法性能有待提升。

Method: 扩展YOLOv8架构，增加卷积分支进行多任务学习：缺货检测、产品分割和深度估计。深度分支使用Depth Anything V2生成的伪标签训练，并提出深度归一化方法稳定训练过程。

Result: 实验结果显示该方法在mAP指标上超过现有最佳方法1.8%。消融研究证实辅助学习提升mAP 3.7%，深度归一化提升4.2%。

Conclusion: OOS-DSD通过多任务辅助学习和深度归一化有效提升了缺货检测性能，证明了辅助学习策略的有效性。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [152] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出一种基于图注意力网络自编码器的图像分类和检索方法，通过构建图像和类别的代表性模型来进行分类和检索。


<details>
  <summary>Details</summary>
Motivation: 开发一种代表性中心的方法，通过构建图像和类别的代表性模型来改进图像分类和检索的性能。

Method: 使用图结构表示图像及其相似关系，利用图注意力网络自编码器构建上下文感知的潜在表示，从中提取类别代表，通过比较查询图像代表与类别代表进行分类，并在识别类别内检索最相似图像。

Result: 通过实验验证了该代表性中心方法的有效性，包括与GAT自编码器和标准特征技术的比较。

Conclusion: 提出的代表性中心方法在图像分类和检索任务中表现有效，能够利用图注意力网络构建更好的上下文感知表示。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [153] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ是一种微调方法，通过添加重构和对齐两个辅助目标来增强视觉语言模型的组合推理能力，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在组合推理方面表现不佳，主要原因是文本编码器倾向于关注单个单词而非它们之间的关系，这种限制被对比训练进一步加强。

Method: READ方法在对比学习基础上添加两个辅助目标：(1) 标记级重构目标：使用冻结预训练解码器基于原始标题嵌入重构替代标题；(2) 句子级对齐目标：在嵌入空间中显式对齐改写句子。

Result: READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强传统微调基线提升高达4.1%。应用于现有CLIP变体也能提升性能。

Conclusion: 重构和对齐目标提供互补优势：重构鼓励编码器捕获标题内单词间关系，对齐确保不同表述的改写句子具有一致表示。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [154] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出GaitRDAE框架，通过动态搜索运动区域并分配自适应时间尺度来提升步态识别精度，解决了现有方法使用固定时间尺度难以建模动态变化运动区域的问题。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法使用预定义区域和固定时间尺度进行时序建模，难以适应动态变化的运动区域和特定模式，特别是当协变量影响视觉外观时。

Method: 提出区域感知动态聚合与激励框架(GaitRDAE)，包含两个核心模块：RDA模块动态搜索每个区域的最佳时间感受野，RDE模块强调学习包含稳定行为模式的运动区域，同时抑制对易受协变量影响的静态区域的关注。

Result: 在多个基准数据集上实现了最先进的性能。

Conclusion: GaitRDAE通过自动搜索运动区域、分配自适应时间尺度和应用相应注意力，有效提升了步态识别的准确性。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [155] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 该研究基于真实世界政治深度伪造事件数据库，首次系统评估了学术、政府和工业界最先进的深度伪造检测器在真实政治深度伪造内容上的表现。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散加剧了政治深度伪造带来的错误信息风险，但现有检测模型主要基于实验室合成数据训练，难以泛化到真实社交媒体上传播的政治深度伪造内容。

Method: 使用政治深度伪造事件数据库作为基准，系统评估学术、政府和工业界的最先进深度伪造检测器在真实世界政治深度伪造内容上的表现。

Result: 学术和政府检测器表现相对较差，付费检测工具性能相对较高，但所有检测器都难以有效泛化到真实政治深度伪造内容，且容易受到简单操作的影响，特别是在视频领域。

Conclusion: 需要开发政治情境化的深度伪造检测框架，以在真实世界环境中更好地保护公众。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [156] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD是一个无需训练的训练框架，通过重新加权视觉token、引入噪声衍生token和应用对抗攻击来缓解大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在跨模态任务中表现出色，但物体幻觉问题（模型产生看似合理但不准确的物体描述）仍然是一个重大挑战。本文首次将LVLM幻觉追溯到视觉编码器，并识别出三个关键问题：统计偏差、固有偏差和脆弱性。

Method: 提出SHIELD框架，采用三种策略：重新加权视觉token以减少统计偏差、引入噪声衍生token来对抗固有偏差、应用带有对比解码的对抗攻击来解决脆弱性问题。

Result: 实验表明SHIELD能有效缓解各种基准测试和LVLM家族中的物体幻觉问题，并在通用LVLM基准上表现优异，显示出广泛的适用性。

Conclusion: SHIELD是一个有效的训练免费框架，能够显著缓解LVLM中的物体幻觉问题，具有广泛的适用性和良好的性能表现。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [157] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector是一个轻量级的可插拔框架，通过端到端可学习的决策过程解决多模态大语言模型中视觉令牌压缩问题，使用可微Top-K机制和课程退火策略实现高效自适应令牌选择。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像或多图像输入时面临计算和内存瓶颈，现有令牌压缩技术受限于启发式规则，可能丢弃关键信息并存在注意力沉没问题。

Method: 提出VisionSelector评分器模块，与MLLM主干解耦，包含可微Top-K机制和课程退火策略，实现端到端可学习的令牌压缩决策过程。

Result: 仅需12.85M可训练参数，在MME基准上保持100%准确率（30%保留预算），在10%保留预算下比现有方法提升12.14%，预填充速度翻倍。

Conclusion: VisionSelector在各种压缩率下都表现出优越性能，能够自适应识别关键令牌，是高效的令牌压缩解决方案。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [158] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出一个用于实时医学图像分析的深度学习框架，集成U-Net、EfficientNet和Transformer等先进神经网络架构，通过模型剪枝、量化和GPU加速优化，在多个影像模态上实现高精度和快速推理。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理技术缺乏实时临床应用所需的精度、鲁棒性和速度，临床诊断耗时且存在变异性，需要更高效的AI解决方案。

Method: 集成U-Net、EfficientNet和Transformer等神经网络架构，采用模型剪枝、量化和GPU加速等实时优化策略，支持边缘设备、本地服务器和云基础设施的灵活部署。

Result: 在公共基准数据集上达到最先进性能：分类准确率超过92%，分割Dice分数超过91%，推理时间低于80毫秒，并提供Grad-CAM和分割叠加等可视化解释工具。

Conclusion: 该框架能显著加速诊断工作流程，减轻临床医生负担，支持在时间关键的医疗环境中可信赖的AI集成。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [159] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出基于视觉的小型无人机自主飞行系统，结合语义分割和单目深度估计，在室内环境中实现避障、场景探索和自主安全降落，无需GPS或昂贵传感器。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限平台上视觉导航的挑战，特别是在缺乏GPS信号的室内环境中，通过计算机视觉技术实现低成本、高效的自主飞行。

Method: 采用知识蒸馏框架，使用SVM教师网络生成训练数据，训练轻量级U-Net学生网络进行实时语义分割；结合自适应尺度因子算法将非度量深度预测转换为精确度量距离。

Result: 在5x4米实验室环境中测试，平均距离误差14.4厘米；30次真实环境飞行测试和100次数字孪生环境测试显示，组合方法增加监视距离、减少任务时间，保持100%成功率。

Conclusion: 该工作推进了结构化环境中基于视觉的无人机导航实践，解决了度量深度估计和计算效率挑战，可在资源受限平台上部署。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [160] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个新的多轮对话基准测试，包含647个对话，涵盖12个VLM评估基准的484个任务，使用GPT-4o作为自动评估器评估18个VLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话数据集无法充分捕捉真实世界对话场景的广度和深度，需要更全面的评估基准。

Method: 从12个VLM评估基准中提取647个对话，每个对话平均4轮，使用基于检查表的评估方法，通过GPT-4o评估37个关键方面。

Result: 即使最强的模型（如GPT-4o）在复杂多轮对话中也仅达到50%的成功率，提供完整对话上下文能显著提升较弱模型的性能。

Conclusion: MultiVerse是评估VLM多轮交互能力的有效基准，揭示了当前模型在多轮对话中的局限性，并强调了上下文学习的重要性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [161] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出了一种使用检索增强生成方法，通过图数据库和Cypher查询语言接口来连接大语言模型与3D场景图，解决自然语言在机器人系统中的落地问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将场景图序列化为文本放入LLM上下文窗口，但这种方法无法扩展到大型或复杂的3D场景图。需要一种更高效的连接方式来实现自然语言与机器人世界表示之间的连接。

Method: 使用检索增强生成方法，将3D场景图编码在图数据库中，为LLM提供Cypher查询语言作为工具接口，通过查询检索相关数据来完成语言落地任务。

Result: 在指令跟随和场景问答任务上的评估显示，相比基线方法，使用Cypher接口的方法在大规模复杂图上具有更好的扩展性，显著提高了语言落地任务的性能，同时大幅减少了场景图内容的token数量。

Conclusion: Cypher作为3D场景图的接口在大规模复杂图上具有显著优势，能够有效提升语言落地任务的性能并减少计算开销。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [162] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了针对病理学基础模型的通用可迁移对抗扰动(UTAP)，这是一种固定的弱噪声模式，能够系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型的关键脆弱性，建立高标准的模型鲁棒性评估基准，为AI在病理学中的安全可靠部署提供必要的对抗训练资源。

Method: 使用深度学习优化生成固定的弱噪声模式UTAP，该扰动具有通用性和可迁移性，能够跨不同视野和数据集应用，并能成功攻击未见过的黑盒病理学基础模型。

Result: UTAP在多个最先进的病理学基础模型上进行了系统评估，通过在输入图像中添加视觉不可察觉的固定噪声模式，导致模型性能显著下降。

Conclusion: UTAP构成了对各种新兴病理学基础模型及其应用的广泛威胁，强调了推进防御机制的必要性，并为确保AI在病理学中安全可靠部署提供了对抗训练资源。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [163] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出HYDRA方法，通过教师-学生模型和知识蒸馏技术，从三通道彩色图像重建高光谱图像，在精度和推理速度上都达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有多尺度注意力方法只能处理稀疏光谱的问题，适应现代高光谱传感器数百个通道的需求。

Method: 使用教师模型封装潜在高光谱数据，学生模型学习从自然图像到教师编码域的映射，结合新型训练方法实现高质量光谱重建。

Result: 在所有指标上达到SOTA性能，精度提升18%，在不同通道深度下推理速度均优于当前SOTA模型。

Conclusion: HYDRA方法成功克服了先前光谱重建模型的关键限制，为高光谱图像应用提供了高效准确的解决方案。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [164] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MSSR是一个双代理框架，通过构建最小充分信息集来解决VLMs在空间推理中的3D理解不足和信息冗余问题，显著提升了空间推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在空间推理中的两个基本瓶颈：基于2D预训练导致的3D理解能力不足，以及冗余3D信息引发的推理失败。

Method: 采用双代理框架：感知代理使用感知工具箱程序化查询3D场景提取充分信息，包括新颖的SOG模块；推理代理迭代精炼信息以追求最小化，在闭环中修剪冗余细节并请求缺失信息，直到构建出最小充分集。

Result: 在两个具有挑战性的基准测试中，该方法通过明确追求充分性和最小化，显著提高了准确性并实现了最先进的性能。

Conclusion: 该框架不仅提升了空间推理性能，还产生了可解释的推理路径，为未来模型提供了高质量训练数据源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [165] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出SDPA++框架，仅使用噪声OCT图像进行自监督去噪，通过自融合生成伪真实图像，然后训练集成去噪模型提升图像质量。


<details>
  <summary>Details</summary>
Motivation: OCT成像中固有的散斑噪声和临床环境限制使得获取成对干净/噪声图像数据集困难，需要开发仅使用噪声图像的自监督去噪方法。

Method: SDPA++框架：首先通过自融合和自监督去噪生成伪真实图像，然后使用基于块的策略训练集成去噪模型。

Result: 在IEEE SPS VIP Cup真实世界数据集上验证，通过CNR、MSR、TP和EP等指标显示性能提升，该数据集仅包含噪声OCT图像无干净参考。

Conclusion: 该方法在临床实践中具有改善图像质量和诊断结果的潜力，特别适用于缺乏干净参考图像的真实世界场景。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [166] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 提出了领域连接对比学习(DCCL)方法来解决领域泛化问题，通过增强类内连接性来提升模型在未见测试域上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在领域泛化(DG)场景中，直接应用对比学习会降低性能，原因是缺乏类内连接性。作者发现类分离表示虽然直观上应该改善DG，但实际效果相反。

Method: 提出DCCL方法：数据端使用更激进的数据增强和跨域正样本；模型端采用模型锚定技术利用预训练表示中的类内连接性，并结合生成变换损失。

Result: 在五个标准DG基准测试上的实验表明，DCCL在无需领域监督的情况下优于现有最先进方法。

Conclusion: DCCL通过增强类内连接性有效解决了对比学习在领域泛化中的性能下降问题，为DG提供了有效的解决方案。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [167] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一个基于一致性模型的一步式人体运动预测框架，通过单步生成实现高效推理，相比扩散模型减少多达两个数量级的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型需要多步去噪过程，推理效率较低。HumanCM旨在开发一种高效的一步式运动预测方法，保持准确性的同时大幅提升推理速度。

Method: 采用基于Transformer的时空架构，结合时间嵌入来建模长程依赖关系并保持运动连贯性，学习噪声和干净运动状态之间的自一致映射。

Result: 在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM达到了与最先进扩散模型相当或更优的精度，同时推理步骤减少了多达两个数量级。

Conclusion: HumanCM证明了一致性模型在人体运动预测任务中的有效性，能够在保持高质量生成的同时实现显著的速度提升，为实时应用提供了可能性。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [168] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出了SCENECOT框架，通过将复杂的3D场景推理任务分解为更简单的问题，并基于多模态专家模块构建视觉线索，实现了类似人类的逐步推理。开发了首个大规模接地CoT推理数据集SCENECOT-185K，在多个3D场景推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的3D大语言模型在接地问答方面仍存在困难，主要原因是缺乏对人类场景-对象接地推理机制的深入探索。本文旨在填补这一空白。

Method: 提出了接地思维链推理方法SCENECOT，将复杂推理任务分解为更简单的问题，利用多模态专家模块构建视觉线索。开发了包含18.5万高质量实例的SCENECOT-185K数据集。

Result: 在多个复杂3D场景推理基准测试中，新框架实现了强大的性能，并具有高度的接地问答一致性。这是首次成功将CoT推理应用于3D场景理解。

Conclusion: 该框架实现了逐步的人类式推理，并显示出扩展到更广泛3D场景理解场景的潜力，是CoT推理在3D场景理解中的首次成功应用。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [169] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM是一个隐式残差世界模型，专注于建模世界的当前状态和演化，通过仅预测变化量而非完整重建未来场景来提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统中的视觉中心世界模型存在效率问题，它们完全重建未来场景，耗费大量能力在冗余的静态背景建模上。

Method: 1. 从视觉观察建立鲁棒的鸟瞰图表示当前状态；2. 利用前一时刻BEV特征作为强时序先验，仅预测基于自车动作和场景上下文的变化量（残差）；3. 使用对齐模块校准语义和动态错位以减轻误差累积；4. 研究不同的预测-规划耦合方案。

Result: 在nuScenes基准测试中，IR-WM在4D占用预测和轨迹规划方面均达到顶级性能。

Conclusion: 隐式残差世界模型通过专注于变化建模而非完整重建，显著提高了自动驾驶系统的效率和规划准确性。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [170] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer是一种基于UKAN架构的语义分割模型，通过全局-局部Transformer模块在噪声监督下实现珊瑚礁高精度制图，在珊瑚类IoU和像素精度上优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有全球珊瑚礁分布产品（如Allen Coral Atlas）存在空间精度和语义一致性不足的问题，特别是在需要精细边界划分的区域。需要开发能够在噪声监督下实现高精度制图的模型。

Method: 在UKAN架构基础上，解码器中引入全局-局部Transformer模块，能够同时提取全局语义结构和局部边界细节，在Allen Coral Atlas的噪声标签下进行训练。

Result: UKANFormer在珊瑚类IoU达到67.00%，像素精度达到83.98%，在相同噪声标签设置下优于传统基线方法，生成的预测在视觉和结构上都比训练用的噪声标签更准确。

Conclusion: 模型架构设计可以缓解标签噪声的影响，支持在非完美监督下进行可扩展制图，为生态监测提供了可靠标签稀缺情况下的解决方案。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [171] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个关于具身AI中世界模型的统一框架，包括问题形式化、学习目标和三轴分类法，系统化整理了数据资源和评估指标，并指出了当前面临的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器，能够捕捉环境动态，支持感知、预测和决策制定。

Method: 提出了一个三轴分类法：(1)功能：决策耦合vs通用目的；(2)时间建模：序列模拟与推理vs全局差异预测；(3)空间表示：全局潜在向量、令牌特征序列、空间潜在网格和分解渲染表示。

Result: 系统化整理了机器人、自动驾驶和通用视频设置中的数据资源和指标，包括像素预测质量、状态级理解和任务性能，并对最先进模型进行了定量比较。

Conclusion: 指出了关键开放挑战：统一数据集的稀缺性、需要评估物理一致性而非像素保真度的指标、模型性能与实时控制计算效率之间的权衡，以及实现长期时间一致性同时减轻误差积累的核心建模困难。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [172] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 本文证明离散自回归模型比连续扩散模型更适合推理时搜索优化，通过beam search使2B参数模型在文本到图像生成任务中超越12B参数扩散模型。


<details>
  <summary>Details</summary>
Motivation: 虽然推理时搜索策略在大型语言模型中取得了革命性进展，但在图像生成领域应用类似策略却面临困难，特别是连续扩散模型中的搜索效果有限。

Method: 采用离散自回归视觉模型，利用beam search进行推理时搜索优化，充分利用离散token空间的优势实现早期剪枝和计算重用。

Result: beam search显著提升文本到图像生成质量，2B参数自回归模型在多个基准测试中超越12B参数扩散模型。

Conclusion: 模型架构（而不仅仅是规模）对于视觉生成中的推理时优化至关重要，离散自回归模型为搜索策略提供了更有效的实现基础。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [173] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 该论文提出了一个基于人类感知的SR伪影显著性评估方法，构建了包含1302个伪影示例的数据集，并训练了一个轻量级回归器来生成空间显著性热图。


<details>
  <summary>Details</summary>
Motivation: 随着生成式图像超分辨率模型能力的提升，它们倾向于产生伪影。这些伪影对人类感知的影响各不相同，有些几乎不可察觉，而有些则严重降低图像质量。因此需要根据伪影对人类观察者的显著性来表征，而不是将其视为统一的二元缺陷。

Method: 构建了包含1302个来自11种当代图像SR方法的伪影示例数据集，每个伪影都配有众包显著性评分。基于此数据集训练了一个轻量级回归器，用于生成空间显著性热图。

Result: 训练的回归器在检测显著伪影方面优于现有方法，能够生成准确的空间显著性热图。

Conclusion: 该研究为SR伪影的显著性感知评估和缓解提供了数据集和工具，强调了根据人类感知重要性来评估伪影的必要性。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [174] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR是一个新颖的图像修复框架，通过全局多尺度小波变换卷积扩大感受野，结合Mamba通道感知模块捕获长距离依赖关系，并使用多尺度纹理增强损失函数，有效提升纹理细节的恢复质量。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在图像修复中由于感受野小且缺乏通道特征建模，难以充分恢复精细纹理细节。

Method: 提出GMWTConvs扩大感受野提取图像特征，MCAM模块捕获通道间长距离依赖关系，MTELoss损失函数指导模型保留纹理结构。

Result: 实验表明WaMaIR在图像修复任务中优于现有先进方法，同时保持高效的计算性能。

Conclusion: 该框架通过扩大感受野和增强通道感知能力，显著提升了图像纹理细节的恢复效果。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [175] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出Region in Context框架，通过多层级语义对齐实现文本引导的图像编辑，确保局部修改与全局场景的协调性


<details>
  <summary>Details</summary>
Motivation: 现有方法孤立处理图像区域，仅依赖局部线索，导致编辑不一致、过渡不自然或整体连贯性丧失

Method: 引入双层级引导机制：区域在完整图像上下文中表示并与详细区域级描述对齐，同时整个图像与视觉语言模型生成的场景级描述匹配

Result: 实验表明该方法能产生更连贯且与指令对齐的结果

Conclusion: 该方法通过全局上下文理解实现了更精确和协调的图像编辑

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [176] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出EMRRG框架，使用参数高效方法微调预训练的Mamba网络进行X光报告生成，在多个基准数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成模型主要依赖大语言模型，对预训练视觉基础模型和先进微调技术探索有限，且忽视了非Transformer架构如Mamba网络的潜力。

Method: 将X光图像分块标记化，通过SSM-based视觉骨干提取特征，使用Partial LoRA进行参数高效微调，结合混合解码器LLM生成报告。

Result: 在三个广泛使用的基准数据集上进行了广泛实验，完全验证了所提策略对X光医疗报告生成的有效性。

Conclusion: EMRRG框架通过微调预训练Mamba网络，实现了端到端训练，在基准数据集上取得了强劲结果，为医疗报告生成提供了新思路。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [177] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE是一种基于3D高斯散射和束调整原理的6D物体姿态估计新方法，通过可微渲染管道迭代优化姿态，在纹理缺失和光照变化条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统6D姿态估计方法依赖2D-3D特征对应，在纹理缺失物体和变化光照条件下表现不佳，需要更鲁棒的解决方案。

Method: 结合束调整原理和Lie代数，开发基于3D高斯散射的姿态可微渲染管道，通过比较输入图像与渲染图像迭代优化姿态，同时更新3DGS模型的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别实现了1.4%、2.8%和2.5%的精度提升。

Conclusion: GS2POSE通过可微渲染和光照自适应机制，有效解决了纹理缺失和光照变化下的6D姿态估计问题，显著提升了估计精度。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [178] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型的语义先验和经典机器学习算法，将视频理解重新定义为高维语义特征空间中的自监督时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据进行任务特定训练，成本高昂且扩展性有限。大型视觉语言模型在静态图像上的零样本推理能力尚未充分应用于视频领域。

Method: 使用预训练VLM的冻结视觉编码器将视频转换为语义特征轨迹，然后使用核时间分割(KTS)将连续特征流分割为离散的语义连贯事件段，最后通过无监督密度聚类识别重复出现的宏观场景和主题。

Result: 框架能够自动从每个发现的聚类中选择代表性关键帧，并利用VLM的生成能力生成文本描述，从而自动生成视频内容的结构化多模态摘要。

Conclusion: 该方法为零样本、自动化的视频内容结构分析提供了一条有效、可解释且模型无关的途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [179] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种新颖的即插即用解决方案，通过为冻结的MLLM附加轻量级可训练头部，利用注意力图中的空间线索提取关键点，实现像素级分割，同时完全保留模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要微调MLLM以产生与掩码解码器兼容的特定输出，这会改变模型的输出空间并损害其内在泛化能力，违背了构建统一模型的目标。

Method: 在完全冻结的MLLM上附加轻量级可训练头部，通过精炼注意力图中嵌入的空间线索来提取关键点，并将其描述为与掩码解码器直接兼容的点级特征。

Result: LENS实现了与基于重新训练方法相竞争甚至更优的分割性能，同时完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。

Conclusion: LENS的可附加设计为扩展MLLM建立了一个高效强大的范式，为真正多才多艺的统一模型铺平了道路。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [180] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的二元道路分割方法，利用场景几何和时间一致性来区分道路与非道路区域，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 消除对昂贵人工标注数据集的依赖，为自动驾驶提供可扩展的无监督道路分割解决方案。

Method: 基于几何先验生成弱标签（地平线上方为非道路，车辆前方四边形为道路），然后通过跨帧跟踪局部特征点并使用互信息最大化来增强时间一致性。

Result: 在Cityscapes数据集上实现了0.82的IoU，表现出高精度和简单设计。

Conclusion: 结合几何约束和时间一致性在无监督道路分割中具有巨大潜力，为自动驾驶提供了可扩展的解决方案。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [181] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种个性化图像滤镜（PIF），基于预训练的文本到图像扩散模型，通过文本反转技术学习参考图像的摄影风格，并能够有效保持内容图像的内容完整性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法从参考图像中学习有意义的摄影概念，要么无法保持内容图像的内容完整性。为了解决这些问题，需要一种能够学习摄影风格并有效转移的方法。

Method: 基于预训练的文本到图像扩散模型，使用文本反转技术优化摄影概念的提示词，学习参考图像的摄影风格。

Result: PIF在提取和转移各种摄影风格方面表现出色。

Conclusion: PIF能够有效学习摄影风格并保持内容完整性，在摄影风格转移任务中表现优异。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [182] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 构建了一个荔枝检测和成熟度分类的开源数据集，包含11,414张图像，涵盖不同品种、天气条件和成熟阶段的荔枝，并进行了详细的统计分析和深度学习模型评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中具有一致性和全面标注的荔枝开源数据集，这限制了基于视觉的荔枝采摘机器人的开发。高质量的数据对于开发此类机器人至关重要。

Method: 收集了不同品种（糯米糍、妃子笑、黑叶、怀枝）在不同天气条件和一天中不同时间的彩色（RGB）图像，包含三个成熟度阶段。数据集包括878张原始RGB图像、8,780张增强RGB图像和1,756张深度图像，共11,414张图像，标注了9,658对荔枝检测和成熟度分类标签。为确保标注一致性，三人独立标注后由第四人审核汇总。

Result: 构建了包含11,414张图像的数据集，涵盖不同荔枝品种、成熟阶段和环境条件。进行了详细的统计分析，并使用三种代表性深度学习模型评估了数据集性能。

Conclusion: 该数据集填补了荔枝检测和成熟度分类领域的数据空白，为开发基于视觉的荔枝采摘机器人提供了高质量的数据支持，已公开供学术使用。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [183] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大型公共珊瑚礁图像数据集，包含925,000个属级硬珊瑚标注，映射到世界海洋物种名录，用于促进珊瑚礁自动监测和领域泛化研究。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化等人为压力导致珊瑚礁快速衰退，急需可扩展的自动化监测方法。现有数据集在规模、地理覆盖和标签粒度方面存在局限。

Method: 整合76个CoralNet来源和红海Al Wajh站点的图像，提供细粒度分类标注。提出两种评估设置：源内基准和跨源基准测试领域泛化能力。

Result: 监督学习在源内表现良好，但跨域性能显著下降；零样本模型整体表现较差，特别是对于稀有和视觉相似属类。

Conclusion: ReefNet为领域泛化和细粒度珊瑚分类提供了具有挑战性的基准，将推动稳健、领域自适应的全球珊瑚礁监测和保护。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [184] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出了一种名为AdaptMoist的领域自适应方法，通过纹理特征预测木屑水分含量，解决了不同来源木屑数据分布变化的问题，在跨域预测中准确率提升23%。


<details>
  <summary>Details</summary>
Motivation: 现有木屑水分含量预测方法存在局限性：直接方法（烘箱干燥）处理时间长且破坏样本；间接方法（近红外光谱、电容、图像）在木屑来源多样时准确性不足。需要一种能够有效应对来源变异性的鲁棒方法。

Method: 1. 从木屑图像中提取五种不同类型的纹理特征；2. 提出AdaptMoist领域自适应方法，利用纹理特征将知识从一个木屑数据源迁移到另一个；3. 提出基于调整互信息的模型保存标准。

Result: 1. 组合所有五种纹理特征在预测水分含量时达到95%准确率，优于单个特征；2. AdaptMoist方法将跨域预测准确率从57%提升至80%，提高了23%；3. 该方法在应对不同领域变异性方面表现出色。

Conclusion: AdaptMoist方法为木屑水分含量估计提供了一个鲁棒的跨域解决方案，对依赖木屑的产业具有潜在应用价值，能够有效处理不同来源木屑的数据分布变化问题。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [185] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo框架，将人台视频转换为身份可控、逼真的人类视频，解决头身运动不协调和身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 人台服装展示成本低但缺乏真实感和表现细节，需要将人台视频转换为逼真的人类视频。

Method: 使用动态姿态感知头部编码器融合面部语义和身体姿态，引入镜像损失和分布感知适配器增强时间一致性。

Result: 在多个数据集上验证，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有方法。

Conclusion: M2HVideo框架有效解决了人台到人类视频生成中的关键挑战，实现了高质量的视频合成。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [186] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 2DGS-R通过分层训练方法提升2D高斯泼溅的渲染质量，同时保持几何精度，仅需1%额外存储和少量训练时间即可实现高质量渲染和精细几何结构。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)渲染质量高但难以准确表示表面，2D高斯泼溅(2DGS)几何保真度提升但渲染质量受损，目前无法在单一训练阶段同时优化几何和渲染质量。

Method: 采用分层训练：1) 用法向一致性正则化训练原始2D高斯；2) 选择渲染质量不足的2D高斯进行原位克隆操作；3) 冻结不透明度微调模型。

Result: 相比原始2DGS，仅需1%额外存储和最小额外训练时间，实现了高质量渲染结果同时保持精细几何结构。

Conclusion: 该方法有效平衡了效率与性能，在视觉保真度和几何重建精度方面均有提升。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [187] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer是一个轻量级基于Transformer的语义分割框架，通过集成CBAM注意力模块和MixVisionTransformer架构，在保持计算效率的同时实现武器像素级精确分割，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法只能提供粗略的边界框定位，缺乏细粒度分割能力；现有语义分割模型要么牺牲精度追求效率，要么计算资源需求过高，不适用于边缘部署场景。

Method: 结合CBAM增强的编码器骨干网络和注意力集成的hamburger解码器，采用MixVisionTransformer架构，实现五类武器（手枪、步枪、刀、左轮手枪、人体）的语义分割。

Result: 在保持实时推理速度82.26 FPS的同时，达到80.64% mIoU和89.13% mFscore的SOTA性能，仅需4.886G FLOPs和3.66M参数，比重量级模型节省高达48倍计算量。

Conclusion: ArmFormer是部署在便携安全摄像头、监控无人机和嵌入式AI加速器等分布式安全基础设施中的最优解决方案。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [188] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: 提出BARL框架，通过表示空间和标签空间的双边对齐来改进半监督医学图像分割，在减少标注成本的同时达到全监督性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法主要依赖标签空间一致性，但忽略了表示空间对齐的重要性，导致模型难以学习具有区分性和空间一致性的表示。

Method: BARL框架包含两个协作分支，在标签空间采用双路径正则化和渐进认知偏差校正，在表示空间进行区域级和病灶实例级匹配。

Result: 在四个公共基准数据集和一个私有CBCT数据集上的实验表明，BARL持续超越最先进的半监督医学图像分割方法。

Conclusion: BARL通过双边对齐策略有效提升了半监督医学图像分割性能，消融研究验证了各组件的重要性。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [189] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种基于点云配准的旋转不变特征提取框架，将配准学习与基于内存库的异常检测相结合，解决了现有方法在特征变换不一致和局部几何细节捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于内存库的3D异常检测方法存在特征变换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些限制会导致不可靠的检测结果。

Method: 提出配准诱导的旋转不变特征提取框架，将点云配准与基于内存的异常检测目标集成。通过将特征提取嵌入到配准学习过程中，联合优化对齐和表示学习。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，该方法在有效性和泛化性方面始终优于现有方法。

Conclusion: 点云配准不仅对几何结构对齐至关重要，还能指导特征提取获得旋转不变和局部判别性表示，通过集成配准与异常检测任务可以显著提升检测性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [190] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一个神经元级别的分析框架，通过人类大脑活动研究视觉语言模型的多模态信息处理机制，揭示了ANN与生物神经元在功能网络、冗余性、极性模式和架构影响等方面的相似性。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络与人类大脑处理之间相似性的理解有限：单模态ANN研究无法捕捉大脑固有的多模态处理能力，而多模态ANN研究主要关注高层模型输出，忽视了单个神经元的关键作用。

Method: 提出了一个结合精细人工神经元分析和基于fMRI的体素编码的神经元级别分析框架，研究了CLIP和METER两种架构不同的视觉语言模型。

Result: 发现了四个关键发现：ANNs能成功预测多个功能网络的生物神经元活动；ANNs和BNs都表现出功能冗余；ANNs表现出与BNs平行的极性模式；CLIP和METER的架构驱动不同的BNs激活模式。

Conclusion: 这些结果为视觉语言模型中存在类脑层次处理提供了有力证据，表明在神经元级别上存在共享的表征机制和相似的信息处理特性。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [191] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff分类诱导扩散模型，在生成皮肤镜图像的同时进行分类，提高生成图像的类别准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型难以准确生成特定医疗类别的图像，限制了在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成，实现更好的类别控制。

Result: 模型能生成更真实多样的图像，分类器性能也得到提升，适用于下游诊断任务。

Conclusion: Class-N-Diff是增强扩散模型合成皮肤镜图像质量和实用性的强大工具。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [192] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的后训练框架，通过DiffusionNFT方法和MLLM奖励模型提升指令图像编辑的泛化能力，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 基于监督微调的图像编辑模型容易过拟合到标注模式，限制了其在训练分布之外的泛化能力。

Method: 使用Diffusion Negative-aware Finetuning策略优化方法，结合多模态大语言模型作为统一奖励模型，并设计了低方差组过滤机制减少评分噪声。

Result: UniWorld-V2在ImgEdit和GEdit-Bench基准测试中分别获得4.49和7.83分，达到最先进水平，且框架具有模型无关性。

Conclusion: Edit-R1框架有效提升了指令图像编辑模型的泛化能力和性能，可广泛应用于不同类型的基座模型。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [193] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种基于地面摄像机的尾迹-航班归因框架，用于将观测到的尾迹与生成它们的航班进行关联，解决了卫星观测中空间和时间分辨率有限的问题。


<details>
  <summary>Details</summary>
Motivation: 航空业的非CO2效应（特别是尾迹）对气候影响显著，但验证和校准尾迹物理模型需要将观测到的尾迹与生成航班关联起来。卫星观测由于空间和时间分辨率有限，难以准确进行尾迹-航班归因。

Method: 利用地面摄像机在尾迹形成初期捕获高分辨率图像，结合GVCCS数据集，开发模块化框架，使用多种几何表示和距离度量，包含时间平滑处理，支持基于概率的分配策略。

Result: 建立了一个稳健的基线框架，能够有效地将地面摄像机观测到的尾迹与理论尾迹进行关联，为尾迹-航班归因研究提供了基础。

Conclusion: 该工作为未来研究尾迹与源航班关联提供了模块化框架和强基线，有助于更好地理解和量化航空尾迹的气候影响。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [194] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本文评估了四种基于Transformer的架构（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer）在热成像武器分割任务上的表现，在9,711张热成像图像数据集上取得了显著性能提升，其中SegFormer-b5达到最高mIoU（94.15%），SegFormer-b0提供最快推理速度（98.32 FPS）。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下对监控和安全应用至关重要，而传统CNN方法在捕获长距离依赖和精细结构细节方面存在局限。虽然Vision Transformers在RGB分割任务中表现出色，但在热成像武器分割领域的潜力尚未充分探索。

Method: 使用MMSegmentation框架，采用标准数据增强策略，在包含9,711张真实世界监控视频热成像图像的自定义数据集上，评估四种Transformer架构：SegFormer、DeepLabV3+、SegNeXt和Swin Transformer。数据集使用SAM2自动标注。

Result: SegFormer-b5达到最高mIoU（94.15%）和像素精度（97.04%），SegFormer-b0提供最快推理速度（98.32 FPS）和竞争性mIoU（90.84%）。SegNeXt-mscans在85.12 FPS下达到92.24% mIoU，DeepLabV3+ R101-D8达到92.76% mIoU（29.86 FPS）。

Conclusion: Transformer架构在低光照和遮挡热成像环境中展现出强大的武器检测泛化能力，提供了灵活的精度-速度权衡，适用于多样化的实时安全应用场景。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [195] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: Res-Bench是一个评估多模态大语言模型分辨率鲁棒性的基准测试，包含14,400个样本，涵盖12个分辨率级别和6个核心能力维度，引入了新的鲁棒性评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法主要关注语义性能，忽略了分辨率鲁棒性——即模型在不同输入分辨率下性能是否保持稳定。

Method: 设计了包含Spearman相关性和绝对/相对连续误差(ACE/RCE)的评估框架，对领先MLLMs进行大规模评估，包括模型和任务中心鲁棒性分析、预处理策略研究和微调稳定性增强探索。

Result: 建立了全面的分辨率鲁棒性评估基准，提供了多维度鲁棒性指标来衡量模型在不同分辨率下的性能稳定性。

Conclusion: Res-Bench填补了MLLMs分辨率鲁棒性评估的空白，为模型开发和优化提供了重要的评估工具和指导。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [196] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文对医学图像分析中的基础模型进行了系统性综述，分析了视觉专用和视觉语言基础模型的架构、训练策略和临床应用，并讨论了领域适应、高效微调等挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像分析领域快速发展，但该领域仍缺乏对架构演进、训练范式和临床应用的系统性综合梳理，需要统一的分析框架来整合现有研究成果。

Method: 采用系统性分类方法将研究分为视觉专用和视觉语言基础模型，基于架构基础、训练策略和下游临床任务进行分析，并进行定量元分析以描述数据集利用和应用领域的时间趋势。

Result: 基础模型在医学图像分析中展现出强大的零样本和少样本性能，能够通过最小微调适应各种下游临床应用，但面临领域适应、计算约束等挑战。

Conclusion: 基础模型有望加速向真实世界医疗实践的转化，未来研究应关注增强模型的鲁棒性、可解释性和临床集成能力。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [197] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出了Di-Bregman框架，通过Bregman散度密度比匹配来加速扩散模型采样，实现高效的一步生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但计算成本昂贵，现有蒸馏方法缺乏统一的理论基础。

Method: 将扩散蒸馏表述为基于Bregman散度的密度比匹配问题，提供凸分析视角。

Result: 在CIFAR-10和文生图任务上，Di-Bregman相比reverse-KL蒸馏获得更好的一步FID，保持高视觉保真度。

Conclusion: Bregman密度比匹配是实现高效一步扩散生成的实际且理论基础扎实的路径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [198] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出CARE框架，通过序列-图像对比对齐方法解决ADL识别中序列和图像表示方法的局限性，实现更好的跨表示对齐和分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有ADL识别方法存在局限性：序列方法保持时间顺序但对噪声敏感且缺乏空间感知，图像方法捕获全局模式和空间相关性但压缩时间动态并扭曲传感器布局。简单融合方法未能充分利用两种表示的互补优势。

Method: CARE框架包含：(1)时间感知、抗噪声的序列编码；(2)空间感知和频率敏感的图像表示；(3)联合对比-分类目标函数，通过序列-图像对比对齐(SICA)和交叉熵损失进行端到端学习。

Result: 在三个CASAS数据集上达到最先进性能：Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%，并展示了对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过对比对齐有效结合序列和图像表示的互补优势，为智能家居中可靠的ADL识别提供了有前景的解决方案。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [199] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 提出BaGLM方法，利用大型多模态模型的零样本能力进行在线视频步骤定位，无需训练即可超越基于训练的离线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统视频步骤定位方法需要标注训练数据和离线处理的限制，探索无需训练且能在线决策的方案。

Method: 使用LMM预测有限帧的步骤，结合贝叶斯滤波原理，通过LLM提取的依赖矩阵和步骤进度估计来注入历史帧知识。

Result: 在三个数据集上的实验表明，BaGLM优于最先进的基于训练的离线方法。

Conclusion: 证明了利用LMM的零样本能力进行在线视频步骤定位的可行性，BaGLM方法在无需训练的情况下实现了优越性能。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [200] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文通过实证研究探讨了不同视频特征对时序视频定位任务的影响，发现仅改变视频编码器就能显著影响模型性能，并揭示了特征互补的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前时序视频定位研究过于集中在少数视频表示方法上，可能导致架构过拟合。需要研究不同视频特征对经典架构的影响。

Method: 在三个基准数据集上使用基于CNN、时序推理和Transformer的不同视频编码器提取特征，并在经典架构上进行对比实验。

Result: 结果显示仅改变视频编码器就能导致模型性能的显著差异，同时揭示了使用特定特征产生的明显模式和错误。

Conclusion: 不同视频特征之间存在互补性，这为改进时序视频定位任务提供了新的方向。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [201] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 本文系统性地挑战了特定领域基础模型优于通用视觉基础模型的观念，通过设计基准测试和训练自监督模型，发现在小规模下遥感专用模型并未带来一致改进。


<details>
  <summary>Details</summary>
Motivation: 研究遥感专用基础模型是否真的比通用视觉基础模型更有用，特别是针对遥感图像的独特特征、特定应用和鲁棒性需求。

Method: 1. 设计简单基准测试，衡量遥感模型在低分辨率图像上的泛化能力；2. 在MillionAID数据集上训练iBOT自监督视觉编码器，并针对遥感进行特定修改。

Result: 在ViT-B规模下，所有预训练模型都没有在通用基线基础上带来一致的改进。

Conclusion: 至少在较小规模下，特定遥感基础模型并不比通用视觉基础模型更有用。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [202] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种利用多模态大语言模型进行细粒度视频时序定位的方法，通过两阶段处理将文本查询转化为包含更多细节的丰富句子，然后使用轻量级解码器进行精确定位。


<details>
  <summary>Details</summary>
Motivation: 利用多模态大语言模型的能力来联合处理文本和视频，有效定位视频中的自然语言查询，解决直接定位时信息不足的问题。

Method: 两阶段方法：第一阶段将语言查询转化为包含缺失细节的丰富句子；第二阶段使用轻量级解码器基于丰富查询的上下文表示预测准确边界。采用多实例学习目标动态选择最佳查询版本。

Result: 在视频时序定位和段落定位的各种基准测试中达到最先进水平，显著优于所有先前提出的基于LLM的时序定位方法，在零样本评估场景中保持明显优势。

Conclusion: ED-VTG方法在视频时序定位任务中表现出色，既优于或可与专用模型相媲美，又在零样本评估中具有明显优势，证明了多模态LLM在视频理解任务中的有效性。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [203] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出W2R2训练框架解决多模态3D定位中的2D语义偏差问题，通过解耦表示学习和针对性捷径抑制，在不改变推理架构的情况下提升3D定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D定位模型存在严重的"2D语义偏差"，过度依赖2D图像特征进行粗定位，而忽视3D几何输入，导致融合性能不佳。

Method: W2R2框架将2D特征作为"What"识别的语义信标，3D特征作为"Where"定位的空间锚点，采用双目标损失函数：对齐损失通过适应交叉熵监督融合预测，伪标签损失通过基于边界的机制惩罚2D主导的伪输出。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2显著提升了定位精度和鲁棒性，特别是在杂乱的室外场景中。

Conclusion: W2R2通过解耦表示学习有效解决了2D语义偏差问题，为多模态3D定位提供了新的训练范式。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [204] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种使用条件StyleGAN2-ADA和StyleGAN3生成高分辨率合成活体指纹，并通过CycleGANs转换为逼真假指纹的方法，解决了生物特征数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 大型指纹数据集收集耗时、昂贵且需要严格的隐私保护措施，研究人员正在探索使用合成指纹数据来解决这些问题。

Method: 使用条件StyleGAN2-ADA和StyleGAN3架构生成高分辨率合成活体指纹，并通过CycleGANs将其转换为模拟各种呈现攻击材料的逼真假指纹。

Result: 创建了两个合成数据集（DB2和DB3），每个包含1,500个指纹图像，StyleGAN3模型实现FID低至5，生成的指纹在0.01% FAR下达到99.47% TAR，且匹配实验证实了强大的隐私保护特性。

Conclusion: 该方法成功生成了高质量的合成指纹数据集，在保持强隐私保护的同时，为开发鲁棒的假指纹检测系统提供了重要资源。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [205] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一个临床医生参与循环的深度学习管道，用于肺癌CT图像分割和预后预测。VNet模型在分割性能、放射组学稳定性和预测准确性方面表现最佳，半监督学习优于监督学习，放射科医生更倾向于使用AI生成的初始掩模进行精炼。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主要原因，CT成像在筛查、预后和治疗中至关重要。手动分割存在变异性大且耗时的问题，而深度学习虽然提供自动化但面临临床采用障碍。

Method: 使用来自12个公共数据集的999名患者的多中心CT数据，评估了5种深度学习模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），并与专家轮廓进行基准测试。通过497个PySERA提取的放射组学特征评估分割可重复性，比较监督学习和半监督学习的预后建模。6名医生在7个领域对掩模进行定性评估。

Result: VNet实现了最佳性能（Dice = 0.83，IoU = 0.71）、放射组学稳定性（平均相关性=0.76，ICC=0.65）和半监督学习下的预测准确性（准确率=0.88，F1=0.83）。半监督学习在所有模型中始终优于监督学习。放射科医生更青睐VNet的瘤周表示和更平滑的边界。

Conclusion: 将VNet与半监督学习相结合，可产生准确、可重复且临床可信的基于CT的肺癌预后结果，突出了实现以医生为中心的AI转化的可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [206] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出了一种广义选择方法，通过选择不限于类质心的表示来改进行人重识别性能，在准确率和平均精度均值之间取得平衡，显著超越现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 虽然特征提取方法和目标函数改进已显著提升行人重识别性能，但选择更好的类代表是一个研究不足的领域。现有方法主要使用类质心作为代表，但效果不理想。

Method: 提出广义选择方法，不限于类质心，可选择多种表示形式。该方法允许调整每类的实际表示数量以满足特定应用需求，并在多种重识别嵌入上应用。

Result: 该方法在准确率和平均精度均值之间取得平衡，在所有测试的重识别嵌入上都显著超越了当代结果。

Conclusion: 广义表示选择方法能有效提升行人重识别性能，为选择类代表提供了更灵活和优化的解决方案。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [207] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出V-Reason方法，通过熵信号优化LMM在视频推理中的微探索和微利用行为，无需RL训练即可提升推理性能


<details>
  <summary>Details</summary>
Motivation: 现有视频推理LMM依赖昂贵的RL训练和冗长的思维链，计算开销大且思维过程控制机制有限

Method: 使用输出熵作为信号，通过小型可训练控制器优化LMM的值缓存，采用基于熵的目标函数调整推理时的探索-利用行为

Result: 在多个视频推理数据集上显著超越基础指令调优模型，与RL模型差距缩小至0.6%，输出token减少58.6%

Conclusion: 基于熵的推理时优化可有效提升LMM视频推理性能，无需额外训练且效率大幅提升

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [208] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 本文研究了通用视觉基础模型与专用模型之间的权衡，通过比较Hiera通用编码器和SAM2分割专用模型的特征适应性，量化了专业化的信息论成本。


<details>
  <summary>Details</summary>
Motivation: 理解通用基础视觉模型与专用模型之间的权衡对于高效特征编码设计至关重要，但目前尚未完全理解这一权衡关系。

Method: 使用轻量级可训练neck层来探测冻结特征的适应性，通过信息论方法量化专业化成本，并对SAM2进行跨neck分析。

Result: 结果显示SAM2在空间相关任务（如深度估计）上表现出色，但在概念较远的任务（如姿态估计和图像描述）上表现不如通用模型Hiera，表明专业化会损失更广泛的语义信息。

Conclusion: 专业化在特定任务上有效但会损失通用性，跨neck分析显示每个适应层级都会产生表示瓶颈，为设计高效特征编码和适应策略提供了量化基础。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [209] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 提出ProDAT方法，通过密度感知的尾丢弃机制实现点云的渐进式编码，支持单一模型下的多码率渐进解码，在编码效率上优于现有学习方法


<details>
  <summary>Details</summary>
Motivation: 3D点云在自动驾驶、增强现实等应用中需要实时处理和低延迟，但大数据量和带宽限制阻碍了在资源受限环境中的高质量服务部署。现有学习方法虽然成功但固定潜在表示不支持渐进解码

Method: 提出密度感知尾丢弃机制(ProDAT)，利用密度信息作为指导信号，根据重要性自适应解码潜在特征和坐标，实现单一模型下的多码率渐进解码

Result: 在基准数据集上的实验结果显示，ProDAT不仅支持渐进编码，而且在编码效率上优于最先进的学习方法，在SemanticKITTI上PSNR-D2的BD-rate提升超过28.6%，在ShapeNet上超过18.15%

Conclusion: ProDAT成功解决了点云渐进编码问题，通过密度感知机制实现了高效的渐进解码，在保持编码效率的同时支持多码率解码

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [210] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: FMCAF是一种用于RGB和红外图像融合的预处理架构，通过频域滤波和跨注意力融合提升多模态目标检测性能，在多个数据集上表现优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测在挑战性条件下通过利用多个传感器模态的互补线索来提高鲁棒性。传统方法往往针对特定数据集设计，缺乏通用性。

Method: 提出FMCAF架构，包含频域滤波块(Freq-Filter)来抑制冗余光谱特征，以及基于跨注意力的融合模块(MCAF)来改善模态间特征共享。

Result: 在LLVIP(低光行人检测)和VEDAI(航空车辆检测)数据集上，FMCAF优于传统拼接融合方法，VEDAI上mAP@50提升13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF有潜力作为未来检测流程中稳健多模态融合的灵活基础架构，具有良好的通用性。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [211] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane是一种改进高斯泼溅(Gaussian Splatting)的方法，专门针对3D场景中的平面区域进行优化，通过引入平面先验和动态高斯重分类器，显著提升平面区域的几何精度和网格拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法在重建平面区域时存在平滑度和精度不足的问题，而平面是人工环境中的重要基元，需要结构化表示以支持场景编辑和物理模拟。

Method: 利用现成的分割和法线预测模型提取平面先验，建立结构化平面高斯坐标表示；引入动态高斯重分类器自适应重新分类高梯度平面高斯；利用优化后的平面先验优化网格布局。

Result: 在不牺牲渲染质量的前提下，显著提高了提取网格的几何精度，减少了顶点和面片数量，改善了拓扑结构，并支持平面对象的解耦和灵活操作。

Conclusion: GSPlane通过引入平面先验有效解决了高斯泼溅在平面重建中的局限性，为下游应用提供了更准确和结构化的3D场景表示。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [212] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出一种针对预训练扩散模型的优化策略，通过潜在空间精炼和双向交互机制，在保持感知真实性的同时显著提升内容保真度，特别适用于低光场景的图像恢复任务。


<details>
  <summary>Details</summary>
Motivation: 现有预训练扩散方法在低层视觉任务中虽然感知质量高，但往往牺牲内容保真度，特别是在低光场景下，黑暗导致的严重信息退化限制了有效控制。主要问题包括缺乏合适的条件潜在建模以及条件潜在与噪声潜在之间缺乏双向交互。

Method: 1) 引入潜在精炼管道，利用生成先验恢复VAE编码过程中丢失的空间细节；2) 建立精炼潜在条件与噪声潜在之间的动态双向交互机制；3) 即插即用设计，可无缝集成到现有扩散网络中。

Result: 大量实验证明该方法在预训练扩散方法中实现了显著的内容保真度提升，同时保持了真实感和美学质量。

Conclusion: 提出的优化策略有效解决了预训练扩散模型在内容保真度方面的局限性，通过改进的条件机制和双向交互，在低光图像恢复等任务中取得了更好的性能。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [213] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出一种使用LED环境照明为消费级相机生成视觉不可见水印的方法，通过优化LED光源的光谱特性，使其对人眼几乎不可见但对相机高度可检测。


<details>
  <summary>Details</summary>
Motivation: 传统可见光通信需要高频调制，而本方法旨在开发一种在标准帧率下工作的不可见水印技术，支持隐私保护和内容验证。

Method: 采用光谱调制而非强度调制，联合考虑人眼视觉系统敏感性、相机传感器光谱敏感性和LED产生宽带白光的能力，优化LED光谱配置。

Result: 能够在10秒视频片段中嵌入128位信息，信息传输速率适中但足以支持基本元数据需求，水印在标准30-60fps帧率下可提取。

Conclusion: 该方法成功实现了对人眼不可见但对消费级相机可检测的水印嵌入，为隐私保护和内容验证提供了实用解决方案。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [214] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出GOOD框架，通过双层级引导机制直接指导扩散采样轨迹生成OOD样本，提升OOD检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扰动文本条件嵌入生成OOD样本，存在语义不稳定和偏移多样性不足的问题，限制了在真实OOD场景中的泛化能力

Method: 使用现成的ID分类器，结合图像级引导（基于对数分割梯度）和特征级引导（基于k-NN距离），在像素空间和特征空间驱动采样到低密度区域

Result: GOOD生成的样本能显著提升OOD检测性能，通过定量和定性分析验证了其有效性

Conclusion: GOOD框架通过双层级引导机制实现了更可控和多样化的OOD样本生成，提出的统一OOD评分增强了检测鲁棒性

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [215] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D是一个统一的框架，通过运动学感知扩散模型从单视图输入重建多样化铰接物体形状并估计姿态。


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如笔记本电脑和抽屉）由于多部件几何结构和可变关节配置，在3D重建和姿态估计方面面临挑战，这些结构在不同状态下表现出多样性。

Method: 首先通过运动学感知VAE将完整几何、关节角度和部件分割编码到结构化潜在空间；然后使用两个条件扩散模型分别回归全局姿态和关节参数，以及从部分观测生成运动学感知潜在代码；最后通过迭代优化模块双向优化重建精度和运动学参数。

Result: 在合成、半合成和真实世界数据集上的实验结果表明，该方法能准确重建铰接物体并估计其运动学特性。

Conclusion: KineDiff3D框架有效解决了铰接物体的3D重建和姿态估计问题，通过运动学感知的扩散模型实现了准确的重建和参数估计。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [216] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD是一个两阶段后训练框架，通过深度和法线图作为几何先验，结合强化学习中的组长度奖励，从单张图像生成可编辑的CAD模型，在几何精度和建模简洁性方面达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在从2D图像准确推断3D几何方面存在困难，限制了工业概念设计的应用潜力。

Method: 采用两阶段后训练框架：监督微调阶段使用深度和表面法线图作为密集几何先验；强化学习阶段引入组长度奖励来促进简洁建模序列的生成。

Result: 在DeepCAD和Fusion360数据集上的实验表明，GACO-CAD在代码有效性、几何精度和建模简洁性方面均优于现有方法。

Conclusion: GACO-CAD框架有效解决了MLLM在3D几何推理方面的局限性，为从单张图像生成高质量CAD模型提供了可行方案。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [217] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 该论文研究了人脸识别系统中预处理对对抗攻击可迁移性的影响，发现人脸检测模型的选择能显著降低攻击成功率，并提出了一种预处理不变的方法来提升攻击可迁移性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统存在对抗样本漏洞，但现有研究往往忽视了预处理环节在黑盒攻击中的重要性。论文旨在探究不同预处理技术对对抗攻击可迁移性的影响。

Method: 研究了多种最先进的对抗攻击方法在不同预处理技术下的可迁移性，分析了人脸检测模型和降采样插值方法的影响，并提出基于输入变换的预处理不变方法。

Result: 人脸检测模型选择可使攻击成功率降低高达78%，而插值方法影响较小。提出的预处理不变方法可将攻击可迁移性提升高达27%。

Conclusion: 预处理在人脸识别系统中至关重要，考虑预处理因素有助于提高面部对抗样本的对抗泛化能力。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [218] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出GtR分层采样策略，通过生成阶段建立全局语义框架，重建阶段高效补全细节，实现3.72倍加速且保持生成质量


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归模型在视觉生成中并行生成能力受限于空间相关视觉标记建模复杂性的问题

Method: 采用两阶段方法：结构生成建立全局语义框架，细节重建高效补全剩余标记；结合频率加权标记选择为图像细节分配更多计算资源

Result: 在ImageNet类条件生成和文本到图像生成任务中实现3.72倍加速，FID保持1.59，IS从299.1提升至304.4

Conclusion: GtR策略显著优于现有加速方法，在不同模型规模和生成任务中均表现出色

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [219] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文针对浮游生物识别中分布偏移问题，基于DYB-PlanktonNet数据集构建了系统性的OoD基准测试，评估了22种OoD检测方法，发现ViM方法表现最优。


<details>
  <summary>Details</summary>
Motivation: 浮游生物识别模型在真实部署中面临分布偏移挑战，由于浮游生物形态复杂、物种多样性大且新物种不断发现，导致推理时出现不可预测错误。该领域缺乏最新计算机视觉技术的系统整合和大规模评估基准。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准测试，系统评估了22种OoD检测方法。

Result: 大量实验结果表明，ViM方法在构建的基准测试中显著优于其他方法，特别是在Far-OoD场景下关键指标有显著提升。

Conclusion: 这项研究为自动浮游生物识别中的算法选择提供了可靠参考，为浮游生物OoD检测的未来研究奠定了坚实基础，是浮游生物识别领域首次大规模系统性评估OoD检测方法的研究。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [220] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出一个联合学习详细头部化身和手脸交互引起的非刚性变形的框架，解决了现有方法忽略自然手脸交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了传达认知状态（如思考）的自然手脸交互，如手托下巴或手指轻触脸颊等动作。

Method: 结合深度顺序损失和接触正则化进行姿态跟踪，学习手引起面部变形的PCA基，并引入基于物理模拟的接触损失来减少穿插伪影。

Result: 在iPhone拍摄的RGB(D)视频和合成数据集上评估，相比SOTA表面重建方法，能捕捉更好的外观和更准确的面部变形几何。

Conclusion: 该方法能够有效建模手脸交互引起的面部变形，生成物理上更合理的结果，解决了现有方法在自然交互建模方面的局限性。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [221] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC是一个双曲表示学习框架，用于解决领域泛化与广义类别发现问题，无需复杂的训练模拟，通过GPT引导的扩散增强和切线空间插值实现高效泛化。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法假设训练和测试数据来自同一领域，限制了在开放世界分布偏移场景中的应用。DG-GCD要求模型泛化到包含新类别的未见领域，但现有方法计算成本高且存在误差累积。

Method: 使用GPT引导的扩散进行领域增强，引入切线CutMix进行曲率感知插值，结合惩罚Busemann对齐、混合双曲对比正则化和自适应离群点排斥的统一损失函数，以及可学习的曲率参数。

Result: 在PACS、Office-Home和DomainNet数据集上达到最先进水平，持续优于现有的欧几里得和双曲(DG)-GCD基线方法。

Conclusion: HIDISC通过双曲表示学习实现了领域和类别级别的泛化，避免了复杂的训练模拟，在多个基准数据集上表现出优越性能。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [222] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出一种零样本的视觉token剪枝方法，通过平衡任务相关性和信息多样性，在保持性能的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理能力增强，视觉token冗余导致推理成本急剧上升，现有方法忽视文本提示的指导，无法有效识别任务相关token。

Method: 采用分层方法：首先选择任务相关的核心视觉token，然后补充多样性token以保留更广泛的上下文信息。

Result: 在多个模型和基准测试中，该方法在剪枝高达90%token的情况下，性能达到或超越最先进方法，仅产生最小精度损失，同时显著降低GPU内存占用和推理延迟。

Conclusion: 提出的提示感知视觉token剪枝方法有效平衡了任务相关性和信息多样性，在保持模型性能的同时大幅降低了推理成本。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [223] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 使用Segment Anything Model (SAM)开发了一个专门用于监测孟加拉国河流侵蚀的AI模型，通过历史Google Earth图像构建了首个包含消失定居点标注的数据集，实现了86.30%的IoU和92.60%的Dice分数的高精度侵蚀检测。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国的主要河流既是商业和生计的动脉，也是持续破坏的媒介，每年吞没整个村庄和大片农田，导致数千家庭流离失所。传统的人工分析难以有效追踪这一缓慢发生的灾难。

Method: 首先使用简单的颜色通道分析进行粗略的土地和水域分割，然后微调SAM的掩码解码器以识别河岸侵蚀的细微特征。构建了包含2003-2025年历史Google Earth图像的新数据集，首次包含消失定居点的手动标注。

Result: 开发出的模型对河流侵蚀过程具有敏锐的识别能力，平均IoU达到86.30%，Dice分数达到92.60%，显著优于传统方法和现成的深度学习模型。

Conclusion: 这项工作提供了三个关键贡献：首个孟加拉国河流侵蚀导致定居点消失的标注数据集、专门针对此关键任务微调的AI模型、以及通过视觉证据量化土地损失的方法，为政策制定者和灾害管理机构提供了监测侵蚀、预测轨迹和保护脆弱社区的有力工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [224] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 基于TimeSformer视频识别模型，通过分析VALORANT比赛录像中的小地图信息，提取战术特征来预测回合结果，准确率达到约81%。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多基于比赛日志数据和统计信息，本研究针对需要复杂策略的FPS游戏VALORANT，旨在通过分析比赛录像中的小地图信息来构建回合结果预测模型。

Method: 基于TimeSformer视频识别模型，通过分析比赛录像中的小地图信息，提取角色位置信息和其他游戏内事件的详细战术特征，并尝试将这些特征融入模型以提高预测准确性。

Result: 在增强战术事件标签的数据集上训练的模型达到了约81%的预测准确率，特别是在回合中期阶段表现尤为突出，显著优于仅基于小地图信息本身训练的模型。

Conclusion: 利用比赛录像中的战术特征对于预测VALORANT回合结果非常有效，证明了从视觉数据中提取战术信息的方法具有重要价值。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [225] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL是一个专门为内窥镜图像诊断设计的类增量学习框架，通过三个关键组件解决领域差异和类别不平衡问题，在四个公共数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析需要模型能够持续适应新的临床数据，同时保持对已学习类别的性能。现有重放方法由于内窥镜图像固有的领域差异和类别不平衡问题，无法有效缓解灾难性遗忘。

Method: EndoCIL包含三个核心组件：基于最大均值差异的重放选择策略（MDBR）、先验正则化类别平衡损失（PRCBL）和全连接层梯度校准（CFG）。

Result: 在四个公共内窥镜数据集上的广泛实验表明，EndoCIL在不同缓冲区大小和评估指标下普遍优于最先进的CIL方法。

Conclusion: 该框架有效平衡了终身内窥镜诊断中的稳定性和可塑性，显示出良好的临床可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [226] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出基于DINOv2的活体检测方法，通过注意力机制抑制扰动，有效区分真实人脸和欺骗攻击图像。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统易受欺骗攻击，攻击者可能使用注册用户的照片绕过认证，因此需要在人脸识别前检测此类欺骗攻击。

Method: 使用带寄存器的DINOv2模型提取可泛化特征，抑制注意力机制中的扰动，专注于关键细微特征。

Result: 在ICCV2025第六届人脸反欺骗研讨会数据集和SiW数据集上验证了方法的有效性。

Conclusion: 基于DINOv2的活体检测方法能够有效检测欺骗攻击，提升人脸识别系统的安全性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [227] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文提出VisiPruner训练无关的剪枝框架，通过分析MLLMs的三阶段跨模态交互过程，可减少99%视觉相关注意力计算和53.9% FLOPs，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中表现优异，但由于注意力计算的二次增长导致显著计算开销。现有方法缺乏对MLLMs如何处理和融合多模态信息的根本理解。

Method: 通过系统分析发现三阶段跨模态交互过程：浅层识别任务意图、中层关键视觉token驱动融合、深层仅关注语言精炼。基于此提出VisiPruner训练无关剪枝框架。

Result: 在LLaVA-v1.5 7B上减少99%视觉相关注意力计算和53.9% FLOPs，显著优于现有token剪枝方法，并在多种MLLMs上具有良好泛化性。

Conclusion: 研究洞察为训练高效MLLMs提供了可操作指南，通过将模型架构与其内在层间处理动态对齐来提升效率。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [228] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 该论文提出了多时刻检索(MMR)任务，创建了QV-M²数据集和FlashMMR框架，解决了现有单时刻检索方法在真实应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有时刻检索方法主要关注单时刻检索，但真实应用中一个查询可能对应多个相关时刻，现有数据集和方法不足以满足视频时序定位需求。

Method: 提出了FlashMMR框架，包含多时刻后验证模块来精炼时刻边界，通过约束时序调整和验证模块重新评估候选片段，通过精细过滤管道修剪低置信度提议。

Result: 在QV-M²数据集上，FlashMMR相比之前SOTA方法在G-mAP上提升3.00%，在mAP@3+tgt上提升2.70%，在mR@3上提升2.56%。

Conclusion: QV-M²数据集和FlashMMR方法为推进更现实和挑战性的视频时序定位场景研究奠定了基础。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [229] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出一个公平感知的深度伪造检测框架，通过时间特征学习和人口统计感知数据增强来提高公平性和可解释性，在多个数据集上实现了公平性和准确性的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度、无法捕捉时间信息，导致不同人口统计群体的决策偏见和不可靠结果。

Method: 集成时间特征学习和人口统计感知数据增强，使用序列聚类进行时间建模，概念提取提高检测可靠性，并引入人口统计感知数据增强方法平衡代表性不足群体。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上使用Xception、ResNet等先进架构进行广泛实验，证明该方法在公平性和准确性之间获得最佳平衡。

Conclusion: 所提出的公平感知深度伪造检测框架有效解决了现有方法的偏见问题，提高了检测的公平性、可靠性和可解释性。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [230] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集、整理和统一的2400万样本语料库，是同类最大的开放资源。通过半自动化、人工参与的流程统一了200多个来源，应用严格的去重和去污染处理，包含代理/GUI任务。在FineVision上训练的模型在广泛评估中表现优于现有开放混合数据集。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的发展受到碎片化、不一致和受污染的公共数据集的阻碍。为了解决这个问题，需要构建一个统一、高质量的数据集来推动VLM研究。

Method: 采用半自动化、人工参与的流程：自动化执行批量摄取和模式映射，审核人员检查映射和抽样检查输出，验证注释的忠实消费、适当格式和多样性以及安全性；发现问题时触发针对性修复和重新运行。工作流程还应用严格的源内和跨源去重，以及针对66个公共基准的去污染处理。

Result: FineVision包含2400万个样本，统一了200多个来源到185个子集。在FineVision上训练的模型在广泛评估套件中始终优于在现有开放混合数据集上训练的模型。

Conclusion: FineVision展示了规模、数据卫生以及平衡自动化与人工监督的好处。发布语料库和整理工具以加速以数据为中心的VLM研究。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [231] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出Plug-and-Forecast方法，通过多模态大语言模型增强现有运动预测模型，无需微调即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在标准条件下表现可靠，但难以经济高效地泛化到多样化现实场景。自然语言能更有效地描述和处理复杂场景。

Method: 设计提示词从MLLMs提取结构化场景理解，将其蒸馏为可学习嵌入来增强行为预测模型，利用MLLMs的零样本推理能力。

Result: 在Waymo和nuScenes数据集上验证，两个最先进的运动预测模型均获得一致的性能提升。

Conclusion: PnF方法通过MLLMs增强现有模型，实现了无需微调的性能改进，具有实际应用价值。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [232] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出SG-CLDFF框架，通过显著性引导的预处理和多尺度深度特征融合，提高白细胞分割和分类的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决白细胞显微图像分析中染色变异、复杂背景和类别不平衡等挑战，提高诊断准确性。

Method: 使用显著性先验突出白细胞区域，结合EfficientSwin风格混合主干和ResNeXt-CC跨层融合模块，多任务训练分割和分类头，采用类别加权损失和显著性对齐正则化。

Result: 在BCCD、LISC、ALL-IDB基准测试中，IoU、F1和分类准确率均优于CNN和Transformer基线模型。

Conclusion: SG-CLDFF为临床工作流程提供了实用且可解释的白细胞自动分析路径。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [233] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出基于YOLOv11的自动手术照明系统，通过检测蓝色标记自动调整LED光源位置，减少外科医生疲劳并提高照明一致性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统依赖手动调整，导致外科医生疲劳、颈部劳损以及因漂移和阴影造成的不一致照明问题。

Method: 使用YOLOv11物体检测算法识别手术部位上方的蓝色标记，通过两个带有倾斜-平移支架的伺服电机将高功率LED光源对准识别位置。

Result: YOLO模型在验证集上达到96.7% mAP@50，验证集包含带有蓝色球形标记的模拟手术场景标注图像。

Conclusion: 这种基于机器视觉的自动化照明解决方案减少了外科医生的身体负担，提高了照明一致性，有助于改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [234] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习中存在一个反直觉现象：训练时间过长会损害密集预测任务的性能，称为自监督密集退化(SDD)。作者提出了DSE指标来无监督评估密集表示质量，并基于此开发了模型选择策略和正则化方法。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习中训练时间过长反而会降低密集预测任务性能的反常现象，需要解决无监督条件下评估密集表示质量的挑战。

Method: 提出了DSE（密集表示结构估计器），包含类相关性度量和有效维度度量。基于DSE开发了模型选择策略和正则化方法。

Result: 在16种自监督方法和4个基准测试上的实验表明，模型选择平均提升mIoU 3.0%，DSE正则化能持续缓解密集退化效应。

Conclusion: DSE指标能有效评估密集表示质量，提出的方法能显著改善自监督学习在密集预测任务上的性能。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [235] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于长视频理解的基准测试，包含约1000个信息密集的长视频和6个挑战性任务场景，评估模型在视觉、音频和文本多模态融合中的表现。实验表明当前全模态模型在时间定位和长距离因果推理方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估长视频理解能力的基准测试，特别是针对包含丰富语言内容（如讲座、访谈、vlog）的视频，需要综合视觉、音频和文本多模态信息。

Method: 从FineVideo数据集中精选约1000个长视频，设计6个挑战性任务场景（包括事件内和事件间任务），并开发三步半自动数据质量保证流程确保问题质量。

Result: 实验结果显示全模态模型在精确时间定位（T-Loc）和长距离因果推理（CE-Caus）任务上表现不佳，扩展实验揭示了多模态融合中的信息丢失和处理偏差问题。

Conclusion: LongInsightBench填补了长视频理解基准测试的空白，揭示了当前全模态模型在复杂多模态任务中的局限性，为未来研究提供了重要参考。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [236] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一个可扩展的fMRI因果推断框架，通过两阶段方法解决BOLD信号失真和计算复杂性问题，在模拟数据上比DCM准确率高37%，在真实数据中能恢复88%的已知神经通路。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI因果推断中的两个基本限制：从血液动力学失真的BOLD信号推断神经因果关系的病态问题，以及现有方法（如动态因果建模DCM）的计算不可行性。

Method: 将复杂逆问题分解为两个可处理阶段：BOLD反卷积恢复潜在神经活动，然后使用新颖的条件Mamba架构进行因果图推断。

Result: 在模拟数据上比DCM准确率高37%；在真实任务fMRI数据中恢复88%的已知神经通路，而传统方法在99%以上的受试者中无法识别这些典型回路；工作记忆数据分析显示大脑根据刺激策略性地转移其主要因果枢纽。

Conclusion: 为神经科学家提供了一个实用的工具，用于大规模因果推断，能够捕捉认知功能背后的基本回路模式和灵活网络动态。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [237] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文评估了现有对抗防御方法对抗对抗性衣物的效果，发现这些防御方法在面对大尺寸、自然的对抗性衣物时表现不佳，揭示了现有防御方法的共同漏洞。


<details>
  <summary>Details</summary>
Motivation: 实验发现简单地增大对抗性补丁的大小就能使现有防御方法失效，因此需要评估防御方法对抗覆盖人体大面积的对抗性衣物的效果。

Method: 通过制作对抗性衣物，在数字世界和物理世界中测试多种防御方法的效果，并制作单一衣物攻击多个防御模型。

Result: 所有防御方法在对抗性衣物面前表现都很差，单一衣物在物理世界中对抗未防御检测器的攻击成功率达96.06%，对抗九个防御模型的攻击成功率均超过64.84%。

Conclusion: 现有对抗防御方法在面对大尺寸、自然的对抗性衣物时存在共同漏洞，防御效果不理想。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [238] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出CharDiff框架，通过字符级引导的扩散模型有效恢复和识别严重退化的车牌图像，在恢复质量和识别准确率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅用于车牌识别系统的预处理，还能提高证据价值、增强视觉界面清晰度，促进车牌图像的进一步利用。

Method: 使用基于扩散的框架，通过外部分割和OCR模块提取细粒度字符级先验，并引入字符引导注意力区域掩码模块(CHARM)确保每个字符的引导仅限于自身区域。

Result: 在Roboflow-LP数据集上，相比最佳基线模型实现了28%的相对CER降低，在恢复质量和识别准确率方面显著优于基线恢复模型。

Conclusion: 结构化字符引导条件化有效增强了基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [239] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出了iDETEX多模态大语言模型，能够同时执行质量定位、感知和描述三个关键任务，在ViDA-UGC基准测试中达到最先进性能，并在ICCV MIPI 2025挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估从标量质量预测向更可解释、与人感知对齐的评估范式发展的挑战，实现详细且可解释的图像质量评估。

Method: 设计了任务特定的离线增强模块和数据混合策略，辅以在线增强策略来充分利用多源监督，构建统一的多模态大语言模型iDETEX。

Result: 在ViDA-UGC基准测试中实现所有子任务的最先进性能，在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。

Conclusion: iDETEX模型在提供准确且可解释的质量评估方面表现出有效性和鲁棒性，为详细图像质量评估提供了统一解决方案。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [240] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出了一种后处理开放集识别方法，通过比较模型特征和预测logits之间的一致性来识别未知类别，无需重新训练预训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物分类模型在封闭世界设置下训练，当遇到未知类别时仍会过度自信预测。现有OSR方法大多需要重新训练模型，这限制了实际应用。

Method: 使用最近类均值(NCM)距离构建概率分布，然后与softmax概率进行比较来衡量NCM和分类头之间的一致性。

Result: 在两个数据集上均排名前三，AUROC分别达到93.41(非洲动物)和95.35(瑞典动物)，性能稳定优于现有方法。

Conclusion: 提出的后处理OSR方法有效解决了开放集识别问题，无需重新训练模型，在两个数据集上均表现出色且性能一致。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [241] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出Semantic-E2VID框架，通过跨模态特征对齐和语义感知特征融合，将视觉语义知识从帧模态迁移到事件模态，显著提升事件到视频重建的质量和语义信息恢复能力。


<details>
  <summary>Details</summary>
Motivation: 事件相机仅捕捉强度变化，忽略静态物体和背景，导致捕获的事件模态缺乏语义信息。现有E2V方法往往忽视语义信息在视频重建中的重要作用。

Method: 1) 跨模态特征对齐模块将SAM模型的视觉语义迁移到事件编码器；2) 语义感知特征融合块整合学习到的语义特征；3) 语义感知E2V监督利用SAM生成的类别标签帮助重建语义细节。

Result: 在多个基准测试中显著提升帧质量，优于最先进的E2V方法。

Conclusion: Semantic-E2VID通过有效利用视觉语义知识，成功解决了事件到视频重建中语义信息缺失的问题，为事件相机应用提供了更高质量的视觉重建能力。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [242] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H是一个用于单目图像多任务学习的轻量级框架，支持语义分割、深度、边缘和表面法线估计，通过窗口跨任务注意力模块实现结构化特征交换，在保持计算效率的同时提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，能够利用互补任务信息同时最小化计算开销。传统方法要么使用独立单任务模型，要么采用共享编码器-解码器架构，无法有效平衡任务间特征交换和任务特定细节保留。

Method: 提出Multi-Mono-Hydra (M2H)框架，基于轻量级ViT DINOv2骨干网络，引入窗口跨任务注意力模块实现结构化特征交换，同时保留任务特定细节。该框架专为实时部署优化，支持动态环境中的3D场景图构建。

Result: 在NYUDv2数据集上超越最先进的多任务模型，在Hypersim上超越单任务深度和语义基线，在Cityscapes数据集上实现卓越性能，同时在笔记本电脑硬件上保持计算效率。在实际数据验证中展示了在空间感知任务中的实用性。

Conclusion: M2H通过创新的跨任务注意力机制有效解决了多任务学习中的特征交换与细节保留平衡问题，为边缘设备上的实时空间感知提供了高效实用的解决方案。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [243] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，使标准视频大语言模型能够处理流式长视频，通过注意力机制选择关键视觉标记、递归处理历史标记和基于描述的问答，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理流式长视频时面临挑战，需要在视频在线处理过程中及时回答问题，而传统方法无法满足这种实时性要求。

Method: 1) 基于LLM注意力机制选择关键视觉标记，丢弃约95%不重要标记；2) 递归处理历史选择的标记以保持时间连贯性；3) 使用基于描述的轻量级问答机制。

Result: 在流式视频基准测试中达到最先进性能，在效率和效果之间取得良好平衡。

Conclusion: 该方法为视频大语言模型在流式场景中的应用提供了有效的解决方案，无需额外训练即可显著提升处理效率。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [244] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 本研究通过系统评估合成人脸识别数据集，证明高性能合成数据集（VariFace、VIGFace）的识别准确率可达95.67%和94.91%，超过真实数据集CASIA-WebFace（94.70%），确立了合成人脸数据作为隐私保护替代方案的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决人脸识别系统中的伦理困境：高精度需要大量未经同意收集的真实人脸数据，导致数据集撤回和GDPR等法规下的法律风险。合成人脸数据作为隐私保护替代方案缺乏全面实证证据。

Method: 系统文献回顾识别25个合成人脸识别数据集（2018-2025），结合严格实验验证。评估七个隐私保护合成数据关键要求：身份泄露预防、类内变异性、身份可分离性、数据集规模、伦理数据来源、偏见缓解和基准可靠性。

Result: 超过1000万合成样本实验显示，最佳合成数据集识别准确率超过真实数据集。合成数据确保适当的类内变异性同时保持身份可分离性，且提供前所未有的偏见缓解控制能力。

Conclusion: 合成人脸数据是科学可行且伦理必要的面部识别研究替代方案，能够替代真实数据集同时解决隐私和伦理问题。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [245] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出一种基于多面部表情特征的帕金森病严重程度诊断方法，通过注意力机制融合不同表情特征，并采用自适应类别平衡策略解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部表情的PD诊断方法依赖单一表情类型易导致误诊，且忽略不同PD阶段的类别不平衡问题，多数方法仅进行二元分类而非严重程度诊断。

Method: 集成多种面部表情特征通过注意力机制进行特征融合，采用自适应类别平衡策略根据类别分布和分类难度动态调整训练样本贡献度。

Result: 实验结果表明该方法在PD严重程度诊断方面表现优异，注意力特征融合和自适应类别平衡策略均有效。

Conclusion: 该方法能够有效诊断PD严重程度，解决了单一表情依赖和类别不平衡问题，为PD早期检测和个性化干预提供了便利工具。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [246] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans是一个闭环框架，通过双向知识转移（从外中心到自我中心再返回）来增强弱监督的affordance定位能力，解决了传统单向转移在复杂交互场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅从外中心图像单向转移到自我中心图像，限制了在复杂交互场景中的应用。人类能够通过观察他人与物体的互动来学习新的交互方式，这启发了双向知识转移的需求。

Method: 提出LoopTrans闭环框架，包含统一跨模态定位和去噪知识蒸馏等创新机制，桥接以物体为中心的自我中心图像和以交互为中心的外中心图像之间的领域差距。

Result: 实验表明，LoopTrans在图像和视频基准测试的所有指标上都实现了持续改进，甚至能够处理人类身体完全遮挡物体交互区域的挑战性场景。

Conclusion: LoopTrans通过双向知识转移有效提升了弱监督affordance定位的性能，证明了闭环框架在复杂交互场景中的优越性。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [247] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 开发了一个基于视觉的自动化监控系统，用于检测和跟踪马厩中的马匹和人员，使用YOLOv11和BoT-SORT技术，能够识别五种事件类型并考虑摄像头盲区。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监测方法劳动密集且耗时，需要自动化解决方案来早期发现健康福利问题。

Method: 利用YOLOv11进行目标检测，BoT-SORT进行多目标跟踪，基于物体轨迹和空间关系推断事件状态，使用CLIP和GroundingDINO辅助构建自定义数据集。

Result: 定性评估显示系统在马相关事件检测上表现可靠，但人员检测因数据稀缺存在局限性。

Conclusion: 该系统为马术设施实时行为监测奠定了基础，对动物福利和厩舍管理具有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [248] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一个智能、全能的密集关键点检测器，通过融合传统检测器的优势并结合深度学习，解决了现有方法在光度变化敏感性、关键点密度和可重复性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器（如SIFT、SURF、ORB等）和学习方法（如SuperPoint、R2D2等）存在对光度变化敏感、关键点密度低、可重复性差、对挑战性场景适应能力有限以及缺乏语义理解等问题。

Method: 首先融合7种关键点检测器和2种边缘检测器的输出创建真实掩码，提取从角点、斑点到突出边缘和纹理的多样化视觉线索；然后使用轻量高效的ESPNet模型，以这些掩码作为标签进行训练。

Result: 在Oxford Affine Covariant Regions数据集上的评估显示，DeepDetect在关键点密度、可重复性和正确匹配数量方面均优于其他检测器，分别达到0.5143、0.9582和59,003的最大值。

Conclusion: DeepDetect能够语义地关注图像，产生高度密集的关键点，并能适应多样化和视觉退化条件，在关键点检测任务中表现出色。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [249] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 该论文重新利用AV1运动向量来生成密集的亚像素对应关系和经过余弦一致性过滤的短轨迹。在短视频上，这种压缩域前端在CPU使用量远低于顺序SIFT的情况下表现相当，并产生具有竞争性对偶几何的密集匹配。


<details>
  <summary>Details</summary>
Motivation: 探索压缩域对应关系作为实际、资源高效的前端，为完整流程的扩展提供清晰路径，减少计算资源消耗。

Method: 重新利用AV1运动向量生成密集亚像素对应关系，通过余弦一致性过滤短轨迹，作为压缩域前端进行匹配。

Result: 在117帧片段的小型SfM演示中，MV匹配成功注册所有图像并重建46-62万个点，重投影误差为0.51-0.53像素；束调整时间随匹配密度增加。

Conclusion: 压缩域对应关系是一个实用且资源高效的前端，在完整流程中具有明确的扩展路径，计算效率优于传统方法。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [250] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文提出了一个高质量夜间图像去雨基准数据集HQ-NightRain，并开发了颜色空间转换网络CST-Net，通过可学习的颜色空间转换器和隐式光照引导来有效去除夜间复杂雨纹。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨相比白天更具挑战性，主要由于夜间场景的固有复杂性以及缺乏准确表示雨和光照耦合效应的高质量数据集。

Method: 提出CST-Net网络，包含可学习的颜色空间转换器（CSC）在Y通道进行更有效的雨纹去除，并引入隐式光照引导来捕获光照信息，提高模型在复杂场景中的鲁棒性。

Result: 大量实验验证了所提数据集的价值和方法的有效性，在夜间图像去雨任务上表现出色。

Conclusion: HQ-NightRain数据集提供了更高的和谐性和真实性，CST-Net方法通过颜色空间转换和光照引导有效解决了夜间去雨的挑战。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [251] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文提出了一种改进稀疏视图3D高斯泼溅(3DGS)初始化质量的方法，通过频率感知SfM、3DGS自初始化和点云正则化来补充传统SfM在稀疏视图下的不足。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3DGS容易过拟合训练视图，导致新视角渲染出现模糊等伪影。现有方法要么改进初始化，要么添加训练时约束，但研究发现初始化是决定性因素，决定了性能上限。

Method: 1) 频率感知SfM：通过低频视图增强和宽松多视图对应改进低纹理区域覆盖；2) 3DGS自初始化：利用光度监督生成额外点，补充SfM稀疏区域；3) 点云正则化：通过几何/可见性先验确保多视图一致性和均匀空间覆盖。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视图设置下取得了持续的性能提升，证明了其作为更强初始化策略的有效性。

Conclusion: 初始化是稀疏视图3DGS的关键因素，本文提出的综合初始化方法显著提升了渲染质量，代码已开源。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [252] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld是一个新颖的4D占用世界模型，通过稀疏动态查询实现灵活、自适应和高效的场景理解，在感知、预测和规划任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态固定嵌入或网格，限制了感知的灵活性，且其基于网格的"原位分类"与真实场景的动态连续性存在潜在不匹配。

Method: 提出Range-Adaptive Perception模块，通过可学习查询结合车辆状态调制和时空关联实现扩展范围感知；设计State-Conditioned Forecasting模块，用回归引导的预测替代基于分类的预测；开发Temporal-Aware Self-Scheduling训练策略。

Result: 在感知、预测和规划任务中达到最先进性能，验证了模型在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询成功解决了现有占用世界模型的局限性，为4D环境建模提供了更灵活、自适应和高效的解决方案。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [253] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet通过熵引导的双聚类头和最优传输技术，在无监督显著目标检测中生成高质量伪掩码，AutoSOD端到端管道在多个基准测试中显著优于现有无监督和弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测作为计算机视觉基础任务，现有方法依赖像素级标注。作者认为当有可靠伪掩码时，无监督方法可接近监督方法的精度，但现有原型方法存在边界和内部像素几何特性差异、最优传输全局一致性利用不足的问题。

Method: 提出POTNet，改进原型最优传输方法：使用熵引导双聚类头（高熵像素用谱聚类，低熵像素用k-means），通过最优传输对齐两个原型集，生成更清晰的伪掩码。AutoSOD基于此构建端到端无监督管道，用标准MaskFormer架构训练。

Result: 在5个基准测试中，AutoSOD在F-measure指标上比无监督方法提升26%，比弱监督方法提升36%，大幅缩小与全监督模型的差距。

Conclusion: POTNet的双聚类设计和最优传输对齐能有效生成高质量伪掩码，AutoSOD证明了无监督显著目标检测可以达到接近监督方法的性能，无需人工先验或离线投票。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [254] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于评分准则和伪标签的零样本视频摘要框架，通过将少量真实标注转化为伪标签来指导LLM进行可解释的场景评估，在无需训练的情况下实现了接近监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且跨数据集泛化能力有限，无监督方法难以捕捉高层次语义，零样本方法对提示模板敏感且需要数据集特定的分数归一化。

Method: 使用评分准则引导的伪标签提示框架，将少量真实标注转化为高置信度伪标签，构建结构化评分准则。推理时对首尾片段基于描述评分，中间片段结合相邻场景上下文评估叙事进展和冗余度。

Result: 在SumMe和TVSum数据集上分别达到57.58和63.05的F1分数，超越了无监督和先前零样本基线，接近监督方法性能。

Conclusion: 评分准则引导的伪标签有效稳定了基于LLM的评分，为视频摘要建立了一个通用、可解释的零样本范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [255] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出了一个优化大规模视频生成模型训练的四支柱框架，包括数据处理、模型架构、训练策略和基础设施，开发出MUG-V 10B模型，在电商视频生成任务上超越开源基线，并开源了完整技术栈。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频生成模型训练面临的挑战：跨模态文本-视频对齐、长序列处理和复杂时空依赖，这些导致训练资源密集且困难。

Method: 优化四个支柱：数据处理、模型架构、训练策略和基础设施，包括数据预处理、视频压缩、参数缩放、课程式预训练和对齐后训练，使用Megatron-Core实现高效训练和近线性多节点扩展。

Result: MUG-V 10B模型整体达到最新视频生成器水平，在电商视频生成任务上超越领先开源基线的人类评估结果。

Conclusion: 成功开发出高效的大规模视频生成训练框架和模型，并开源完整技术栈，为社区提供了首个基于Megatron-Core的大规模视频生成训练代码。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [256] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出了MambaX-Net，一种用于前列腺癌主动监测的半监督双扫描3D分割架构，通过利用先前时间点的MRI和分割掩码来改进当前时间点的分割，解决了纵向分析中多时间点和专家标注稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型通常基于单时间点和专家标注数据集训练，不适合纵向主动监测分析，因为多时间点和专家标注稀缺限制了有效微调。

Method: 提出MambaX-Net架构，包含Mamba增强的交叉注意力模块和形状提取器模块，采用半监督自训练策略利用预训练nnU-Net生成的伪标签进行学习。

Result: 在纵向主动监测数据集上的评估显示，MambaX-Net显著优于最先进的U-Net和Transformer模型，即使在有限和噪声数据下也能实现优异的前列腺区域分割。

Conclusion: MambaX-Net通过有效利用时间信息和半监督学习，为前列腺癌主动监测提供了准确的分割解决方案，解决了纵向分析中的关键挑战。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [257] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一种弱监督的道路裂缝检测方法，仅使用图像级标签即可实现像素级检测，通过分类器、重建器和检测器的对抗学习机制提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵像素级标注的依赖，实现智能城市基础设施维护中可扩展的道路裂缝检测。

Method: 集成三个组件：分类器生成CAM、重建器测量特征可推断性、检测器生成像素级结果；采用对抗学习机制，并设计了路径感知注意力模块和中心增强CAM一致性模块。

Result: 在三个图像级数据集上的实验表明，WP-CrackNet达到与监督方法相当的结果，优于现有弱监督方法。

Conclusion: 该方法显著推进了可扩展道路检测的发展，源代码和数据集已公开。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [258] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D扩展了VGGT模型，通过动态感知聚合器解决动态场景中的多任务冲突，在相机姿态估计、深度预测和点云重建方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D前馈模型在静态数据集上训练，难以处理包含动态元素（如移动人物、可变形物体）的真实世界场景。

Method: 提出动态感知聚合器，通过预测动态感知掩码来分离静态和动态信息：抑制运动线索用于姿态估计，增强运动线索用于几何重建。

Result: 在动态场景中持续优于原始VGGT，在相机姿态估计、单目和视频深度估计、密集点云重建方面取得更优结果。

Conclusion: PAGE-4D成功将VGGT扩展到动态场景，解决了多任务4D重建中的内在冲突，无需后处理即可实现相机姿态估计、深度预测和点云重建。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [259] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 本文提出了首个水下伪装实例分割数据集UCIS4K，并基于Segment Anything Model开发了UCIS-SAM网络，通过三个关键模块解决水下环境中颜色失真、低对比度和模糊等问题，在伪装实例分割任务上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着水下探测和海洋保护的发展，水下视觉任务日益重要。但由于水下环境的退化特性（颜色失真、低对比度、模糊），伪装实例分割面临更大挑战。传统方法在陆地数据集上训练，水下样本有限，在水下场景中表现不佳。

Method: 提出了UCIS-SAM网络，包含三个关键模块：1）通道平衡优化模块（CBOM）增强通道特征以改进水下特征学习；2）频域真值集成模块（FDTIM）强调内在物体特征并减少伪装模式干扰；3）多尺度特征频率聚合模块（MFFAM）在多频带中加强低对比度伪装实例的边界。

Result: 在提出的UCIS4K数据集和公共基准测试上的广泛实验表明，UCIS-SAM优于最先进的方法。

Conclusion: 本文成功构建了首个水下伪装实例分割数据集，并开发了有效的UCIS-SAM网络，显著提升了水下伪装实例分割的性能。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [260] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个多代理框架，通过图形化程序形状表示将自然语言转换为结构化、可交互的3D资产，解决了现有方法生成非结构化网格和交互性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D生成方法通常产生非结构化网格且交互性差，难以满足艺术工作流程需求，需要一种能生成结构化、可交互3D资产的方法。

Method: 提出基于图形的程序形状表示，将复杂自然语言分解为子任务图，通过LLM代理分层解析用户输入并迭代优化程序建模和绘制过程。

Result: 定性和定量实验显示ShapeCraft在生成几何精确和语义丰富的3D资产方面优于现有基于LLM的代理方法，支持动画和用户定制编辑。

Conclusion: ShapeCraft通过结构化表示和多代理框架实现了高质量文本到3D生成，具有广泛的交互应用潜力。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [261] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出基于机器学习的框架，结合无人机扫描点云和BIM合成数据，实现3D基础设施模型的自动分割，提高结构健康监测效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动标注方法在无人机3D模型分割中耗时且易出错的问题，推进无人机与BIM技术在基础设施管理中的集成应用。

Method: 利用真实无人机扫描点云与BIM合成数据的互补优势，开发机器学习框架进行自动分割，采用小规模数据集结合BIM数据减少训练时间。

Result: 在铁路轨道数据集验证中，对铁轨和轨枕等主要组件实现了高精度识别和分割，同时保持合理分割精度下显著缩短训练时间。

Conclusion: 该自动化方法提升了3D基础设施模型分割的精度和效率，推动了无人机与BIM技术在结构健康监测和基础设施管理中的集成发展。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [262] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是首个全谱图像无监督异常检测统一框架，通过五个简单元素的协同工作，在标准重建框架中实现卓越性能，解决了多类模型性能差距问题，并自然扩展到多种数据模态和任务设置。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测领域存在多类模型性能显著落后于单类模型的问题，且方法碎片化到特定场景，需要统一的解决方案。

Method: 采用"少即是多"理念，在标准重建框架中协调五个简单元素，实现方法最小主义，无需修改即可自然扩展到多样化任务。

Result: 在12个UAD基准测试中表现出全谱优越性：多类模型在MVTec-AD和VisA上分别达到99.9%和99.3%的I-AUROC；多视图和多模态检测达到SOTA性能；仅用每类8个正常样本就超越之前全样本模型。

Conclusion: Dinomaly2通过最小化设计、计算可扩展性和通用适用性，成为现实世界异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [263] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT是一个细粒度数据集，捕捉汽车模型的时间演变，支持监督和自监督学习。研究发现静态预训练在跨年测试时性能下降，提出了时间增量分类设置和两种策略来提升时间鲁棒性，并探索了时间感知图像生成。


<details>
  <summary>Details</summary>
Motivation: AI系统需要适应不断变化的视觉环境，特别是在物体外观随时间变化的领域。汽车模型作为代表性技术制品，其时间演变需要被研究。

Method: 1) 构建CaMiT数据集(787K标注样本和5.1M未标注样本)；2) 静态预训练与跨年测试；3) 提出时间增量分类设置；4) 评估两种策略：时间增量预训练和时间增量分类器学习；5) 探索时间感知图像生成。

Result: 静态预训练在域内数据上表现竞争性且更高效，但跨年测试时准确率下降。时间增量策略显著提升了时间鲁棒性。时间感知图像生成产生更真实的输出。

Conclusion: CaMiT为研究细粒度视觉识别和生成中的时间适应提供了丰富的基准，时间增量学习策略能有效应对视觉环境的时间演变。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [264] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: DINO-CV是一个基于自监督学习的干石墙分割框架，使用高分辨率LiDAR DEM数据解决植被遮挡和标注数据稀缺问题，在澳大利亚文化遗产地实现了68.6%的mIoU分割精度。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要的遗产和环境价值，但由于植被遮挡和人工测绘成本高昂，许多墙体未被识别。需要开发能够克服视觉遮挡和数据稀缺的自动测绘方法。

Method: 提出DINO-CV分割框架，使用LiDAR DEM数据捕获植被下地形结构，采用基于知识蒸馏的自监督跨视图预训练策略，学习多DEM衍生物的视觉和几何表示不变性，支持多种视觉骨干网络。

Result: 在维多利亚Budj Bim UNESCO世界遗产地应用中，识别出澳大利亚最密集的殖民时期干石墙，测试区域mIoU达到68.6%，仅使用10%标注数据微调后仍保持63.8% mIoU。

Conclusion: 自监督学习结合高分辨率DEM衍生物在植被覆盖和遗产丰富的环境中具有自动化干石墙测绘的潜力，能够有效应对标注稀缺的挑战。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [265] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较两种节俭联邦学习方法用于暴力检测：零样本和联邦微调的视觉语言模型(VLMs)与个性化训练的紧凑3D卷积神经网络(CNN3D)。两者准确率均超90%，CNN3D在ROC AUC和log loss上略优于LoRA调优的VLMs且能耗更低，而VLMs在上下文推理和多模态推理方面更优。


<details>
  <summary>Details</summary>
Motivation: 研究资源高效的联邦学习方法用于暴力检测，特别关注能源效率和环境可持续性，填补了LoRA调优视觉语言模型与个性化CNN在联邦暴力检测中比较研究的空白。

Method: 使用LLaVA-7B和65.8M参数的CNN3D作为代表性案例，在非独立同分布设置下评估准确性、校准性和能耗。量化训练和推理过程中的能源消耗和二氧化碳排放。

Result: 两种方法准确率均超过90%。CNN3D在ROC AUC和log loss上略微优于LoRA调优的VLMs，同时能耗更低。VLMs在上下文推理和多模态推理方面保持优势。

Conclusion: 提出混合模型：轻量级CNN用于常规分类，选择性激活VLM用于复杂或描述性场景。该框架为视频监控中负责任、资源感知的AI提供了可复现的基线。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [266] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA是一个基础模型，通过混合动作机制将GUI原始操作与高级程序化工具调用无缝集成，解决了计算机使用代理在视觉定位和执行链方面的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理仅依赖原始操作（点击、输入、滚动），需要精确的视觉定位和冗长的执行链，导致级联失败和性能瓶颈。同时，这些代理无法利用丰富的程序化接口（API、MCP服务器、工具）。

Method: 包含四个关键组件：(1)从软件文档、开源仓库和代码生成中扩展程序化工具的自动化流水线；(2)生成超过17,000个可验证任务的合成数据引擎；(3)大规模高质量混合动作轨迹收集；(4)结合监督微调和在线强化学习的两阶段训练流程。

Result: 7B和32B模型在OSWorld上相比基线模型平均提升22%相对性能，执行步骤减少11%。在WindowsAgentArena的域外评估中达到21.7%成功率，优于在Windows数据上训练的基线模型。

Conclusion: 混合动作机制在减少错误传播的同时保持执行效率，证明了其关键作用，为计算机使用代理提供了更高效可靠的解决方案。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [267] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer是一个用于4D全景分割的流式处理框架，采用双线程系统实现实时处理能力，可集成到现有3D/4D分割方法中，在动态环境中表现出优越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在高度动态环境中（如人群疏散、复杂自动驾驶场景）需要实时、细粒度的感知能力，但现有方法在受限时间预算下难以满足实时需求。

Method: 采用双线程系统：预测线程利用历史运动几何信息提取特征并预测未来动态；推理线程通过对齐最新记忆并补偿自运动和动态物体移动来确保对传入帧的及时预测。

Result: 在室内HOI4D数据集和室外SemanticKITTI、nuScenes数据集上的综合实验表明，该方法在复杂场景中准确预测动态物体方面表现优异，特别是在高FPS条件下。

Conclusion: 4DSegStreamer框架为4D全景分割提供了有效的流式处理解决方案，能够无缝集成到现有方法中，在动态环境中实现实时感知能力。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [268] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph框架通过将长文本渲染为图像，利用视觉语言模型处理，实现3-4倍文本压缩，在保持准确性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理百万级token长上下文时面临的计算和内存成本过高问题，提供更实用的长上下文处理方案。

Method: 提出Glyph框架：将长文本渲染成图像，使用视觉语言模型处理；设计基于LLM的遗传搜索算法优化视觉渲染配置，平衡准确性和压缩率。

Result: 实现3-4倍token压缩，准确性与Qwen3-8B相当；预填充和解码速度提升约4倍，SFT训练速度提升约2倍；128K上下文VLM可处理1M token级任务。

Conclusion: 视觉上下文缩放是解决长上下文建模挑战的有效方法，Glyph框架在保持性能的同时显著提升了计算效率，并有益于文档理解等多模态任务。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [269] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: PICABench是一个评估图像编辑物理真实性的基准，涵盖8个物理维度，并提出了PICAEval评估协议和PICA-100K训练数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要关注指令完成度，但忽视了物理效果的真实性，如移除物体时应同时移除其阴影、反射等物理交互。

Method: 构建PICABench基准系统评估8个物理维度；提出PICAEval评估协议使用VLM作为评判；创建PICA-100K训练数据集从视频中学习物理知识。

Result: 评估主流模型后发现物理真实性仍是一个具有挑战性的问题，存在很大改进空间。

Conclusion: 该研究为从简单内容编辑向物理一致性真实感的转变提供了基础，推动了图像编辑物理真实性的研究。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [270] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC-MoE模型，通过混合专家架构和语义引导对比学习，解决医学图像分割中高层特征表示不足和预训练权重结构完整性问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割基础模型微调方法存在两个关键问题：1）高层特征表示不足；2）微调过程破坏预训练权重的结构完整性。

Method: 1）构建基础专家、语义专家和自适应专家，采用像素概率自适应投票策略进行专家选择和融合；2）提出语义引导对比学习方法解决对比学习中弱监督问题。

Result: 在三个公共医学图像分割数据集上的广泛实验表明，IC-MoE优于其他SOTA模型，有效补充了基础医学图像分割模型的高层特征和预训练结构完整性。

Conclusion: IC-MoE在多样医学图像分割场景中展现出优越的泛化能力，为医学图像分割基础模型提供了高层特征增强和结构完整性保持的有效解决方案。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [271] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了多语言文本到图像人物检索任务，开发了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决模态异质性问题，在多语言TIPR数据集上达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像人物检索方法存在两个问题：全局方法忽略细粒度差异，局部方法需要先验信息；且当前方法主要面向英语，限制了多语言应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过掩码图像和文本的双向预测增强跨语言和跨模态的局部关系建模）和多维全局对齐模块（桥接模态异质性）。

Result: 在所有多语言TIPR数据集上取得了新的最先进结果。

Conclusion: Bi-IRRA框架有效解决了多语言文本到图像人物检索中的模态异质性问题，为多语言场景下的跨模态检索提供了有效解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [272] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det是一个无需文本提示的开放世界3D检测器，能够检测3D场景中的任何物体，包括训练时未见的新物体，通过结合2D语义先验和3D几何先验实现类无关的物体发现。


<details>
  <summary>Details</summary>
Motivation: 现有3D目标检测和新型类别检测虽有进展，但在学习通用3D物体性方面研究不足。传统闭集3D检测器难以泛化到开放世界场景，而直接使用3D开放词汇模型又面临词汇扩展和语义重叠问题。

Method: 提出OP3Det，利用2D基础模型的强泛化能力和零样本能力，结合2D语义先验和3D几何先验生成类无关的候选区域。通过跨模态专家混合集成点云和RGB图像的互补信息，动态路由单模态和多模态特征来学习通用3D物体性。

Result: 大量实验表明OP3Det表现卓越，在AR指标上显著超越现有开放世界3D检测器达16.0%，相比闭集3D检测器提升13.5%。

Conclusion: OP3Det通过结合2D和3D先验以及跨模态融合，成功实现了开放世界3D物体发现，在检测新物体方面表现出色。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [273] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了一种名为广义对抗求解器的新方法，通过结合原始蒸馏损失和对抗训练，在减少扩散模型采样计算成本的同时提高细节保真度。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型虽然生成质量优秀，但采样计算成本高昂。现有方法依赖复杂训练技巧且未能明确关注细粒度细节的保留。

Method: 引入广义求解器参数化ODE采样器，无需额外训练技巧；结合原始蒸馏损失与对抗训练来减少伪影并增强细节保真度。

Result: 在相似资源约束下，该方法相比现有求解器训练方法表现出更优越的性能。

Conclusion: 广义对抗求解器提供了一种简单有效的解决方案，在降低计算成本的同时显著提升了生成质量。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [274] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT是一种后预训练结构化剪枝方法，能够在不同计算预算下实现弹性推理，无需重新训练或标签数据。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型只有有限的预定义尺寸，无法灵活适应实际部署中的计算约束。

Method: 结合梯度信息与跨网络结构相关性，通过进化算法近似Hessian非对角结构，使用自监督重要性评分机制。

Result: 在DINO、SigLIPv2、DeIT和AugReg模型上表现优于现有方法，单张A100 GPU不到5分钟即可生成弹性模型。

Conclusion: SnapViT提供了一种高效、无需重新训练的剪枝策略，使预训练视觉Transformer能够灵活适应各种计算预算。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [275] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出一种基于手绘图像的两阶段帕金森病检测方法，通过图像分块和集成学习策略，显著提高了对未见患者数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病早期检测方法存在两个主要局限：数据集不足和对未见患者数据的鲁棒性差。

Method: 采用两阶段方法：第一阶段按绘图类型分类，第二阶段将图像分为2x2块分别提取特征，使用集成方法合并各块决策进行最终分类。

Result: 在NewHandPD数据集上，对已见患者准确率达97.08%，对未见患者达94.91%，性能差距仅2.17个百分点，优于现有方法。

Conclusion: 提出的分块策略和集成方法有效解决了数据不足和泛化问题，在帕金森病检测方面表现出色。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [276] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 该研究开发了一个用于自动分析循环血细胞簇图像的计算框架，通过YOLOv11模型进行细胞簇分类，并结合多通道荧光染色识别细胞类型，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 循环血细胞簇是血栓、感染和炎症等疾病的重要生物标志物，但目前缺乏自动分析细胞簇图像的工具，特别是针对不规则形状和异质细胞类型的细胞簇。

Method: 采用两步分析策略：首先使用YOLOv11模型对图像进行细胞簇与非细胞簇分类，然后通过叠加细胞簇轮廓与多通道荧光染色区域来识别细胞类型。

Result: 该方法在细胞簇分类和表型识别方面均达到超过95%的准确率，优于传统的CNN和ViT模型。

Conclusion: 该自动化框架能有效分析流式细胞术中的细胞簇图像，不仅适用于血细胞分析，还有潜力扩展到免疫细胞和肿瘤细胞簇的分析，支持多种疾病的细胞研究。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [277] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS是一个针对3D高斯溅射在雨滴条件下的综合基准测试，解决了真实场景中雨滴对相机姿态估计和点云初始化的干扰问题，并揭示了现有方法的性能局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常使用合成雨滴图像和已知相机姿态进行评估，但真实场景中雨滴会干扰相机姿态估计和点云初始化，且合成与真实雨滴存在显著域差距，影响泛化能力。

Method: 构建完整的基准测试流程，包括数据准备、数据处理和雨滴感知的3DGS评估。收集真实世界雨滴重建数据集，每个场景包含三种对齐的图像集：雨滴聚焦、背景聚焦和无雨地面真值。

Result: 通过全面实验和分析，揭示了现有3DGS方法在无约束雨滴图像上的性能局限，以及不同流程组件的影响：相机焦点位置对3DGS重建性能的影响，以及不准确的姿态和点云初始化对重建的干扰。

Conclusion: 这些见解为开发在雨滴条件下更鲁棒的3DGS方法确立了明确方向。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [278] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench是一个用于评估多模态大语言模型在多轮视频对话中表现的综合基准，包含987个精心策划的多轮对话，涵盖六个核心能力维度。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准主要局限于单轮问答，无法反映真实场景中多轮对话的复杂性，需要开发专门针对多轮视频对话的评估标准。

Method: 构建了MT-Video-Bench基准，包含987个多轮对话，评估六个核心能力（感知性和交互性），涵盖体育分析、视频智能辅导等实际应用场景。

Result: 对多种开源和闭源MLLMs进行了广泛评估，发现在处理多轮视频对话方面存在显著的性能差异和局限性。

Conclusion: MT-Video-Bench揭示了当前MLLMs在多轮视频对话理解方面的不足，该基准将公开可用以促进未来研究。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [279] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 该研究探讨了离线签名验证中的特征学习策略，重点关注提高跨数据集泛化能力。使用三个公开基准数据集，开发了基于原始签名图像和壳预处理两种实验流程，发现原始图像模型性能更好，但壳预处理模型显示出改进潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习方法在签名验证方面取得了显著进展，但大多数离线签名验证方法在跨数据集泛化方面仍然存在困难，因为手写风格和采集协议的变化往往会降低性能。

Method: 使用CEDAR、ICDAR和GPDS Synthetic三个公开基准数据集，开发了两种实验流程：基于原始签名图像的方法和采用壳预处理的方法。

Result: 原始图像模型在跨基准测试中实现了更高的性能，而基于壳预处理的模型显示出未来改进的潜力，但两种方法之间没有明确的优劣之分。

Conclusion: 研究结果表明原始图像模型性能更优，但壳预处理方法为未来开发鲁棒的跨域签名验证系统提供了有前景的方向。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [280] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 该研究探索了基于扩散变换器的图像到视频模型是否能够生成拥挤公共场景中真实的行人运动模式，通过使用行人轨迹基准中的关键帧来条件化模型，并评估其轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证当前高性能的图像到视频模型是否能够捕捉和生成真实的行人动态行为，特别是在拥挤的公共场景中。

Method: 方法包括：1）从行人轨迹基准中提取关键帧；2）使用这些关键帧条件化基于扩散变换器的图像到视频模型；3）通过量化行人动力学指标来评估模型的轨迹预测性能。

Result: 研究结果表明，这些模型确实能够生成现实的行人运动模式，并在轨迹预测任务中表现出良好的性能。

Conclusion: 结论是扩散变换器模型在训练大规模视频数据集后，具备了内在的世界建模能力，能够有效生成和预测拥挤场景中的行人运动轨迹。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [281] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种无需训练的描述符无关方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配，用于多参考视觉位置识别。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉位置识别中，在训练和部署时增加数据多样性和模型复杂性带来的高计算成本问题，同时避免现有描述符级融合方法的局限性。

Method: 采用训练自由的描述符无关方法，通过矩阵分解将多个参考描述符联合建模为基表示，使用投影基残差匹配进行位置识别。

Result: 在多外观数据上，该方法比单参考方法Recall@1提升约18%，在非结构化数据上提升约5%，在视觉外观和视角变化下均优于多参考基线方法。

Conclusion: 该方法在保持轻量级的同时，在多种视觉变化条件下表现出强大的泛化能力，显著提升了多参考视觉位置识别的性能。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [282] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出一种基于双编码器和注意力的框架，结合分割病灶和临床元数据，提升皮肤病变分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测至关重要，但现有深度学习模型存在"黑盒"问题，且皮肤病变图像存在类内差异大、类间差异小等挑战。

Method: 使用带双注意力门和ASPP的Deep-UNet分割病灶，分类阶段采用两个DenseNet201编码器分别处理原图和分割病灶，通过多头交叉注意力融合特征，并加入基于transformer的元数据模块。

Result: 在HAM10000、ISIC 2018和2019数据集上取得最先进的分割性能，显著提升分类准确率和平均AUC，Grad-CAM可视化证实模型基于病灶区域进行预测。

Conclusion: 整合精确病灶分割、临床数据和注意力融合机制，可构建更准确且可解释的皮肤癌分类模型。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [283] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种高效的视觉语言模型推理范式，通过在预填充阶段剪枝冗余视觉令牌，在解码阶段仅检索查询相关令牌，实现端到端2.6倍加速，同时保持多轮对话的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的推理延迟主要受视觉令牌数量增长的限制，需要一种既能加速推理又不牺牲模型能力的高效方法。

Method: 采用解耦的视觉稀疏化设计：预填充阶段进行查询无关的令牌剪枝，解码阶段进行查询感知的令牌检索，结合AWQ优化的推理管道。

Result: 在长上下文视频任务上实现预填充4.0倍加速、解码2.5倍加速、端到端2.6倍加速，同时在文档理解和推理任务上提高准确性。

Conclusion: SparseVILA通过解耦查询无关剪枝和查询感知检索，为高效多模态推理开辟了新方向，提供无需训练、架构无关的加速框架。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [284] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 提出ConsistEdit方法，针对MM-DiT架构的注意力控制机制，解决现有训练自由编辑方法在编辑强度与一致性保持之间的平衡问题，支持多轮和视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由注意力控制方法在保持编辑强度与源图像一致性方面存在不足，特别是在多轮和视频编辑中错误会累积。MM-DiT架构的出现为解决这些问题提供了新机会。

Method: 基于MM-DiT注意力机制分析，提出三种关键技术：视觉专用注意力控制、掩码引导预注意力融合、以及查询/键/值令牌的差异化操作。

Result: 在广泛图像和视频编辑任务中达到最先进性能，支持结构一致和不一致场景，首次实现全推理步骤和注意力层的无人工编辑。

Conclusion: ConsistEdit显著提升了编辑的可靠性和一致性，支持稳健的多轮和多区域编辑，并能渐进调整结构一致性，实现更精细的控制。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [285] [Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS](https://arxiv.org/abs/2510.16152)
*Mason Smetana,Lev Khazanovich*

Main category: cs.DL

TL;DR: 该研究提出了一种基于大语言模型（LLM）的框架，用于量化科学文献中的主题趋势并绘制知识演化图谱。通过对PNAS工程类文章的分析，该框架能够揭示跨主题的潜在联系，比传统NLP方法更有效地映射高度多样化的科学领域。


<details>
  <summary>Details</summary>
Motivation: 科学文献因复杂语言、静态学科结构和稀疏关键词系统而日益孤立，难以捕捉现代科学的动态本质。需要开发新方法来量化主题趋势和绘制科学知识演化图谱。

Method: 采用两阶段分类流程：首先基于摘要建立主要主题分类，然后通过全文分析分配次要分类以揭示跨主题联系。使用传统NLP方法（BoW、TF-IDF）验证结果，并通过分离图表示展示主题间隐含连接。

Result: 该方法独立恢复了期刊编辑嵌入的大部分结构，无需预先了解其现有的双重分类模式。研究发现仅基于词频的分析可能不足以映射高度多样化的领域，而LLM驱动的框架能够有效揭示抽象或关键词分析中不明显的主题间隐含连接。

Conclusion: 该框架为检测潜在主题趋势和提供科学进展的高层概览提供了强大工具，能够更好地捕捉科学知识的动态演化特性。

Abstract: Scientific literature is increasingly siloed by complex language, static
disciplinary structures, and potentially sparse keyword systems, making it
cumbersome to capture the dynamic nature of modern science. This study
addresses these challenges by introducing an adaptable large language model
(LLM)-driven framework to quantify thematic trends and map the evolving
landscape of scientific knowledge. The approach is demonstrated over a 20-year
collection of more than 1,500 engineering articles published by the Proceedings
of the National Academy of Sciences (PNAS), marked for their breadth and depth
of research focus. A two-stage classification pipeline first establishes a
primary thematic category for each article based on its abstract. The
subsequent phase performs a full-text analysis to assign secondary
classifications, revealing latent, cross-topic connections across the corpus.
Traditional natural language processing (NLP) methods, such as Bag-of-Words
(BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the
resulting topical structure and also suggest that standalone word-frequency
analyses may be insufficient for mapping fields with high diversity. Finally, a
disjoint graph representation between the primary and secondary classifications
reveals implicit connections between themes that may be less apparent when
analyzing abstracts or keywords alone. The findings show that the approach
independently recovers much of the journal's editorially embedded structure
without prior knowledge of its existing dual-classification schema (e.g.,
biological studies also classified as engineering). This framework offers a
powerful tool for detecting potential thematic trends and providing a
high-level overview of scientific progress.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [286] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: U-Codec是一个超低帧率（5Hz）的神经语音编解码器，通过Transformer长期依赖模块和优化RVQ配置，在保持高质量重建的同时实现快速语音生成，并将LLM-based TTS扩展到32层RVQ，推理速度提升约3倍。


<details>
  <summary>Details</summary>
Motivation: 解决5Hz极端压缩导致的语音可懂度和频谱细节严重损失问题，探索在极低帧率下实现高保真语音重建和快速合成的可行性。

Method: 1. 引入基于Transformer的帧间长期依赖模块；2. 系统探索残差向量量化（RVQ）深度和码本大小的最优配置；3. 将U-Codec应用于基于LLM的自回归TTS模型，采用全局和局部层次架构捕捉多层token间的依赖关系。

Result: U-Codec将LLM-based TTS从3层RVQ（50Hz）扩展到32层RVQ（5Hz），推理速度比高帧率编解码器提升约3倍，同时保持了语音相似性和自然度。

Conclusion: 验证了使用高度压缩的5Hz离散token进行快速高保真语音合成的可行性，为极低帧率语音处理提供了有效解决方案。

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [287] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现大型音频语言模型在情感变化下的安全性存在不一致性，不同情感和强度会引发不同程度的危险响应，中等强度情感表达风险最高。


<details>
  <summary>Details</summary>
Motivation: 虽然大型音频语言模型在感知、推理和任务性能方面得到广泛研究，但其在副语言变化下的安全对齐仍未被充分探索，特别是说话者情感的影响。

Method: 构建包含多种情感和强度表达的恶意语音指令数据集，并评估多个最先进的大型音频语言模型。

Result: 发现显著的安全不一致性：不同情感引发不同程度的危险响应，强度影响呈非单调性，中等强度表达通常风险最大。

Conclusion: 这些发现揭示了大型音频语言模型中被忽视的脆弱性，呼吁开发专门针对情感变化的对齐策略，这是在实际场景中可信部署的前提条件。

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [288] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: SAKE是首个专门针对大型音频-语言模型中听觉属性知识编辑的基准，突破了传统文本和视觉模态的限制，评估了七种编辑方法在四个维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑研究主要集中在文本或视觉模态，缺乏对听觉模态的关注。SAKE旨在填补这一空白，探索如何高效更新音频-语言模型中的抽象听觉属性知识。

Method: 在两种大型音频-语言模型上，对七种知识编辑方法进行了系统性评估，从可靠性、泛化性、音频/文本局部性和可移植性四个维度进行分析。

Result: 结果揭示了多个挑战：保持与编辑无关的同类属性知识、将编辑泛化到多模态推理、以及在顺序更新下维持编辑效果。

Conclusion: SAKE为研究知识编辑如何扩展到听觉模态提供了原则性框架，为在更广泛的现实场景中维护和适应音频-语言模型开辟了新方向。

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [289] [DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model](https://arxiv.org/abs/2510.17662)
*Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: DELULU是一个说话人感知的自监督基础模型，通过在伪标签生成过程中整合外部监督，显著提升了说话人区分能力，在说话人验证等任务上比现有SSL模型取得62%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督语音模型在内容驱动任务上表现出色，但在说话人区分特征方面存在局限，无法满足说话人验证、聚类和分析等应用的需求。

Method: 利用ReDimNet的帧级嵌入指导k-means聚类过程，引入说话人区分性归纳偏置；采用掩码预测和去噪的双重目标函数训练，增强鲁棒性和泛化能力。

Result: 在说话人验证任务上EER相对改进达62%；在性别、年龄、口音和说话人计数等零样本分析任务上持续获得提升；无需任务特定微调即可实现优越性能。

Conclusion: DELULU是一个强大的通用说话人感知语音处理编码器，证明了整合外部监督可以有效增强自监督模型的说话人区分能力。

Abstract: Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [290] [Comparing LLMs for Sentiment Analysis in Financial Market News](https://arxiv.org/abs/2510.15929)
*Lucas Eduardo Pereira Teles,Carlos M. S. Figueiredo*

Main category: q-fin.ST

TL;DR: 比较大型语言模型与传统方法在金融新闻情感分析任务中的性能表现，结果显示LLM在大多数情况下优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型在金融领域自然语言处理任务中的性能差异，量化比较不同模型在金融新闻情感分析中的表现。

Method: 采用对比研究方法，将大型语言模型与经典方法在金融市场新闻情感分析任务中进行比较。

Result: 大型语言模型在绝大多数情况下表现优于传统模型。

Conclusion: 大型语言模型在金融新闻情感分析任务中具有显著优势，能够提供更准确的分析结果。

Abstract: This article presents a comparative study of large language models (LLMs) in
the task of sentiment analysis of financial market news. This work aims to
analyze the performance difference of these models in this important natural
language processing task within the context of finance. LLM models are compared
with classical approaches, allowing for the quantification of the benefits of
each tested model or approach. Results show that large language models
outperform classical models in the vast majority of cases.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [291] [Taming Modality Entanglement in Continual Audio-Visual Segmentation](https://arxiv.org/abs/2510.17234)
*Yuyang Hong,Qi Yang,Tao Zhang,Zili Wang,Zhaojin Fu,Kun Ding,Bin Fan,Shiming Xiang*

Main category: cs.MM

TL;DR: 本文提出了持续音频-视觉分割任务，通过碰撞式多模态复现框架解决多模态语义漂移和共现混淆问题，在三个音频-视觉增量场景中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态持续学习方法主要关注粗粒度任务，在细粒度持续学习场景中处理模态纠缠存在局限性。

Method: 提出碰撞式多模态复现框架：1）多模态样本选择策略选择模态一致性高的样本进行复现；2）碰撞式样本复现机制增加易混淆类的复现样本频率。

Result: 在三个音频-视觉增量场景中的综合实验表明，该方法显著优于单模态持续学习方法。

Conclusion: 所提出的CMR框架有效解决了多模态语义漂移和共现混淆问题，在多模态持续学习任务中表现出色。

Abstract: Recently, significant progress has been made in multi-modal continual
learning, aiming to learn new tasks sequentially in multi-modal settings while
preserving performance on previously learned ones. However, existing methods
mainly focus on coarse-grained tasks, with limitations in addressing modality
entanglement in fine-grained continual learning settings. To bridge this gap,
we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to
continuously segment new classes guided by audio. Through comprehensive
analysis, two critical challenges are identified: 1) multi-modal semantic
drift, where a sounding objects is labeled as background in sequential tasks;
2) co-occurrence confusion, where frequent co-occurring classes tend to be
confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework
is designed to address these challenges. Specifically, for multi-modal semantic
drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select
samples with high modal consistency for rehearsal. Meanwhile, for co-occurence
confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,
allowing for the increase of rehearsal sample frequency of those confusable
classes during training process. Moreover, we construct three audio-visual
incremental scenarios to verify effectiveness of our method. Comprehensive
experiments demonstrate that our method significantly outperforms single-modal
continual learning methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [292] [Lung Cancer Classification from CT Images Using ResNet](https://arxiv.org/abs/2510.16310)
*Olajumoke O. Adekunle,Joseph D. Akinyemi,Khadijat T. Ladoja,Olufade F. W. Onifade*

Main category: eess.IV

TL;DR: 提出了一种基于ResNet50的深度学习方法来改进肺癌CT图像的多分类，将肺组织分为三类（两种恶性亚型和一种良性），在LC25000数据集上达到了98.8%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有肺癌分类研究主要关注恶性与良性的二分类，而临床需要更精细的亚型分类。现有自动化系统的预测效果尚未达到临床应用标准。

Method: 使用预训练的ResNet50模型，在其架构顶部添加自定义层，并进行精细的超参数调优。数据集包含15,000张肺部CT图像，按10,200/2,550/2,250分割用于训练、验证和测试。

Result: 模型在测试集上达到了98.8%的准确率，相比之前在同一数据集上的模型性能有显著提升。

Conclusion: 该方法在肺癌多分类任务中表现出色，准确率显著优于现有方法，为临床肺癌亚型诊断提供了有前景的解决方案。

Abstract: Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed
and classified using medical imaging techniques, particularly computed
tomography (CT). Despite the integration of machine learning and deep learning
methods, the predictive efficacy of automated systems for lung cancer
classification from CT images remains below the desired threshold for clinical
adoption. Existing research predominantly focuses on binary classification,
distinguishing between malignant and benign lung nodules. In this study, a
novel deep learning-based approach is introduced, aimed at an improved
multi-class classification, discerning various subtypes of lung cancer from CT
images. Leveraging a pre-trained ResNet model, lung tissue images were
classified into three distinct classes, two of which denote malignancy and one
benign. Employing a dataset comprising 15,000 lung CT images sourced from the
LC25000 histopathological images, the ResNet50 model was trained on 10,200
images, validated on 2,550 images, and tested on the remaining 2,250 images.
Through the incorporation of custom layers atop the ResNet architecture and
meticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was
recorded. This represents a notable enhancement over the performance of prior
models on the same dataset.

</details>


### [293] [Time-Embedded Algorithm Unrolling for Computational MRI](https://arxiv.org/abs/2510.16321)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出了一种时间嵌入的算法展开方案，通过将迭代相关的近端操作和Onsager校正构建为时间嵌入神经网络，有效减少伪影和噪声放大，在MRI重建中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统算法展开方法中共享近端算子网络会引入伪影或模糊，而使用不同网络又会显著增加参数导致过拟合。受AMP中变化阈值近端算子和扩散模型中时间嵌入的启发，需要一种平衡的方法。

Method: 将向量AMP中的迭代相关近端操作和Onsager校正构建为时间嵌入神经网络，数据保真度操作中的标量权重也作为时间相关可学习参数。

Result: 在fastMRI数据集上的广泛实验表明，该方法有效减少混叠伪影和噪声放大，在各种加速率和数据集上达到最先进性能。

Conclusion: 时间嵌入策略可扩展到现有算法展开方法，在不显著增加计算复杂度的情况下提高重建质量。

Abstract: Algorithm unrolling methods have proven powerful for solving the regularized
least squares problem in computational magnetic resonance imaging (MRI). These
approaches unfold an iterative algorithm with a fixed number of iterations,
typically alternating between a neural network-based proximal operator for
regularization, a data fidelity operation and auxiliary updates with learnable
parameters. While the connection to optimization methods dictate that the
proximal operator network should be shared across unrolls, this can introduce
artifacts or blurring. Heuristically, practitioners have shown that using
distinct networks may be beneficial, but this significantly increases the
number of learnable parameters, making it challenging to prevent overfitting.
To address these shortcomings, by taking inspirations from proximal operators
with varying thresholds in approximate message passing (AMP) and the success of
time-embedding in diffusion models, we propose a time-embedded algorithm
unrolling scheme for inverse problems. Specifically, we introduce a novel
perspective on the iteration-dependent proximal operation in vector AMP (VAMP)
and the subsequent Onsager correction in the context of algorithm unrolling,
framing them as a time-embedded neural network. Similarly, the scalar weights
in the data fidelity operation and its associated Onsager correction are cast
as time-dependent learnable parameters. Our extensive experiments on the
fastMRI dataset, spanning various acceleration rates and datasets, demonstrate
that our method effectively reduces aliasing artifacts and mitigates noise
amplification, achieving state-of-the-art performance. Furthermore, we show
that our time-embedding strategy extends to existing algorithm unrolling
approaches, enhancing reconstruction quality without increasing the
computational complexity significantly.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [294] [Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude](https://arxiv.org/abs/2510.16948)
*Ruiming Guo,Ayush Bhandari*

Main category: cs.IT

TL;DR: 本文提出了一种基于无限制传感框架(USF)的数字超分辨率方法，通过模数编码增强测量精度，突破了传统数字采集在强-弱振幅差异下的限制，实现了幅度和时间维度的超分辨率恢复。


<details>
  <summary>Details</summary>
Motivation: 传统数字采集在处理强-弱振幅差异的脉冲信号时存在根本性限制：强分量可能被削波，弱分量可能被量化噪声淹没。在固定比特预算下，这种信息损失是不可避免的，因此需要同时解决幅度和时间结构的超分辨率问题。

Method: 基于无限制传感框架(USF)，采用模数编码技术增强测量精度，开发了适用于非带限核的新理论结果，并提出了鲁棒的离网格稀疏恢复算法。在飞行时间成像场景中实例化该框架。

Result: 数值模拟和硬件实验均验证了该方法在低位量化条件下的有效性，能够实现幅度和时间维度的超分辨率恢复，突破了传统方法的限制。

Conclusion: 无限制传感框架通过模数编码克服了传统数字采集的基本限制，为幅度和时间超分辨率提供了可行的解决方案，在低比特量化条件下展现出实际应用价值。

Abstract: The recovery of Dirac impulses, or spikes, from filtered measurements is a
classical problem in signal processing. As the spikes lie in the continuous
domain while measurements are discrete, this task is known as super-resolution
or off-the-grid sparse recovery. Despite significant theoretical and
algorithmic advances over the past decade, these developments often overlook
critical challenges at the analog-digital interface. In particular, when spikes
exhibit strong-weak amplitude disparity, conventional digital acquisition may
result in clipping of strong components or loss of weak ones beneath the
quantization noise floor. This motivates a broader perspective:
super-resolution must simultaneously resolve both amplitude and temporal
structure. Under a fixed bit budget, such information loss is unavoidable. In
contrast, the emerging theory and practice of the Unlimited Sensing Framework
(USF) demonstrate that these fundamental limitations can be overcome. Building
on this foundation, we demonstrate that modulo encoding within USF enables
digital super-resolution by enhancing measurement precision, thereby unlocking
temporal super-resolution beyond conventional limits. We develop new
theoretical results that extend to non-bandlimited kernels commonly encountered
in practice and introduce a robust algorithm for off-the-grid sparse recovery.
To demonstrate practical impact, we instantiate our framework in the context of
time-of-flight imaging. Both numerical simulations and hardware experiments
validate the effectiveness of our approach under low-bit quantization, enabling
super-resolution in amplitude and time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [295] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 该论文引入了一个包含1,893个用户问题的数据集，涵盖家庭机器人的12个类别和70个子类别，揭示了用户对机器人问答能力的需求差异，特别是新手与有经验用户的问题类型差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和对话界面在人机交互中的广泛应用，机器人回答用户问题的能力变得至关重要。现有可解释机器人研究主要关注"为什么"问题，而缺乏对其他类型用户问题的系统研究。

Method: 通过创建15个视频刺激和7个文本刺激，描绘机器人执行各种家庭任务的情景，在Prolific平台上向100名参与者收集他们在每个情境下想要询问机器人的问题。

Result: 数据集中最常见的类别是任务执行细节问题(22.5%)、机器人能力问题(12.7%)和性能评估问题(11.3%)。尽管关于机器人处理困难场景和确保正确行为的问题较少，但用户认为这些是最重要的。新手用户更倾向于询问简单事实问题。

Conclusion: 该数据集为识别机器人需要记录和暴露给对话界面的信息、基准测试问答模块以及设计符合用户期望的解释策略提供了宝贵基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [296] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA是一个用于单臂操作任务的统一评估生态系统，通过细粒度能力测试和系统性压力测试来诊断VLA代理的技能并测量其鲁棒性，解决了传统端任务成功指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA代理评估存在两个主要问题：1）粗粒度的端任务成功指标无法提供精确的技能诊断或测量对现实世界扰动的鲁棒性；2）碎片化的数据环境阻碍了可重复研究和通用模型的发展。

Method: 1）引入双轴评估协议：结合细粒度能力测试进行精确技能诊断和系统性压力测试测量鲁棒性；2）提供标准化API和大规模聚合数据集以减少碎片化；3）支持跨数据集训练和公平比较。

Result: 使用NEBULA评估发现，表现最佳的VLA代理在空间推理和动态适应等关键能力上存在困难，这些缺陷被传统端任务成功指标所掩盖。

Conclusion: NEBULA通过同时测量代理能做什么以及何时能可靠地执行，为构建鲁棒、通用的具身代理提供了实用基础。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [297] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA是一个多模态目标条件行为克隆框架，用于实现自主导管导航，融合视觉观察和操纵杆运动学，减少对操作员的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前心脏导管手术主要依赖手动操作，现有机器人系统缺乏智能自主性，导致操作者疲劳、辐射暴露增加和手术结果不一致。

Method: 提出DINO-CVA框架，将视觉观察和操纵杆运动学融合到联合嵌入空间，通过专家演示自回归预测动作，目标条件引导导航到指定目的地。

Result: 在合成血管模型上的实验显示，DINO-CVA能高精度预测动作，性能与仅使用运动学的基线相当，同时将预测基于解剖环境。

Conclusion: 多模态目标条件架构在导管导航中具有可行性，是减少操作员依赖、提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [298] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强的自动驾驶框架，通过度量引导对齐显式桥接认知推理和端到端规划，结合VLA模型的世界知识和E2E模型的物理可行性。


<details>
  <summary>Details</summary>
Motivation: 传统E2E驾驶模型能生成物理可行轨迹但缺乏世界知识处理长尾场景，而VLA模型有世界知识但3D推理能力有限导致物理不可行动作。需要结合两者优势。

Method: 1) VLA模块直接生成语义基础的驾驶轨迹；2) E2E模块使用密集轨迹词汇确保物理可行性；3) 度量引导轨迹评分器指导和对齐两个模块输出。

Result: 在ICCV 2025自动驾驶大挑战排行榜上，DiffVLA++实现了49.12的EPDMS评分。

Conclusion: DiffVLA++成功整合了认知推理和端到端规划的优势，通过度量引导对齐机制实现了更好的自动驾驶性能。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [299] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON是一个新的视觉-语言-动作模型范式，通过向动作头注入丰富的3D空间标记来解决现有VLA模型的空间推理缺陷，无需依赖专用传感器即可从RGB图像获取几何先验。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型基于2D编码器构建，存在空间推理差距，限制了泛化能力和适应性。现有的3D集成技术要么需要专用传感器且跨模态迁移差，要么注入缺乏几何信息的弱线索并损害视觉-语言对齐。

Method: FALCON利用空间基础模型从RGB图像获取强几何先验，包含可选的具身空间模型融合深度或姿态信息，并通过空间增强动作头处理空间标记以保持语言推理能力。

Result: 在三个仿真基准和十一个真实世界任务中，FALCON实现了最先进的性能，始终超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化下保持鲁棒性。

Conclusion: FALCON通过创新的空间标记注入范式有效解决了空间表示、模态可迁移性和对齐方面的限制，为VLA模型提供了强大的3D空间推理能力。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [300] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot系统使用立体相机、数字转台、工业机器人臂和3D分割高斯泼溅模型，通过操纵叶片来拍摄被遮挡植物细节的高分辨率图像，构建详细的"注释数字孪生"植物模型。


<details>
  <summary>Details</summary>
Motivation: 商用植物表型系统因叶片遮挡无法感知许多植物细节，需要开发能获取被遮挡区域详细图像的系统。

Method: 使用立体相机、数字转台、工业机器人臂和3D分割高斯泼溅模型，开发机器人算法来操纵叶片，拍摄被遮挡细节的高分辨率图像。

Result: 叶片分割准确率90.8%，叶片检测准确率86.2%，叶片提升/推动准确率77.9%，详细正反面图像拍摄准确率77.3%。

Conclusion: Botany-Bot系统能够有效构建详细的植物数字孪生模型，代码、视频和数据集已开源。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [301] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: 本文提出基于辩证行为疗法（DBT）的AI聊天机器人框架，旨在通过心理学原理解决当前AI在情绪适应和实时响应中的幻觉、错误输出等问题。


<details>
  <summary>Details</summary>
Motivation: 当前个性化AI聊天机器人开发存在维护困难、易产生幻觉和错误输出的问题，需要更稳健的解决方案。

Method: 应用辩证行为疗法（DBT）原则来调节聊天机器人对多样化用户输入的响应，建立基于心理学原理的框架。

Result: 研究验证DBT框架对AI聊天机器人性能的影响，评估其在提高可靠性、安全性和准确性方面的效果。

Conclusion: 基于心理学治疗模式的框架比纯技术干预能提供更稳健和可持续的AI聊天机器人解决方案。

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [302] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhüschen,Max Zettl,Antonia Lüönd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: SHIELD是一个基于LLM的监督系统，用于检测和缓解AI伴侣中的情感风险模式，包括情感过度依恋、边界侵犯等五个维度，在测试中显著减少了问题内容。


<details>
  <summary>Details</summary>
Motivation: 现有安全系统主要关注明显危害，很少处理早期阶段的问题行为，这些行为可能助长不健康的情感动态，如过度依恋或强化社交孤立。

Method: 开发了SHIELD系统，基于五个关注维度（情感过度依恋、同意和边界侵犯、伦理角色扮演违规、操纵性互动、社交孤立强化），使用特定系统提示来检测和缓解风险情感模式。

Result: 在五个主流LLM上的测试显示，问题内容的基础率（10-16%）显著降低至3-8%，相对减少50-79%，同时保留了95%的适当互动。系统达到59%的敏感性和95%的特异性。

Conclusion: 这个概念验证表明，透明、可部署的监督系统能够解决AI伴侣中的微妙情感操纵问题。大多数开发材料已作为开源材料提供。

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [303] [HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents](https://arxiv.org/abs/2510.15898)
*Farnaz Nouraei,Zhuorui Yong,Timothy Bickmore*

Main category: cs.HC

TL;DR: HealthDial是一个对话创作工具，帮助医疗工作者创建虚拟代理，通过多轮对话提供健康教育和咨询。它利用LLM自动生成基于文本健康材料的会话计划和对话，输出为有限状态机以确保内容安全。


<details>
  <summary>Details</summary>
Motivation: 解决医疗工作者创建虚拟健康教育代理的困难，确保教育内容完整覆盖且避免LLM幻觉带来的不安全建议。

Method: 使用LLM基于文本健康材料自动生成初始对话计划和内容，通过无代码界面让作者编辑，输出为有限状态机格式的虚拟代理对话。

Result: 可行性研究显示，HealthDial能帮助咨询师确保健康教育材料完整覆盖，同时创建患者可理解和执行的虚拟代理对话。

Conclusion: HealthDial为医疗工作者提供了一个有前景的工具，能够有效创建安全、完整的健康教育虚拟对话代理。

Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare
providers and educators create virtual agents that deliver health education and
counseling to patients over multiple conversations. HealthDial leverages large
language models (LLMs) to automatically create an initial session-based plan
and conversations for each session using text-based patient health education
materials as input. Authored dialogue is output in the form of finite state
machines for virtual agent delivery so that all content can be validated and no
unsafe advice is provided resulting from LLM hallucinations. LLM-drafted
dialogue structure and language can be edited by the author in a no-code user
interface to ensure validity and optimize clarity and impact. We conducted a
feasibility and usability study with counselors and students to test our
approach with an authoring task for cancer screening education. Participants
used HealthDial and then tested their resulting dialogue by interacting with a
3D-animated virtual agent delivering the dialogue. Through participants'
evaluations of the task experience and final dialogues, we show that HealthDial
provides a promising first step for counselors to ensure full coverage of their
health education materials, while creating understandable and actionable
virtual agent dialogue with patients.

</details>


### [304] [Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models](https://arxiv.org/abs/2510.16952)
*Austin Drake,Hang Dong*

Main category: cs.HC

TL;DR: 提出了一种将大型语言模型安全集成到交互式游戏引擎中的架构，允许玩家使用自然语言"编程"新行为，通过LLM将命令转换为受限领域特定语言来配置自定义实体-组件系统。


<details>
  <summary>Details</summary>
Motivation: 安全地将LLM集成到游戏引擎中，让玩家能够通过自然语言创造新行为，同时通过受限DSL降低风险。

Method: 使用LLM将自然语言命令转换为受限领域特定语言，配置运行时自定义实体-组件系统，并在2D法术制作游戏原型中评估不同模型家族和提示策略。

Result: 验证的LLM评估显示，较大模型能更好地捕捉创意意图，最佳提示策略因任务而异：思维链提升创意对齐，而少样本示例对生成复杂DSL脚本是必要的。

Conclusion: 这项工作为涌现式游戏玩法提供了经过验证的LLM-ECS模式，并为开发者提供了定量性能比较。

Abstract: We present a novel architecture for safely integrating Large Language Models
(LLMs) into interactive game engines, allowing players to "program" new
behaviors using natural language. Our framework mitigates risks by using an LLM
to translate commands into a constrained Domain-Specific Language (DSL), which
configures a custom Entity-Component-System (ECS) at runtime. We evaluated this
system in a 2D spell-crafting game prototype by experimentally assessing models
from the Gemini, GPT, and Claude families with various prompting strategies. A
validated LLM judge qualitatively rated the outputs, showing that while larger
models better captured creative intent, the optimal prompting strategy is
task-dependent: Chain-of-Thought improved creative alignment, while few-shot
examples were necessary to generate more complex DSL scripts. This work offers
a validated LLM-ECS pattern for emergent gameplay and a quantitative
performance comparison for developers.

</details>


### [305] [Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation](https://arxiv.org/abs/2510.17599)
*Hendric Voss,Lisa Michelle Bohnenkamp,Stefan Kopp*

Main category: cs.HC

TL;DR: 本研究比较了AQ-GT和AQ-GT-a两种语音手势生成框架，发现缺乏语义输入的AQ-GT在训练领域内概念传达更有效，而语义增强的AQ-GT-a在形状和尺寸表示方面泛化能力更好，但语义增强并不总能提升手势生成质量。


<details>
  <summary>Details</summary>
Motivation: 评估两种语音手势生成框架传达意义的能力以及人类对生成手势的感知，探索语义注释与性能之间的复杂关系。

Method: 使用SAGA空间通信语料库中的句子、上下文相似句子和新颖运动聚焦句子，进行以用户为中心的概念识别和拟人性评估。

Result: 原始AQ-GT框架在训练领域内概念传达更有效，AQ-GT-a框架在形状和尺寸表示方面泛化能力更好；参与者认为AQ-GT-a手势更具表现力和帮助性，但不认为更拟人。

Conclusion: 显式语义增强不能保证手势生成质量的提升，其有效性高度依赖上下文，表明专业化和泛化之间存在潜在权衡。

Abstract: This study explores two frameworks for co-speech gesture generation, AQ-GT
and its semantically-augmented variant AQ-GT-a, to evaluate their ability to
convey meaning through gestures and how humans perceive the resulting
movements. Using sentences from the SAGA spatial communication corpus,
contextually similar sentences, and novel movement-focused sentences, we
conducted a user-centered evaluation of concept recognition and human-likeness.
Results revealed a nuanced relationship between semantic annotations and
performance. The original AQ-GT framework, lacking explicit semantic input, was
surprisingly more effective at conveying concepts within its training domain.
Conversely, the AQ-GT-a framework demonstrated better generalization,
particularly for representing shape and size in novel contexts. While
participants rated gestures from AQ-GT-a as more expressive and helpful, they
did not perceive them as more human-like. These findings suggest that explicit
semantic enrichment does not guarantee improved gesture generation and that its
effectiveness is highly dependent on the context, indicating a potential
trade-off between specialization and generalization.

</details>


### [306] [ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input](https://arxiv.org/abs/2510.17617)
*Hendric Voss,Stefan Kopp*

Main category: cs.HC

TL;DR: 本文提出了一种零样本系统，能够从语言输入和图像信息生成语义连贯的图标性和指示性手势，显著改善了虚拟代理的多模态交流能力。


<details>
  <summary>Details</summary>
Motivation: 当前手势生成方法仅限于简单的节拍手势，无法传达语义意义。本文旨在解决语音手势合成中的核心挑战：生成与语言表达语义一致的图标性或指示性手势。

Method: 整合图像分析流程提取物体属性（形状、对称性、对齐），结合语义匹配模块将视觉细节与口语文本关联，通过逆运动学引擎合成图标性和指示性手势，并与自然节拍手势结合。

Result: 用户研究表明，在语音表达模糊的情况下，系统生成的手势显著提高了参与者识别物体属性的能力，证实了手势的可解释性和交流价值。

Conclusion: 虽然复杂形状表示仍存挑战，但结果表明情境感知的语义手势对于创建表达性和协作性虚拟代理至关重要，标志着向高效、稳健的具身人机交互迈出了重要一步。

Abstract: Human communication combines speech with expressive nonverbal cues such as
hand gestures that serve manifold communicative functions. Yet, current
generative gesture generation approaches are restricted to simple, repetitive
beat gestures that accompany the rhythm of speaking but do not contribute to
communicating semantic meaning. This paper tackles a core challenge in
co-speech gesture synthesis: generating iconic or deictic gestures that are
semantically coherent with a verbal utterance. Such gestures cannot be derived
from language input alone, which inherently lacks the visual meaning that is
often carried autonomously by gestures. We therefore introduce a zero-shot
system that generates gestures from a given language input and additionally is
informed by imagistic input, without manual annotation or human intervention.
Our method integrates an image analysis pipeline that extracts key object
properties such as shape, symmetry, and alignment, together with a semantic
matching module that links these visual details to spoken text. An inverse
kinematics engine then synthesizes iconic and deictic gestures and combines
them with co-generated natural beat gestures for coherent multimodal
communication. A comprehensive user study demonstrates the effectiveness of our
approach. In scenarios where speech alone was ambiguous, gestures generated by
our system significantly improved participants' ability to identify object
properties, confirming their interpretability and communicative value. While
challenges remain in representing complex shapes, our results highlight the
importance of context-aware semantic gestures for creating expressive and
collaborative virtual agents or avatars, marking a substantial step forward
towards efficient and robust, embodied human-agent interaction. More
information and example videos are available here:
https://review-anon-io.github.io/ImaGGen.github.io/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [307] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX提出了一种基于快慢思维的适应性程序修复方法，通过问题描述响应来指导修复工作流，在SWE-bench Lite上达到60.67%的pass@1性能


<details>
  <summary>Details</summary>
Motivation: 利用快慢思维增强大型语言模型代理在复杂任务（如程序修复）上的能力，平衡修复效率和准确性

Method: 设计基于问题描述响应的适应性程序修复方法SIADAFIX，使用慢思维bug修复代理完成复杂任务，快思维工作流决策组件优化和分类问题描述，根据问题复杂度自适应选择三种修复模式（简单、中等、困难）

Result: 在SWE-bench Lite测试中，使用Claude-4 Sonnet模型达到60.67%的pass@1性能，在所有开源方法中达到最先进水平

Conclusion: SIADAFIX有效平衡了修复效率和准确性，为自动化程序修复提供了新的思路

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [308] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 研究发现代码翻译任务中存在"多示例悖论"：虽然静态相似度指标随示例增多略有提升，但功能正确性在少示例（5-25个）时达到峰值，过多示例反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在代码翻译任务中，提供大量上下文示例（多示例提示）是否能真正提升性能的假设。

Method: 通过大规模实证研究，对超过90,000个翻译进行系统评估，从零示例到多达625个示例（约10万到80万tokens）的不同配置下测试代码翻译性能。

Result: 发现静态相似度指标随示例数量增加有轻微改善，但功能正确性在5-25个示例时达到最佳，继续增加示例会降低功能性能。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了"越多越好"的通用ICL假设，强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [309] [Attention to Non-Adopters](https://arxiv.org/abs/2510.15951)
*Kaitlyn Zhou,Kristina Gligorić,Myra Cheng,Michelle S. Lam,Vyoma Raman,Boluwatife Aminu,Caeley Woo,Michael Brockman,Hannah Cha,Dan Jurafsky*

Main category: cs.CY

TL;DR: 论文指出当前LLM开发主要依赖采纳者数据，但66%美国人从未使用ChatGPT，建议必须纳入非采纳者视角以避免任务遗漏、加剧不平等和开发盲点。


<details>
  <summary>Details</summary>
Motivation: 大多数美国人（66%）从未使用过基于聊天的LLM，但LLM开发和评估主要依赖采纳者数据，这可能导致忽视非采纳者的需求和任务，加剧不平等。

Method: 通过与非采纳者进行案例研究，分析其需求与当前用户的差异，识别新的推理任务，并探索通过以人为中心的方法系统整合非采纳者需求。

Result: 案例研究表明非采纳者的需求与当前用户存在显著差异，这些需求指向了新的推理任务类型。

Conclusion: 必须将非采纳者视角纳入LLM开发过程，以确保模型具有广泛实用性和能力，避免不平等和开发盲点。

Abstract: Although language model-based chat systems are increasingly used in daily
life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,
66% had never used ChatGPT. At the same time, LLM development and evaluation
rely mainly on data from adopters (e.g., logs, preference data), focusing on
the needs and tasks for a limited demographic group of adopters in terms of
geographic location, education, and gender. In this position paper, we argue
that incorporating non-adopter perspectives is essential for developing broadly
useful and capable LLMs. We contend that relying on methods that focus
primarily on adopters will risk missing a range of tasks and needs prioritized
by non-adopters, entrenching inequalities in who benefits from LLMs, and
creating oversights in model development and evaluation. To illustrate this
claim, we conduct case studies with non-adopters and show: how non-adopter
needs diverge from those of current users, how non-adopter needs point us
towards novel reasoning tasks, and how to systematically integrate non-adopter
needs via human-centered methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [310] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，通过检索增强评估研究想法的合理性和贡献度，并在多领域数据集上验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中日益普及，需要建立稳健的评估框架来确保生成想法的有效性和实用性。

Method: 引入ScholarEval检索增强评估框架，基于两个核心标准评估研究想法：合理性（基于现有文献的方法有效性）和贡献度（相对于先前研究的进步程度）。

Result: 在ScholarIdeas数据集上的评估显示，ScholarEval在覆盖专家标注要点方面显著优于所有基线方法，并在可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。

Conclusion: ScholarEval框架在文献参与度、想法精炼和实用性方面显著优于深度研究方法，为研究想法评估提供了有效工具。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [311] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 这篇论文是关于基于强化学习的代理搜索的综述，系统梳理了这一新兴领域，从功能角色、优化策略和应用范围三个维度进行组织，旨在构建可靠且可扩展的RL驱动代理搜索系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。代理搜索通过多步交互解决了这些限制，而强化学习为自适应和自我改进的搜索行为提供了强大机制。

Method: 从三个互补维度组织该领域：(i) RL的功能角色，(ii) RL的优化策略，(iii) RL的应用范围。总结了代表性方法、评估协议和应用。

Result: 提供了首个关于基于RL的代理搜索的全面概述，建立了该领域的系统框架，并创建了相关的GitHub存储库。

Conclusion: 讨论了构建可靠和可扩展RL驱动代理搜索系统的开放挑战和未来方向，希望激发RL与代理搜索集成方面的未来研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [312] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能同时感知和生成视觉、文本、语音和动作，实现更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态信息，实现更自然的人类行为模拟。

Method: 采用新颖的SA-MoE架构，将各模态路由到专门专家，通过统一注意力骨干网络进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA与模态特定基线表现相当，同时支持高级多模态和全双工行为。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [313] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一的框架，通过分层组织图信息和使用规划代理协调文本与视觉模态，解决了视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在图理解中面临输入令牌限制导致的可扩展性瓶颈，以及缺乏有效的文本和视觉模态协调机制。

Method: GraphVista采用分层方法将图信息组织到轻量级GraphRAG基础中，仅检索任务相关的文本描述和高分辨率视觉子图；同时引入规划代理，根据任务复杂度路由到最适合的模态。

Result: GraphVista能够扩展到比现有基准大200倍的大型图，在质量上比现有最先进基线提升4.4倍，持续优于基于文本、视觉和融合的方法。

Conclusion: GraphVista通过充分利用两种模态的互补优势，显著提升了图理解的可扩展性和性能。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [314] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的智能LLM代理，能够自动完成从数据源到分析师级深度研究报告的端到端流程，仅需80亿参数即可超越基于工作流的先进专有LLM代理。


<details>
  <summary>Details</summary>
Motivation: 现有基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流而无法实现完全自主的数据科学。随着强大LLM的出现，从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出基于课程学习的智能代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力；同时引入数据驱动的轨迹合成框架构建高质量训练数据。

Result: 实验表明，仅80亿参数的DeepAnalyze在性能上超越了基于最先进专有LLM构建的先前工作流代理，能够执行从数据问答、专业分析任务到开放式数据研究的广泛数据任务。

Conclusion: DeepAnalyze模型、代码和训练数据已开源，为自主数据科学的发展铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [315] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练视觉语言模型（VLM）代理构建内部世界模型，通过状态估计和转移建模分解推理过程，在五个代理基准测试中达到0.82分，显著优于未训练模型和专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 解决从文本状态到复杂视觉观察的转变带来的部分可观测性问题，探索VLM代理能否通过显式视觉状态推理构建内部世界模型。

Method: 使用强化学习将推理过程建模为部分可观测马尔可夫决策过程（POMDP），分解为状态估计和转移建模，设计世界建模奖励和双层次广义优势估计（Bi-Level GAE）。

Result: 3B参数模型在五个代理基准测试中达到0.82分，相比未训练模型（0.21）提升3倍，优于GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）。

Conclusion: 代理的内部信念表示是任务依赖的，自然语言擅长捕捉语义关系，结构化格式对精确操作至关重要，通过视觉状态推理可显著提升VLM代理性能。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [316] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 本文研究了一个基于网页部署、工具增强的LLM健康教练系统，通过真实用户实验发现统一的重工具策略虽然提升平均价值，但对特定用户群体（特别是低健康素养/高自我效能用户）产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 研究真实环境中工具增强LLM健康教练的表现，探索个性化策略对用户群体的影响，特别是识别和解决平均指标掩盖的亚群体伤害问题。

Method: 使用离线策略评估（OPE）分析因子化决策头（工具/风格），通过包含隐藏原型的轻量级模拟器测试信息增益奖励策略。

Result: 统一重工具策略在日志上提升平均价值但损害特定亚群体；添加早期信息增益奖励可缩短特质识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出了评估优先的个性化路径：冻结生成器，在类型化奖励上学习亚群体感知决策头，并始终报告按原型分类的指标以揭示平均指标掩盖的亚群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [317] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个推理时、模型可插拔的代理框架，通过四个顺序模块分解多模态验证：视觉真实性评估、跨模态一致性分析、检索增强的事实检查以及校准判断模块，在MMFakeBench验证集上达到81.65% F1分数，优于最强零样本基线7.65个百分点。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿结合文本和图像的多模态帖子传播错误信息，超出了人工事实核查能力。监督检测模型需要特定领域训练数据，且无法泛化到不同的操纵策略。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：视觉真实性评估检测AI生成图像，跨模态一致性分析识别上下文不当的重新利用，检索增强的事实检查通过迭代问题生成将声明基于网络证据，校准判断模块整合所有信号。该框架协调视觉语言模型推理与定向网络检索，输出结构化且带有引用的推理过程。

Result: 在MMFakeBench验证集（1,000个样本）上，MIRAGE与GPT-4o-mini组合达到81.65% F1分数和75.1%准确率，优于最强零样本基线（GPT-4V与MMD-Agent的74.0% F1）7.65个百分点，同时保持34.3%的假阳性率，而仅判断基线的假阳性率为97.3%。测试集结果（5,000个样本）确认了泛化能力，达到81.44% F1和75.08%准确率。消融研究显示视觉验证贡献5.18 F1点，检索增强推理贡献2.97点。

Conclusion: 分解的代理推理与网络检索可以在没有特定领域训练的情况下匹配监督检测器的性能，使得在标记数据稀缺的多模态场景中能够进行错误信息检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [318] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该论文提出了一种将超大型语言模型的推理能力蒸馏到更小、更高效模型的方法，通过结构感知损失优化来提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 代码生成不仅需要准确的标记预测，更需要理解解决方案级别的结构关系。超大型语言模型具有复杂的推理能力，但部署成本高，因此需要将其能力蒸馏到更小的模型中。

Method: 通过结构感知损失优化方法，训练小模型模拟超大型语言模型的推理和问题解决能力，学习识别正确解决方案路径，建立问题定义与解决方案之间的结构对应关系。

Result: 实验结果显示，经过微调的模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过简单易实现的廉价过程开发的微调模型，能够超越标记级生成，深入掌握给定问题解决方案的整体结构，实现高效的代码生成。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [319] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统研究了LLMs作为预测工具的能力，构建了Prophet Arena评估基准，发现LLMs已具备不错的预测能力，但也存在事件回忆不准确、数据源误解等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在大规模互联网数据上训练的LLMs快速发展，探索LLMs预测现实世界未来事件的潜力，这种新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建Prophet Arena评估基准，持续收集实时预测任务，并将每个任务分解为不同的流程阶段，以支持受控的大规模实验。

Result: 综合评估显示，许多LLMs已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。

Conclusion: LLMs作为预测工具已具备相当能力，但在实现卓越预测智能方面仍面临关键瓶颈，如事件回忆不准确、数据源误解以及在接近解决时信息聚合速度较市场慢。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [320] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出HyCAM框架，通过上下文注意力调制机制解决LLMs多任务适应问题，在保持通用知识的同时增强任务特定特征，平均性能提升3.65%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多任务适应中存在知识保留与任务专业化之间的平衡问题，传统微调方法存在灾难性遗忘和资源消耗大的缺陷。

Method: 提出上下文注意力调制(CAM)机制，动态调节自注意力模块表示；构建HyCAM框架，结合共享CAM模块和多个轻量级专用CAM模块，使用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的实验表明，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: HyCAM框架有效解决了LLMs在多任务适应中的挑战，实现了更好的知识保留与任务专业化平衡。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [321] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: SELECT是一个动态锚点选择框架，通过两阶段评估机制自动发现最优锚点进行精确概念擦除，同时识别关键边界锚点以保护相关概念，解决了固定锚点策略导致的概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的概念擦除方法通常依赖固定锚点策略，这会导致概念重现和侵蚀等关键问题。通过因果追踪发现擦除对锚点选择具有内在敏感性，因此需要更优的锚点选择方法。

Method: 提出SELECT框架，基于兄弟排他概念作为更优锚点类别，采用新颖的两阶段评估机制：自动发现最优擦除锚点，同时识别关键边界锚点以保护相关概念。

Result: 广泛评估表明SELECT作为通用锚点解决方案，不仅高效适应多个擦除框架，还在关键性能指标上持续优于现有基线，单个概念的锚点挖掘平均仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了固定锚点策略的局限性，实现了更精确的概念擦除和更好的概念保护。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [322] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型在输出错误答案时仍能感知到正确的视觉证据，这种现象称为'看见但不相信'。通过选择性注意力掩码干预，无需训练即可提升多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 系统研究视觉语言模型失败的原因：是未能感知视觉证据还是未能有效利用证据。

Method: 通过层间注意力动态分析，发现浅层主要关注文本，深层稀疏但可靠地关注局部证据区域。引入基于选择性注意力掩码的推理时干预方法。

Result: 干预方法在LLaVA、Qwen、Gemma和InternVL等多个VLM家族中一致提升准确率。

Conclusion: VLMs内部编码了可靠的证据但未充分利用，使这些信号显式化可以弥合感知与推理之间的差距。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [323] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: MA-SAPO是一个多代理框架，通过将评估结果与结构化推理相结合来指导系统化的提示优化，相比传统方法提供更透明、可审计和可控的提示改进。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将评估视为黑盒，仅依赖数值分数而缺乏对提示成功或失败原因的解释，且严重依赖难以解释和控制的试错式改进。

Method: MA-SAPO采用两阶段多代理框架：推理阶段代理协作解释指标分数、诊断弱点并合成针对性改进；测试阶段代理检索推理资产分析优化提示并应用基于证据的编辑。

Result: 在HelpSteer1/2基准测试中，相比单次提示、检索增强基线和先前多代理策略，MA-SAPO实现了持续改进。

Conclusion: 通过将评估信号转化为可解释的推理链，MA-SAPO能够产生更透明、可审计和可控的提示改进，验证了该方法的有效性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [324] [Detecting streaks in smart telescopes images with Deep Learning](https://arxiv.org/abs/2510.17540)
*Olivier Parisot,Mahmoud Jaziri*

Main category: astro-ph.IM

TL;DR: 本文测试和改进了多种深度学习方法，用于检测智能望远镜拍摄的天文图像中的卫星条纹。


<details>
  <summary>Details</summary>
Motivation: 卫星在夜空中的可见性对天文学和天文摄影产生负面影响，会在观测图像中引入条纹，需要额外的后处理来减轻这种不良影响。

Method: 测试和改进了多种深度学习方法，使用2022年3月至2023年2月期间智能望远镜拍摄的原始天文数据进行卫星条纹检测。

Result: 开发了有效的卫星条纹检测方法，能够识别天文图像中的卫星轨迹。

Conclusion: 深度学习方法可以有效检测天文图像中的卫星条纹，有助于减轻卫星对天文观测的负面影响。

Abstract: The growing negative impact of the visibility of satellites in the night sky
is influencing the practice of astronomy and astrophotograph, both at the
amateur and professional levels. The presence of these satellites has the
effect of introducing streaks into the images captured during astronomical
observation, requiring the application of additional post processing to
mitigate the undesirable impact, whether for data loss or cosmetic reasons. In
this paper, we show how we test and adapt various Deep Learning approaches to
detect streaks in raw astronomical data captured between March 2022 and
February 2023 with smart telescopes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [325] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: Long Exposure系统通过解决微调中的Shadowy Sparsity问题，加速参数高效微调(PEFT)，实现最高2.49倍的端到端微调加速。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)技术在时间投入和运营成本方面存在效率低下的问题，特别是微调过程中出现的Shadowy Sparsity问题尚未得到充分解决。

Method: Long Exposure系统包含三个核心组件：Shadowy-sparsity Exposer使用长感知范围捕获更多稀疏细节；Sequence-oriented Predictor处理大序列输入和动态参数；Dynamic-aware Operator优化计算模式和内存访问。

Result: 广泛评估表明，Long Exposure在端到端微调中实现了最高2.49倍的加速，优于现有最优方法。

Conclusion: Long Exposure为加速LLMs的PEFT提供了有前景的进展，有效解决了微调过程中的效率瓶颈问题。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [326] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE框架通过提示引导的数据增强进行幻觉检测，使用对比马氏距离评分评估LLM生成内容的真实性，无需人工标注即可显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM产生幻觉信息的问题，由于缺乏标注的幻觉数据集，需要开发低成本的数据增强方法来改善幻觉检测。

Method: 使用提示引导从LLM生成真实和幻觉数据作为数据增强，引入基于激活空间分布的对比马氏距离评分来评估中间嵌入的真实性。

Result: PALE在幻觉检测性能上显著优于基线方法，提升幅度达6.55%，且无需额外人工标注。

Conclusion: PALE框架提供了一种有效且实用的幻觉检测方法，通过低成本数据增强和创新的评分机制，显著提升了检测准确性。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [327] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法作为RLVR的核心方法，在提升LLM推理能力方面存在局限性：只能强化预训练偏差，无法发现全新解决方案，OOD泛化仅在目标任务与预训练偏差一致时有效。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO在提升LLM推理能力时表现不一致的原因，探究其在什么条件下能改善推理并实现OOD泛化。

Method: 从数据分布角度进行理论证明和受控实验，训练transformer模型并评估在推理深度、输入长度、token表示和组合性等方面的泛化能力。

Result: GRPO是一种保守的重加权方案，受限于基础模型分布，无法发现全新解决方案；OOD改进仅在目标任务与模型预训练偏差一致时出现，ID任务收益随性能饱和而减少。

Conclusion: GRPO不是通用推理增强器，而是强化预训练偏差的工具，需要开发能扩展模型能力超越预训练起源的新算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [328] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种连接零阶优化和锐度感知最小化(SAM)的新方法，通过指数倾斜目标在平均损失和最大损失之间实现平滑过渡，开发了解决软SAM目标的零阶算法。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的函数（扰动参数下的期望目标），而SAM方法关注邻域内的最大损失以获得平坦最小值。本文旨在明确连接这两种方法。

Method: 提出指数倾斜目标作为过渡桥梁，开发新的零阶算法来求解参数化的软SAM目标，精确刻画倾斜SAM框架的锐度概念。

Result: 该方法可作为SAM变体的无梯度和内存高效替代方案，在分类、多项选择QA和语言生成等下游任务上比传统零阶基线获得更好的泛化性能。

Conclusion: 通过指数倾斜目标成功连接了零阶优化和SAM方法，提出的零阶算法在保持内存效率的同时实现了更好的泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [329] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 该论文通过层间因果修补分析发现，语言模型的对齐过程是空间局部化的，主要集中在中间层激活，而非扩散到整个网络。


<details>
  <summary>Details</summary>
Motivation: 虽然基于人类反馈的强化学习（RLHF）已成为语言模型对齐的主流方法，但其内部工作机制仍不透明，需要系统分析对齐过程如何影响模型内部表示。

Method: 在Llama-3.2-1B模型上应用层间因果修补技术，分析基础模型与对齐模型在人类偏好对上的差异，并使用LASSO回归识别关键层。

Result: 发现对齐过程空间局部化：中间层激活编码了决定奖励一致行为的独特子空间，而早期和晚期层基本不受影响；只有少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个方向性的低秩过程，而非扩散的参数化过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [330] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个可扩展的强化学习Web代理训练环境，通过紧凑的浏览器环境和高效的服务端管理，解决了现有环境在上下文噪声、动作确定性和扩展性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RL Web代理训练环境存在以下问题：上下文信息过多且嘈杂；动作执行缺乏确定性；无法有效扩展并行RL训练。需要构建一个既能提供真实浏览器交互，又能控制服务端状态的可扩展环境。

Method: 1) 构建紧凑、站点无关的浏览器环境，平衡上下文复杂度和动作复杂度；2) 通过高效启动和重置Web服务器，创建可扩展的RL训练环境。

Result: 在WebArena的购物CMS和Gitlab任务中达到最先进的单提示成功率，同时将启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，支持单主机200+并发容器。

Conclusion: WEBSERV提供了一个高效、可扩展的RL Web代理训练解决方案，显著提升了训练效率和性能。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [331] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出了C-SMILES分子表示方法，通过分解SMILES为元素-令牌对并引入复制增强机制，显著提升了逆合成预测的准确性和分子生成的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测精度降低。

Method: 引入C-SMILES表示法分解SMILES为元素-令牌对，结合复制增强机制动态决定生成新令牌或保留未变化分子片段，并集成SMILES对齐指导增强注意力一致性。

Result: 在USPTO-50K数据集上达到67.2%的top-1准确率，在USPTO-FULL数据集上达到50.8%准确率，生成分子有效性达99.9%。

Conclusion: 为结构感知分子生成建立了新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [332] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种基于多任务学习和分层高斯过程的方法来预测NLP模型的学习曲线，通过潜在变量建模任务间相关性，支持零样本预测，并应用主动学习策略降低预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线有助于在满足特定性能目标的同时，减少计算开销和数据集获取与整理的成本。

Method: 将学习曲线预测任务构建为多任务学习问题，采用分层结构建模任务数据，使用潜在变量多输出高斯过程来捕捉任务间相关性和层次依赖关系。

Result: 该方法能够在三个小型NLP数据集（包含最多30条学习曲线）上实现接近真实缩放规律的预测，验证了框架的有效性。

Conclusion: 该框架支持以较低成本开发概率缩放规律，通过主动学习策略可进一步降低预测不确定性，为NLP模型的性能预测提供了有效工具。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [333] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: UDS框架用于SFT中的高效在线批次选择，通过核范数捕获数据效用和样本内多样性，使用轻量级内存缓冲区估计样本间多样性，无需外部资源且计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决现有在线批次选择方法的三个主要问题：(i)仅依赖数据效用而忽略多样性，(ii)需要外部资源如参考模型或验证集，(iii)训练时间超过全数据集训练。

Method: UDS框架利用对数矩阵的核范数捕获数据效用和样本内多样性，通过低维嵌入比较与历史样本的轻量级内存缓冲区估计样本间多样性，无需外部资源且避免不必要的反向传播。

Result: 在多个基准测试中，UDS在不同数据预算下始终优于最先进的在线批次选择方法，并显著减少了与全数据集微调相比的训练时间。

Conclusion: UDS提供了一个无需外部资源、计算高效的在线批次选择框架，在保持性能的同时显著减少训练时间，解决了现有方法的局限性。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [334] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级别的评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，引入了决策变量和约束的精确度、召回率等细粒度指标。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法通常将公式视为整体，依赖解决方案准确性或运行时间等粗粒度指标，掩盖了结构或数值错误，需要更精细的评估框架。

Method: 开发了包含决策变量和约束的精确度、召回率、约束和目标函数RMSE、基于token使用和延迟的效率指标的综合评估框架，在六种提示策略下评估GPT-5、LLaMA 3.1 Instruct和DeepSeek Math模型。

Result: GPT-5持续优于其他模型，思维链、自一致性和模块化提示策略最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精确度和决策变量指标起次要作用，简洁输出提高计算效率。

Conclusion: 确立了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率，为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [335] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，通过分析专家路由模式来识别模型是否经过知识蒸馏，在黑白盒设置下均能有效工作，检测准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的知识蒸馏检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险，需要更可靠的检测手段。

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，分析专家在不同输入下的专业化和协作模式；提出Shadow-MoE黑盒方法，通过辅助蒸馏构建代理MoE表示来比较任意模型对之间的模式。

Result: 建立了全面的可复现基准，提供多样化的蒸馏检查点；实验显示在各种场景下检测准确率超过94%，对基于提示的规避具有强鲁棒性，优于现有基线方法。

Conclusion: 该方法有效检测知识蒸馏，突出了LLM中结构习惯转移的重要性，为知识产权保护和模型多样性提供了可靠解决方案。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [336] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型遗忘过程的隐蔽攻击方法——后门遗忘攻击，攻击者可以在模型正常遗忘过程中植入后门，使模型在触发条件出现时恢复已遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重大语言模型的兴起，需要研究遗忘过程本身是否可能被植入后门，使模型在表面上成功遗忘，但在特定触发条件下恢复原有行为。

Method: 通过分析注意力汇聚现象，将后门触发器放置在注意力汇聚位置，并调整其注意力值来增强后门持久性。

Result: 实验证明，基于注意力汇聚的后门遗忘攻击能够可靠地在触发条件出现时恢复遗忘的知识，而在无触发条件下与正常遗忘模型无法区分。

Conclusion: 注意力汇聚现象为后门遗忘攻击提供了有效途径，揭示了遗忘过程的安全脆弱性，需要开发更安全的遗忘机制。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [337] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 该论文提出了一个评估LLM发现潜在信息的统一基准，通过三智能体框架在三个场景中测试LLM通过对话揭示用户隐藏偏好的能力。


<details>
  <summary>Details</summary>
Motivation: LLM在生成通用文本方面表现出色，但在需要用户特定偏好的场景中，用户往往不会明确表达所有偏好，需要模型能够发现和推理这些潜在信息。

Method: 采用三智能体框架（用户、助手、评判者）构建统一基准，涵盖20 Questions游戏、个性化问答和个性化文本摘要三个渐进式场景，进行多轮交互评估。

Result: LLM确实能够通过对话发现潜在信息，但成功率差异很大（32%-98%），取决于任务复杂度、主题和隐藏属性数量等因素。

Conclusion: 有效的偏好推理仍然是构建真正自适应AI系统的开放前沿，该基准为研究个性化交互中的潜在信息发现提供了首个系统框架。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [338] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出了一种名为软掩码（Soft-Masking, SM）的新方法，用于改进基于掩码扩散的语言模型。该方法通过动态混合掩码标记嵌入与先前解码步骤中预测的前k个标记嵌入，为模型提供更丰富的信息先验，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散语言模型在解码时采用二元选择（保留掩码或替换为预测标记），这会在保留掩码时丢弃有价值的预测信息。作者旨在解决这一限制，通过保留先前计算的上下文信息，让掩码标记的部分信息能够传播到多个步骤。

Method: 提出软掩码方法，动态混合掩码标记嵌入与先前解码步骤中预测的前k个标记嵌入。开发了相应的训练方法，将预训练的掩码扩散语言模型适配为包含SM。在169M参数模型上继续预训练，并对Dream-7B和Dream-Coder-7B两个最先进的扩散模型进行微调。

Result: 在169M参数模型上，SM改善了困惑度和MAUVE分数。在Dream-7B和Dream-Coder-7B模型上，SM在多个编码基准测试中持续提升性能，特别是在高吞吐量设置下表现更佳。

Conclusion: 软掩码方法有效解决了传统掩码扩散模型中信息丢失的问题，通过提供更丰富的先验信息，显著提升了语言模型的性能，尤其在代码生成任务中表现出色。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [339] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言在环框架，使用大语言模型将自然语言反馈转换为标量效用，用于在数值搜索空间上进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要将复杂、细微或主观的目标转化为可量化的优化目标，而现有方法只能接受受限的反馈格式且需要为每个领域特定问题定制模型。

Method: 利用大语言模型将各种类型的文本反馈转换为一致的效用信号，并轻松包含灵活的用户先验，无需手动设计核函数，同时保持贝叶斯优化的样本效率和原则性不确定性量化。

Result: 该混合方法不仅为决策者提供了更自然的接口，而且在反馈受限的情况下优于传统的贝叶斯优化基线和仅使用大语言模型的优化器。

Conclusion: 语言在环框架通过结合大语言模型和贝叶斯优化，有效解决了将自然语言反馈转化为优化目标的问题，在反馈受限场景下表现优异。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [340] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 本文提出了一个样本级别的评估框架来衡量后训练对预训练知识的影响，重点关注知识遗忘和反向转移现象，并通过大规模分析揭示了不同后训练阶段对模型知识保留的影响模式。


<details>
  <summary>Details</summary>
Motivation: 当前规模化后训练虽然能显著提升语言模型能力，但其对预训练知识的影响机制尚不明确。传统任务平均指标会掩盖知识遗忘和反向转移的具体变化，需要更精细的评估方法。

Method: 提出样本级别的评估范式：使用1->0转换（正确变错误）量化知识遗忘，0->1转换（错误变正确）量化反向转移。对于选择题基准，还引入了机会调整变体来消除随机猜测的影响。

Result: 大规模分析发现：领域持续预训练导致中度遗忘和低至中度反向转移；RL/SFT后训练在数学和逻辑任务上产生中至高度反向转移且遗忘较低；对指令调优模型应用RL/SFT的效果对数据规模敏感；模型融合不能可靠缓解遗忘。

Conclusion: 该框架为系统评估后训练如何改变预训练知识提供了实用标准，有助于推动通用AI系统的发展。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [341] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出FedPURIN框架，通过整数规划识别关键参数进行传输，结合稀疏聚合方案，在保持性能的同时显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中通信效率低下的问题，现有方法通信负担重，阻碍实际部署。

Method: 使用整数规划策略识别关键参数，结合稀疏聚合方案，实现通信效率优化。

Result: 在标准图像分类基准测试中，在多种非IID条件下表现出与最先进方法相当的性能，同时通过稀疏聚合实现可量化的通信减少。

Conclusion: 为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [342] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 本文提出了一种基于半监督学习和正未标记学习策略的深度学习模型，用于考古遗址预测建模，解决了考古学中标签稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 考古预测建模面临结构性标签稀缺的挑战：正样本稀少且大多数位置未标记。需要开发能够在这种数据稀缺情况下有效工作的预测方法。

Method: 采用半监督正未标记学习策略，实现为语义分割模型。使用动态伪标签技术，并通过RNN实现的CRF进行精炼，在严重类别不平衡下提高标签置信度。

Result: 在数字高程模型数据集上，模型性能与最先进的LAMAP相当，但获得更高的Dice分数。在原始卫星图像上，通过分层k折交叉验证评估，模型保持性能并产生具有更好可解释性的预测表面。

Conclusion: 半监督学习为在大规模、稀疏标注的景观中识别未发现遗址提供了一种有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [343] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉回路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间，同时达到或超越现有最优方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续表示学习方法直接使用预训练特征进行下游任务时，在相似性匹配阶段容易遇到多重共线性问题，而更先进的方法在实时低延迟应用中计算成本过高。

Method: 提出Fly-CL框架，受果蝇嗅觉回路启发，与多种预训练主干网络兼容，通过渐进式解决多重共线性问题，实现更有效的相似性匹配。

Result: Fly-CL显著减少了训练时间，同时在各种网络架构和数据机制下的广泛模拟实验中，性能达到或超过了当前最优方法。

Conclusion: Fly-CL通过生物启发设计有效解决了持续表示学习中的多重共线性挑战，具有低时间复杂度，适用于实时应用。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [344] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 该论文提出了领域泛化持续学习(DGCL)新设置，并开发了自适应领域变换(DoT)方法，通过解耦语义和领域信息来提升模型在动态环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法假设训练和测试领域相同，无法适应真实世界中动态变化的领域环境，需要开发能同时处理任务序列和领域变化的泛化方法。

Method: 提出自适应领域变换(DoT)，基于预训练模型，解耦语义和领域相关信息，通过自适应变换任务表示来实现输出对齐，确保平衡和泛化的预测。

Result: DoT作为插件策略显著提升了现有持续学习基线方法在DGCL设置下的性能，在完整参数调优和参数高效调优范式下均有效，且能积累领域泛化知识并保持资源效率。

Conclusion: DoT方法成功解决了领域泛化持续学习的挑战，为动态环境中的智能系统提供了有效的学习和泛化解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [345] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的新型自编码器正则化方法，通过优化代码矩阵的奇异值分布使其接近高斯分布，提高泛化能力并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在代码分布控制方面存在不足，需要一种能确保代码矩阵具有高斯分布特性的正则化方法，以提升模型泛化性能。

Method: 使用矩阵自由能作为可微损失函数，通过随机矩阵理论优化代码矩阵的奇异值分布，使其匹配独立同分布高斯随机矩阵的分布特性。

Result: 经验模拟显示，通过随机梯度下降最小化负矩阵自由能可获得高斯化代码，在训练和测试集上均表现出良好的泛化能力。

Conclusion: 基于矩阵自由能最大化的自编码器能可靠生成高斯代码，在欠定逆问题中具有应用价值，为自编码器正则化提供了新思路。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [346] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文探讨了生成视觉模型中内部表征的演变，分析了从GANs和VAEs到扩散模型的转变，提出严格意义合成与广义合成的区分，并论证扩散模型通过分层表征挑战了统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解生成视觉模型中内部表征的演变，特别是从GANs和VAEs到扩散模型的转变，以及这种转变如何影响我们对生成AI的理解。

Method: 通过模型架构的详细分析和针对性的实验设置，干预分层表征，展示扩散模型如何分散表征负担。

Result: 研究发现扩散模型将表征负担分散到不同层次，挑战了统一内部空间的假设，表明生成AI是专门化过程的涌现配置。

Conclusion: 结论是生成AI应被理解为专门化过程的涌现配置，而非内容的直接合成，这需要对生成AI的理解进行重新定位。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [347] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出MILES方法，通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，网络过度依赖某一模态导致性能不佳，限制了多模态学习的潜力

Method: MILES利用训练过程中模态条件利用率差异，动态调整学习率以平衡各模态学习速度

Result: 在四个多模态联合融合任务中，MILES优于七个最先进基线方法，有效平衡模态使用并提升性能

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES方法能产生更强的模态编码器

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [348] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种0.25M参数的视觉变换器变体，用于区分心源性肺水肿与非心源性肺水肿和正常肺部，在肺超声视频分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的视觉变异性高，使得在肺超声视频中区分心源性肺水肿变得困难，重叠的B线和胸膜伪影进一步增加了自动分类的复杂性。

Method: 提出ZACH-ViT，移除位置嵌入和[CLS]标记，使其完全置换不变；引入ShuffleStrides数据增强，在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在380个肺超声视频上评估，ZACH-ViT获得最高的验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而所有竞争模型都崩溃为平凡分类。

Conclusion: 结果表明，将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [349] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 本文提出PrivacyPAD框架，通过强化学习动态路由文本块，在保护隐私和任务性能之间取得最优平衡。


<details>
  <summary>Details</summary>
Motivation: 解决用户向大语言模型提交查询时的隐私困境：使用强大但可能泄露数据的云端模型，还是使用本地模型但性能下降。

Method: 将隐私保护委托问题重新定义为顺序决策问题，引入强化学习框架训练智能体动态路由文本块，区分可替换PII和任务关键PII。

Result: 在隐私-效用边界上达到新的最先进水平，验证了在敏感环境中部署LLM时需要学习自适应策略的必要性。

Conclusion: PrivacyPAD框架成功解决了隐私保护与任务性能的权衡问题，为敏感环境中的LLM部署提供了有效解决方案。

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [350] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 该论文系统比较了生成式和判别式分类器在成员推理攻击中的脆弱性，发现生成式分类器由于显式建模联合概率P(X,Y)而具有更高的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式和判别式分类器在成员推理攻击中的系统性比较有限，本文旨在填补这一空白，从理论和实证角度分析两类分类器的隐私脆弱性差异。

Method: 通过理论分析生成式分类器易受攻击的原因，并在9个基准数据集上对判别式、生成式和伪生成式文本分类器进行综合实证评估，使用多种MIA策略进行测试。

Result: 完全生成式分类器（显式建模P(X,Y)）对成员推理攻击最为脆弱，其标准推理方法显著放大了隐私风险。生成式分类器在不同训练数据量下都表现出更高的隐私泄露。

Conclusion: 生成式分类器存在固有的效用-隐私权衡，在隐私敏感应用中需要谨慎使用。研究结果推动了开发既能保持效用又能缓解成员推理漏洞的隐私保护生成式分类器的未来研究方向。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [351] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出可验证微调协议，通过零知识证明确保模型是从公开初始化模型按照声明的训练程序和可审计数据集承诺获得的，解决了当前模型发布实践中的数据使用和更新计算缺乏保证的问题。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调的发布实践无法提供关于使用数据和更新计算的可靠保证，存在信任缺口，特别是在受监管和去中心化部署场景中。

Method: 结合五个要素：数据源承诺、可验证采样器、限制参数高效微调的更新电路、递归聚合证明、来源绑定和可信执行属性卡。

Result: 在英语和双语指令混合任务中，方法在严格预算内保持实用性，实现实际证明性能。策略配额零违规，私有采样窗口无索引泄露。

Conclusion: 端到端可验证微调对于真实参数高效管道是可行的，为受监管和去中心化部署解决了关键信任问题。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [352] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 本文提出了一个信息论框架来量化大语言模型(LLMs)在对抗攻击中的信息泄露风险，通过互信息计算可安全披露的信息量，为审计和防御提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击通过恶意指令推断LLMs的未知目标属性，如触发有害响应的标志或未学习信息的恢复程度。当前信息泄露程度缺乏系统量化，导致审计无原则指导，防御者难以权衡透明度与风险。

Method: 建立信息论框架，将观测信号Z与目标属性T之间的互信息I(Z;T)作为每次查询泄露的比特数，推导出达到误差ε所需的最小查询次数为log(1/ε)/I(Z;T)。

Result: 在7个LLMs上的实验表明：仅暴露答案标记需要约1000次查询；添加logits可减少到约100次；揭示完整思维过程仅需几十次查询。泄露率的小幅增加可使攻击成本从二次方降至对数级。

Conclusion: 该框架为部署LLMs时平衡透明度与安全性提供了首个原则性衡量标准，揭示了信息泄露与攻击效率之间的量化关系。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [353] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一个基于变分推理的多模态越狱发现框架，通过学习文本-图像提示的联合后验分布来生成隐蔽的对抗输入，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态红队方法依赖脆弱的模板，专注于单攻击设置，只能暴露有限的漏洞。需要更系统的方法来发现视觉语言模型的新漏洞。

Method: 使用变分推理框架将多模态越狱发现转化为学习文本-图像提示的联合后验分布，训练轻量级攻击器来近似后验，结合排版文本提示、扩散图像合成和结构化干扰三种策略。

Result: 在HarmBench和HADES基准测试中，VERA-V在开源和前沿VLM上始终优于最先进基线，在GPT-4o上攻击成功率比最佳基线高出53.75%。

Conclusion: VERA-V提供了一个概率框架来系统发现多模态漏洞，证明了变分推理在多模态安全评估中的有效性。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


### [354] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一种实用的卡上人脸验证匹配设计，使用64/128位紧凑模板，通过PCA-ITQ离线生成模板，在卡上通过恒定时间汉明距离进行比较，满足ISO/IEC传输约束和隐私目标。


<details>
  <summary>Details</summary>
Motivation: 解决人脸验证在智能卡应用中的实际问题，包括模板存储、传输效率和隐私保护，同时满足ISO/IEC标准要求。

Method: 使用PCA-ITQ生成紧凑的64/128位二进制模板，在卡上通过恒定时间汉明距离进行匹配，采用固定长度的APDU命令和仅决策状态字，避免分数泄露。

Result: 在FAR=1%时，两种模板长度均达到TPR=0.836，128位模板比64位模板有更低的EER。验证时间在9.6kbps下为43.9ms(64b)和52.3ms(128b)，在38.4kbps下均小于14ms。

Conclusion: 短二进制模板、固定载荷的仅决策APDU和恒定时间匹配能够满足ISO/IEC传输约束，具有宽裕的时间余量，并符合ISO/IEC 24745隐私目标。当前局限是单数据集评估和设计级时序分析。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [355] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus是一个针对文本到图像模型的安全防御框架，通过内部调节器和非可微调学习机制来抵御白盒攻击者的恶意微调攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型安全措施在面临知道模型参数并能调整的白盒攻击者时失效，需要新的防御机制。

Method: 设计了内部调节器将不安全输入特征解码为零向量，同时采用非可微调学习机制强化模型对齐，防止恶意微调。

Result: 实验验证了在安全内容生成上的性能保持完整，同时有效拒绝不安全内容生成，并能抵抗各种白盒微调攻击。

Conclusion: Patronus框架为文本到图像模型提供了全面的保护，能够有效防御白盒攻击者的恶意行为。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [356] [Filtering of Small Components for Isosurface Generation](https://arxiv.org/abs/2510.16684)
*Devin Zhao,Rephael Wenger*

Main category: cs.GR

TL;DR: 该论文研究了如何通过简单的预过滤方法去除等值面中的微小干扰组件，同时保留主要可视化结构。


<details>
  <summary>Details</summary>
Motivation: 从CT扫描或MRI等扫描数据构建的等值面通常包含许多极小的组件，这些组件会分散可视化注意力且不属于任何几何模型。

Method: 使用简单的数据预过滤技术来移除等值面中的微小组件。

Result: 实验结果表明该方法能有效去除小组件，同时对构成可视化主体的大组件没有影响。

Conclusion: 简单的预过滤方法可以有效改善等值面可视化质量，消除干扰性的微小组件。

Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface
is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some
$\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$.
Isosurfaces constructed from scanned data such as CT scans or MRIs often
contain extremely small components that distract from the visualization and do
not form part of any geometric model produced from the data. Simple
prefiltering of the data can remove such small components while having no
effect on the large components that form the body of the visualization. We
present experimental results on such filtering.

</details>


### [357] [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](https://arxiv.org/abs/2510.17101)
*Lu Yin,Ziying Shi,Yinghao Wu,Xinyu Yi,Feng Xu,Shihui Guo*

Main category: cs.GR

TL;DR: SAIP是首个考虑身体形状差异的稀疏惯性运动捕捉方法，通过分解传感器测量中的形状和姿态信息，能够有效处理不同体型（如儿童）的运动捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖标准成人身体模型，难以泛化到体型差异大的个体（如儿童），因为身体形状变化会影响IMU测量的加速度数据。

Method: 1. 训练回归模型将真实身体的IMU加速度转换为匹配标准成人身体模型；2. 使用现有方法估计标准体型身体的全身体运动；3. 通过第二个回归模型将关节速度映射回真实身体，结合形状感知物理优化策略计算全局运动。

Result: SAIP能够有效处理不同体型的运动捕捉任务，并提出了首个惯性形状估计方案。创建了包含10名儿童和10成人的IMU运动捕捉数据集，总时长400分钟。

Conclusion: SAIP通过形状感知的方法解决了惯性运动捕捉中的体型泛化问题，为不同体型个体的运动捕捉提供了有效解决方案。

Abstract: Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [358] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 本文通过Yelp评论的HSAN分类器分析食源性疾病信号，并与纽约市官方餐厅检查结果对比，发现在人口普查区级别两者相关性极小。


<details>
  <summary>Details</summary>
Motivation: 食源性疾病是严重的公共卫生问题，餐厅是调查疫情的关键场所。传统报告渠道有限，而社交媒体平台提供丰富的用户生成内容，可以及时提供公共卫生信号。

Method: 使用分层S形注意力网络(HSAN)分类器分析Yelp评论信号，并与纽约市卫生局2023年官方餐厅检查结果进行比较，在人口普查区级别评估相关性，比较HSAN分数分布，并绘制空间模式。

Result: 在人口普查区级别，HSAN信号与检查分数之间相关性极小，且C级餐厅数量不同的区域间HSAN分数无显著差异。

Conclusion: 社交媒体信号与官方检查结果在宏观层面相关性有限，需要进一步进行地址级别的分析来探索更精细的关联。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>
