{"id": "2509.09699", "pdf": "https://arxiv.org/pdf/2509.09699", "abs": "https://arxiv.org/abs/2509.09699", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6587\u6863\u7ea7\u77e5\u8bc6\u56fe\u8c31(KG)\u6765\u8868\u793a\u4e34\u5e8a\u6587\u6863\uff0c\u7528\u4e8e\u81ea\u52a8\u5316ICD-9\u7f16\u7801\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u80fd\u4fdd\u755990%\u4fe1\u606f\u7684\u540c\u65f6\u51cf\u5c1123%\u6587\u672c\u91cf\uff0c\u5728PLM-ICD\u67b6\u6784\u4e0a\u5b9e\u73b0Macro-F1\u63d0\u53473.20%\uff0c\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e34\u5e8a\u6587\u6863\u6807\u51c6\u5316\u7f16\u7801\u5bf9\u4e34\u5e8a\u7814\u7a76\u3001\u533b\u9662\u7ba1\u7406\u548c\u60a3\u8005\u62a4\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u7f16\u7801\u8017\u65f6\u8d39\u529b\u3002\u81ea\u52a8\u5316\u7f16\u7801\u9762\u4e34\u9ad8\u7ef4\u957f\u5c3e\u76ee\u6807\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u8868\u793a\u8f93\u5165\u6587\u6863\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u6587\u6863\u7ea7\u77e5\u8bc6\u56fe\u8c31\u6765\u7ed3\u6784\u5316\u8868\u793a\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u8f93\u5165\u6587\u6863\uff0c\u5c06KG\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684PLM-ICD\u7f16\u7801\u67b6\u6784\u4e2d\uff0c\u5229\u7528KG\u4e2d\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\u4fe1\u606f\u589e\u5f3a\u6587\u6863\u8868\u793a\u3002", "result": "KG\u8868\u793a\u4ec5\u9700\u539f\u6587\u672c23%\u7684\u7bc7\u5e45\u5373\u53ef\u4fdd\u755990%\u4fe1\u606f\uff0c\u5728ICD-9\u81ea\u52a8\u5316\u7f16\u7801\u4efb\u52a1\u4e2dMacro-F1\u5f97\u5206\u63d0\u5347\u6700\u9ad8\u8fbe3.20%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u6587\u6863\u8868\u793a\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4e34\u5e8a\u7f16\u7801\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u6587\u6863\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.09700", "pdf": "https://arxiv.org/pdf/2509.09700", "abs": "https://arxiv.org/abs/2509.09700", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.", "AI": {"tldr": "\u63d0\u51faCross-Layer Attention Probing (CLAP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406LLM\u6574\u4e2a\u6b8b\u5dee\u6d41\u4e2d\u7684\u6fc0\u6d3b\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u548c\u68c0\u6d4b\u540e\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u751f\u6210\u4e0d\u51c6\u786e\u6587\u672c\uff08\u5e7b\u89c9\uff09\u7684\u53ef\u9760\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u8de8\u5c42\u6ce8\u610f\u529b\u63a2\u6d4b(CLAP)\u6280\u672f\uff0c\u5c06LLM\u6574\u4e2a\u6b8b\u5dee\u6d41\u4e2d\u7684\u6fc0\u6d3b\u4f5c\u4e3a\u8054\u5408\u5e8f\u5217\u8fdb\u884c\u5904\u7406\uff0c\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u3002", "result": "\u57285\u4e2aLLM\u548c3\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLAP\u5728\u8d2a\u5a6a\u89e3\u7801\u548c\u9ad8\u6e29\u91c7\u6837\u54cd\u5e94\u4e2d\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u68c0\u6d4b\uff0c\u5e76\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u4fdd\u6301\u9ad8\u53ef\u9760\u6027\u3002", "conclusion": "CLAP\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u652f\u6301\u68c0\u6d4b\u540e\u7f13\u89e3\u7b56\u7565\uff0c\u76f8\u6bd4\u76f4\u63a5\u7f13\u89e3\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8LLM\u53ef\u9760\u6027\u3002"}}
{"id": "2509.09701", "pdf": "https://arxiv.org/pdf/2509.09701", "abs": "https://arxiv.org/abs/2509.09701", "authors": ["JungHo Jung", "Junhyun Lee"], "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "categories": ["cs.CL"], "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset.", "AI": {"tldr": "\u672c\u6587\u4ece\u6b63\u5219\u5316\u89d2\u5ea6\u7814\u7a76\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u63a2\u7d22\u8de8\u6a21\u6001\u548c\u540c\u6a21\u6001\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u6982\u5ff5\uff0c\u5728MuST-C\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u63a5\u8fd1SOTA\u7684\u6027\u80fd\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u9762\u4e34\u914d\u5bf9\u8bed\u97f3-\u6587\u672c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u5229\u7528\u673a\u5668\u7ffb\u8bd1\u7684\u53cc\u8bed\u6587\u672c\u6570\u636e\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u514b\u670d\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u4ece\u6b63\u5219\u5316\u89c6\u89d2\u6784\u5efa\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7814\u7a76\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08\u8de8\u6a21\u6001\uff09\u548cR-drop\uff08\u540c\u6a21\u6001\uff09\u7684\u4f5c\u7528\uff0c\u5206\u6790\u673a\u5668\u7ffb\u8bd1\u635f\u5931\u7cfb\u6570\u4f5c\u4e3a\u53e6\u4e00\u79cd\u6b63\u5219\u5316\u6e90\uff0c\u63d0\u51fa\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u6982\u5ff5\u6765\u4f18\u5316\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u5185\u8c03\u4f18\u8d85\u53c2\u6570\u80fd\u591f\u5728MuST-C\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4e09\u79cd\u6b63\u5219\u5316\u6e90\uff08\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3001R-drop\u548cMT\u635f\u5931\u7cfb\u6570\uff09\u5e76\u57fa\u4e8e\u6b63\u5219\u5316\u5730\u5e73\u7ebf\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2509.09702", "pdf": "https://arxiv.org/pdf/2509.09702", "abs": "https://arxiv.org/abs/2509.09702", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "Creativity Benchmark\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8425\u9500\u521b\u610f\u9886\u57df\u8868\u73b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u521b\u610f\u4e13\u5bb6\u5bf911,012\u6b21\u533f\u540d\u6bd4\u8f83\u7684\u504f\u597d\u5206\u6790\uff0c\u663e\u793a\u6a21\u578b\u8868\u73b0\u7d27\u5bc6\u805a\u96c6\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u54c1\u724c\u6216\u63d0\u793a\u7c7b\u578b\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u8425\u9500\u521b\u610f\u9886\u57df\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u54c1\u724c\u7ea6\u675f\u521b\u610f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6bd4\u8f83\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u7684\u5dee\u5f02\u3002", "method": "\u8986\u76d6100\u4e2a\u54c1\u724c\uff0812\u4e2a\u7c7b\u522b\uff09\u548c\u4e09\u79cd\u63d0\u793a\u7c7b\u578b\uff08\u6d1e\u5bdf\u3001\u60f3\u6cd5\u3001\u75af\u72c2\u60f3\u6cd5\uff09\uff0c\u6536\u96c6678\u540d\u521b\u610f\u4ece\u4e1a\u8005\u7684\u4eba\u7c7b\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u4f7f\u7528Bradley-Terry\u6a21\u578b\u5206\u6790\uff0c\u5e76\u8ba1\u7b97\u4f59\u5f26\u8ddd\u79bb\u6765\u8bc4\u4f30\u6a21\u578b\u591a\u6837\u6027\u3002", "result": "\u6a21\u578b\u8868\u73b0\u7d27\u5bc6\u805a\u96c6\uff08\u0394\u03b8\u22480.45\uff09\uff0c\u5934\u5bf9\u5934\u83b7\u80dc\u6982\u7387\u4e3a61%\uff1b\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u5f31\u4e14\u4e0d\u4e00\u81f4\uff1b\u4f20\u7edf\u521b\u9020\u529b\u6d4b\u8bd5\u4ec5\u90e8\u5206\u9002\u7528\u4e8e\u54c1\u724c\u7ea6\u675f\u4efb\u52a1\u3002", "conclusion": "\u9700\u8981\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u4f30\u548c\u591a\u6837\u6027\u611f\u77e5\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u8bc4\u4f30\u3002"}}
{"id": "2509.09720", "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "ASOS\u662f\u4e00\u4e2a\u5305\u542b50\u79cd\u5e38\u89c1\u8d85\u5e02\u7269\u54c1\u7684\u9ad8\u8d28\u91cf3D\u7eb9\u7406\u7f51\u683c\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\uff0c\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u4f9d\u8d56\u5408\u6210\u6a21\u578b\u6216\u96be\u4ee5\u83b7\u53d6\u7684\u4e13\u4e1a\u7269\u54c1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u8d85\u5e02\u73af\u5883\u4e2d\u5e38\u89c1\u7269\u54c1\u7684\u9ad8\u8d28\u91cf3D\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u6293\u53d6\u3001\u7269\u4f53\u68c0\u6d4b\u7b49\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528\u8fd0\u52a8\u7ed3\u6784\u91cd\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u91c7\u96c6\u7269\u54c1\u76843D\u7f51\u683c\uff0c\u751f\u6210\u6c34\u5bc6\u7f51\u683c\u6a21\u578b\uff0c\u6db5\u76d610\u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u7269\u54c1\uff0c\u5177\u6709\u591a\u6837\u5316\u7684\u5f62\u72b6\u3001\u5c3a\u5bf8\u548c\u91cd\u91cf\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b50\u79cd\u6613\u83b7\u53d6\u8d85\u5e02\u7269\u54c1\u7684\u5168\u97623D\u6570\u636e\u96c6\uff0c\u6240\u6709\u7269\u54c1\u5747\u53ef\u4ece\u6fb3\u5927\u5229\u4e9a\u4e3b\u8981\u8d85\u5e02\u8fde\u9501\u5e97\u91c7\u8d2d\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u7f51\u683c\u6a21\u578b\u3002", "conclusion": "ASOS\u6570\u636e\u96c6\u5f3a\u8c03\u53ef\u8bbf\u95ee\u6027\u548c\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\uff0c\u4e3a\u7269\u4f53\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.09703", "pdf": "https://arxiv.org/pdf/2509.09703", "abs": "https://arxiv.org/abs/2509.09703", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.", "AI": {"tldr": "CTCC\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u8a00\u6a21\u578b\u6307\u7eb9\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7f16\u7801\u6765\u5b9e\u73b0\u6240\u6709\u6743\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9690\u853d\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u90e8\u7f72\u5f15\u53d1\u4e86\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u62c5\u5fe7\uff0c\u73b0\u6709\u6307\u7eb9\u65b9\u6cd5\u5b58\u5728\u53ef\u68c0\u6d4b\u6027\u3001\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u6a21\u578b\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6848\u3002", "method": "\u63d0\u51faCTCC\u6846\u67b6\uff0c\u91c7\u7528\u89c4\u5219\u9a71\u52a8\u65b9\u6cd5\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7f16\u7801\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff08\u5982\u53cd\u4e8b\u5b9e\u5173\u7cfb\uff09\uff0c\u800c\u975e\u4f9d\u8d56\u8bcd\u7ea7\u6216\u5355\u8f6e\u89e6\u53d1\uff0c\u652f\u6301\u9ed1\u76d2\u8bbf\u95ee\u4e0b\u7684\u6307\u7eb9\u9a8c\u8bc1\u3002", "result": "\u5728\u591aLLM\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCTCC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5 consistently \u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6709\u6548\u51cf\u5c11\u8bef\u62a5\u548c\u6307\u7eb9\u6cc4\u9732\u98ce\u9669\u3002", "conclusion": "CTCC\u4e3a\u5b9e\u9645LLM\u90e8\u7f72\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u9760\u5b9e\u7528\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5728\u90e8\u5206\u89e6\u53d1\u66b4\u9732\u60c5\u51b5\u4e0b\u57fa\u4e8e\u5171\u4eab\u8bed\u4e49\u89c4\u5219\u7684\u6301\u7eed\u6784\u5efa\u3002"}}
{"id": "2509.09721", "pdf": "https://arxiv.org/pdf/2509.09721", "abs": "https://arxiv.org/abs/2509.09721", "authors": ["Jiayi Miao", "Dingxin Lu", "Zhuqi Wang"], "title": "A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "After natural disasters, accurate evaluations of damage to housing are\nimportant for insurance claims response and planning of resources. In this\nwork, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)\nframework. On top of classical RAG architecture, we further the framework to\ndevise a two-branch multimodal encoder structure that the image branch employs\na visual encoder composed of ResNet and Transformer to extract the\ncharacteristic of building damage after disaster, and the text branch harnesses\na BERT retriever for the text vectorization of posts as well as insurance\npolicies and for the construction of a retrievable restoration index. To impose\ncross-modal semantic alignment, the model integrates a cross-modal interaction\nmodule to bridge the semantic representation between image and text via\nmulti-head attention. Meanwhile, in the generation module, the introduced modal\nattention gating mechanism dynamically controls the role of visual evidence and\ntext prior information during generation. The entire framework takes end-to-end\ntraining, and combines the comparison loss, the retrieval loss and the\ngeneration loss to form multi-task optimization objectives, and achieves image\nunderstanding and policy matching in collaborative learning. The results\ndemonstrate superior performance in retrieval accuracy and classification index\non damage severity, where the Top-1 retrieval accuracy has been improved by\n9.6%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6(MM-RAG)\uff0c\u7528\u4e8e\u81ea\u7136\u707e\u5bb3\u540e\u623f\u5c4b\u635f\u574f\u8bc4\u4f30\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7f16\u7801\u5668\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\u5b9e\u73b0\u56fe\u50cf\u548c\u6587\u672c\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5728\u68c0\u7d22\u51c6\u786e\u7387\u548c\u635f\u574f\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u540e\u51c6\u786e\u8bc4\u4f30\u623f\u5c4b\u635f\u574f\u5bf9\u4e8e\u4fdd\u9669\u7406\u8d54\u548c\u8d44\u6e90\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u591a\u6a21\u6001\u7f16\u7801\u5668\u7ed3\u6784\uff1a\u56fe\u50cf\u5206\u652f\u4f7f\u7528ResNet\u548cTransformer\u63d0\u53d6\u5efa\u7b51\u635f\u574f\u7279\u5f81\uff0c\u6587\u672c\u5206\u652f\u4f7f\u7528BERT\u68c0\u7d22\u5668\u5904\u7406\u5e16\u5b50\u548c\u4fdd\u9669\u653f\u7b56\u6587\u672c\uff1b\u96c6\u6210\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff1b\u5f15\u5165\u6a21\u6001\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\u52a8\u6001\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9\u8bc1\u636e\u548c\u6587\u672c\u5148\u9a8c\u4fe1\u606f\u4f5c\u7528\u3002", "result": "\u5728\u68c0\u7d22\u51c6\u786e\u7387\u548c\u635f\u574f\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff0cTop-1\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u9ad8\u4e869.6%\u3002", "conclusion": "\u8be5MM-RAG\u6846\u67b6\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u4f18\u5316\u76ee\u6807\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u56fe\u50cf\u7406\u89e3\u548c\u653f\u7b56\u5339\u914d\u7684\u534f\u540c\u5b66\u4e60\uff0c\u4e3a\u707e\u540e\u623f\u5c4b\u635f\u574f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09704", "pdf": "https://arxiv.org/pdf/2509.09704", "abs": "https://arxiv.org/abs/2509.09704", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.", "AI": {"tldr": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u671f\u9009\u62e9\u4e2d\u8868\u73b0\u51fa\u7684\u672a\u6765vs\u73b0\u5728\u5bfc\u5411\u504f\u597d\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u597d\u662f\u5426\u53ef\u88ab\u7cfb\u7edf\u6027\u64cd\u7eb5\u3002\u901a\u8fc7\u4eba\u7c7b\u5b9e\u9a8c\u534f\u8bae\u8bc4\u4f30\u591a\u6a21\u578b\uff0c\u63d0\u51fa\u53ef\u64cd\u7eb5\u6027\u65f6\u95f4\u5bfc\u5411\u6307\u6807(MTO)\uff0c\u53d1\u73b0\u63a8\u7406\u578b\u6a21\u578b\u5728\u9762\u5411\u672a\u6765\u7684\u63d0\u793a\u4e0b\u66f4\u503e\u5411\u9009\u62e9\u5ef6\u8fdf\u9009\u9879\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u504f\u597d\u51b3\u7b56\u4e2d\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4e86\u89e3\u5176\u662f\u5426\u5177\u6709\u53ef\u9884\u6d4b\u7684\u65f6\u95f4\u5bfc\u5411\u504f\u597d\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u597d\u662f\u5426\u53ef\u4ee5\u88ab\u63d0\u793a\u8bcd\u64cd\u7eb5\uff0c\u4e3aAI\u52a9\u624b\u4e0e\u4eba\u7c7b\u957f\u671f\u76ee\u6807\u5bf9\u9f50\u63d0\u4f9b\u8bbe\u8ba1\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u6539\u7f16\u7684\u4eba\u7c7b\u5b9e\u9a8c\u534f\u8bae\uff0c\u5728\u65f6\u95f4\u6743\u8861\u4efb\u52a1\u4e2d\u8bc4\u4f30\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u51b3\u7b56\u8005\u8fdb\u884c\u57fa\u51c6\u6bd4\u8f83\u3002\u5f15\u5165MTO\u6307\u6807\u6765\u8861\u91cf\u6a21\u578b\u5728\u9762\u5411\u672a\u6765\u548c\u73b0\u5728\u63d0\u793a\u4e0b\u7684\u504f\u597d\u53d8\u5316\u3002", "result": "\u63a8\u7406\u578b\u6a21\u578b(\u5982DeepSeek-Reasoner\u548cgrok-3-mini)\u5728\u9762\u5411\u672a\u6765\u7684\u63d0\u793a\u4e0b\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u5ef6\u8fdf\u9009\u9879\uff0c\u4f46\u5728\u8de8\u8eab\u4efd\u6216\u5730\u7406\u4f4d\u7f6e\u7684\u4e2a\u6027\u5316\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002\u80fd\u591f\u6b63\u786e\u63a8\u7406\u65f6\u95f4\u5bfc\u5411\u7684\u6a21\u578b\u4f1a\u5185\u5316\u4f5c\u4e3aAI\u51b3\u7b56\u8005\u7684\u672a\u6765\u5bfc\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u5e94\u4e0e\u5f02\u8d28\u957f\u671f\u76ee\u6807\u5bf9\u9f50\u7684AI\u52a9\u624b\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e2a\u6027\u5316\u60c5\u5883\u6821\u51c6\u548c\u793e\u4f1a\u610f\u8bc6\u90e8\u7f72\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u548c\u9002\u5e94\u4e0d\u540c\u7528\u6237\u65f6\u95f4\u504f\u597d\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2509.09722", "pdf": "https://arxiv.org/pdf/2509.09722", "abs": "https://arxiv.org/abs/2509.09722", "authors": ["Taylor Archibald", "Tony Martinez"], "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u56fe\u50cf\u589e\u5f3a\u53d8\u4f53\u8f6c\u5f55\u548cNeedleman-Wunsch\u5bf9\u9f50\u5668\u878d\u5408\uff0c\u63d0\u9ad8\u566a\u58f0\u5386\u53f2\u6587\u6863\u6587\u672c\u63d0\u53d6\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027", "motivation": "\u89e3\u51b3\u566a\u58f0\u5386\u53f2\u6587\u6863\u4e2dLLM\u6587\u672c\u63d0\u53d6\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u8f6c\u5f55\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "method": "\u4f7f\u7528Gemini 2.0 Flash\u5bf9\u6bcf\u4e2a\u56fe\u50cf\u7684\u591a\u4e2a\u589e\u5f3a\u53d8\u4f53\u8fdb\u884c\u8f6c\u5f55\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49Needleman-Wunsch\u98ce\u683c\u5bf9\u9f50\u5668\u878d\u5408\u8f93\u51fa\uff0c\u751f\u6210\u5171\u8bc6\u8f6c\u5f55\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570", "result": "\u5728622\u4efd\u5bbe\u5915\u6cd5\u5c3c\u4e9a\u5dde\u6b7b\u4ea1\u8bb0\u5f55\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u5355\u6b21\u8f6c\u5f55\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u53474\u4e2a\u767e\u5206\u70b9\uff1b\u586b\u5145\u548c\u6a21\u7cca\u5904\u7406\u5bf9\u63d0\u9ad8\u51c6\u786e\u6027\u6700\u6709\u6548\uff0c\u7f51\u683c\u626d\u66f2\u6270\u52a8\u6700\u9002\u5408\u533a\u5206\u9ad8\u4f4e\u7f6e\u4fe1\u5ea6\u60c5\u51b5", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u53ef\u6269\u5c55\uff0c\u53ef\u7acb\u5373\u90e8\u7f72\u5230\u5176\u4ed6\u6587\u6863\u96c6\u5408\u548c\u8f6c\u5f55\u6a21\u578b\uff0c\u4e3a\u5386\u53f2\u6587\u6863\u6570\u5b57\u5316\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09705", "pdf": "https://arxiv.org/pdf/2509.09705", "abs": "https://arxiv.org/abs/2509.09705", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c0f\u578bLLM\uff082B-8B\u53c2\u6570\uff09\u5728\u91cd\u590d\u56de\u7b54\u76f8\u540c\u95ee\u9898\u65f6\u7684\u7a33\u5b9a\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\u3001\u6a21\u578b\u5927\u5c0f\u3001\u5fae\u8c03\u72b6\u6001\u7b49\u56e0\u7d20\u5bf9\u7b54\u6848\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u8bc4\u4f30\u5c0f\u578bLLM\u5728\u91cd\u590d\u56de\u7b54\u76f8\u540c\u95ee\u9898\u65f6\u7684\u7b54\u6848\u4e00\u81f4\u6027\uff0c\u4e86\u89e3\u4e0d\u540c\u56e0\u7d20\uff08\u5982\u63a8\u7406\u6e29\u5ea6\u3001\u6a21\u578b\u5927\u5c0f\u3001\u5fae\u8c03\u72b6\u6001\uff09\u5bf9\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u4e00\u81f4\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5f00\u6e90LLM\u5bf9MMLU-Redux\u548cMedQA\u591a\u9009\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u95ee\u9898\u8fdb\u884c10\u6b21\u91cd\u590d\u56de\u7b54\uff0c\u5206\u6790\u4e0d\u540c\u63a8\u7406\u6e29\u5ea6\uff080.1-1.0\uff09\u3001\u5c0f\u578bvs\u4e2d\u578b\u6a21\u578b\uff0850B-80B\uff09\u3001\u5fae\u8c03vs\u57fa\u7840\u6a21\u578b\u7b49\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u65b0\u7684\u5206\u6790\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728\u4f4e\u63a8\u7406\u6e29\u5ea6\u4e0b\uff0c\u80fd\u591f\u4e00\u81f4\u56de\u7b54\u7684\u95ee\u9898\u6bd4\u4f8b\u901a\u5e38\u572850%-80%\u4e4b\u95f4\uff1b\u4e00\u81f4\u7b54\u6848\u7684\u51c6\u786e\u6027\u4e0e\u603b\u4f53\u51c6\u786e\u6027\u5b58\u5728\u5408\u7406\u76f8\u5173\u6027\uff1b\u4e2d\u578b\u6a21\u578b\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u7b54\u6848\u4e00\u81f4\u6027\u6c34\u5e73\u3002", "conclusion": "LLM\u7684\u7b54\u6848\u4e00\u81f4\u6027\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u5c0f\u578b\u6a21\u578b\u5728\u4f4e\u6e29\u8bbe\u7f6e\u4e0b\u8868\u73b0\u76f8\u5bf9\u7a33\u5b9a\uff0c\u4e00\u81f4\u6027\u4e0e\u51c6\u786e\u6027\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u4e2d\u578b\u6a21\u578b\u5728\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u9700\u8981\u6743\u8861\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u6765\u9009\u62e9\u6700\u4f73\u6a21\u578b\u3002"}}
{"id": "2509.09730", "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\u9886\u57df\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6MITS\uff0c\u5305\u542b17\u4e07\u5f20\u771f\u5b9e\u4ea4\u901a\u76d1\u63a7\u56fe\u50cf\u548c500\u4e07\u6761\u6307\u4ee4\u8ddf\u968f\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u4ea4\u901a\u76d1\u63a7\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u5927\u6a21\u578b\u5728\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\u9886\u57df\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u4e13\u95e8\u7684\u4ea4\u901a\u76d1\u63a7\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u63d0\u5347\u6a21\u578b\u5728\u4ea4\u901a\u76d1\u63a7\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u6536\u96c6\u4e86170,400\u5f20\u771f\u5b9e\u4ea4\u901a\u76d1\u63a7\u56fe\u50cf\uff0c\u6807\u6ce8\u4e868\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c24\u4e2a\u5b50\u7c7b\u522b\u7684\u4ea4\u901a\u5bf9\u8c61\u548c\u4e8b\u4ef6\u3002\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u63cf\u8ff0\u548c500\u4e07\u6761\u6307\u4ee4\u8ddf\u968f\u7684\u89c6\u89c9\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d65\u4e2a\u5173\u952e\u4ea4\u901a\u76d1\u63a7\u4efb\u52a1\u3002", "result": "\u5728MITS\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u4e3b\u6d41\u5927\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1aLLaVA-1.5\u4ece0.494\u63d0\u5347\u52300.905(+83.2%)\uff0cLLaVA-1.6\u4ece0.678\u52300.921(+35.8%)\uff0cQwen2-VL\u4ece0.584\u52300.926(+58.6%)\uff0cQwen2.5-VL\u4ece0.732\u52300.930(+27.0%)\u3002", "conclusion": "MITS\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\u9886\u57df\u7f3a\u4e4f\u4e13\u95e8\u591a\u6a21\u6001\u6570\u636e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u4ea4\u901a\u76d1\u63a7\u548c\u5927\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u4ef7\u503c\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.09708", "pdf": "https://arxiv.org/pdf/2509.09708", "abs": "https://arxiv.org/abs/2509.09708", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u673a\u5236\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u641c\u7d22\u6d41\u7a0b\u6765\u8bc6\u522b\u5bfc\u81f4\u6709\u5bb3\u63d0\u793a\u4ece\u62d2\u7edd\u8f6c\u5411\u5408\u89c4\u7684\u5173\u952e\u7279\u5f81\u96c6\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6f5c\u5728\u7a7a\u95f4\u3002", "motivation": "\u7406\u89e3\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\u7684\u5b89\u5168\u884c\u4e3a\u5185\u90e8\u673a\u5236\uff0c\u76ee\u524d\u8fd9\u65b9\u9762\u7684\u56e0\u679c\u7406\u89e3\u8fd8\u5f88\u7f3a\u4e4f\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u6b8b\u5dee\u6d41\u6fc0\u6d3b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u641c\u7d22\u6d41\u7a0b\uff1a(1)\u62d2\u7edd\u65b9\u5411\u8bc6\u522b\uff0c(2)\u8d2a\u5a6a\u8fc7\u6ee4\u6700\u5c0f\u7279\u5f81\u96c6\uff0c(3)\u4ea4\u4e92\u53d1\u73b0\u4f7f\u7528\u56e0\u5b50\u5206\u89e3\u673a\u6355\u6349\u975e\u7ebf\u6027\u4ea4\u4e92\u3002", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u5bfc\u81f4jailbreak\u7684\u5173\u952e\u7279\u5f81\u96c6\uff0c\u53d1\u73b0\u4e86\u5197\u4f59\u7279\u5f81\u7684\u5b58\u5728\uff0c\u8fd9\u4e9b\u7279\u5f81\u5728\u65e9\u671f\u7279\u5f81\u88ab\u6291\u5236\u65f6\u624d\u4f1a\u6fc0\u6d3b\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u64cd\u7eb5\u53ef\u89e3\u91ca\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5ba1\u8ba1\u548c\u9488\u5bf9\u6027\u5e72\u9884\u5b89\u5168\u884c\u4e3a\u7684\u6f5c\u529b\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u62d2\u7edd\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.09732", "pdf": "https://arxiv.org/pdf/2509.09732", "abs": "https://arxiv.org/abs/2509.09732", "authors": ["Sary Elmansoury", "Islam Mesabah", "Gerrit Gro\u00dfmann", "Peter Neigel", "Raj Bhalwankar", "Daniel Kondermann", "Sebastian J. Vollmer"], "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs", "categories": ["cs.CV"], "comment": null, "summary": "Vision language models (VLMs) excel at zero-shot visual classification, but\ntheir performance on fine-grained tasks and large hierarchical label spaces is\nunderstudied. This paper investigates whether structured, tree-based reasoning\ncan enhance VLM performance. We introduce a framework that decomposes\nclassification into interpretable decisions using decision trees and evaluates\nit on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the\nmodel achieves 98.2% accuracy in understanding the tree knowledge, tree-based\nreasoning consistently underperforms standard zero-shot prompting. We also\nexplore enhancing the tree prompts with LLM-generated classes and image\ndescriptions to improve alignment. The added description enhances the\nperformance of the tree-based and zero-shot methods. Our findings highlight\nlimitations of structured reasoning in visual classification and offer insights\nfor designing more interpretable VLM systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u51b3\u7b56\u6811\u7684\u7ed3\u6784\u5316\u63a8\u7406\u662f\u5426\u80fd\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u867d\u7136\u6a21\u578b\u80fd\u5f88\u597d\u5730\u7406\u89e3\u6811\u72b6\u77e5\u8bc6\uff0c\u4f46\u6811\u57fa\u63a8\u7406\u59cb\u7ec8\u4e0d\u5982\u6807\u51c6\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u89c6\u89c9\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u5c42\u6b21\u6807\u7b7e\u7a7a\u95f4\u4e2d\u7684\u6027\u80fd\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u7ed3\u6784\u5316\u63a8\u7406\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u5206\u7c7b\u5206\u89e3\u4e3a\u4f7f\u7528\u51b3\u7b56\u6811\u7684\u53ef\u89e3\u91ca\u51b3\u7b56\uff0c\u5e76\u5728\u7ec6\u7c92\u5ea6(GTSRB)\u548c\u7c97\u7c92\u5ea6(CIFAR-10)\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u540c\u65f6\u63a2\u7d22\u4f7f\u7528LLM\u751f\u6210\u7684\u7c7b\u522b\u548c\u56fe\u50cf\u63cf\u8ff0\u6765\u589e\u5f3a\u6811\u63d0\u793a\u3002", "result": "\u6a21\u578b\u5728\u7406\u89e3\u6811\u77e5\u8bc6\u65b9\u9762\u8fbe\u523098.2%\u7684\u51c6\u786e\u7387\uff0c\u4f46\u6811\u57fa\u63a8\u7406\u59cb\u7ec8\u8868\u73b0\u4e0d\u5982\u6807\u51c6\u96f6\u6837\u672c\u63d0\u793a\u3002\u6dfb\u52a0\u56fe\u50cf\u63cf\u8ff0\u540e\uff0c\u6811\u57fa\u65b9\u6cd5\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6027\u80fd\u90fd\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u5728\u89c6\u89c9\u5206\u7c7b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.09709", "pdf": "https://arxiv.org/pdf/2509.09709", "abs": "https://arxiv.org/abs/2509.09709", "authors": ["Jing Ren", "Weiqi Wang"], "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u5185\u5bb9\u8d28\u91cf\u548c\u53c2\u8003\u6587\u732e\u6709\u6548\u6027\u7684\u91cf\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u7684\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86ChatGPT\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u8868\u73b0\u5e76\u51cf\u5c11\u4e86\u53c2\u8003\u6587\u732e\u9519\u8bef\u548c\u4f2a\u9020\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5b58\u5728\u53c2\u8003\u6587\u732e\u9519\u8bef\u6216\u4f2a\u9020\u7b49\u4f26\u7406\u95ee\u9898\uff0c\u4e14\u5f53\u524d\u7684\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u4eba\u5de5\u5224\u65ad\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u8bc4\u4f30\u6307\u6807\uff08\u5185\u5bb9\u8d28\u91cf\u548c\u53c2\u8003\u6587\u732e\u6709\u6548\u6027\uff09\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u7684\u5f97\u5206\u8bbe\u8ba1\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u8bc4\u4f30ChatGPT\u5199\u4f5c\u6027\u80fd\u63d0\u4f9b\u4e86\u5ba2\u89c2\u7684\u91cf\u5316\u6846\u67b6\uff0c\u8fed\u4ee3\u63d0\u793a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u8003\u6587\u732e\u7684\u4e0d\u51c6\u786e\u6027\u548c\u4f2a\u9020\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u672f\u73af\u5883\u4e2dChatGPT\u4f7f\u7528\u4e2d\u7684\u5173\u952e\u4f26\u7406\u6311\u6218\uff0c\u4e3aLLM\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u91cf\u5316\u8bc4\u4f30\u548c\u8d28\u91cf\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.09737", "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.", "AI": {"tldr": "PSI\u662f\u4e00\u4e2a\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u63a7\u3001\u53ef\u63d0\u793a\u4e16\u754c\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u6b65\u5faa\u73af\u6784\u5efa\u6982\u7387\u56fe\u6a21\u578b\uff0c\u63d0\u53d6\u5e95\u5c42\u7ed3\u6784\uff0c\u5e76\u6574\u5408\u4e3a\u65b0\u7684\u63a7\u5236\u4fe1\u53f7\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u4ece\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u4e30\u5bcc\u63a7\u5236\u7ed3\u6784\u548c\u7075\u6d3b\u63d0\u793a\u529f\u80fd\u7684\u4e16\u754c\u6a21\u578b\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u89c6\u9891\u9884\u6d4b\u3001\u7406\u89e3\u548c\u5e95\u5c42\u7ed3\u6784\u63d0\u53d6\u3002", "method": "\u4e09\u6b65\u5faa\u73af\uff1a1) \u6982\u7387\u9884\u6d4b - \u6784\u5efa\u968f\u673a\u8bbf\u95ee\u81ea\u56de\u5f52\u5e8f\u5217\u6a21\u578b\uff1b2) \u7ed3\u6784\u63d0\u53d6 - \u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u96f6\u6837\u672c\u63d0\u53d6\u5e95\u5c42\u4f4e\u7ef4\u5c5e\u6027\uff1b3) \u6574\u5408 - \u5c06\u7ed3\u6784\u8f6c\u6362\u4e3a\u65b0token\u7c7b\u578b\u5e76\u91cd\u65b0\u878d\u5165\u8bad\u7ec3\u3002", "result": "\u57281.4\u4e07\u4ebftoken\u4e92\u8054\u7f51\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3PSI\u5b9e\u4f8b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5149\u6d41\u3001\u81ea\u76d1\u7763\u6df1\u5ea6\u548c\u5bf9\u8c61\u5206\u5272\uff0c\u5e76\u652f\u6301\u5b8c\u6574\u7684\u9884\u6d4b\u6539\u8fdb\u5faa\u73af\u3002", "conclusion": "PSI\u7cfb\u7edf\u901a\u8fc7\u5faa\u73af\u589e\u5f3a\u7684\u65b9\u5f0f\uff0c\u4e0d\u4ec5\u6539\u5584\u4e86\u5e95\u5c42\u6570\u636e\u5efa\u6a21\u80fd\u529b\uff0c\u8fd8\u521b\u5efa\u4e86\u7c7b\u4f3cLLM\u7684\u901a\u7528\u63d0\u793a\u8bed\u8a00\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2509.09710", "pdf": "https://arxiv.org/pdf/2509.09710", "abs": "https://arxiv.org/abs/2509.09710", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u4f53\u51fa\u884c\u65e5\u8bb0\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u751f\u6210\u865a\u62df\u4eba\u7269\u5e76\u5408\u6210\u51fa\u884c\u8bb0\u5f55\uff0c\u9a8c\u8bc1\u663e\u793aLLM\u5728\u51fa\u884c\u76ee\u7684\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6574\u4f53\u771f\u5b9e\u6027\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u51fa\u884c\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u4e13\u6709\u5bb6\u5ead\u51fa\u884c\u8c03\u67e5\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u83b7\u53d6\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u57fa\u4e8e\u5f00\u6e90\u6570\u636e\u7684LLM\u65b9\u6848\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u751f\u6210\u5177\u6709\u7edf\u8ba1\u4ee3\u8868\u6027\u7684\u4e2a\u4f53\u51fa\u884c\u65e5\u8bb0\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u793e\u533a\u8c03\u67e5\u548c\u667a\u80fd\u4f4d\u7f6e\u6570\u636e\u5e93\u5f00\u6e90\u6570\u636e\u968f\u673a\u751f\u6210\u865a\u62df\u4eba\u7269\uff0c\u901a\u8fc7\u76f4\u63a5\u63d0\u793a\u8bcd\u751f\u6210\u51fa\u884c\u65e5\u8bb0\u3002\u63d0\u51fa\u56db\u6307\u6807\u7efc\u5408\u771f\u5b9e\u6027\u8bc4\u5206\uff08\u51fa\u884c\u6b21\u6570\u3001\u65f6\u95f4\u95f4\u9694\u3001\u76ee\u7684\u3001\u65b9\u5f0f\uff09\uff0c\u91c7\u7528Jensen-Shannon\u6563\u5ea6\u9a8c\u8bc1\u751f\u6210\u65e5\u8bb0\u4e0e\u771f\u5b9e\u65e5\u8bb0\u7684\u5206\u5e03\u76f8\u4f3c\u6027\u3002", "result": "LLM\u751f\u6210\u65e5\u8bb0\u6574\u4f53\u771f\u5b9e\u6027\u8bc4\u52060.485\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd50.455\u76f8\u5f53\u3002LLM\u5728\u51fa\u884c\u76ee\u7684\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4e00\u81f4\u6027\u66f4\u9ad8\uff08\u8bc4\u5206\u5206\u5e03\u66f4\u7a84\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u51fa\u884c\u6b21\u6570\u548c\u6d3b\u52a8\u65f6\u957f\u6570\u503c\u4f30\u8ba1\u65b9\u9762\u66f4\u4f18\u3002\u805a\u5408\u9a8c\u8bc1\u663e\u793aLLM\u7edf\u8ba1\u4ee3\u8868\u6027\u66f4\u597d\uff080.612 vs 0.435\uff09\u3002", "conclusion": "LLM\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u53ef\u884c\uff0c\u5efa\u7acb\u4e86\u53ef\u91cf\u5316\u7684\u65e5\u8bb0\u771f\u5b9e\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u672a\u6765\u5408\u6210\u51fa\u884c\u65e5\u8bb0\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.09742", "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u89c6\u9891\u6570\u636e\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u98ce\u9669\uff0c\u53d1\u73b0\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u63d0\u4f9b\u66f4\u597d\u4fdd\u62a4\u4f46\u4ecd\u6709\u6cc4\u6f0f\u53ef\u80fd\uff0c\u8d85\u5206\u8fa8\u7387\u6280\u672f\u53ef\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u901a\u8fc7\u53ea\u4ea4\u6362\u6a21\u578b\u66f4\u65b0\u800c\u975e\u539f\u59cb\u6570\u636e\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u80fd\u4ece\u4e2d\u91cd\u5efa\u654f\u611f\u6570\u636e\u3002\u867d\u7136\u5df2\u77e5\u5bf9\u56fe\u50cf\u3001\u6587\u672c\u548c\u8868\u683c\u6570\u636e\u7684\u5a01\u80c1\uff0c\u4f46\u5bf9\u89c6\u9891\u6570\u636e\u7684\u5f71\u54cd\u5c1a\u672a\u7814\u7a76\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u7684\u65b9\u6cd5\u548c\u5904\u7406\u539f\u59cb\u89c6\u9891\u5e27\u7684\u7b80\u5355\u53d8\u6362\u65b9\u6cd5\u3002\u6d4b\u8bd5\u5728\u4e0d\u540c\u653b\u51fb\u573a\u666f\u4e0b\uff08\u96f6\u53c2\u8003\u5e27\u3001\u4e00\u4e2a\u53c2\u8003\u5e27\u3001\u591a\u4e2a\u53c2\u8003\u5e27\uff09\u7684\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u6548\u679c\uff0c\u5e76\u5e94\u7528\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u7279\u5f81\u63d0\u53d6\u5668\u5bf9\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u5177\u6709\u66f4\u5f3a\u97e7\u6027\uff0c\u4f46\u82e5\u5206\u7c7b\u5668\u590d\u6742\u5ea6\u4e0d\u8db3\u4ecd\u53ef\u80fd\u6cc4\u6f0f\u3002\u8d85\u5206\u8fa8\u7387\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u653b\u51fb\u8005\u91cd\u5efa\u7684\u89c6\u9891\u8d28\u91cf\u3002\u89c6\u9891\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u53ef\u884c\u7684\u6cc4\u6f0f\u5a01\u80c1\u3002", "conclusion": "\u89c6\u9891\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6cc4\u6f0f\u662f\u771f\u5b9e\u5b58\u5728\u7684\u5a01\u80c1\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u53d1\u751f\u6761\u4ef6\u548c\u9632\u62a4\u63aa\u65bd\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5668\u4f46\u4ecd\u9700\u786e\u4fdd\u5206\u7c7b\u5668\u8db3\u591f\u590d\u6742\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.09711", "pdf": "https://arxiv.org/pdf/2509.09711", "abs": "https://arxiv.org/abs/2509.09711", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications.", "AI": {"tldr": "PsychiatryBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u6743\u5a01\u7cbe\u795e\u75c5\u5b66\u6559\u79d1\u4e66\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5300\u591a\u4e2a\u4e13\u5bb6\u6807\u6ce8\u9879\u76ee\uff0c\u8bc4\u4f30\u663e\u793a\u524d\u6cbfLLMs\u5728\u4e34\u5e8a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u8d44\u6e90\u4e3b\u8981\u4f9d\u8d56\u5c0f\u578b\u4e34\u5e8a\u8bbf\u8c08\u8bed\u6599\u5e93\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u6216\u5408\u6210\u5bf9\u8bdd\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u6709\u6548\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u7cbe\u795e\u75c5\u5b66\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "method": "\u57fa\u4e8e\u6743\u5a01\u4e13\u5bb6\u9a8c\u8bc1\u7684\u7cbe\u795e\u75c5\u5b66\u6559\u79d1\u4e66\u548c\u6848\u4f8b\u96c6\uff0c\u6784\u5efa11\u4e2a\u4e0d\u540c\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u5305\u62ec\u8bca\u65ad\u63a8\u7406\u3001\u6cbb\u7597\u8ba1\u5212\u7b49\uff0c\u4f7f\u7528\u4f20\u7edf\u6307\u6807\u548cLLM-as-judge\u76f8\u4f3c\u6027\u8bc4\u5206\u6846\u67b6\u8bc4\u4f30\u591a\u79cdLLMs\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5728\u4e34\u5e8a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u968f\u8bbf\u548c\u7ba1\u7406\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u6a21\u578b\u8c03\u4f18\u548c\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u8303\u5f0f\uff0cPsychiatryBench\u4e3a\u9ad8\u98ce\u9669\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684LLM\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.09750", "pdf": "https://arxiv.org/pdf/2509.09750", "abs": "https://arxiv.org/abs/2509.09750", "authors": ["Hossein Yazdanjouei", "Arash Mansouri", "Mohammad Shokouhifar"], "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534a\u76d1\u7763\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u5bc6\u96c6\u96f6\u552e\u73af\u5883\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u7ed3\u5408Faster R-CNN\u548cYOLO\u8fdb\u884c\u4f2a\u6807\u7b7e\u4ea4\u6362\uff0c\u96c6\u6210\u591a\u79cd\u5206\u7c7b\u5668\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u5728SKU-110k\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u96f6\u552e\u73af\u5883\u4e2d\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u6761\u4ef6\uff08\u906e\u6321\u3001\u91cd\u53e0\u7269\u4f53\uff09\u5e26\u6765\u7684\u6311\u6218\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u9002\u5e94\u96f6\u552e\u573a\u666f\u4e2d\u9891\u7e41\u7684\u4ea7\u54c1\u548c\u5e03\u5c40\u53d8\u5316", "method": "\u91c7\u7528\u534a\u76d1\u7763\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff1a1) Faster R-CNN\uff08ResNet\u4e3b\u5e72\uff09\u7cbe\u786e\u5b9a\u4f4d + YOLO\uff08Darknet\u4e3b\u5e72\uff09\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u76f8\u4e92\u4ea4\u6362\u4f2a\u6807\u7b7e\uff1b2) \u96c6\u6210XGBoost\u3001\u968f\u673a\u68ee\u6797\u548cSVM\u5206\u7c7b\u5668\uff1b3) \u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570", "result": "\u5728SKU-110k\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5b9e\u9645\u96f6\u552e\u5e94\u7528\uff0c\u5982\u81ea\u52a8\u5316\u5e93\u5b58\u8ddf\u8e2a\u3001\u4ea7\u54c1\u76d1\u63a7\u548c\u7ed3\u8d26\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u5e76\u9002\u5e94\u96f6\u552e\u73af\u5883\u53d8\u5316"}}
{"id": "2509.09712", "pdf": "https://arxiv.org/pdf/2509.09712", "abs": "https://arxiv.org/abs/2509.09712", "authors": ["Talha Tahir"], "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86SFT\u548cORPO\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u5c0f\u578bLLM\u8fdb\u884cACT\u6cbb\u7597\u7684\u80fd\u529b\u5f71\u54cd\uff0c\u53d1\u73b0ORPO\u65b9\u6cd5\u5728\u6cbb\u7597\u5fe0\u8bda\u5ea6\u548c\u5171\u60c5\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFT\u548c\u57fa\u7840\u6a21\u578b\uff0c\u800c\u601d\u7ef4\u94fe\u63a8\u7406\u4ec5\u5bf9SFT\u6a21\u578b\u6709\u663e\u8457\u5e2e\u52a9\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u540e\u8bad\u7ec3\u65b9\u6cd5\u548c\u663e\u5f0f\u63a8\u7406\u5bf9\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u63a5\u7eb3\u627f\u8bfa\u7597\u6cd5(ACT)\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347AI\u5728\u5fc3\u7406\u6cbb\u7597\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u4f7f\u7528Mistral-Large\u751f\u6210\u768450\u7ec4\u5408\u6210ACT\u8f6c\u5f55\u672c\uff0c\u5bf9Llama-3.2-3b-Instruct\u8fdb\u884c\u4e24\u79cd\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u51e0\u7387\u6bd4\u7b56\u7565\u4f18\u5316(ORPO)\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u5305\u542b\u6709/\u65e0\u601d\u7ef4\u94fe\u63a8\u7406\u6b65\u9aa4\u3002\u901a\u8fc7\u6a21\u62df\u6cbb\u7597\u4f1a\u8bdd\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "ORPO\u8bad\u7ec3\u6a21\u578b\u5728ACT\u5fe0\u8bda\u5ea6(\u03c7\u00b2=185.15, p<.001)\u548c\u6cbb\u7597\u5171\u60c5(\u03c7\u00b2=140.37, p<.001)\u65b9\u9762\u663e\u8457\u4f18\u4e8eSFT\u548c\u57fa\u7840\u6a21\u578b\u3002\u601d\u7ef4\u94fe\u63a8\u7406\u4ec5\u5bf9SFT\u6a21\u578b\u6709\u663e\u8457\u76ca\u5904(\u5e73\u5747\u63d0\u53472.68\u5206\uff0cp<.001)\uff0c\u5bf9ORPO\u6a21\u578b\u65e0\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u504f\u597d\u5bf9\u9f50\u7b56\u7565\u4f18\u5316\u80fd\u6709\u6548\u57f9\u517b\u5c0f\u578bLLM\u7684ACT\u80fd\u529b\uff0c\u663e\u5f0f\u63a8\u7406\u7684\u6548\u7528\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5e95\u5c42\u8bad\u7ec3\u8303\u5f0f\uff0cORPO\u901a\u8fc7\u5b66\u4e60\u6cbb\u7597\u8fc7\u7a0b\u800c\u975e\u6a21\u4eff\u5185\u5bb9\u5c55\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2509.09785", "pdf": "https://arxiv.org/pdf/2509.09785", "abs": "https://arxiv.org/abs/2509.09785", "authors": ["Moslem Yazdanpanah", "Ali Bahri", "Mehrdad Noori", "Sahar Dastani", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging", "categories": ["cs.CV"], "comment": null, "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}", "AI": {"tldr": "\u63d0\u51faToken Purging(PG)\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6280\u672f\uff0c\u901a\u8fc7\u79fb\u9664\u53d7\u57df\u504f\u79fb\u5f71\u54cd\u4e25\u91cd\u7684token\u6765\u63d0\u53473D\u70b9\u4e91\u5206\u7c7b\u6027\u80fd", "motivation": "\u89e3\u51b33D\u70b9\u4e91\u5206\u7c7b\u4e2d\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u73b0\u6709TTA\u65b9\u6cd5\u9700\u8981\u8fed\u4ee3\u66f4\u65b0\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8", "method": "\u63d0\u51fa\u4e24\u79cd\u53d8\u4f53\uff1aPG-SP\u5229\u7528\u6e90\u57df\u7edf\u8ba1\u4fe1\u606f\uff0cPG-SF\u662f\u5b8c\u5168\u65e0\u6e90\u7248\u672c\uff0c\u4f9d\u8d56CLS-token\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u3002\u5728\u6ce8\u610f\u529b\u5c42\u524d\u79fb\u9664\u53d7\u57df\u504f\u79fb\u5f71\u54cd\u7684token", "result": "\u5728ModelNet40-C\u3001ShapeNet-C\u548cScanObjectNN-C\u6570\u636e\u96c6\u4e0a\uff0cPG-SP\u6bd4\u6700\u5148\u8fdb\u7684\u65e0\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u9ad810.3%\uff0cPG-SF\u4e3a\u65e0\u6e90\u81ea\u9002\u5e94\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002\u901f\u5ea6\u63d0\u534712.4\u500d\uff0c\u5185\u5b58\u6548\u7387\u63d0\u9ad85.5\u500d", "conclusion": "Token Purging\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5185\u5b58\u53cb\u597d\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c"}}
{"id": "2509.09713", "pdf": "https://arxiv.org/pdf/2509.09713", "abs": "https://arxiv.org/abs/2509.09713", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.", "AI": {"tldr": "HANRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u8def\u7531\u3001\u5b50\u67e5\u8be2\u5206\u89e3\u548c\u566a\u58f0\u8fc7\u6ee4\uff0c\u6709\u6548\u89e3\u51b3\u591a\u8df3\u67e5\u8be2\u4e2d\u7684\u566a\u58f0\u79ef\u7d2f\u548c\u68c0\u7d22\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8df3\u67e5\u8be2\u65f6\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u8fed\u4ee3\u68c0\u7d22\u3001\u65e0\u6cd5\u6355\u83b7\u5b50\u67e5\u8be2\u76f8\u5173\u5185\u5bb9\u5bfc\u81f4\u566a\u58f0\u79ef\u7d2f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHANRAG\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5927\u7684revealator\u8fdb\u884c\u67e5\u8be2\u8def\u7531\u3001\u67e5\u8be2\u5206\u89e3\u4e3a\u5b50\u67e5\u8be2\uff0c\u5e76\u5bf9\u68c0\u7d22\u6587\u6863\u8fdb\u884c\u566a\u58f0\u8fc7\u6ee4\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHANRAG\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u884c\u4e1a\u9886\u5148\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "HANRAG\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u5904\u7406\u591a\u6837\u5316\u67e5\u8be2\u7684\u9002\u5e94\u6027\u548c\u6297\u566a\u58f0\u80fd\u529b\u3002"}}
{"id": "2509.09792", "pdf": "https://arxiv.org/pdf/2509.09792", "abs": "https://arxiv.org/abs/2509.09792", "authors": ["Zimin Xia", "Chenghao Xu", "Alexandre Alahi"], "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors", "categories": ["cs.CV"], "comment": null, "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u786e\u4e14\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u8de8\u89c6\u89d2\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u5730\u9762\u56fe\u50cf\u4e0e\u53c2\u8003\u822a\u62cd\u56fe\u50cf\u7684\u5c40\u90e8\u7279\u5f81\u6765\u4f30\u8ba13\u81ea\u7531\u5ea6\u4f4d\u59ff\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u9e1f\u77b0\u56fe\u8f6c\u6362\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5730\u9762\u56fe\u50cf\u8f6c\u6362\u4e3a\u9e1f\u77b0\u56fe\u8868\u793a\u518d\u4e0e\u822a\u62cd\u56fe\u50cf\u5bf9\u9f50\uff0c\u8fd9\u79cd\u8f6c\u6362\u5e38\u56e0\u900f\u89c6\u7578\u53d8\u6216\u9ad8\u5ea6\u4fe1\u606f\u538b\u7f29\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\uff0c\u5f71\u54cd\u5bf9\u9f50\u8d28\u91cf\u3002", "method": "\u76f4\u63a5\u5728\u539f\u59cb\u56fe\u50cf\u95f4\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ec5\u5c06\u5339\u914d\u7684\u5173\u952e\u70b9\u4f7f\u7528\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\u63d0\u5347\u5230\u9e1f\u77b0\u7a7a\u95f4\uff0c\u652f\u6301\u5ea6\u91cf\u548c\u76f8\u5bf9\u6df1\u5ea6\uff0c\u91c7\u7528\u5c3a\u5ea6\u611f\u77e5\u7684Procrustes\u5bf9\u9f50\u6765\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u9700\u76f8\u673a\u4f4d\u59ff\u7684\u5f31\u76d1\u7763\uff0c\u8be5\u65b9\u6cd5\u5c31\u80fd\u5b66\u4e60\u51c6\u786e\u7684\u5c40\u90e8\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u8de8\u533a\u57df\u6cdb\u5316\u548c\u672a\u77e5\u65b9\u5411\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u517c\u5bb9\u5404\u79cd\u76f8\u5bf9\u6df1\u5ea6\u6a21\u578b\u4e14\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5177\u6709\u7075\u6d3b\u6027\u548c\u5f3a\u5927\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.09714", "pdf": "https://arxiv.org/pdf/2509.09714", "abs": "https://arxiv.org/abs/2509.09714", "authors": ["Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline B\u00e9n\u00e9dicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8618\u79cd\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e38\u7528\u6307\u6807\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u67d0\u4e9b\u5d4c\u5165\u65b9\u6cd5\u5c06\u8bed\u4e49\u76f8\u53cd\u7684\u6587\u672c\u8bef\u5224\u4e3a\u76f8\u4f3c\u5ea6\u9ad8\u8fbe99.9%\uff0c\u800cLLM\u65b9\u6cd5\u5728\u533a\u5206\u8bed\u4e49\u5dee\u5f02\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u641c\u7d22\u3001API\u63a8\u8350\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u5e94\u7528\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u4f30\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u7406\u89e3\u8bed\u4e49\u5173\u7cfb\u8fd8\u662f\u4ec5\u8bc6\u522b\u8868\u9762\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u6d4b\u8bd5\u4e8618\u79cd\u4e0d\u540c\u65b9\u6cd5\uff08\u57fa\u4e8e\u8bcd\u3001\u5d4c\u5165\u3001LLM\u548c\u7ed3\u6784\u611f\u77e5\u7b97\u6cd5\uff09\uff0c\u5efa\u7acb\u4e86\u7cfb\u7edf\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u53d7\u63a7\u6587\u672c\u548c\u4ee3\u7801\u53d8\u5316\u6765\u8bc4\u4f30\u5404\u65b9\u6cd5\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u8bed\u4e49\u5173\u7cfb\u7684\u80fd\u529b\u3002", "result": "\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5c06\u8bed\u4e49\u76f8\u53cd\u5185\u5bb9\u8bef\u5224\u4e3a99.9%\u76f8\u4f3c\uff1b\u4ece\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5207\u6362\u5230\u4f59\u5f26\u76f8\u4f3c\u5ea6\u53ef\u63d0\u5347\u6027\u80fd24-66%\uff1bLLM\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u5bf9\u771f\u6b63\u4e0d\u540c\u7684\u542b\u4e49\u7ed9\u51fa\u4f4e\u76f8\u4f3c\u5ea6\u5206\u6570\uff080.00-0.29\uff09\u3002", "conclusion": "\u5f53\u524d\u5e38\u7528\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff1b\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u7684\u9009\u62e9\u5bf9\u5d4c\u5165\u65b9\u6cd5\u6027\u80fd\u5f71\u54cd\u5f88\u5927\uff1bLLM\u5728\u8bed\u4e49\u533a\u5206\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\u3002"}}
{"id": "2509.09808", "pdf": "https://arxiv.org/pdf/2509.09808", "abs": "https://arxiv.org/abs/2509.09808", "authors": ["Judith Massmann", "Alexander Lichtenstein", "Francisco M. L\u00f3pez"], "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IEEE ICDL 2025. 6 pages, 7 figures, 2 tables", "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u667a\u80fd\u624b\u673a\u5e94\u7528KidsVisionCheck\uff0c\u901a\u8fc7\u7ea2\u773c\u53cd\u5c04\u56fe\u50cf\u8fdb\u884c\u513f\u7ae5\u89c6\u529b\u7b5b\u67e5\uff0c\u51c6\u786e\u7387\u8fbe90%\uff0c\u65e0\u9700\u4e13\u4e1a\u8bbe\u5907\u3002", "motivation": "\u5229\u7528\u667a\u80fd\u624b\u673a\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u91cd\u73b0\u4f20\u7edfBruckner\u6d4b\u8bd5\uff0c\u4f7f\u513f\u7ae5\u89c6\u529b\u7b5b\u67e5\u66f4\u52a0\u53ef\u8fbe\u548c\u666e\u53ca\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\uff0c\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u773c\u79d1\u533b\u751f\u6536\u96c6\u6807\u6ce8\u7684\u513f\u7ae5\u773c\u7c92\u56fe\u50cf\u3002", "result": "\u5728\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u4e0a\u8fbe\u523090%\u7684\u51c6\u786e\u7387\uff0c\u80fd\u591f\u8bc6\u522b\u6700\u4f73\u6570\u636e\u6536\u96c6\u6761\u4ef6\u5e76\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5168\u7403\u8303\u56f4\u5185\u5b9e\u73b0\u53ef\u8fbe\u7684\u513f\u79d1\u89c6\u529b\u7b5b\u67e5\u548c\u65e9\u671f\u5e72\u9884\u89c6\u529b\u5f02\u5e38\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.09715", "pdf": "https://arxiv.org/pdf/2509.09715", "abs": "https://arxiv.org/abs/2509.09715", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc6\u522b\u5e76\u8868\u5f81\u4e86\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u5173\u952e\u7b26\u53f7\u5c5e\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u589e\u5927\u80fd\u964d\u4f4e\u5e7b\u89c9\u7387\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\uff0c\u4fee\u9970\u8bcd\u548c\u547d\u540d\u5b9e\u4f53\u662f\u4e3b\u8981\u7684\u5e7b\u89c9\u6765\u6e90\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5bfc\u81f4\u6a21\u578b\u5185\u5728\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u5c5e\u6027\u5c1a\u672a\u88ab\u8bc6\u522b\u548c\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u627e\u51fa\u8fd9\u4e9b\u5173\u952e\u5c5e\u6027\u5e76\u5206\u6790\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u8106\u5f31\u6027\u3002", "method": "\u4f7f\u7528HaluEval\u548cTruthfulQA\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u5c06\u539f\u6709\u7684\u95ee\u7b54\u683c\u5f0f\u8f6c\u6362\u4e3a\u591a\u79cd\u5176\u4ed6\u683c\u5f0f\uff0c\u4ee5\u786e\u5b9a\u5bfc\u81f4\u5e7b\u89c9\u7684\u5c5e\u6027\u3002\u6d4b\u8bd5\u4e86Gemma-2\u7cfb\u5217\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b(2B, 9B, 27B)\u3002", "result": "Gemma-2-2B\u7684\u5e73\u5747\u5e7b\u89c9\u7387\u4e3a79.0%\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0cGemma-2-9B\u964d\u81f373.6%\uff0cGemma-2-27B\u964d\u81f363.9%\u3002\u4fee\u9970\u8bcd(84.76%-94.98%)\u548c\u547d\u540d\u5b9e\u4f53(83.87%-93.96%)\u5728\u6240\u6709\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u90fd\u662f\u4e3b\u8981\u7684\u5e7b\u89c9\u6765\u6e90\u3002", "conclusion": "\u7b26\u53f7\u5143\u7d20\u6301\u7eed\u4f7f\u6a21\u578b\u6df7\u6dc6\uff0c\u8fd9\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6b64\u7c7b\u8f93\u5165\u65f6\u5b58\u5728\u6839\u672c\u6027\u5f31\u70b9\uff0c\u4e14\u8fd9\u79cd\u5f31\u70b9\u4e0d\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u5b8c\u5168\u6d88\u9664\u3002"}}
{"id": "2509.09828", "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "\u63d0\u51faDGFusion\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8bed\u4e49\u611f\u77e5\u6027\u80fd\uff0c\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u5206\u5272\u6548\u679c", "motivation": "\u73b0\u6709\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u5728\u7a7a\u95f4\u4e0a\u5747\u5300\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\uff0c\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u6839\u636e\u6df1\u5ea6\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u878d\u5408\u7b56\u7565", "method": "\u63d0\u51fa\u6df1\u5ea6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edcDGFusion\uff0c\u5c06\u591a\u6a21\u6001\u5206\u5272\u4f5c\u4e3a\u591a\u4efb\u52a1\u95ee\u9898\uff0c\u5229\u7528LiDAR\u6d4b\u91cf\u4f5c\u4e3a\u8f93\u5165\u548c\u6df1\u5ea6\u771f\u503c\uff0c\u5b66\u4e60\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u5e76\u901a\u8fc7\u5c40\u90e8\u6df1\u5ea6token\u548c\u5168\u5c40\u6761\u4ef6token\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u878d\u5408", "result": "\u5728MUSES\u548cDELIVER\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5168\u666f\u5206\u5272\u548c\u8bed\u4e49\u5206\u5272\u6027\u80fd", "conclusion": "\u6df1\u5ea6\u5f15\u5bfc\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u4e0d\u540c\u6df1\u5ea6\u6761\u4ef6\u4e0b\u7684\u4f20\u611f\u5668\u53ef\u9760\u6027\u53d8\u5316\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027"}}
{"id": "2509.09723", "pdf": "https://arxiv.org/pdf/2509.09723", "abs": "https://arxiv.org/abs/2509.09723", "authors": ["Kai R. Larsen", "Sen Yan", "Roland M\u00fcller", "Lan Sang", "Mikko R\u00f6nkk\u00f6", "Ravi Starzl", "Donald Edmondson"], "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis.", "AI": {"tldr": "ALIGNS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b55\u4e07+\u6307\u6807\u7684\u7efc\u5408nomological\u7f51\u7edc\uff0c\u89e3\u51b3\u5fc3\u7406\u6d4b\u91cf\u4e2d\u6784\u5efa\u7406\u8bba\u7f51\u7edc\u7684\u957f\u671f\u6311\u6218\u3002", "motivation": "\u81eaCronbach\u548cMeehl\u63d0\u51fanomological\u7f51\u7edc70\u5e74\u6765\uff0c\u6784\u5efa\u7406\u8bba\u7f51\u7edc\u4e00\u76f4\u662f\u6d4b\u91cf\u9a8c\u8bc1\u7684\u6838\u5fc3\u6311\u6218\uff0c\u8fd9\u5bfc\u81f4\u4e34\u5e8a\u8bd5\u9a8c\u53ef\u80fd\u65e0\u6cd5\u68c0\u6d4b\u6cbb\u7597\u6548\u679c\uff0c\u516c\u5171\u653f\u7b56\u53ef\u80fd\u9488\u5bf9\u9519\u8bef\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u95ee\u5377\u6d4b\u91cf\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edfALIGNS\uff0c\u63d0\u4f9b\u4e09\u4e2a\u7efc\u5408nomological\u7f51\u7edc\uff0c\u6db5\u76d6\u5fc3\u7406\u5b66\u3001\u533b\u5b66\u3001\u793e\u4f1a\u653f\u7b56\u7b49\u9886\u57df\u3002", "result": "\u7cfb\u7edf\u5728\u4e09\u4e2a\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff1a\u53d1\u73b0NIH PROMIS\u7126\u8651\u548c\u6291\u90c1\u5de5\u5177\u6536\u655b\u4e3a\u5355\u4e00\u60c5\u7eea\u56f0\u6270\u7ef4\u5ea6\uff1b\u8bc6\u522b\u513f\u7ae5\u6c14\u8d28\u6d4b\u91cf\u7684\u56db\u4e2a\u65b0\u7ef4\u5ea6\u5e76\u8d28\u7591\u73b0\u6709\u7ef4\u5ea6\uff1b\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "ALIGNS\u662f\u9996\u4e2a\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6d4b\u91cf\u9a8c\u8bc1\u57fa\u7840\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u514d\u8d39\u63d0\u4f9b\u4e8enomologicalnetwork.org\uff0c\u4e3a\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u63d0\u4f9b\u5927\u89c4\u6a21nomological\u5206\u6790\u8865\u5145\u3002"}}
{"id": "2509.09841", "pdf": "https://arxiv.org/pdf/2509.09841", "abs": "https://arxiv.org/abs/2509.09841", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework", "categories": ["cs.CV"], "comment": null, "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u81ea\u52a8\u9152\u6e23\u9f3b\u68c0\u6d4b\u7b56\u7565\uff0c\u4f7f\u7528ResNet-18\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u4e0d\u540c\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u4f4d\u7f6e\u7684\u9762\u90e8\u56fe\u50cf\u5757\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u51c6\u786e\u7387\u548c\u7075\u654f\u5ea6\u3002", "motivation": "\u9152\u6e23\u9f3b\u662f\u4e00\u79cd\u6162\u6027\u708e\u75c7\u6027\u76ae\u80a4\u75c5\uff0c\u9700\u8981\u7cbe\u786e\u65e9\u671f\u68c0\u6d4b\u6765\u63d0\u9ad8\u6cbb\u7597\u6548\u679c\u3002\u4f20\u7edf\u5168\u56fe\u50cf\u65b9\u6cd5\u53ef\u80fd\u5305\u542b\u8fc7\u591a\u65e0\u5173\u4fe1\u606f\u4e14\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u51c6\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528ResNet-18\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u9762\u90e8\u56fe\u50cf\u4e2d\u63d0\u53d6\u5404\u79cd\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u4f4d\u7f6e\u7684\u56fe\u50cf\u5757\uff0c\u7814\u7a76\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u5168\u56fe\u50cf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u7b56\u7565\u80fd\u591f\u5f15\u5bfc\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5173\u6ce8\u4e34\u5e8a\u76f8\u5173\u533a\u57df\uff0c\u5728\u51c6\u786e\u7387\u548c\u7075\u654f\u5ea6\u65b9\u9762\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u4f18\u4e8e\u5168\u56fe\u50cf\u65b9\u6cd5\uff0c\u540c\u65f6\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u7b56\u7565\u4e3a\u6539\u8fdb\u81ea\u52a8\u5316\u76ae\u80a4\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u901a\u8fc7\u4f7f\u7528\u5c40\u90e8\u56fe\u50cf\u5757\u65e2\u4fdd\u62a4\u4e86\u60a3\u8005\u9690\u79c1\uff0c\u53c8\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.09724", "pdf": "https://arxiv.org/pdf/2509.09724", "abs": "https://arxiv.org/abs/2509.09724", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities.", "AI": {"tldr": "\u57fa\u4e8e\u6280\u672f\u95f4\u65f6\u5e8f\u5173\u7cfb\u8bc6\u522b\u65b0\u5174\u6280\u672f\u673a\u4f1a\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5229\u6587\u672c\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u6280\u672f\u4e3b\u9898\u5e76\u8ffd\u8e2a\u5176\u6f14\u53d8", "motivation": "\u6280\u672f\u673a\u4f1a\u662f\u63a8\u52a8\u6280\u672f\u3001\u4ea7\u4e1a\u548c\u521b\u65b0\u8fdb\u6b65\u7684\u5173\u952e\u4fe1\u606f\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u8bc6\u522b\u65b0\u5174\u6280\u672f\u8d8b\u52bf\u548c\u53d1\u5c55\u673a\u4f1a", "method": "\u4ece\u4e13\u5229\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6587\u672c\uff0c\u5c06\u57fa\u4e8e\u6587\u672c\u7684\u4e3b\u9898\u6620\u5c04\u4ee5\u53d1\u73b0\u6280\u672f\u95f4\u5173\u7cfb\uff0c\u901a\u8fc7\u8ffd\u8e2a\u4e3b\u9898\u968f\u65f6\u95f4\u53d8\u5316\u6765\u8bc6\u522b\u6280\u672f\u673a\u4f1a\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e3b\u9898\u548c\u804a\u5929\u6a21\u578b\u63d0\u793a\u652f\u6301\u673a\u4f1a\u53d1\u73b0", "result": "\u4f7f\u7528\u7f8e\u56fd\u4e13\u5229\u5546\u6807\u5c40\u63d0\u4f9b\u7684\u4eba\u5de5\u667a\u80fd\u4e13\u5229\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6b63\u5728\u5411\u4fbf\u4e8e\u65e5\u5e38\u8bbf\u95ee\u7684\u5f62\u5f0f\u6f14\u53d8", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u8bc6\u522b\u672a\u6765\u6280\u672f\u673a\u4f1a\u7684\u6f5c\u529b\uff0c\u4e3a\u6280\u672f\u53d1\u5c55\u548c\u521b\u65b0\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5206\u6790\u5de5\u5177"}}
{"id": "2509.09844", "pdf": "https://arxiv.org/pdf/2509.09844", "abs": "https://arxiv.org/abs/2509.09844", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection", "categories": ["cs.CV"], "comment": null, "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u548c\u4e34\u5e8a\u5148\u9a8c\u77e5\u8bc6\u7684\u9690\u79c1\u4fdd\u62a4\u578b\u73ab\u7470\u75e4\u75ae\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea2\u8272\u901a\u9053\u5f3a\u5ea6\u6784\u5efa\u8bca\u65ad\u76f8\u5173\u533a\u57df\u63a9\u7801\uff0c\u4f7f\u7528ResNet-18\u5728\u63a9\u7801\u5408\u6210\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5168\u8138\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73ab\u7470\u75e4\u75ae\u662f\u4e00\u79cd\u5e38\u89c1\u4f46\u8bca\u65ad\u4e0d\u8db3\u7684\u708e\u75c7\u6027\u76ae\u80a4\u75c5\uff0c\u81ea\u52a8\u68c0\u6d4b\u9762\u4e34\u75c7\u72b6\u5f25\u6563\u6027\u3001\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u9762\u90e8\u56fe\u50cf\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u51c6\u786e\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "1. \u57fa\u4e8e\u4e34\u5e8a\u5148\u9a8c\u77e5\u8bc6\uff08\u73ab\u7470\u75e4\u75ae\u4e3b\u8981\u8868\u73b0\u4e3a\u9762\u90e8\u4e2d\u592e\u7ea2\u6591\uff09\u6784\u5efa\u56fa\u5b9a\u7ea2\u8272\u4fe1\u606f\u63a9\u7801\uff0c\u9009\u62e9\u9762\u90e8\u56fe\u50cf\u4e2d\u7ea2\u8272\u901a\u9053\u5f3a\u5ea6\u6301\u7eed\u8f83\u9ad8\u7684\u533a\u57df\uff1b2. \u4f7f\u7528ResNet-18\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u63a9\u7801\u5408\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff1b3. \u4e13\u6ce8\u4e8e\u8138\u988a\u3001\u9f3b\u5b50\u548c\u989d\u5934\u7b49\u8bca\u65ad\u76f8\u5173\u533a\u57df\uff0c\u6392\u9664\u8eab\u4efd\u8bc6\u522b\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u5168\u8138\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u548c\u4e34\u5e8a\u5148\u9a8c\u77e5\u8bc6\u53ef\u4ee5\u5171\u540c\u5b9e\u73b0\u51c6\u786e\u4e14\u7b26\u5408\u4f26\u7406\u7684\u76ae\u80a4\u75c5AI\u7cfb\u7edf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fdc\u7a0b\u533b\u7597\u548c\u5927\u89c4\u6a21\u7b5b\u67e5\u7b49\u9690\u79c1\u654f\u611f\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.09725", "pdf": "https://arxiv.org/pdf/2509.09725", "abs": "https://arxiv.org/abs/2509.09725", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "categories": ["cs.CL"], "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53cc\u9636\u6bb5\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u7cfb\u7edfBIBERT-Pipe\uff0c\u4e13\u95e8\u5904\u7406\u591a\u8bed\u8a00\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u94fe\u63a5\u95ee\u9898\uff0c\u5728BioNNE 2025\u8bc4\u6d4b\u4e2d\u6392\u540d\u7b2c\u4e09", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u5355\u8bed\u548c\u5e73\u5766\u5b9e\u4f53\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00\u548c\u5d4c\u5957\u5b9e\u4f53\u7684\u7cfb\u7edf\u7814\u7a76", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u68c0\u7d22-\u6392\u5e8f\u67b6\u6784\uff1a\u68c0\u7d22\u9636\u6bb5\u4f7f\u7528\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6392\u5e8f\u9636\u6bb5\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff1b\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8fb9\u754c\u6807\u8bb0[Ms]/[Me]\u5904\u7406\u5d4c\u5957\u5b9e\u4f53\uff1b\u901a\u8fc7\u4e09\u79cd\u6570\u636e\u6e90\u81ea\u52a8\u6269\u5145\u8bad\u7ec3\u8bed\u6599", "result": "\u5728BioNNE 2025\u591a\u8bed\u8a00\u8d5b\u9053\u6392\u540d\u7b2c\u4e09\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6700\u5c0f\u4f46\u539f\u5219\u6027\u4fee\u6539\u7684\u6709\u6548\u6027\u548c\u7ade\u4e89\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4fee\u6539\u5b9e\u73b0\u4e86\u5bf9\u591a\u8bed\u8a00\u5d4c\u5957\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u7684\u6709\u6548\u5904\u7406\uff0c\u4e3a\u8fd9\u4e00\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09849", "pdf": "https://arxiv.org/pdf/2509.09849", "abs": "https://arxiv.org/abs/2509.09849", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking", "categories": ["cs.CV"], "comment": null, "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.", "AI": {"tldr": "\u672c\u6587\u5bf9ULW\u8179\u8154\u955c\u56fe\u50cf\u53bb\u70df\u6846\u67b6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u53ef\u5b66\u4e60\u7ef4\u7eb3\u6ee4\u6ce2\u5668\u6a21\u5757\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u4e2d\u5404\u635f\u5931\u9879\u7684\u8d21\u732e", "motivation": "\u4e3a\u4e86\u4e25\u683c\u8bc4\u4f30ULW\u6846\u67b6\u4e2d\u5404\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u548c\u5fc5\u8981\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5206\u6790\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u6574\u4f53\u6027\u80fd\u7684\u5177\u4f53\u8d21\u732e", "method": "\u91c7\u7528\u6d88\u878d\u7814\u7a76\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a(1)\u79fb\u9664\u53ef\u5b66\u4e60\u7ef4\u7eb3\u6ee4\u6ce2\u5668\u6a21\u5757\uff1b(2)\u9009\u62e9\u6027\u4f7f\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\u4e2d\u7684\u5355\u4e2a\u635f\u5931\u9879\uff08MSE\u3001SSIM\u635f\u5931\u548c\u611f\u77e5\u635f\u5931\uff09\u3002\u6240\u6709\u53d8\u4f53\u5728\u516c\u5f00\u7684\u8179\u8154\u955c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5", "result": "\u4f7f\u7528\u5b9a\u91cf\u6307\u6807\uff08SSIM\u3001PSNR\u3001MSE\u548cCIEDE-2000\uff09\u548c\u5b9a\u6027\u89c6\u89c9\u6bd4\u8f83\u6765\u8bc4\u4f30\u4e0d\u540c\u53d8\u4f53\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86ULW\u6846\u67b6\u4e2d\u5404\u4e2a\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u8179\u8154\u955c\u56fe\u50cf\u53bb\u70df\u6280\u672f\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3"}}
{"id": "2509.09726", "pdf": "https://arxiv.org/pdf/2509.09726", "abs": "https://arxiv.org/abs/2509.09726", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "categories": ["cs.CL"], "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u7684\u975e\u5f62\u5f0f\u5316\u548c\u6458\u8981\u80fd\u529b\uff0c\u5c06\u673a\u5668\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u7ffb\u8bd1\u4e3a\u81ea\u7136\u8bed\u8a00\u7684\u65b9\u6cd5", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f62\u5f0f\u5316\u8bc1\u660e\u96be\u4ee5\u88ab\u4eba\u7c7b\u7406\u89e3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u673a\u5668\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u63d0\u9ad8\u8bc1\u660e\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u7406\u89e3\u6027", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u975e\u5f62\u5f0f\u5316(\u5f62\u5f0f\u8bed\u8a00\u8bc1\u660e\u6b65\u9aa4\u7684\u8a00\u8bed\u5316)\u548c\u6458\u8981\u80fd\u529b\uff0c\u5c06\u5f62\u5f0f\u5316\u8bc1\u660e\u7ffb\u8bd1\u4e3a\u81ea\u7136\u8bed\u8a00", "result": "\u5728\u672c\u79d1\u6559\u79d1\u4e66\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u6570\u636e\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\uff1b\u5728Lean\u8bc1\u660e\u52a9\u624b\u7684\u73b0\u6709\u5f62\u5f0f\u8bc1\u660e\u5e93\u4e0a\u5e94\u7528\uff0c\u8bc1\u660e\u80fd\u8f93\u51fa\u9ad8\u53ef\u8bfb\u6027\u548c\u51c6\u786e\u6027\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5c06\u673a\u5668\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\uff0c\u63d0\u9ad8\u4e86\u8bc1\u660e\u7684\u53ef\u8bfb\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2509.09859", "pdf": "https://arxiv.org/pdf/2509.09859", "abs": "https://arxiv.org/abs/2509.09859", "authors": ["Razvan Stefanescu", "Ethan Oh", "Ruben Vazquez", "Chris Mesterharm", "Constantin Serban", "Ritu Chadha"], "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector", "categories": ["cs.CV", "cs.LG", "68W99"], "comment": "11 pages, 11 figures", "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.", "AI": {"tldr": "WAVE-DETR\u662f\u4e00\u4e2a\u7ed3\u5408\u53ef\u89c1\u5149RGB\u548c\u58f0\u5b66\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u65e0\u4eba\u673a\u68c0\u6d4b\u5668\uff0c\u4f7f\u7528Deformable DETR\u548cWav2Vec2\u67b6\u6784\uff0c\u5728\u771f\u5b9eARDrone\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u95e8\u63a7\u878d\u5408\u673a\u5236\u5c06mAP\u63d0\u534711.1%-15.3%\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u9c81\u68d2\u68c0\u6d4b\u65e0\u4eba\u673a\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u89c9\u548c\u58f0\u5b66\u4fe1\u53f7\u7684\u4e92\u8865\u6027\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u57fa\u4e8eDeformable DETR\u548cWav2Vec2\u67b6\u6784\uff0c\u5f00\u53d1\u4e86\u56db\u79cd\u878d\u5408\u914d\u7f6e\uff08\u95e8\u63a7\u673a\u5236\u3001\u7ebf\u6027\u5c42\u3001MLP\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\uff0c\u5c06\u58f0\u5b66\u5d4c\u5165\u4e0e\u591a\u5206\u8fa8\u7387\u7279\u5f81\u6620\u5c04\u878d\u5408\u3002", "result": "\u95e8\u63a7\u878d\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5728\u5c0f\u65e0\u4eba\u673a\u4e0amAP\u63d0\u534711.1%-15.3%\uff0c\u4e2d\u5927\u578b\u65e0\u4eba\u673amAP\u63d0\u53473.27%-5.84%\uff0c\u5728\u6240\u6709IoU\u9608\u503c(0.5-0.9)\u4e0a\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "\u58f0\u5b66\u4fe1\u606f\u80fd\u591f\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5c0f\u5c3a\u5bf8\u65e0\u4eba\u673a\u68c0\u6d4b\u65b9\u9762\uff0c\u591a\u6a21\u6001\u878d\u5408\u5728\u771f\u5b9e\u73af\u5883\u6761\u4ef6\u4e0b\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.09727", "pdf": "https://arxiv.org/pdf/2509.09727", "abs": "https://arxiv.org/abs/2509.09727", "authors": ["Andy Zhu", "Yingjun Du"], "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "categories": ["cs.CL", "cs.CE"], "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u63d0\u5347\u91d1\u878d\u95ee\u7b54\u7684\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u57fa\u7ebf\u63d0\u9ad8\u4e866.6-8.3%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u6559\u80b2\u95ee\u7b54\u4e2d\u96be\u4ee5\u6355\u6349\u4e13\u4e1a\u63a8\u7406\u9700\u6c42\uff0c\u91d1\u878d\u9886\u57df\u9700\u8981\u591a\u6b65\u5b9a\u91cf\u63a8\u7406\u3001\u4e13\u4e1a\u672f\u8bed\u7406\u89e3\u548c\u73b0\u5b9e\u573a\u666f\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5305\u542b\u57fa\u7840\u751f\u6210\u5668\u3001\u8bc1\u636e\u68c0\u7d22\u5668\u548c\u4e13\u5bb6\u8bc4\u5ba1\u5668\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408RAG\u4ece6\u672c\u91d1\u878d\u6559\u79d1\u4e66\u83b7\u53d6\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u91c7\u7528\u5355\u6b21\u8fed\u4ee3\u751f\u6210\u7cbe\u70bc\u7b54\u6848\u3002", "result": "\u5728Study.com\u76843,532\u4e2a\u91d1\u878d\u6559\u80b2\u95ee\u9898\u4e0a\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u6279\u5224\u7684\u7ec6\u5316\u65b9\u6cd5\u6bd4\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u9ad86.6-8.3%\uff0cGemini-2.0-Flash\u8868\u73b0\u6700\u4f73\uff0cGPT-4o-mini\u8fbe\u5230\u4e0e\u91d1\u878d\u8c03\u4f18\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u91d1\u878d\u95ee\u7b54\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u9014\u5f84\uff0c\u5e76\u4e3a\u591a\u667a\u80fd\u4f53\u91d1\u878dLLM\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.09869", "pdf": "https://arxiv.org/pdf/2509.09869", "abs": "https://arxiv.org/abs/2509.09869", "authors": ["Yihao Liu", "Junyu Chen", "Lianrui Zuo", "Shuwen Wei", "Brian D. Boyd", "Carmen Andreescu", "Olusola Ajilore", "Warren D. Taylor", "Aaron Carass", "Bennett A. Landman"], "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u4ee3\u7406\u76d1\u7763\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u4f30\u8ba1\u7684\u7a7a\u95f4\u53d8\u6362\u5e94\u7528\u4e8e\u4ee3\u7406\u56fe\u50cf\uff0c\u5c06\u8f93\u5165\u57df\u4e0e\u76d1\u7763\u57df\u89e3\u8026\uff0c\u4ece\u800c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u914d\u51c6\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u914d\u51c6\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5bf9\u8f93\u5165\u56fe\u50cf\u7279\u6027\u53d8\u5316\uff08\u5982\u4f2a\u5f71\u3001\u89c6\u91ce\u4e0d\u5339\u914d\u3001\u6a21\u6001\u5dee\u5f02\uff09\u654f\u611f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u63d0\u9ad8\u7f51\u7edc\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u7684\u901a\u7528\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4ee3\u7406\u76d1\u7763\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u57df\u4e0e\u76d1\u7763\u57df\u89e3\u8026\uff0c\u901a\u8fc7\u5c06\u4f30\u8ba1\u7684\u7a7a\u95f4\u53d8\u6362\u5e94\u7528\u4e8e\u4ee3\u7406\u56fe\u50cf\u6765\u786e\u4fdd\u5728\u76f8\u4f3c\u6027\u5b9a\u4e49\u826f\u597d\u7684\u57df\u4e2d\u8fdb\u884c\u76d1\u7763\u8ba1\u7b97\uff0c\u5141\u8bb8\u5728\u5f02\u6784\u8f93\u5165\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u5e94\u7528\u4e2d\uff08\u6297\u4f2a\u5f71\u8111MR\u914d\u51c6\u3001\u63a9\u7801\u65e0\u5173\u80baCT\u914d\u51c6\u3001\u591a\u6a21\u6001MR\u914d\u51c6\uff09\u5747\u8868\u73b0\u51fa\u5bf9\u8f93\u5165\u53d8\u5316\u7684\u5f3a\u97e7\u6027\uff0c\u5305\u62ec\u4e0d\u5747\u5300\u573a\u3001\u4e0d\u4e00\u81f4\u89c6\u91ce\u548c\u6a21\u6001\u5dee\u5f02\uff0c\u540c\u65f6\u5728\u7cbe\u5fc3\u6574\u7406\u7684\u6570\u636e\u4e0a\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "\u4ee3\u7406\u76d1\u7763\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u5728\u4e0d\u589e\u52a0\u590d\u6742\u6027\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u914d\u51c6\u6a21\u578b\uff0c\u4e3a\u66f4\u9c81\u68d2\u7684\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2509.09728", "pdf": "https://arxiv.org/pdf/2509.09728", "abs": "https://arxiv.org/abs/2509.09728", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "categories": ["cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u835f\u8403\u5206\u6790\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5728Twitter\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5e73\u5747\u51c6\u786e\u7387\u4e3a0.80\uff0c\u5f3a\u8c03\u9700\u8981\u6807\u51c6\u5316\u6027\u80fd\u62a5\u544a\u548c\u8003\u8651\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5728Twitter\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6574\u4f53\u6027\u80fd\u8868\u73b0\uff0c\u5206\u6790\u7814\u7a76\u95f4\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u63a2\u8ba8\u7814\u7a76\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\u8fdb\u884c\u6587\u732e\u68c0\u7d22\uff0c\u4ece20\u9879\u7814\u7a76\u4e2d\u7b5b\u9009195\u4e2a\u8bd5\u9a8c\uff0c\u4f7f\u7528\u53cc\u53cd\u6b63\u5f26\u53d8\u6362\u548c\u4e09\u5c42\u6b21\u968f\u673a\u6548\u5e94\u6a21\u578b\u5206\u6790\u603b\u4f53\u51c6\u786e\u7387\u3002", "result": "AIC\u4f18\u5316\u6a21\u578b\u7684\u5e73\u5747\u603b\u4f53\u51c6\u786e\u7387\u4e3a0.80 [0.76, 0.84]\uff0c\u53d1\u73b0\u603b\u4f53\u51c6\u786e\u7387\u56e0\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u7c7b\u522b\u6570\u91cf\u7684\u654f\u611f\u6027\u800c\u5bb9\u6613\u4ea7\u751f\u8bef\u5bfc\u3002", "conclusion": "\u9700\u8981\u6807\u51c6\u5316\u6a21\u578b\u6027\u80fd\u62a5\u544a\uff08\u5305\u62ec\u72ec\u7acb\u6d4b\u8bd5\u96c6\u7684\u6df7\u6dc6\u77e9\u9635\uff09\uff0c\u5e76\u5bf9\u51c6\u786e\u7387\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\uff0c\u4ee5\u5b9e\u73b0\u8de8\u7814\u7a76\u7684\u53ef\u9760\u6bd4\u8f83\u3002"}}
{"id": "2509.09911", "pdf": "https://arxiv.org/pdf/2509.09911", "abs": "https://arxiv.org/abs/2509.09911", "authors": ["Barkin Buyukcakir", "Jannick De Tobel", "Patrick Thevissen", "Dirk Vandermeulen", "Peter Claes"], "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars", "categories": ["cs.CV", "cs.AI", "68T07 (Primary)"], "comment": "21 pages, 11 figures, Scientific Reports", "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548cVision Transformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u7259\u9f7f\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u9488\u5bf9\u7b2c\u4e8c\u548c\u7b2c\u4e09\u78e8\u7259\u7684\u6027\u80fd\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u6cd5\u533b\u5e74\u9f84\u4f30\u8ba1\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9762\u4e34'\u9ed1\u76d2'\u95ee\u9898\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u900f\u660e\u5ea6\u6765\u652f\u6301\u4e13\u5bb6\u51b3\u7b56\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668(AE)\u4e0eVision Transformer(ViT)\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790AE\u7684\u6f5c\u5728\u7a7a\u95f4\u5ea6\u91cf\u548c\u56fe\u50cf\u91cd\u5efa\u6765\u63d0\u4f9b\u591a\u65b9\u9762\u7684\u8bca\u65ad\u6d1e\u5bdf\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff1a\u7259\u9f7f37\u4ece0.712\u63d0\u9ad8\u52300.815\uff0c\u7259\u9f7f38\u4ece0.462\u63d0\u9ad8\u52300.543\u3002\u5206\u6790\u8868\u660e\u5269\u4f59\u6027\u80fd\u5dee\u8ddd\u4e3b\u8981\u662f\u7531\u4e8e\u7259\u9f7f38\u6570\u636e\u96c6\u7684\u9ad8\u7c7b\u5185\u5f62\u6001\u53d8\u5f02\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u8bc1\u636e\uff0c\u5f3a\u8c03\u4e86\u5355\u4e00\u53ef\u89e3\u91ca\u6027\u6a21\u5f0f\u7684\u4e0d\u8db3\uff0c\u4e3a\u6cd5\u533b\u5e74\u9f84\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2509.09729", "pdf": "https://arxiv.org/pdf/2509.09729", "abs": "https://arxiv.org/abs/2509.09729", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias M\u00fcller", "Rico Sennrich", "Sarah Ebling"], "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters.", "AI": {"tldr": "MultimodalHugs\u662f\u4e00\u4e2a\u57fa\u4e8eHugging Face\u6784\u5efa\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u8bed\u5904\u7406\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u4ee3\u7801\u590d\u6742\u3001\u53ef\u590d\u73b0\u6027\u4f4e\u548c\u6bd4\u8f83\u4e0d\u516c\u5e73\u7b49\u95ee\u9898\uff0c\u901a\u8fc7\u652f\u6301\u66f4\u591a\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\u6765\u63d0\u5347\u5b9e\u9a8c\u7075\u6d3b\u6027\u3002", "motivation": "\u624b\u8bed\u5904\u7406\u7814\u7a76\u76f8\u6bd4\u53e3\u8bed\u8bed\u8a00\u7814\u7a76\u9762\u4e34\u66f4\u591a\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7684\u4e34\u65f6\u4ee3\u7801\u3001\u4f4e\u53ef\u590d\u73b0\u6027\u548c\u4e0d\u516c\u5e73\u6bd4\u8f83\u3002\u73b0\u6709\u5de5\u5177\u5982Hugging Face\u5728\u624b\u8bed\u5b9e\u9a8c\u96c6\u6210\u65b9\u9762\u4e0d\u591f\u7075\u6d3b\uff0c\u8fd9\u901a\u8fc7\u5bf9SLP\u7814\u7a76\u4eba\u5458\u7684\u8c03\u67e5\u5f97\u5230\u8bc1\u5b9e\u3002", "method": "\u5728Hugging Face\u57fa\u7840\u4e0a\u6784\u5efaMultimodalHugs\u6846\u67b6\uff0c\u589e\u52a0\u62bd\u8c61\u5c42\u4ee5\u652f\u6301\u66f4\u591a\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\uff0c\u540c\u65f6\u7ee7\u627fHugging Face\u751f\u6001\u7cfb\u7edf\u7684\u4f18\u52bf\u3002", "result": "MultimodalHugs\u80fd\u591f\u9002\u5e94\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5982\u624b\u8bed\u7684\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u548c\u6587\u672c\u5b57\u7b26\u7684\u50cf\u7d20\u6570\u636e\uff0c\u901a\u8fc7\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u9002\u7528\u6027\u3002", "conclusion": "MultimodalHugs\u4e0d\u4ec5\u4e13\u6ce8\u4e8e\u624b\u8bed\u5904\u7406\uff0c\u5176\u62bd\u8c61\u5c42\u8bbe\u8ba1\u4f7f\u5176\u66f4\u5e7f\u6cdb\u9002\u7528\u4e8e\u5176\u4ed6\u4e0d\u7b26\u5408Hugging Face\u6807\u51c6\u6a21\u677f\u7684\u7528\u4f8b\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b9e\u9a8c\u7684\u7075\u6d3b\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2509.09935", "pdf": "https://arxiv.org/pdf/2509.09935", "abs": "https://arxiv.org/abs/2509.09935", "authors": ["Chirayu Agrawal", "Snehasis Mukherjee"], "title": "SCoDA: Self-supervised Continual Domain Adaptation", "categories": ["cs.CV"], "comment": "Submitted to ICVGIP 2025", "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.", "AI": {"tldr": "SCoDA\u662f\u4e00\u79cd\u65e0\u9700\u6e90\u57df\u6570\u636e\u7684\u81ea\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u6d41\u5f62\u5bf9\u9f50\u548cEMA\u66f4\u65b0\u89e3\u51b3\u4f20\u7edfSFDA\u65b9\u6cd5\u4e22\u5931\u51e0\u4f55\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4f20\u7edfSFDA\u65b9\u6cd5\u4f9d\u8d56\u5168\u76d1\u7763\u9884\u8bad\u7ec3\u6e90\u6a21\u578b\uff0c\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5bf9\u9f50\u5b9e\u4f8b\u7ea7\u7279\u5f81\u65f6\u4f1a\u4e22\u5f03\u6e90\u6a21\u578b\u6f5c\u5728\u6d41\u5f62\u7684\u5173\u952e\u51e0\u4f55\u4fe1\u606f", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u521d\u59cb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u6d41\u5f62\u5bf9\u9f50\u539f\u5219\uff0c\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u7279\u5f81\u5339\u914d\u548c\u7a7a\u95f4\u76f8\u4f3c\u6027\u635f\u5931\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u91c7\u7528EMA\u66f4\u65b0\u6559\u5e08\u53c2\u6570\u4ee5\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSCoDA\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684SFDA\u65b9\u6cd5", "conclusion": "SCoDA\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u51e0\u4f55\u6d41\u5f62\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86SFDA\u4e2d\u7684\u51e0\u4f55\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2509.09731", "pdf": "https://arxiv.org/pdf/2509.09731", "abs": "https://arxiv.org/abs/2509.09731", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring.", "AI": {"tldr": "AncientDoc\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e2a\u4efb\u52a1\uff08\u9875\u9762\u7ea7OCR\u3001\u767d\u8bdd\u7ffb\u8bd1\u3001\u63a8\u7406\u95ee\u7b54\u3001\u77e5\u8bc6\u95ee\u7b54\u3001\u8bed\u8a00\u53d8\u4f53\u95ee\u7b54\uff09\uff0c\u6db5\u76d614\u79cd\u6587\u6863\u7c7b\u578b\u3001100\u591a\u672c\u4e66\u7c4d\u548c\u7ea63000\u9875\u5185\u5bb9\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u7c4d\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u4e2d\u6587\u53e4\u7c4d\u4f5c\u4e3a\u5343\u5e74\u5386\u53f2\u6587\u5316\u7684\u5b9d\u8d35\u8f7d\u4f53\uff0c\u5305\u542b\u4e30\u5bcc\u77e5\u8bc6\u4f46\u9762\u4e34\u6570\u5b57\u5316\u548c\u7406\u89e3\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u626b\u63cf\u56fe\u50cf\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u5176\u89c6\u89c9\u548c\u8bed\u8a00\u590d\u6742\u6027\uff0c\u4e14\u73b0\u6709\u6587\u6863\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u82f1\u6587\u5370\u5237\u6587\u672c\u6216\u7b80\u4f53\u4e2d\u6587\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u6587\u53e4\u7c4d\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u6784\u5efaAncientDoc\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u9875\u9762\u7ea7OCR\u8bc6\u522b\u3001\u6587\u8a00\u6587\u5230\u767d\u8bdd\u6587\u7ffb\u8bd1\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u95ee\u7b54\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u95ee\u7b54\u3001\u8bed\u8a00\u53d8\u4f53\u95ee\u7b54\u3002\u6570\u636e\u96c6\u6db5\u76d614\u79cd\u6587\u6863\u7c7b\u578b\u3001\u8d85\u8fc7100\u672c\u4e66\u7c4d\u548c\u7ea63000\u9875\u5185\u5bb9\u3002\u4f7f\u7528\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8f85\u4ee5\u4eba\u5de5\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5AncientDoc\uff0c\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u7c4d\u5904\u7406\u9886\u57df\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "AncientDoc\u586b\u8865\u4e86\u4e2d\u6587\u53e4\u7c4d\u6587\u6863\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u63a8\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u7c4d\u6570\u5b57\u5316\u548c\u7406\u89e3\u65b9\u9762\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u4fdd\u62a4\u548c\u4f20\u627f\u4e2d\u534e\u6587\u5316\u9057\u4ea7\u3002"}}
{"id": "2509.09943", "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edg\u00fc", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSAM2\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u7ec6\u80de\u8ffd\u8e2a\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u57282D\u548c3D\u663e\u5fae\u955c\u89c6\u9891\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u9002\u5e94\u663e\u5fae\u955c\u6570\u636e\u7684\u5de8\u5927\u591a\u6837\u6027", "method": "\u5c06Segment Anything 2 (SAM2)\u5927\u578b\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u8ffd\u8e2a\u6d41\u7a0b\u4e2d\uff0c\u4f5c\u4e3a\u5b8c\u5168\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u96c6", "result": "\u57282D\u548c\u5927\u89c4\u6a213D\u5ef6\u65f6\u663e\u5fae\u955c\u89c6\u9891\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u7cbe\u5ea6\uff0c\u65e0\u9700\u6570\u636e\u96c6\u7279\u5b9a\u9002\u914d", "conclusion": "\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u663e\u5fae\u955c\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2509.09734", "pdf": "https://arxiv.org/pdf/2509.09734", "abs": "https://arxiv.org/abs/2509.09734", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.", "AI": {"tldr": "MCP-AgentBench\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9MCP\u534f\u8bae\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b33\u4e2a\u670d\u52a1\u5668\u3001188\u4e2a\u5de5\u5177\u548c600\u4e2a\u67e5\u8be2\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u4ee3\u7406\u5728\u5de5\u5177\u4ea4\u4e92\u4e2d\u7684\u771f\u5b9e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30MCP\u534f\u8bae\u4e0bAI\u4ee3\u7406\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u5bfc\u81f4\u5bf9\u5176\u64cd\u4f5c\u4ef7\u503c\u7684\u8bef\u89e3\u548c\u80fd\u529b\u5dee\u5f02\u7684\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u3002", "method": "\u5efa\u7acb\u4e86\u5305\u542b33\u4e2a\u64cd\u4f5c\u670d\u52a1\u5668\u548c188\u4e2a\u4e0d\u540c\u5de5\u5177\u7684MCP\u6d4b\u8bd5\u5e8a\uff0c\u5f00\u53d1\u4e86600\u4e2a\u7cfb\u7edf\u8bbe\u8ba1\u7684\u67e5\u8be2\uff0c\u5206\u5e03\u57286\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4ea4\u4e92\u7c7b\u522b\u4e2d\uff0c\u5e76\u5f15\u5165\u4e86MCP-Eval\u7ed3\u679c\u5bfc\u5411\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u8bed\u8a00\u4ee3\u7406\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u4ee3\u7406\u5728MCP\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "MCP-AgentBench\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u548c\u53ef\u9760\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u9a8c\u8bc1\u548c\u63a8\u8fdb\u80fd\u591f\u5145\u5206\u5229\u7528MCP\u53d8\u9769\u6027\u4f18\u52bf\u7684AI\u4ee3\u7406\uff0c\u52a0\u901f\u5b9e\u73b0\u771f\u6b63\u6709\u80fd\u529b\u4e14\u53ef\u4e92\u64cd\u4f5c\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2509.09946", "pdf": "https://arxiv.org/pdf/2509.09946", "abs": "https://arxiv.org/abs/2509.09946", "authors": ["Vu-Minh Le", "Thao-Anh Tran", "Duc Huy Do", "Xuan Canh Do", "Huong Ninh", "Hai Tran"], "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation", "categories": ["cs.CV"], "comment": "Accepted at ICCVW 2025", "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u73b0\u67092D\u591a\u6444\u50cf\u5934\u8ddf\u8e2a\u7cfb\u7edf\u6269\u5c55\u52303D\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u4fe1\u606f\u91cd\u5efa\u76ee\u6807\u70b9\u4e91\u5e76\u6062\u590d3D\u8fb9\u754c\u6846\uff0c\u5728AI City Challenge\u4e2d\u83b7\u5f97\u7b2c\u4e09\u540d", "motivation": "3D\u7a7a\u95f4\u8ddf\u8e2a\u9700\u8981\u5b8c\u5168\u91cd\u5efa2D\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u8fd9\u5bf9\u73b0\u6709MTMC\u7cfb\u7edf\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u73b0\u67092D\u7cfb\u7edf\u6269\u5c55\u52303D\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u91cd\u5efa\u76ee\u6807\u70b9\u4e91\u7a7a\u95f4\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u504f\u822a\u89d2\u7ec6\u5316\u6062\u590d3D\u8fb9\u754c\u6846\uff0c\u5f15\u5165\u589e\u5f3a\u7684\u5728\u7ebf\u6570\u636e\u5173\u8054\u673a\u5236\u5229\u7528\u76ee\u6807\u5c40\u90e8ID\u4e00\u81f4\u6027\u5206\u914d\u5168\u5c40ID", "result": "\u57282025 AI City Challenge\u76843D MTMC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u83b7\u5f97\u6392\u884c\u699c\u7b2c\u4e09\u540d", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c062D\u591a\u6444\u50cf\u5934\u8ddf\u8e2a\u7cfb\u7edf\u6269\u5c55\u52303D\u7a7a\u95f4\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u5efa\u7cfb\u7edf\uff0c\u5728\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02"}}
{"id": "2509.09735", "pdf": "https://arxiv.org/pdf/2509.09735", "abs": "https://arxiv.org/abs/2509.09735", "authors": ["Willem Huijzer", "Jieying Chen"], "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "categories": ["cs.CL"], "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u80cc\u666f\u3001\u6027\u522b\u548c\u5e74\u9f84\u65b9\u9762\u7684\u504f\u89c1\uff0c\u53d1\u73b0GPT-3.5\u548cGPT-4o\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u504f\u597d\u5973\u6027\u3001\u5e74\u8f7b\u5e74\u9f84\u548c\u7279\u5b9a\u80cc\u666f\uff08\u5982\u975e\u88d4\u7f8e\u56fd\u4eba\uff09\uff0c\u800c\u6458\u8981\u4efb\u52a1\u504f\u89c1\u8f83\u5c0f\u3002\u8de8\u8bed\u8a00\u5206\u6790\u663e\u793a\u82f1\u8bed\u548c\u8377\u5170\u8bed\u7684\u504f\u89c1\u6a21\u5f0f\u76f8\u4f3c\uff0c\u4f46\u5b58\u5728\u5dee\u5f02\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u6307\u4ee4\u80fd\u51cf\u5c1127%\u7684\u504f\u89c1\u5dee\u8ddd\uff0cGPT-4o\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u4e2a\u9886\u57df\u7684\u5feb\u901f\u96c6\u6210\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\u548c\u4fe1\u606f\u504f\u89c1\u7684\u62c5\u5fe7\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u6790LLMs\u5728\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u4e0a\u7684\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u51b3\u7b56\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u8de8\u8bed\u8a00\u504f\u89c1\u4f20\u64ad\u548c\u7f13\u89e3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528Tamkin\u7b49\u4eba(2023)\u6570\u636e\u96c6\u7684\u8377\u5170\u8bed\u7ffb\u8bd1\u7248\u672c\uff0c\u521b\u5efa\u4e86151,200\u4e2a\u51b3\u7b56\u4efb\u52a1\u63d0\u793a\u548c176,400\u4e2a\u6458\u8981\u4efb\u52a1\u63d0\u793a\u3002\u6d4b\u8bd5\u4e86\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u53d8\u91cf\u3001\u6307\u4ee4\u3001\u663e\u8457\u5ea6\u6c34\u5e73\u548c\u8bed\u8a00\uff0c\u5728GPT-3.5\u548cGPT-4o\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u90fd\u5b58\u5728\u663e\u8457\u504f\u89c1\uff1a\u504f\u597d\u5973\u6027\u6027\u522b\u3001\u5e74\u8f7b\u5e74\u9f84\u548c\u7279\u5b9a\u80cc\u666f\uff08\u5982\u975e\u88d4\u7f8e\u56fd\u4eba\uff09\u3002\u6458\u8981\u4efb\u52a1\u504f\u89c1\u8f83\u5c0f\uff0c\u4f46GPT-3.5\u5728\u82f1\u8bed\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u5e74\u9f84\u76f8\u5173\u5dee\u5f02\u3002\u8de8\u8bed\u8a00\u5206\u6790\u663e\u793a\u82f1\u8bed\u548c\u8377\u5170\u8bed\u504f\u89c1\u6a21\u5f0f\u76f8\u4f3c\u4f46\u6709\u5dee\u5f02\u3002\u7f13\u89e3\u6307\u4ee4\u80fd\u5e73\u5747\u51cf\u5c1127%\u7684\u6700\u6709\u5229\u548c\u6700\u4e0d\u5229\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8c28\u614e\u91c7\u7528LLMs\u548c\u9488\u5bf9\u5177\u4f53\u60c5\u5883\u8fdb\u884c\u504f\u89c1\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff0c\u9700\u8981\u6301\u7eed\u5f00\u53d1\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u4ee5\u786e\u4fddAI\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002GPT-4o\u76f8\u6bd4GPT-3.5\u5728\u6240\u6709\u82f1\u8bed\u63d0\u793a\u4e2d\u90fd\u663e\u793a\u51fa\u51cf\u5c11\u7684\u504f\u89c1\uff0c\u8868\u660e\u5728\u65b0\u6a21\u578b\u4e2d\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u5177\u6709\u7279\u5b9a\u6f5c\u529b\u3002"}}
{"id": "2509.09958", "pdf": "https://arxiv.org/pdf/2509.09958", "abs": "https://arxiv.org/abs/2509.09958", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u7684\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06REC\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8e\u6846\u7684\u89c6\u89c9\u8bed\u8a00\u9a8c\u8bc1\uff0c\u4f7f\u7528\u901a\u7528\u68c0\u6d4b\u5668\u548cVLM\u8fdb\u884cTrue/False\u67e5\u8be2\uff0c\u65e0\u9700\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u8fbe\u5230\u7ade\u4e89\u6027\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684REC\u65b9\u6cd5\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u8bad\u7ec3\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u96f6\u6837\u672c\u5de5\u4f5c\u6d41\u662f\u5426\u80fd\u8fbe\u5230\u7c7b\u4f3c\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u7279\u5b9a\u9884\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528COCO-clean\u901a\u7528\u68c0\u6d4b\u5668(YOLO-World)\u751f\u6210\u5019\u9009\u6846\uff0c\u7136\u540e\u4f7f\u7528\u901a\u7528VLM\u5bf9\u6bcf\u4e2a\u533a\u57df\u72ec\u7acb\u8fdb\u884cTrue/False\u67e5\u8be2\u9a8c\u8bc1\uff0c\u907f\u514d\u8de8\u6846\u5e72\u6270\uff0c\u652f\u6301\u5f03\u6743\u548c\u591a\u5339\u914d\u3002", "result": "\u5728RefCOCO\u3001RefCOCO+\u548cRefCOCOg\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u96f6\u6837\u672cGroundingDINO\u57fa\u7ebf\uff0c\u8fd8\u8d85\u8fc7\u4e86\u7ecf\u8fc7REC\u8bad\u7ec3\u7684GroundingDINO\u548cGroundingDINO+CRG\u7684\u62a5\u544a\u7ed3\u679c\u3002", "conclusion": "\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u800c\u975e\u4efb\u52a1\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u662f\u5b9e\u73b0\u5f3a\u5927\u96f6\u6837\u672cREC\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.09801", "pdf": "https://arxiv.org/pdf/2509.09801", "abs": "https://arxiv.org/abs/2509.09801", "authors": ["Brennen Hill"], "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.", "AI": {"tldr": "HEFT\u662f\u4e00\u79cd\u5206\u5c42\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408\u6743\u91cd\u7a7a\u95f4\uff08LoRA\uff09\u548c\u8868\u793a\u7a7a\u95f4\uff08ReFT\uff09\u4e24\u79cdPEFT\u65b9\u6cd5\uff0c\u5728BoolQ\u63a8\u7406\u4efb\u52a1\u4e0a\u4ee5\u66f4\u5c11\u7684\u8bad\u7ec3\u5468\u671f\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u63a8\u7406\u4efb\u52a1\u4e2d\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4e0d\u540c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u7684\u534f\u540c\u6548\u5e94", "method": "\u63d0\u51faHEFT\u5206\u5c42\u9002\u5e94\u7b56\u7565\uff1a\u5148\u5728\u6743\u91cd\u7a7a\u95f4\u4f7f\u7528LoRA\u8fdb\u884c\u57fa\u7840\u9002\u914d\uff0c\u7136\u540e\u5728\u8868\u793a\u7a7a\u95f4\u4f7f\u7528ReFT\u8fdb\u884c\u7cbe\u786e\u7ec6\u5316", "result": "\u5728Llama-2-7B\u6a21\u578b\u4e0a\uff0c\u4ec5\u8bad\u7ec33\u4e2a\u5468\u671f\u7684HEFT\u8fbe\u523085.17%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u8bad\u7ec320\u5468\u671f\u7684LoRA-only\uff0885.05%\uff09\u548cReFT-only\uff0883.36%\uff09\u65b9\u6cd5", "conclusion": "PEFT\u65b9\u6cd5\u7684\u7cbe\u5fc3\u7ec4\u5408\u662f\u5f3a\u5927\u7684\u7b97\u6cd5\u521b\u65b0\uff0c\u4e3a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u6709\u6548\u7684\u8def\u5f84"}}
{"id": "2509.09961", "pdf": "https://arxiv.org/pdf/2509.09961", "abs": "https://arxiv.org/abs/2509.09961", "authors": ["Tianqi Wei", "Xin Yu", "Zhi Chen", "Scott Chapman", "Zi Huang"], "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.", "AI": {"tldr": "\u63d0\u51faRPCP\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u5c0f\u9ea6\u53f6\u7247\u75c5\u866b\u5bb3\u5206\u5272\u4e2d\u7684\u50cf\u7d20\u7ea7\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u968f\u673a\u51e0\u4f55\u53d8\u6362\u548c\u6295\u5f71\u6ee4\u6ce2\u6765\u589e\u5f3a\u7f55\u89c1\u7c7b\u522b\u7684\u5b66\u4e60\u6548\u679c", "motivation": "\u5c0f\u9ea6\u53f6\u7247\u75c5\u866b\u5bb3\u5206\u5272\u4e2d\uff0c\u866b\u5bb3\u533a\u57df\u901a\u5e38\u53ea\u5360\u6807\u6ce8\u50cf\u7d20\u7684\u6781\u5c0f\u90e8\u5206\uff0c\u8fd9\u79cd\u6781\u7aef\u50cf\u7d20\u7ea7\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u5e38\u89c1\u7c7b\u522b\u800c\u5ffd\u7565\u7f55\u89c1\u7c7b\u522b\uff0c\u5f71\u54cd\u6574\u4f53\u5206\u5272\u6027\u80fd", "method": "\u968f\u673a\u6295\u5f71\u590d\u5236\u7c98\u8d34(RPCP)\u589e\u5f3a\u6280\u672f\uff1a\u4ece\u8bad\u7ec3\u56fe\u50cf\u4e2d\u63d0\u53d6\u7f55\u89c1\u866b\u5bb3\u6591\u5757\uff0c\u5e94\u7528\u968f\u673a\u51e0\u4f55\u53d8\u6362\u6a21\u62df\u53d8\u5316\uff0c\u5c06\u53d8\u6362\u540e\u7684\u6591\u5757\u7c98\u8d34\u5230\u5408\u9002\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u6295\u5f71\u6ee4\u6ce2\u5668\u4f18\u5316\u5c40\u90e8\u7279\u5f81\uff0c\u786e\u4fdd\u4e0e\u80cc\u666f\u81ea\u7136\u878d\u5408", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u866b\u5bb3\u7c7b\u522b\u7684\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u7565\u5fae\u63d0\u5347\u4e86\u5176\u4ed6\u7c7b\u522b\u7684\u51c6\u786e\u7387", "conclusion": "\u76ee\u6807\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u6781\u7aef\u50cf\u7d20\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u519c\u4e1a\u5206\u5272\u95ee\u9898\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09804", "pdf": "https://arxiv.org/pdf/2509.09804", "abs": "https://arxiv.org/abs/2509.09804", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "categories": ["cs.CL"], "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u5206\u6790\u8bed\u7528\u6846\u67b6\u5982\u4f55\u88ab\u6982\u5ff5\u5316\u548c\u5524\u8d77\uff0c\u6765\u5efa\u6a21\u591a\u6a21\u6001\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8bed\u8a00\u4e0e\u4ea4\u4e92\u624b\u52bf\u4e4b\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u5df2\u88ab\u591a\u4e2a\u9886\u57df\u7814\u7a76\uff0c\u4f46\u7279\u522b\u662f\u7528\u4e8e\u8f6e\u6b21\u7ec4\u7ec7\u7684\u624b\u52bf\u7b56\u7565\u5c1a\u672a\u88ab\u7f16\u7801\u5230\u53ef\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u96c6\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6ce8\u91ca\u65b9\u6cd5\uff0c\u5728Frame2\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u5df2\u6807\u6ce8\u8bed\u4e49\u6846\u67b6\uff09\u7684\u57fa\u7840\u4e0a\uff0c\u589e\u52a0\u4e86\u7528\u4e8e\u5efa\u6a21\u5bf9\u8bdd\u8f6e\u6b21\u7ec4\u7ec7\u7684\u8bed\u7528\u6846\u67b6\u548c\u624b\u52bf\u6807\u6ce8\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u5df4\u897f\u7535\u89c6\u5267\u768410\u96c6\u5185\u5bb9\u3002", "result": "\u7814\u7a76\u8bc1\u5b9e\u9762\u5bf9\u9762\u4ea4\u8c08\u4e2d\u4eba\u4eec\u4f7f\u7528\u624b\u52bf\u4f5c\u4e3a\u4f20\u9012\u3001\u83b7\u53d6\u548c\u4fdd\u6301\u5bf9\u8bdd\u8f6e\u6b21\u7684\u5de5\u5177\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u4e9b\u5148\u524d\u672a\u8bb0\u5f55\u7684\u624b\u52bf\u53d8\u4f53\u3002\u6570\u636e\u8868\u660e\u8bed\u7528\u6846\u67b6\u6807\u6ce8\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u548c\u8bed\u8a00\u3002", "conclusion": "\u624b\u52bf\u7684\u4f7f\u7528\u6e90\u4e8e\u8bed\u7528\u6846\u67b6\u7684\u6982\u5ff5\u5316\uff0c\u6d89\u53ca\u5fc3\u7406\u7a7a\u95f4\u3001\u6982\u5ff5\u6574\u5408\u548c\u6982\u5ff5\u9690\u55bb\u3002\u8bed\u7528\u6846\u67b6\u7684\u6807\u6ce8\u4e3a\u7406\u89e3\u4eba\u7c7b\u591a\u6a21\u6001\u4ea4\u6d41\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.09962", "pdf": "https://arxiv.org/pdf/2509.09962", "abs": "https://arxiv.org/abs/2509.09962", "authors": ["Anne Marthe Sophie Ngo Bibinbe", "Chiron Bang", "Patrick Gagnon", "Jamie Ahloy-Dallaire", "Eric R. Paquet"], "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock", "categories": ["cs.CV"], "comment": "13 pages, 7 figures, 1 table, accepted at CVPR animal workshop 2024,\n  submitted to IJCV", "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u8eab\u4efd\u4fe1\u606f\u548c\u8ddf\u8e2a\u6280\u672f\uff0c\u89e3\u51b3\u957f\u671f\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u8eab\u4efd\u5207\u6362\u95ee\u9898\uff0c\u5728\u7272\u755c\u8ddf\u8e2a\u548c\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u89c6\u9891\u4e2d\u56e0\u8eab\u4efd\u5207\u6362\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u96be\u4ee5\u6ee1\u8db3\u957f\u671f\u8ddf\u8e2a\u9700\u6c42\u3002\u4f46\u5728\u755c\u7267\u4e1a\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5582\u98df\u5668\u7b49\u6765\u6e90\u83b7\u5f97\u52a8\u7269\u7684\u96f6\u661f\u8eab\u4efd\u4fe1\u606f", "method": "\u4f7f\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u6846\u67b6\uff0c\u5c06\u4e0d\u786e\u5b9a\u7684\u8eab\u4efd\u4fe1\u606f\u4e0e\u8ddf\u8e2a\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u5904\u7406\u957f\u671f\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u8eab\u4efd\u4e0d\u786e\u5b9a\u6027", "result": "\u572810\u5206\u949f\u732a\u53ea\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\uff0c\u5373\u4f7f\u53ea\u670921\u4e2a\u5582\u98df\u7ad9\u8eab\u4efd\u8bc6\u522b\uff0c\u4e5f\u63d0\u5347\u4e86ByteTrack\u7684F1\u5206\u6570\uff1b\u5728MOT17\u548cMOT20\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5bf9\u8eab\u4efd\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684HMM\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u96f6\u661f\u8eab\u4efd\u4fe1\u606f\u6539\u5584\u957f\u671f\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u5982\u7272\u755c\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09852", "pdf": "https://arxiv.org/pdf/2509.09852", "abs": "https://arxiv.org/abs/2509.09852", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "categories": ["cs.CL"], "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u9898\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u591a\u6587\u6863\u6458\u8981\u4e2d\u7684\u5185\u5bb9\u9009\u62e9\uff0c\u901a\u8fc7\u5728GRPO\u6846\u67b6\u4e2d\u5f15\u5165\u4e3b\u9898\u5956\u52b1\u6765\u63d0\u5347\u6458\u8981\u4e0e\u6e90\u6587\u6863\u7684\u4e3b\u9898\u5bf9\u9f50\u5ea6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6587\u6863\u6458\u8981\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u597d\u5730\u6574\u5408\u591a\u4e2a\u6765\u6e90\u7684\u4fe1\u606f\u540c\u65f6\u4fdd\u6301\u8fde\u8d2f\u6027\u548c\u4e3b\u9898\u76f8\u5173\u6027", "method": "\u9996\u5148\u901a\u8fc7\u663e\u5f0f\u63d0\u793a\u6a21\u578b\u4f7f\u7528\u4e3b\u9898\u6807\u7b7e\u6765\u589e\u5f3a\u6458\u8981\u4fe1\u606f\u91cf\uff0c\u7136\u540e\u5728Group Relative Policy Optimization\u6846\u67b6\u4e2d\u63d0\u51fa\u65b0\u9896\u7684\u4e3b\u9898\u5956\u52b1\u673a\u5236\u6765\u8861\u91cf\u751f\u6210\u6458\u8981\u4e0e\u6e90\u6587\u6863\u7684\u4e3b\u9898\u5bf9\u9f50\u5ea6", "result": "\u5728Multi-News\u548cMulti-XScience\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5 consistently outperforms strong baselines", "conclusion": "\u5229\u7528\u4e3b\u9898\u7ebf\u7d22\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u662f\u6709\u6548\u7684\uff0c\u4e3b\u9898\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6587\u6863\u6458\u8981\u7684\u8d28\u91cf"}}
{"id": "2509.09971", "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u63a2\u8ba8\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u4f20\u7edf\u5e27\u76f8\u673a\u7684\u878d\u5408\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u8fd9\u79cd\u878d\u5408\u5728\u89c6\u9891\u6062\u590d\u548c3D\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u7a7a\u589e\u5f3a\u65b9\u9762\u7684\u8d21\u732e\uff0c\u5e76\u6574\u7406\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u65b0\u5174\u7684\u751f\u7269\u542f\u53d1\u5f0f\u4f20\u611f\u5668\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u9ad8\u6355\u83b7\u7387\u7b49\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e0e\u4f20\u7edf\u5e27\u76f8\u673a\u878d\u5408\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u4e8b\u4ef6\u6d41\u4e0e\u5e27\u6d41\u878d\u5408\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u5206\u6790\u6df1\u5ea6\u5b66\u4e60\u8d21\u732e\uff1a\u65f6\u95f4\u589e\u5f3a\uff08\u5e27\u63d2\u503c\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\uff09\u548c\u7a7a\u95f4\u589e\u5f3a\uff08\u8d85\u5206\u8fa8\u7387\u3001\u4f4e\u5149\u589e\u5f3a\u3001HDR\u589e\u5f3a\u3001\u4f2a\u5f71\u51cf\u5c11\uff09\u3002\u540c\u65f6\u63a2\u8ba83D\u91cd\u5efa\u9886\u57df\u7684\u878d\u5408\u8fdb\u5c55\u3002", "result": "\u8bba\u6587\u5168\u9762\u56de\u987e\u4e86\u4e8b\u4ef6\u76f8\u673a\u878d\u5408\u6280\u672f\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\uff0c\u5c55\u793a\u4e86\u8be5\u878d\u5408\u5728\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u7684\u663e\u8457\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u3002\u8fd8\u6574\u7406\u4e86\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u8d44\u6e90\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u4e0e\u4f20\u7edf\u76f8\u673a\u7684\u878d\u5408\u6280\u672f\u4e3a\u89c6\u89c9\u5a92\u4f53\u6062\u590d\u548c\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ea7\u89c6\u89c9\u5e94\u7528\u65b9\u9762\u3002"}}
{"id": "2509.09871", "pdf": "https://arxiv.org/pdf/2509.09871", "abs": "https://arxiv.org/abs/2509.09871", "authors": ["Basti\u00e1n Gonz\u00e1lez-Bustamante", "Nando Verelst", "Carla Cisternas"], "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u8c03\u67e5\u56de\u590d\u7684\u53ef\u9760\u6027\uff0c\u4e0e\u667a\u5229\u6982\u7387\u62bd\u6837\u8c03\u67e5\u7684\u771f\u5b9e\u4eba\u7c7b\u56de\u590d\u5bf9\u6bd4\uff0c\u53d1\u73b0LLM\u5728\u4fe1\u4efb\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u9879\u76ee\u7ea7\u5f02\u8d28\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u3002", "motivation": "LLMs\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u5177\u6709\u521b\u65b0\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u751f\u6210\u7684\u5408\u6210\u56de\u590d\u662f\u5426\u80fd\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u56de\u7b54\u5206\u5e03\uff0c\u907f\u514d\u590d\u5236\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u548c\u504f\u89c1\u3002", "method": "\u4f7f\u7528128\u4e2a\u63d0\u793a-\u6a21\u578b-\u95ee\u9898\u4e09\u5143\u7ec4\u751f\u6210189,696\u4e2a\u5408\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u667a\u5229\u6982\u7387\u62bd\u6837\u8c03\u67e5\u7684\u771f\u5b9e\u56de\u590d\u5bf9\u6bd4\uff0c\u901a\u8fc7\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u5143\u5206\u6790\uff0c\u6d4b\u8bd5\u5173\u952e\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u7ef4\u5ea6\u4e0a\u7684\u504f\u5dee\u3002", "result": "1) \u5408\u6210\u56de\u590d\u5728\u4fe1\u4efb\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02(F1\u5206\u6570\u548c\u51c6\u786e\u7387>0.90)\uff1b2) GPT-4o\u3001GPT-4o-mini\u548cLlama 4 Maverick\u8868\u73b0\u76f8\u5f53\uff1b3) 45-59\u5c81\u53d7\u8bbf\u8005\u7684\u5408\u6210-\u4eba\u7c7b\u5bf9\u9f50\u5ea6\u6700\u9ad8\uff1b\u603b\u4f53\u8fd1\u4f3c\u6982\u7387\u6837\u672c\u56de\u590d\u4f46\u5b58\u5728\u9879\u76ee\u7ea7\u5f02\u8d28\u6027\u3002", "conclusion": "LLM\u5408\u6210\u6837\u672c\u80fd\u8fd1\u4f3c\u6982\u7387\u6837\u672c\u56de\u590d\uff0c\u4f46\u8981\u5b8c\u5168\u6355\u6349\u516c\u4f17\u610f\u89c1\u7684\u7ec6\u5fae\u5dee\u522b\u4ecd\u9700\u8c28\u614e\u6821\u51c6\u548c\u989d\u5916\u5206\u5e03\u6d4b\u8bd5\uff0c\u4ee5\u786e\u4fdd\u7b97\u6cd5\u4fdd\u771f\u5ea6\u548c\u51cf\u5c11\u8bef\u5dee\u3002"}}
{"id": "2509.09977", "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "categories": ["cs.CV"], "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "ISTASTrack\u662f\u9996\u4e2a\u57fa\u4e8eTransformer\u7684ANN-SNN\u6df7\u5408\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7ISTA\u9002\u914d\u5668\u5b9e\u73b0RGB\u548c\u4e8b\u4ef6\u6570\u636e\u7684\u6709\u6548\u878d\u5408\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u5e76\u4fdd\u6301\u9ad8\u80fd\u6548\u3002", "motivation": "\u73b0\u6709ANN\u7f51\u7edc\u96be\u4ee5\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6d41\u7684\u7a00\u758f\u548c\u5f02\u6b65\u7279\u6027\uff0c\u800cANN-SNN\u6df7\u5408\u67b6\u6784\u5728RGB-Event\u611f\u77e5\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u8de8\u5f02\u6784\u8303\u5f0f\u7684\u7279\u5f81\u878d\u5408\u4ecd\u5177\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1a\u89c6\u89c9Transformer\u5904\u7406RGB\u8f93\u5165\uff0c\u8109\u51b2Transformer\u5904\u7406\u4e8b\u4ef6\u6d41\uff1b\u8bbe\u8ba1\u57fa\u4e8eISTA\u7b97\u6cd5\u7684\u9002\u914d\u5668\u8fdb\u884c\u53cc\u5411\u7279\u5f81\u4ea4\u4e92\uff1b\u52a0\u5165\u65f6\u5e8f\u4e0b\u91c7\u6837\u6ce8\u610f\u529b\u6a21\u5757\u5bf9\u9f50\u7279\u5f81\u3002", "result": "\u5728FE240hz\u3001VisEvent\u3001COESOT\u548cFELT\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u80fd\u6548\u3002", "conclusion": "ISTASTrack\u8bc1\u660e\u4e86ANN-SNN\u6df7\u5408\u8bbe\u8ba1\u5728\u9c81\u68d2\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u8de8\u6a21\u6001\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09969", "pdf": "https://arxiv.org/pdf/2509.09969", "abs": "https://arxiv.org/abs/2509.09969", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6cd5\u5f8b\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e8616\u4e2a\u6cd5\u5f8bLLM\u7cfb\u5217\u300147\u4e2aLLM\u6846\u67b6\u300115\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c29\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd1\u5e74\u6765\u663e\u8457\u63a8\u52a8\u6cd5\u5f8b\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u548c\u603b\u7ed3LLM\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u7cfb\u7edf\u4ecb\u7ecd\u5e76\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7efc\u8ff016\u4e2a\u6cd5\u5f8bLLM\u7cfb\u5217\u548c47\u4e2a\u57fa\u4e8eLLM\u7684\u6cd5\u5f8b\u4efb\u52a1\u6846\u67b6\uff0c\u6536\u96c615\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c29\u4e2a\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u4e0d\u540c\u7684\u6cd5\u5f8b\u80fd\u529b\uff0c\u5e76\u5bf9\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u8fdb\u884c\u5206\u6790\u8ba8\u8bba\u3002", "result": "\u63d0\u4f9b\u4e86\u6cd5\u5f8bLLM\u9886\u57df\u7684\u7cfb\u7edf\u6027\u8d44\u6e90\u6c47\u603b\uff0c\u5305\u62ec\u6a21\u578b\u3001\u6846\u67b6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\u8d44\u6e90\u3002", "conclusion": "\u672c\u6587\u4e3aLLM\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u4ecb\u7ecd\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u6311\u6218\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u65e8\u5728\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002\u76f8\u5173\u8d44\u6e90\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2509.09988", "pdf": "https://arxiv.org/pdf/2509.09988", "abs": "https://arxiv.org/abs/2509.09988", "authors": ["Yusuke Takagi", "Shunya Nagashima", "Komei Sugiura"], "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction", "categories": ["cs.CV", "astro-ph.SR"], "comment": "Accepted for presentation at ICONIP2025", "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cFLARE\u635f\u5931\u51fd\u6570\u7684\u592a\u9633\u8000\u6591\u9884\u6d4b\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u572811\u5e74\u592a\u9633\u6d3b\u52a8\u5468\u671f\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u5f53\u524d\u592a\u9633\u8000\u6591\u9884\u6d4b\u6027\u80fd\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8000\u6591\u7c7b\u522b\u95f4\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u6a21\u578b\u6765\u4fdd\u62a4\u5173\u952e\u57fa\u7840\u8bbe\u65bd", "method": "\u4f7f\u7528\u591a\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6784\u5efa\u9884\u6d4b\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u9891\u7387\u548c\u5c40\u90e8\u8fb9\u754c\u611f\u77e5\u53ef\u9760\u6027\u635f\u5931\u51fd\u6570\uff08FLARE\u635f\u5931\uff09\u6765\u6539\u5584\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u9760\u6027", "result": "\u5728\u8986\u76d6\u5b8c\u657411\u5e74\u592a\u9633\u6d3b\u52a8\u5468\u671f\u7684\u591a\u6ce2\u957f\u592a\u9633\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728Gandin-Murphy-Gerrity\u5206\u6570\u548c\u771f\u5b9e\u6280\u80fd\u7edf\u8ba1\u91cf\u4e24\u4e2a\u6807\u51c6\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7ed3\u5408FLARE\u635f\u5931\u51fd\u6570\u80fd\u591f\u6709\u6548\u63d0\u5347\u592a\u9633\u8000\u6591\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272"}}
{"id": "2509.09990", "pdf": "https://arxiv.org/pdf/2509.09990", "abs": "https://arxiv.org/abs/2509.09990", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "categories": ["cs.CL"], "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\uff08\u85cf\u8bed\u3001\u7ef4\u543e\u5c14\u8bed\u3001\u8499\u53e4\u8bed\uff09\u7684\u65b0\u95fb\u6807\u9898\u751f\u6210\u6570\u636e\u96c6CMHG\uff0c\u5305\u542b20\u4e07\u6761\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u7531\u6bcd\u8bed\u8005\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u4f5c\u4e3a\u57fa\u51c6\u3002", "motivation": "\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u7531\u4e8e\u4e66\u5199\u7cfb\u7edf\u4e0e\u56fd\u9645\u6807\u51c6\u4e0d\u540c\uff0c\u5bfc\u81f4\u76f8\u5173\u8bed\u6599\u5e93\u4e25\u91cd\u7f3a\u4e4f\uff0c\u7279\u522b\u662f\u5728\u76d1\u7763\u4efb\u52a1\u5982\u6807\u9898\u751f\u6210\u65b9\u9762\u5b58\u5728\u660e\u663e\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86CMHG\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07\u6761\u85cf\u8bed\u548c5\u4e07\u6761\u7ef4\u543e\u5c14\u8bed\u30015\u4e07\u6761\u8499\u53e4\u8bed\u6570\u636e\uff0c\u4e13\u95e8\u7528\u4e8e\u6807\u9898\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u7531\u6bcd\u8bed\u8005\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b20\u4e07\u6761\u6570\u636e\u7684\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6807\u9898\u751f\u6210\u6570\u636e\u96c6\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u6210\u4e3a\u63a8\u52a8\u4e2d\u56fd\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6807\u9898\u751f\u6210\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u6709\u52a9\u4e8e\u76f8\u5173\u57fa\u51c6\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.10005", "pdf": "https://arxiv.org/pdf/2509.10005", "abs": "https://arxiv.org/abs/2509.10005", "authors": ["Xiaodong Guo", "Tong Liu", "Yike Li", "Zi'ang Lin", "Zhihong Deng"], "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion", "categories": ["cs.CV"], "comment": null, "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.", "AI": {"tldr": "TUNI\u662f\u4e00\u4e2aRGB-\u70ed\u6210\u50cf\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7f16\u7801\u5668\u540c\u65f6\u8fdb\u884c\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RGB-T\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4e2d\u70ed\u7279\u5f81\u63d0\u53d6\u6709\u9650\u3001\u8de8\u6a21\u6001\u878d\u5408\u4e0d\u7406\u60f3\u4ee5\u53ca\u7f16\u7801\u5668\u5197\u4f59\u5bfc\u81f4\u7684\u5b9e\u65f6\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faTUNI\u6a21\u578b\uff0c\u4f7f\u7528\u5806\u53e0\u5757\u6784\u5efaRGB-T\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5927\u89c4\u6a21RGB\u548c\u4f2a\u70ed\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u6574\u5408\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\uff1b\u91c7\u7528\u7cbe\u7b80\u7684\u70ed\u5206\u652f\u67b6\u6784\uff1b\u5f15\u5165RGB-T\u5c40\u90e8\u6a21\u5757\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9009\u62e9\u6027\u5730\u5f3a\u8c03\u8de8\u6a21\u6001\u7684\u663e\u8457\u4e00\u81f4\u548c\u4e0d\u540c\u5c40\u90e8\u7279\u5f81\u3002", "result": "\u5728FMB\u3001PST900\u548cCART\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u53c2\u6570\u66f4\u5c11\u3001\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff1b\u5728Jetson Orin NX\u4e0a\u5b9e\u73b027 FPS\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "TUNI\u901a\u8fc7\u7edf\u4e00\u7684\u7f16\u7801\u5668\u67b6\u6784\u548c\u5c40\u90e8\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86RGB-T\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2509.10004", "pdf": "https://arxiv.org/pdf/2509.10004", "abs": "https://arxiv.org/abs/2509.10004", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.", "AI": {"tldr": "IRIS\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528LLM\u5185\u90e8\u8868\u793a\u6765\u8bc6\u522b\u751f\u6210\u5185\u5bb9\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4e0e\u4e8b\u5b9e\u6b63\u786e\u6027\u65e0\u5173\u7684\u4ee3\u7406\u4fe1\u53f7\uff0c\u5bfc\u81f4\u68c0\u6d4b\u504f\u5411\u8868\u9762\u7279\u5f81\uff0c\u9650\u5236\u4e86\u8de8\u6570\u636e\u96c6\u548c\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u901a\u8fc7\u63d0\u793aLLM\u9a8c\u8bc1\u9648\u8ff0\u771f\u5b9e\u6027\uff0c\u83b7\u53d6\u4e0a\u4e0b\u6587\u5d4c\u5165\u4f5c\u4e3a\u7279\u5f81\uff0c\u5e76\u5c06\u54cd\u5e94\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u771f\u5b9e\u6027\u8f6f\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3", "result": "IRIS\u5728\u5b9e\u9a8c\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u5c11\u4e5f\u80fd\u826f\u597d\u5de5\u4f5c\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b", "conclusion": "IRIS\u6846\u67b6\u901a\u8fc7\u5229\u7528LLM\u5185\u90e8\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u5e7b\u89c9\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u65e0\u5173\u4ee3\u7406\u4fe1\u53f7\u7684\u95ee\u9898"}}
{"id": "2509.10006", "pdf": "https://arxiv.org/pdf/2509.10006", "abs": "https://arxiv.org/abs/2509.10006", "authors": ["Masaki Akiba", "Shumpei Takezaki", "Daichi Haraguchi", "Seiichi Uchida"], "title": "Few-Part-Shot Font Generation", "categories": ["cs.CV"], "comment": "ICDAR 2025 Workshop on Machine Learning", "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u8bbe\u8ba1\u5143\u7d20\u7684\u5c11\u90e8\u5206\u5b57\u5f62\u751f\u6210\u6a21\u578b\uff0c\u53ea\u9700\u8f93\u5165\u90e8\u5206\u5f62\u72b6\u800c\u975e\u5b8c\u6574\u5b57\u7b26\u5373\u53ef\u751f\u6210\u6574\u4e2a\u5b57\u4f53", "motivation": "\u4f20\u7edf\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u9700\u8981\u5b8c\u6574\u5b57\u7b26\u5f62\u72b6\uff0c\u800c\u672c\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u90e8\u5206\u8bbe\u8ba1\u5143\u7d20\u63d0\u9ad8\u5b57\u4f53\u521b\u5efa\u6548\u7387\uff0c\u5e76\u63a2\u7d22\u5c40\u90e8\u8bbe\u8ba1\u7ec6\u8282\u5bf9\u6574\u4f53\u5b57\u7b26\u7ed3\u6784\u7684\u5f71\u54cd", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u5f62\u72b6\u8f93\u5165\u7684\u5b57\u4f53\u751f\u6210\u6a21\u578b\uff0c\u5229\u7528\u90e8\u5206\u8bbe\u8ba1\u5143\u7d20\u6765\u63a8\u65ad\u548c\u751f\u6210\u5b8c\u6574\u7684\u5b57\u7b26\u96c6", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5b57\u4f53\u521b\u5efa\u7684\u6548\u7387\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u5c40\u90e8\u8bbe\u8ba1\u7ec6\u8282\u5982\u4f55\u5f71\u54cd\u6574\u4f53\u5b57\u7b26\u7ed3\u6784\u7684\u89c1\u89e3", "conclusion": "\u63d0\u51fa\u7684\u5c11\u90e8\u5206\u5b57\u5f62\u751f\u6210\u6a21\u578b\u4e3a\u5b57\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6df1\u5316\u4e86\u5bf9\u5b57\u4f53\u8bbe\u8ba1\u4e2d\u5c40\u90e8\u4e0e\u6574\u4f53\u5173\u7cfb\u7684\u7406\u89e3"}}
{"id": "2509.10010", "pdf": "https://arxiv.org/pdf/2509.10010", "abs": "https://arxiv.org/abs/2509.10010", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian M\u00f6ller"], "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u4f7f\u7528MultiWOZ 2.1\u6570\u636e\u96c6\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u6bd4\u8f83\u4e86LLama2-7B\u3001Mistral-7B\u548cYi-6B\u7684\u6027\u80fd\uff0c\u5e76\u4e0eBERT\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u7814\u7a76\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5904\u7406\u591a\u6807\u7b7e\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u5b9e\u7528\u6846\u67b6\u3002", "method": "\u4f7f\u7528MultiWOZ 2.1\u6570\u636e\u96c6\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff08\u63d0\u793a\u4e2d\u5305\u542b20\u4e2a\u793a\u4f8b\uff09\u6d4b\u8bd5\u4e09\u4e2a\u5f00\u6e90LLM\uff08LLama2-7B\u3001Mistral-7B\u3001Yi-6B\uff09\uff0c\u5e76\u4e0eBERT\u76d1\u7763\u5206\u7c7b\u5668\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u591a\u79cdF1\u5206\u6570\u3002", "result": "Mistral-7B\u572814\u4e2a\u610f\u56fe\u7c7b\u522b\u4e2d\u768411\u4e2a\u4e0aF\u5206\u6570\u8868\u73b0\u6700\u4f73\uff0c\u52a0\u6743\u5e73\u5747F\u5206\u6570\u4e3a0.50\uff0c\u5177\u6709\u8f83\u4f4e\u7684Hamming Loss\u548c\u8f83\u9ad8\u7684Jaccard\u76f8\u4f3c\u5ea6\u3002\u4f46BERT\u76d1\u7763\u5206\u7c7b\u5668\u7684\u6027\u80fd\u4ecd\u4f18\u4e8e\u6700\u4f73\u5c11\u6837\u672c\u751f\u6210\u5f0fLLM\u3002", "conclusion": "\u867d\u7136Mistral-7B\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u76d1\u7763\u5b66\u4e60\u7684BERT\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4ecd\u5177\u4f18\u52bf\u3002\u7814\u7a76\u4e3a\u5c0f\u578b\u5f00\u6e90LLM\u5728\u590d\u6742\u591a\u610f\u56fe\u5bf9\u8bdd\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2509.10021", "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas K\u00fchne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5fae\u578b\u548c\u7eb3\u7c73\u65e0\u4eba\u673a\u4f18\u5316\u7684\u9ad8\u6548\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1(VIO)\u6d41\u6c34\u7ebf\uff0c\u5728\u8d85\u4f4e\u529f\u8017RISC-V SoC\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6d41\u6c34\u7ebfRMSE\u5e73\u5747\u964d\u4f4e3.65\u500d", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9ad8\u7cbe\u5ea6VIO\u6d41\u6c34\u7ebf\u8ba1\u7b97\u9700\u6c42\u5927\u3001\u65e0\u6cd5\u5728\u5fae\u578b\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u95ee\u9898\uff0c\u586b\u8865\u9ad8\u6027\u80fd\u7cfb\u7edf\u4e0e\u5fae\u63a7\u5236\u5668\u9002\u7528\u5b9e\u73b0\u4e4b\u95f4\u7684\u6280\u672f\u7a7a\u767d", "method": "\u91c7\u7528\u6700\u5148\u8fdb\u7684\u7279\u5f81\u68c0\u6d4b\u548c\u8ddf\u8e2a\u65b9\u6cd5(SuperPoint\u3001PX4FLOW\u3001ORB)\uff0c\u8fdb\u884c\u4f18\u5316\u548c\u91cf\u5316\u4ee5\u9002\u5e94RISC-V\u8d85\u4f4e\u529f\u8017\u5e76\u884cSoC\uff0c\u5e76\u5229\u7528\u521a\u4f53\u8fd0\u52a8\u6a21\u578b\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee", "result": "\u5728GAP9\u4f4e\u529f\u8017SoC\u4e0a\uff0c\u4f7f\u7528ORB\u7279\u5f81\u8ddf\u8e2a\u5668\u65f6RMSE\u5e73\u5747\u964d\u4f4e3.65\u500d\uff1bPX4FLOW\u5728\u79fb\u52a8\u901f\u5ea6\u4f4e\u4e8e24\u50cf\u7d20/\u5e27\u65f6\u4ee5\u66f4\u4f4e\u8fd0\u884c\u65f6\u95f4\u5b9e\u73b0\u4e0eORB\u76f8\u5f53\u7684\u8ddf\u8e2a\u7cbe\u5ea6", "conclusion": "\u8be5\u4f18\u5316\u6d41\u6c34\u7ebf\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u8d85\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6VIO\uff0c\u4e3a\u5fae\u578b\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10035", "pdf": "https://arxiv.org/pdf/2509.10035", "abs": "https://arxiv.org/abs/2509.10035", "authors": ["Laurin Plank", "Armin Zlomuzica"], "title": "Linguistic trajectories of bipolar disorder on social media", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u786e\u5b9a\u53cc\u76f8\u969c\u788d\u8bca\u65ad\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u8bca\u65ad\u524d\u540e\u957f\u8fbe24\u5e74\u7684\u8bed\u8a00\u8f68\u8ff9\u53d8\u5316\uff0c\u53d1\u73b0\u53cc\u76f8\u969c\u788d\u4f34\u968f\u5e7f\u6cdb\u7684\u8bed\u8a00\u6539\u53d8\u548c\u5468\u671f\u6027\u60c5\u7eea\u6ce2\u52a8\u3002", "motivation": "\u4e34\u5e8a\u8bc4\u4f30\u53cc\u76f8\u969c\u788d\u7684\u89c4\u6a21\u6709\u9650\uff0c\u800c\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7eb5\u5411\u8303\u56f4\u7684\u4f18\u52bf\uff0c\u53ef\u4ee5\u7528\u4e8e\u5927\u89c4\u6a21\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u3002", "method": "\u5f15\u5165\u786e\u5b9a\u7528\u6237\u8bca\u65ad\u65f6\u95f4\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u53cc\u76f8\u969c\u788d\u60a3\u8005\u4ece\u8bca\u65ad\u524d3\u5e74\u5230\u8bca\u65ad\u540e21\u5e74\u7684\u8bed\u8a00\u8f68\u8ff9\uff0c\u5e76\u4e0e\u5355\u76f8\u6291\u90c1\u75c7\u60a3\u8005\u548c\u5065\u5eb7\u5bf9\u7167\u7ec4\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0\u53cc\u76f8\u969c\u788d\u8bca\u65ad\u4f34\u968f\u53cd\u6620\u60c5\u7eea\u969c\u788d\u3001\u7cbe\u795e\u5171\u75c5\u3001\u7269\u8d28\u6ee5\u7528\u3001\u4f4f\u9662\u6cbb\u7597\u3001\u533b\u5b66\u5171\u75c5\u3001\u5f02\u5e38\u601d\u7ef4\u5185\u5bb9\u548c\u601d\u7ef4\u7d0a\u4e71\u7684\u5e7f\u6cdb\u8bed\u8a00\u6539\u53d8\uff0c\u5e76\u89c2\u5bdf\u5230\u8bca\u65ad\u540e\u4e8c\u5341\u5e74\u53cd\u590d\u51fa\u73b0\u7684\u60c5\u7eea\u76f8\u5173\u8bed\u8a00\u53d8\u5316\uff0c\u5177\u6709\u660e\u663e\u768412\u4e2a\u6708\u5468\u671f\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u53cc\u76f8\u969c\u788d\u6025\u6027\u548c\u6162\u6027\u671f\u8bed\u8a00\u6539\u53d8\u7684\u8bc1\u636e\uff0c\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u8fdb\u884c\u53ef\u6269\u5c55\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u7684\u6700\u65b0\u52aa\u529b\u3002"}}
{"id": "2509.10024", "pdf": "https://arxiv.org/pdf/2509.10024", "abs": "https://arxiv.org/abs/2509.10024", "authors": ["Danling Cao"], "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images", "categories": ["cs.CV"], "comment": "This work was completed during the author's MPhil studies at the\n  University of Manchester", "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6b21\u5316\u591a\u7ea7\u6ce8\u610f\u529b\u7f51\u7edc(MLANet)\uff0c\u4ece\u5355\u5f20\u91ce\u5916\u56fe\u50cf\u91cd\u5efa3D\u4eba\u8138\u6a21\u578b\uff0c\u9884\u6d4b\u51e0\u4f55\u3001\u7eb9\u7406\u3001\u59ff\u6001\u548c\u5149\u7167\u53c2\u6570", "motivation": "\u89e3\u51b3\u4ece2D\u91ce\u5916\u56fe\u50cf\u6062\u590d3D\u4eba\u8138\u6a21\u578b\u7684\u6311\u6218\uff0c\u5305\u62ec\u7f3a\u4e4f\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u96c6\u548c\u590d\u6742\u771f\u5b9e\u73af\u5883\u7684\u56f0\u96be", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u5c42\u6b21\u5316\u4e3b\u5e72\u7f51\u7edc\uff0c\u5f15\u5165\u591a\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u7ed3\u54083DMM\u53c2\u6570\u548c\u53ef\u5fae\u5206\u6e32\u67d3\u5668\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3", "result": "\u5728AFLW2000-3D\u548cMICC Florence\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5305\u62ec\u5bf9\u6bd4\u548c\u6d88\u878d\u7814\u7a76\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u65b9\u6cd5\u6709\u6548", "conclusion": "\u63d0\u51fa\u7684MLANet\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4ece\u5355\u5f20\u91ce\u5916\u56fe\u50cf\u91cd\u5efa\u9ad8\u8d28\u91cf\u76843D\u4eba\u8138\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218"}}
{"id": "2509.10040", "pdf": "https://arxiv.org/pdf/2509.10040", "abs": "https://arxiv.org/abs/2509.10040", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "categories": ["cs.CL"], "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BAREC 2025\u963f\u62c9\u4f2f\u8bed\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u7684\u83b7\u80dc\u7cfb\u7edf\uff0c\u5728\u516d\u4e2a\u8d5b\u9053\u4e2d\u5747\u83b7\u5f97\u7b2c\u4e00\u540d\u3002\u901a\u8fc7\u56db\u79cdTransformer\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u96c6\u6210\u3001\u9488\u5bf9\u6027\u6570\u636e\u589e\u5f3a\u548c\u540e\u5904\u7406\u6280\u672f\uff0c\u5728\u53e5\u5b50\u548c\u6587\u6863\u7ea7\u522b\u5206\u522b\u8fbe\u523087.5%\u548c87.4%\u7684QWK\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u8bc4\u4f30\u4e2d\u7684\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u53ef\u8bfb\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u4e92\u8865\u7684Transformer\u6a21\u578b\uff08AraBERTv2\u3001AraELECTRA\u3001MARBERT\u3001CAMeLBERT\uff09\u8fdb\u884c\u96c6\u6210\uff0c\u6bcf\u79cd\u6a21\u578b\u91c7\u7528\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\uff1b\u5e94\u7528\u52a0\u6743\u8bad\u7ec3\u3001\u9ad8\u7ea7\u9884\u5904\u7406\u3001SAMER\u8bed\u6599\u5e93\u91cd\u65b0\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7Gemini 2.5 Flash\u751f\u6210\u7ea610,000\u4e2a\u7a00\u6709\u7ea7\u522b\u6837\u672c\u7684\u5408\u6210\u6570\u636e\uff1b\u91c7\u7528\u9488\u5bf9\u6027\u540e\u5904\u7406\u6b65\u9aa4\u6821\u6b63\u9884\u6d4b\u5206\u5e03\u504f\u5dee\u3002", "result": "\u5728\u516d\u4e2a\u8d5b\u9053\u4e2d\u5168\u90e8\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u53e5\u5b50\u7ea7\u522bQWK\u8fbe\u523087.5%\uff0c\u6587\u6863\u7ea7\u522bQWK\u8fbe\u523087.4%\uff0c\u540e\u5904\u7406\u5e26\u67656.3%\u7684QWK\u589e\u76ca\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570\u7684\u591a\u6837\u6027\u3001\u7f6e\u4fe1\u5ea6\u4fe1\u606f\u878d\u5408\u4ee5\u53ca\u667a\u80fd\u6570\u636e\u589e\u5f3a\u5728\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10026", "pdf": "https://arxiv.org/pdf/2509.10026", "abs": "https://arxiv.org/abs/2509.10026", "authors": ["Jing Huang", "Zhiya Tan", "Shutao Gong", "Fanwei Zeng", "Jianshu Li"], "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA", "categories": ["cs.CV"], "comment": "12 Pages, 12 Figures, 2 Tables", "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}", "AI": {"tldr": "LaV-CoT\u662f\u4e00\u4e2a\u8bed\u8a00\u611f\u77e5\u7684\u89c6\u89c9\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\u548c\u591a\u65b9\u9762\u5956\u52b1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u601d\u7ef4\u94fe\uff0c\u5bf9\u591a\u8bed\u8a00\u591a\u6a21\u6001\u63a8\u7406\u652f\u6301\u6709\u9650\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff08\u6587\u672c\u6458\u8981+\u8fb9\u754c\u6846\u3001\u8bed\u8a00\u8bc6\u522b\u3001\u7a7a\u95f4\u5bf9\u8c61\u7ea7\u63cf\u8ff0\u3001\u9010\u6b65\u903b\u8f91\u63a8\u7406\uff09\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u6570\u636e\u6807\u6ce8\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u8bed\u8a00\u611f\u77e5\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728MMMB\u3001Multilingual MMBench\u548cMTVQA\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4\u540c\u89c4\u6a21\u5f00\u6e90\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u5347\u7ea69.5%\uff0c\u751a\u81f3\u8d85\u8d8a\u89c4\u6a21\u59272\u500d\u7684\u6a21\u578b\u7ea62.6%\uff0c\u4f18\u4e8eGPT-4o-0513\u548cGemini-2.5-flash\u7b49\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "LaV-CoT\u901a\u8fc7\u8bed\u8a00\u611f\u77e5\u7684\u89c6\u89c9\u601d\u7ef4\u94fe\u6846\u67b6\u548c\u591a\u65b9\u9762\u5956\u52b1\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u5de5\u4e1a\u90e8\u7f72\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2509.10078", "pdf": "https://arxiv.org/pdf/2509.10078", "abs": "https://arxiv.org/abs/2509.10078", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4f20\u7edf\u5fc3\u7406\u6d4b\u91cf\u95ee\u5377\u4e0e\u751f\u6001\u6548\u5ea6\u95ee\u5377\u5728\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u4eba\u683c\u7279\u8d28\u65f6\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4f20\u7edf\u95ee\u5377\u5b58\u5728\u6d4b\u91cf\u4e0d\u7a33\u5b9a\u3001\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u7b49\u95ee\u9898\uff0c\u5efa\u8bae\u907f\u514d\u4f7f\u7528\u4f20\u7edf\u5fc3\u7406\u95ee\u5377\u8bc4\u4f30LLMs\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u4f20\u7edf\u4eba\u7c7b\u5fc3\u7406\u95ee\u5377\uff08\u5982BFI\u3001PVQ\uff09\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u683c\u7279\u8d28\u548c\u4ef7\u503c\u89c2\uff0c\u4f46\u8fd9\u4e9b\u95ee\u5377\u7f3a\u4e4f\u751f\u6001\u6548\u5ea6\uff0c\u4e0d\u80fd\u53cd\u6620LLMs\u5728\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u6587\u672c\u751f\u6210\u7279\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e24\u79cd\u95ee\u5377\u7684\u5dee\u5f02\u3002", "method": "\u5bf9\u4f20\u7edf\u5fc3\u7406\u95ee\u5377\u548c\u751f\u6001\u6548\u5ea6\u95ee\u5377\u8fdb\u884c\u5168\u9762\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u6d4b\u91cfLLMs\u4eba\u683c\u7279\u8d28\u65f6\u7684\u8868\u73b0\u5dee\u5f02\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u95ee\u5377\uff1a1\uff09\u4ea7\u751f\u4e0e\u751f\u6001\u6548\u5ea6\u95ee\u5377\u663e\u8457\u4e0d\u540c\u7684\u4eba\u683c\u7279\u5f81\u5256\u9762\uff1b2\uff09\u9879\u76ee\u6570\u91cf\u4e0d\u8db3\u5bfc\u81f4\u6d4b\u91cf\u4e0d\u7a33\u5b9a\uff1b3\uff09\u9020\u6210LLMs\u5177\u6709\u7a33\u5b9a\u4eba\u683c\u7ed3\u6784\u7684\u8bef\u5bfc\u5370\u8c61\uff1b4\uff09\u5bf9\u89d2\u8272\u63d0\u793a\u7684LLMs\u4ea7\u751f\u5938\u5f20\u7684\u4eba\u683c\u5256\u9762\u3002", "conclusion": "\u4f20\u7edf\u5fc3\u7406\u95ee\u5377\u4e0d\u9002\u5408\u7528\u4e8e\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5efa\u8bae\u907f\u514d\u4f7f\u7528\uff0c\u7814\u7a76\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2509.10058", "pdf": "https://arxiv.org/pdf/2509.10058", "abs": "https://arxiv.org/abs/2509.10058", "authors": ["Sung-Lin Tsai", "Bo-Lun Huang", "Yu Ting Shen", "Cheng Yu Yeo", "Chiang Tseng", "Bo-Kai Ruan", "Wen-Sheng Lien", "Hong-Han Shuai"], "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation", "categories": ["cs.CV"], "comment": "Accepted to ACM Multimedia 2025 (MM '25)", "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u6a21\u7cca\u989c\u8272\u672f\u8bed\uff0c\u5728CIELAB\u8272\u5f69\u7a7a\u95f4\u4e2d\u4f18\u5316\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u989c\u8272\u51c6\u786e\u6027", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u989c\u8272\u672f\u8bed\uff08\u5982\u8482\u8299\u5c3c\u84dd\u3001\u67e0\u6aac\u7eff\uff09\u65f6\u5b58\u5728\u989c\u8272\u5bf9\u9f50\u95ee\u9898\uff0c\u65e0\u6cd5\u51c6\u786e\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\uff0c\u9700\u8981\u6539\u8fdb\u989c\u8272\u4fdd\u771f\u5ea6", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u63d0\u793a\u8bcd\u4e2d\u7684\u6a21\u7cca\u989c\u8272\u63cf\u8ff0\uff0c\u5728CIELAB\u8272\u5f69\u7a7a\u95f4\u57fa\u4e8e\u7a7a\u95f4\u5173\u7cfb\u4f18\u5316\u6587\u672c\u5d4c\u5165\uff0c\u6307\u5bfc\u989c\u8272\u6df7\u5408\u64cd\u4f5c", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u6539\u5584\u4e86\u989c\u8272\u5bf9\u9f50\u6548\u679c\uff0c\u586b\u8865\u4e86\u6587\u672c\u8bed\u4e49\u4e0e\u89c6\u89c9\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd", "conclusion": "\u8be5\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u989c\u8272\u6a21\u7cca\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u53c2\u8003\u56fe\u50cf\u5373\u53ef\u63d0\u5347\u989c\u8272\u51c6\u786e\u6027"}}
{"id": "2509.10087", "pdf": "https://arxiv.org/pdf/2509.10087", "abs": "https://arxiv.org/abs/2509.10087", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "categories": ["cs.CL"], "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6c14\u5019\u79d1\u5b66\u9886\u57df\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u8bed\u4e49\u67e5\u8be2\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u533a\u57df\u548c\u53d8\u91cf\u4e4b\u95f4\u7684\u7cbe\u786e\u8054\u7cfb\uff0c\u5e76\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u6c14\u5019\u79d1\u5b66\u6587\u732e\u7684\u590d\u6742\u6027\u548c\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u8de8\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u533a\u57df\u548c\u53d8\u91cf\u627e\u5230\u76f8\u5173\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u77e5\u8bc6\u68c0\u7d22\u65b9\u5f0f\u3002", "method": "\u4ece\u6c14\u5019\u51fa\u7248\u7269\u548c\u79d1\u5b66\u6587\u672c\u6784\u5efa\u9886\u57df\u7279\u5b9a\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301Cypher\u67e5\u8be2\u8fdb\u884c\u7ed3\u6784\u5316\u8bed\u4e49\u641c\u7d22\uff0c\u5e76\u4e0eRAG\u7cfb\u7edf\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u3002", "result": "\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u56de\u7b54\u8bf8\u5982\u7279\u5b9a\u533a\u57df\u9a8c\u8bc1\u7684\u6a21\u578b\u3001\u4e0e\u7279\u5b9a\u9065\u76f8\u5173\u6a21\u5f0f\u5e38\u7528\u6570\u636e\u96c6\u7b49\u7cbe\u786e\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6c14\u5019\u77e5\u8bc6\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u8d85\u8d8a\u4e86\u4f20\u7edf\u6784\u5efa\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6c14\u5019\u7814\u7a76\u3001\u6a21\u578b\u5f00\u53d1\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\uff0c\u4e3a\u4f9d\u8d56\u51c6\u786e\u79d1\u5b66\u4fe1\u606f\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2509.10059", "pdf": "https://arxiv.org/pdf/2509.10059", "abs": "https://arxiv.org/abs/2509.10059", "authors": ["Yue Zhou", "Litong Feng", "Mengcheng Lan", "Xue Yang", "Qingyun Li", "Yiping Ke", "Xue Jiang", "Wayne Zhang"], "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 16 figures", "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math", "AI": {"tldr": "AVI-Math\u662f\u9996\u4e2a\u9488\u5bf9\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u50cf\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3,773\u4e2a\u9ad8\u8d28\u91cf\u8f66\u8f86\u76f8\u5173\u95ee\u9898\uff0c\u6db5\u76d66\u4e2a\u6570\u5b66\u5b66\u79d1\u548c20\u4e2a\u4e3b\u9898\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u7c7b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u9065\u611f\u9886\u57df\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff08\u5982\u7cbe\u786e\u8ddd\u79bb\u8ba1\u7b97\u3001\u8f68\u8ff9\u4f30\u8ba1\u548c\u7a7a\u95f4\u5206\u6790\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u6d4b\u8bd5\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e00\u9886\u57df\u7684\u80fd\u529b", "method": "\u6784\u5efaAVI-Math\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u4ece\u4e0d\u540c\u9ad8\u5ea6\u548c\u89d2\u5ea6\u91c7\u96c6\u7684\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u50cf\uff0c\u6db5\u76d6\u51e0\u4f55\u3001\u903b\u8f91\u548c\u4ee3\u6570\u7b49\u6570\u5b66\u9886\u57df\uff0c\u5bf914\u4e2a\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u63a2\u7d22\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u5fae\u8c03\u6280\u672f", "result": "\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5728\u4ee5\u5f80\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6210\u529f\uff0c\u4f46\u5728AVI-Math\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u6323\u624e\uff0c\u66b4\u9732\u51fa\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u663e\u8457\u5c40\u9650\u6027", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u4e3a\u63a8\u8fdb\u65e0\u4eba\u673a\u53ef\u4fe1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u5fae\u8c03\u6280\u672f\u663e\u793a\u51fa\u89e3\u51b3\u8fd9\u4e9b\u63a8\u7406\u6311\u6218\u7684\u6f5c\u529b"}}
{"id": "2509.10095", "pdf": "https://arxiv.org/pdf/2509.10095", "abs": "https://arxiv.org/abs/2509.10095", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "title": "Arabic Large Language Models for Medical Text Generation", "categories": ["cs.CL"], "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u751f\u6210\u7684LLM\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u6536\u96c6\u793e\u4ea4\u5a92\u4f53\u771f\u5b9e\u533b\u60a3\u5bf9\u8bdd\u6570\u636e\uff0c\u5fae\u8c03Mistral-7B\u7b49\u6a21\u578b\uff0c\u5728\u533b\u7597\u54a8\u8be2\u3001\u8bca\u65ad\u548c\u6cbb\u7597\u5efa\u8bae\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u5728\u5b9e\u65f6\u533b\u7597\u5efa\u8bae\u3001\u4e0d\u89c4\u5219\u8f93\u5165\u548c\u5c11\u6570\u8bed\u8a00\u652f\u6301\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u751f\u6210\u65b9\u9762\u7684\u7a7a\u767d\u3002", "method": "\u6536\u96c6\u793e\u4ea4\u5a92\u4f53\u533b\u60a3\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u9884\u5904\u7406\u591a\u963f\u62c9\u4f2f\u65b9\u8a00\u6570\u636e\uff0c\u5fae\u8c03Mistral-7B-Instruct-v0.2\u3001LLaMA-2-7B\u548cGPT-2 Medium\u7b49\u751f\u6210\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\u7684Mistral-7B\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cBERT Score\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u8fbe\u523068.5%\u300169.08%\u548c68.5%\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u533b\u9662\u7ba1\u7406\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u6587\u5316\u591a\u6837\u5316\u7684\u533b\u7597\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10080", "pdf": "https://arxiv.org/pdf/2509.10080", "abs": "https://arxiv.org/abs/2509.10080", "authors": ["Minsang Kong", "Myeongjun Kim", "Sang Gu Kang", "Sang Hun Lee"], "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals", "categories": ["cs.CV", "I.2.9; I.4.8"], "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (under review)", "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.", "AI": {"tldr": "BEVTraj\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u9e1f\u77b0\u56fe\u7a7a\u95f4\u4e2d\u5229\u7528\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5efa\u9ad8\u6e05\u5730\u56fe\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u5730\u56fe\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5efa\u9ad8\u6e05\u5730\u56fe\u6216\u5b9e\u65f6\u5730\u56fe\u6784\u5efa\u6a21\u5757\uff0c\u4f46\u9884\u5efa\u5730\u56fe\u5c40\u9650\u4e8e\u7279\u5b9a\u533a\u57df\u4e14\u65e0\u6cd5\u9002\u5e94\u77ac\u65f6\u53d8\u5316\uff0c\u800c\u5730\u56fe\u6784\u5efa\u6a21\u5757\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u5173\u952e\u573a\u666f\u7ec6\u8282\u6216\u5f15\u5165\u9519\u8bef\uff0c\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faBEVTraj\u6846\u67b6\uff0c\u5728BEV\u7a7a\u95f4\u4e2d\u4f7f\u7528\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\u4ece\u5bc6\u96c6BEV\u7279\u5f81\u4e2d\u63d0\u53d6\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u7a00\u758f\u76ee\u6807\u5019\u9009\u63d0\u8bae\u6a21\u5757\u5b9e\u73b0\u5b8c\u5168\u7aef\u5230\u7aef\u9884\u6d4b\uff0c\u65e0\u9700\u540e\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBEVTraj\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u9ad8\u6e05\u5730\u56fe\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u6d88\u9664\u5bf9\u9884\u5efa\u5730\u56fe\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "BEVTraj\u6846\u67b6\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u5730\u56fe\u4f9d\u8d56\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.10108", "pdf": "https://arxiv.org/pdf/2509.10108", "abs": "https://arxiv.org/abs/2509.10108", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4f7f\u7528ChatGPT-4o\u548cGemini 2.5 Pro\u751f\u6210\u4e8680,000\u4e2a\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u5bf9\uff0c\u5c06\u8bad\u7ec3\u8bed\u6599\u6269\u5c55\u5230100,000\u6761\u8bb0\u5f55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7684\u6027\u80fd\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7684\u53d1\u5c55\u53d7\u5230\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u5148\u524d\u7684\u7814\u7a76\u867d\u7136\u6784\u5efa\u4e8620,000\u6761\u533b\u60a3\u4e92\u52a8\u6570\u636e\uff0c\u4f46\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002", "method": "\u4f7f\u7528ChatGPT-4o\u548cGemini 2.5 Pro\u751f\u621080,000\u4e2a\u4e0a\u4e0b\u6587\u76f8\u5173\u4e14\u533b\u5b66\u4e0a\u8fde\u8d2f\u7684\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u7ecf\u8fc7\u8bed\u4e49\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\u540e\u6574\u5408\u5230\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u5bf9\u5305\u62ecMistral-7B\u548cAraGPT2\u5728\u5185\u7684\u4e94\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "ChatGPT-4o\u751f\u6210\u7684\u6570\u636e\u5728\u6240\u6709\u6a21\u578b\u4e2d\u59cb\u7ec8\u83b7\u5f97\u66f4\u9ad8\u7684F1\u5206\u6570\u548c\u66f4\u5c11\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5408\u6210\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u589e\u5f3a\u662f\u589e\u5f3a\u4f4e\u8d44\u6e90\u533b\u7597NLP\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u66f4\u5177\u5305\u5bb9\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u7684\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.10093", "pdf": "https://arxiv.org/pdf/2509.10093", "abs": "https://arxiv.org/abs/2509.10093", "authors": ["Laura Bragagnolo", "Matteo Terreran", "Leonardo Barcellona", "Stefano Ghidoni"], "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing", "categories": ["cs.CV"], "comment": "ICIAP 2025", "summary": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u89c6\u89d2\u4fe1\u606f\u6539\u8fdb\u591a\u4eba\u89e3\u6790\u6a21\u578b\u5728\u906e\u6321\u573a\u666f\u4e0b\u6027\u80fd\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u548c\u4e00\u81f4\u6027\u635f\u5931\u5b9e\u73b0\uff0c\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u76f8\u5bf9\u57fa\u7ebf\u6a21\u578b\u63d0\u53474.20%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u53e0\u4eba\u4f53\u5206\u5272\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u591a\u89c6\u89d2\u4fe1\u606f\u53ef\u4ee5\u63d0\u4f9b\u5206\u79bb\u7684\u91cd\u53e0\u4eba\u4f53\u89c6\u56fe", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f31\u76d1\u7763\u4eba\u7c7b\u5b9e\u4f8b\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u635f\u5931\u7684\u65b0\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u534a\u81ea\u52a8\u6807\u6ce8\u7b56\u7565\u4ece\u591a\u89c6\u89d2RGB+D\u6570\u636e\u548c3D\u4eba\u4f53\u9aa8\u67b6\u751f\u6210\u6807\u6ce8", "result": "\u5728\u906e\u6321\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0\u4e864.20%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347", "conclusion": "\u591a\u89c6\u89d2\u4fe1\u606f\u80fd\u6709\u6548\u6539\u5584\u591a\u4eba\u89e3\u6790\u6a21\u578b\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u6807\u6ce8\u7b56\u7565\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2509.10116", "pdf": "https://arxiv.org/pdf/2509.10116", "abs": "https://arxiv.org/abs/2509.10116", "authors": ["Julian Linke", "Barbara Schuppler"], "title": "Prominence-aware automatic speech recognition for conversational speech", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper investigates prominence-aware automatic speech recognition (ASR)\nby combining prominence detection and speech recognition for conversational\nAustrian German. First, prominence detectors were developed by fine-tuning\nwav2vec2 models to classify word-level prominence. The detector was then used\nto automatically annotate prosodic prominence in a large corpus. Based on those\nannotations, we trained novel prominence-aware ASR systems that simultaneously\ntranscribe words and their prominence levels. The integration of prominence\ninformation did not change performance compared to our baseline ASR system,\nwhile reaching a prominence detection accuracy of 85.53% for utterances where\nthe recognized word sequence was correct. This paper shows that\ntransformer-based models can effectively encode prosodic information and\nrepresents a novel contribution to prosody-enhanced ASR, with potential\napplications for linguistic research and prosody-informed dialogue systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u91cd\u97f3\u68c0\u6d4b\u548c\u8bed\u97f3\u8bc6\u522b\u6765\u5f00\u53d1\u5965\u5730\u5229\u5fb7\u8bed\u5bf9\u8bdd\u4e2d\u7684\u91cd\u97f3\u611f\u77e5\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528wav2vec2\u6a21\u578b\u8fdb\u884c\u91cd\u97f3\u5206\u7c7b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u81ea\u52a8\u6807\u6ce8\u97f5\u5f8b\u91cd\u97f3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8f6c\u5f55\u5355\u8bcd\u53ca\u5176\u91cd\u97f3\u7ea7\u522b\u7684\u91cd\u97f3\u611f\u77e5ASR\u7cfb\u7edf\uff0c\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u57fa\u4e8e\u97f5\u5f8b\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5fae\u8c03wav2vec2\u6a21\u578b\u5f00\u53d1\u8bcd\u7ea7\u91cd\u97f3\u68c0\u6d4b\u5668\uff0c\u81ea\u52a8\u6807\u6ce8\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u7684\u97f5\u5f8b\u91cd\u97f3\uff0c\u7136\u540e\u8bad\u7ec3\u80fd\u591f\u540c\u65f6\u5904\u7406\u5355\u8bcd\u8f6c\u5f55\u548c\u91cd\u97f3\u7ea7\u522b\u8bc6\u522b\u7684\u65b0\u578bASR\u7cfb\u7edf\u3002", "result": "\u96c6\u6210\u91cd\u97f3\u4fe1\u606f\u540eASR\u6027\u80fd\u4e0e\u57fa\u7ebf\u7cfb\u7edf\u76f8\u5f53\uff0c\u5728\u8bc6\u522b\u6b63\u786e\u7684\u8bed\u53e5\u4e2d\u91cd\u97f3\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523085.53%\u3002", "conclusion": "\u57fa\u4e8etransformer\u7684\u6a21\u578b\u80fd\u6709\u6548\u7f16\u7801\u97f5\u5f8b\u4fe1\u606f\uff0c\u8fd9\u662f\u5bf9\u97f5\u5f8b\u589e\u5f3aASR\u7684\u65b0\u8d21\u732e\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10105", "pdf": "https://arxiv.org/pdf/2509.10105", "abs": "https://arxiv.org/abs/2509.10105", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "title": "VARCO-VISION-2.0 Technical Report", "categories": ["cs.CV", "cs.CL"], "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.", "AI": {"tldr": "VARCO-VISION-2.0\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u97e9\u82f1\u53cc\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u76f8\u6bd4\u524d\u4ee3\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u652f\u6301\u591a\u56fe\u50cf\u7406\u89e3\u3001\u6587\u6863\u56fe\u8868\u5904\u7406\u548c\u5e03\u5c40\u611f\u77e5OCR\uff0c\u572814B\u548c1.7B\u4e24\u4e2a\u89c4\u6a21\u7248\u672c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u53cc\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u97e9\u8bed\u548c\u82f1\u8bed\uff0c\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6587\u6863\u3001\u56fe\u8868\u548c\u8868\u683c\u7b49\u590d\u6742\u8f93\u5165\u7684\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u6838\u5fc3\u80fd\u529b\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\u548c\u5185\u5b58\u9ad8\u6548\u6280\u672f\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u63d0\u5347\u5b89\u5168\u6027\uff0c\u652f\u6301\u591a\u56fe\u50cf\u7406\u89e3\u548c\u5e03\u5c40\u611f\u77e5OCR\uff08\u9884\u6d4b\u6587\u672c\u5185\u5bb9\u53ca\u5176\u7a7a\u95f4\u4f4d\u7f6e\uff09\u3002", "result": "\u6a21\u578b\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u589e\u5f3a\uff0c\u5728\u7a7a\u95f4\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c14B\u6a21\u578b\u5728OpenCompass VLM\u6392\u884c\u699c\u4e0a\u4f4d\u5217\u540c\u89c4\u6a21\u6a21\u578b\u7b2c8\u540d\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u9002\u7528\u4e8e\u8bbe\u5907\u90e8\u7f72\u76841.7B\u8f7b\u91cf\u7248\u672c\u3002", "conclusion": "VARCO-VISION-2.0\u6a21\u578b\u63a8\u52a8\u4e86\u53cc\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u53ca\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u89c4\u6a21\u548c\u9ad8\u6548\u7387\u4e24\u4e2a\u7248\u672c\u4f9b\u5b9e\u9645\u4f7f\u7528\u3002"}}
{"id": "2509.10127", "pdf": "https://arxiv.org/pdf/2509.10127", "abs": "https://arxiv.org/abs/2509.10127", "authors": ["Zhengyu Hu", "Zheyuan Xiao", "Max Xiong", "Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Kaize Ding", "Ziang Xiao", "Nicholas Jing Yuan", "Xing Xie"], "title": "Population-Aligned Persona Generation for LLM-based Social Simulation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0e\u4eba\u53e3\u5206\u5e03\u5bf9\u9f50\u7684LLM\u9a71\u52a8\u793e\u4ea4\u6a21\u62df\u89d2\u8272\u96c6\uff0c\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u751f\u6210\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u91cd\u8981\u6027\u91c7\u6837\u6765\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u73b0\u6709LLM\u793e\u4ea4\u6a21\u62df\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7406\u6846\u67b6\u548c\u6a21\u62df\u73af\u5883\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u89d2\u8272\u751f\u6210\u7684\u590d\u6742\u6027\u548c\u975e\u4ee3\u8868\u6027\u89d2\u8272\u96c6\u5f15\u5165\u7684\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u80fd\u771f\u5b9e\u53cd\u6620\u73b0\u5b9e\u4eba\u7fa4\u591a\u6837\u6027\u548c\u5206\u5e03\u7684\u89d2\u8272\u96c6\u3002", "method": "\u5229\u7528LLM\u4ece\u957f\u671f\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u751f\u6210\u53d9\u4e8b\u89d2\u8272\uff0c\u8fdb\u884c\u4e25\u683c\u8d28\u91cf\u8bc4\u4f30\u7b5b\u9009\u4f4e\u8d28\u91cf\u6863\u6848\uff0c\u5e94\u7528\u91cd\u8981\u6027\u91c7\u6837\u5b9e\u73b0\u4e0e\u53c2\u8003\u5fc3\u7406\u6d4b\u91cf\u5206\u5e03\uff08\u5982\u5927\u4e94\u4eba\u683c\uff09\u7684\u5168\u5c40\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u9488\u5bf9\u5b50\u7fa4\u4f53\u8fdb\u884c\u9002\u914d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u53e3\u5c42\u9762\u7684\u504f\u89c1\uff0c\u80fd\u591f\u4e3a\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u653f\u7b56\u5e94\u7528\u63d0\u4f9b\u51c6\u786e\u3001\u7075\u6d3b\u7684\u793e\u4ea4\u6a21\u62df\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u793e\u4ea4\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u4eba\u53e3\u5bf9\u9f50\u7684\u89d2\u8272\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u89d2\u8272\u96c6\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10114", "pdf": "https://arxiv.org/pdf/2509.10114", "abs": "https://arxiv.org/abs/2509.10114", "authors": ["MohammadAli Hamidi", "Hadi Amirpour", "Luigi Atzori", "Christian Timmerer"], "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss", "categories": ["cs.CV"], "comment": null, "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408MobileNetV3-Small\u548cShuffleNetV2\u7f51\u7edc\uff0c\u4f7f\u7528MSECorrLoss\u635f\u5931\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u4eba\u8138\u7279\u6709\u7684\u9000\u5316\u7279\u5f81\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u4e24\u4e2a\u7d27\u51d1\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08MobileNetV3-Small\u548cShuffleNetV2\uff09\uff0c\u901a\u8fc7\u7b80\u5355\u5e73\u5747\u8fdb\u884c\u9884\u6d4b\u7ea7\u878d\u5408\uff0c\u4f7f\u7528\u7ed3\u5408MSE\u548cPearson\u76f8\u5173\u6b63\u5219\u5316\u7684MSECorrLoss\u635f\u5931\u51fd\u6570\u6765\u66f4\u597d\u5730\u5bf9\u9f50\u4eba\u7c7b\u611f\u77e5\u5224\u65ad\u3002", "result": "\u5728VQualA FIQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SRCC 0.9829\u548cPLCC 0.9894\u7684\u4f18\u79c0\u6027\u80fd\uff0c\u540c\u65f6\u6ee1\u8db3\u8ba1\u7b97\u6548\u7387\u7ea6\u675f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff0c\u4e3a\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d28\u91cf\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10129", "pdf": "https://arxiv.org/pdf/2509.10129", "abs": "https://arxiv.org/abs/2509.10129", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce\n\\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that\ndecouples answer generation from spatial localization. This design makes it\napplicable to existing VLMs, including proprietary systems where fine-tuning is\nnot feasible. Through systematic evaluation, we provide quantitative insights\ninto the gap between textual accuracy and spatial grounding, showing that\ncorrect answers often lack reliable localization. Our standardized framework\nhighlights these shortcomings and establishes a benchmark for future research\ntoward more interpretable and robust document information extraction VLMs.", "AI": {"tldr": "DocExplainerV0\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u8fb9\u754c\u6846\u9884\u6d4b\u6a21\u5757\uff0c\u5c06\u7b54\u6848\u751f\u6210\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u89e3\u8026\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u4e2d\u7684\u7b54\u6848\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u51c6\u786e\u5728\u6587\u6863\u4e2d\u5b9a\u4f4d\u7b54\u6848\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165DocExplainerV0\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u4e0e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u65e0\u6cd5\u5fae\u8c03\u7684\u4e13\u6709\u7cfb\u7edf\uff09\u914d\u5408\u4f7f\u7528\uff0c\u901a\u8fc7\u89e3\u8026\u7b54\u6848\u751f\u6210\u548c\u7a7a\u95f4\u5b9a\u4f4d\u6765\u5b9e\u73b0\u8fb9\u754c\u6846\u9884\u6d4b\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u6587\u672c\u51c6\u786e\u6027\u548c\u7a7a\u95f4\u5b9a\u4f4d\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u6b63\u786e\u7684\u7b54\u6848\u5f80\u5f80\u7f3a\u4e4f\u53ef\u9760\u7684\u7a7a\u95f4\u5b9a\u4f4d\u3002\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u8fd9\u4e9b\u4e0d\u8db3\u5e76\u5efa\u7acb\u4e86\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002", "conclusion": "DocExplainerV0\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u548c\u57fa\u51c6\u3002"}}
{"id": "2509.10122", "pdf": "https://arxiv.org/pdf/2509.10122", "abs": "https://arxiv.org/abs/2509.10122", "authors": ["Zongliang Wu", "Siming Zheng", "Peng-Tao Jiang", "Xin Yuan"], "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.", "AI": {"tldr": "\u63d0\u51faRCOD\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u57df\u5206\u7ec4\u7b56\u7565\u548c\u9000\u5316\u611f\u77e5\u91c7\u6837\uff0c\u5b9e\u73b0\u5355\u6b65\u6269\u6563\u6a21\u578b\u4e2d\u4fdd\u771f\u5ea6\u4e0e\u771f\u5b9e\u611f\u7684\u7075\u6d3b\u6743\u8861\u63a7\u5236", "motivation": "\u4f20\u7edf\u5355\u6b65\u6269\u6563\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u7684\u4fdd\u771f\u5ea6-\u771f\u5b9e\u611f\u6743\u8861\u63a7\u5236\u673a\u5236\uff0c\u65e0\u6cd5\u50cf\u591a\u6b65\u65b9\u6cd5\u90a3\u6837\u901a\u8fc7\u8c03\u6574\u91c7\u6837\u6b65\u9aa4\u6765\u9002\u5e94\u4e0d\u540c\u573a\u666f\u9700\u6c42", "method": "\u63d0\u51faRCOD\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u6f5c\u5728\u57df\u5206\u7ec4\u7b56\u7565\u5b9e\u73b0\u566a\u58f0\u9884\u6d4b\u9636\u6bb5\u7684\u663e\u5f0f\u63a7\u5236\uff1b2\uff09\u9000\u5316\u611f\u77e5\u91c7\u6837\u7b56\u7565\u5bf9\u9f50\u84b8\u998f\u6b63\u5219\u5316\uff1b3\uff09\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u6a21\u5757\u66ff\u4ee3\u6587\u672c\u63d0\u793a", "result": "\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5355\u6b65\u6269\u6563\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u5177\u6709\u7075\u6d3b\u7684\u903c\u771f\u5ea6\u63a7\u5236\u80fd\u529b", "conclusion": "RCOD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5355\u6b65\u6269\u6563\u6a21\u578b\u5728\u4fdd\u771f\u5ea6-\u771f\u5b9e\u611f\u6743\u8861\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10179", "pdf": "https://arxiv.org/pdf/2509.10179", "abs": "https://arxiv.org/abs/2509.10179", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Anna Marklov\u00e1", "V\u00e1clav Cvr\u010dek"], "title": "Benchmark of stylistic variation in LLM-generated texts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Biber\u7684\u591a\u7ef4\u5206\u6790(MDA)\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4eba\u7c7b\u5199\u4f5c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u6587\u672c\u7684\u8bed\u57df\u53d8\u5f02\u5dee\u5f02\uff0c\u521b\u5efa\u4e86\u53ef\u89e3\u91ca\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u751f\u6210\u6587\u672c\u5728\u8bed\u57df\u7279\u5f81\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u7279\u522b\u662f\u524d\u6cbfLLM\u5728\u82f1\u8bed\u4e4b\u5916\u8bed\u8a00(\u5982\u6377\u514b\u8bed)\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u975e\u82f1\u8bed\u8bed\u8a00\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3", "method": "\u5e94\u7528Biber\u7684\u591a\u7ef4\u5206\u6790\u65b9\u6cd5\uff0c\u4f7f\u7528AI-Brown\u8bed\u6599\u5e93(\u4e0eBE-21\u53ef\u6bd4)\u548cAI-Koditex\u6377\u514b\u8bed\u6599\u5e93\uff0c\u5206\u679016\u4e2a\u524d\u6cbfLLM\u5728\u4e0d\u540c\u8bbe\u7f6e\u548c\u63d0\u793a\u4e0b\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b", "result": "\u8bc6\u522b\u51faLLM\u4e0e\u4eba\u7c7b\u6587\u672c\u5dee\u5f02\u6700\u663e\u8457\u548c\u6700\u7cfb\u7edf\u7684\u53d8\u5f02\u7ef4\u5ea6\uff0c\u5efa\u7acb\u4e86\u6a21\u578b\u95f4\u53ef\u6bd4\u6027\u548c\u53ef\u6392\u540d\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6", "conclusion": "\u7814\u7a76\u4e3a\u8bc4\u4f30LLM\u6587\u672c\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u591a\u7ef4\u5ea6\u57fa\u51c6\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u975e\u82f1\u8bed\u8bed\u8a00\u7684LLM\u8868\u73b0\u8bc4\u4f30"}}
{"id": "2509.10134", "pdf": "https://arxiv.org/pdf/2509.10134", "abs": "https://arxiv.org/abs/2509.10134", "authors": ["Rini Smita Thakur", "Rajeev Ranjan Dwivedi", "Vinod K Kurmi"], "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment", "categories": ["cs.CV"], "comment": "Accepted in BMVC 2025", "summary": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.", "AI": {"tldr": "Grad-CL\u662f\u4e00\u4e2a\u6e90\u81ea\u7531\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5f15\u5bfc\u4f2a\u6807\u7b7e\u4f18\u5316\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u65e0\u9700\u6e90\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u89c6\u76d8\u548c\u89c6\u676f\u5206\u5272\u7684\u8de8\u57df\u6027\u80fd", "motivation": "\u89e3\u51b3\u89c6\u76d8\u548c\u89c6\u676f\u5206\u5272\u6a21\u578b\u5728\u4e0d\u540c\u6210\u50cf\u534f\u8bae\u6216\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u6e90\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u57df\u9002\u5e94", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u68af\u5ea6\u673a\u5236\u63d0\u53d6\u7c7b\u522b\u7279\u5b9a\u7279\u5f81\uff0c\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u539f\u578b\u4f30\u8ba1\u6765\u4f18\u5316\u566a\u58f0\u4f2a\u6807\u7b7e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u5bf9\u6bd4\u635f\u5931\u6765\u589e\u5f3a\u89c6\u676f\u548c\u89c6\u676f\u7279\u5f81\u4e4b\u95f4\u7684\u7c7b\u95f4\u53ef\u5206\u79bb\u6027", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u57df\u773c\u5e95\u6210\u50cf\u6570\u636e\u96c6\u4e0a\uff0cGrad-CL\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u548c\u6e90\u81ea\u7531\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u6539\u5584\u7684\u8fb9\u754c\u63cf\u7ed8", "conclusion": "Grad-CL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u573a\u666f\u4e0b\uff0c\u4e3a\u773c\u79d1\u75be\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301"}}
{"id": "2509.10184", "pdf": "https://arxiv.org/pdf/2509.10184", "abs": "https://arxiv.org/abs/2509.10184", "authors": ["Leen Almajed", "Abeer ALdayel"], "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations", "categories": ["cs.CL"], "comment": "This paper is under review", "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u4e0d\u6070\u5f53\u7684\u79ef\u6781\u56de\u5e94\u95ee\u9898\uff0c\u5206\u6790\u4eba\u7c7b\u548cLLM\u751f\u6210\u56de\u5e94\u4e2d\u7684\u4e0d\u534f\u8c03\u79ef\u6781\u6027\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e0bLLM\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u73b0\u5b9e\u7684\u4e50\u89c2\u56de\u5e94\u3002", "motivation": "\u7814\u7a76\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\u5584\u610f\u4f46\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u7684\u79ef\u6781\u56de\u5e94\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u7ebf\u652f\u6301\u5bf9\u8bdd\u4e2dLLM\u751f\u6210\u56de\u5e94\u53ef\u80fd\u663e\u5f97\u8f7b\u63cf\u6de1\u5199\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u4e50\u89c2\uff0c\u9700\u8981\u66f4\u7b26\u5408\u60c5\u611f\u671f\u671b\u7684\u652f\u6301\u65b9\u5f0f\u3002", "method": "\u6536\u96c6Reddit\u771f\u5b9e\u7528\u6237-\u52a9\u624b\u5bf9\u8bdd\uff0c\u6309\u60c5\u611f\u5f3a\u5ea6\u5206\u7c7b\u4e3a\u8f7b\u5ea6\uff08\u5173\u7cfb\u7d27\u5f20\u3001\u4e00\u822c\u5efa\u8bae\uff09\u548c\u91cd\u5ea6\uff08\u60b2\u4f24\u3001\u7126\u8651\uff09\u60c5\u5883\uff1b\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u989d\u5916\u56de\u5e94\uff1b\u5fae\u8c03LLM\u5e76\u5f00\u53d1\u5f31\u76d1\u7763\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u96c6\u6210\uff08DeBERTa\u548cMentalBERT\uff09\u6765\u68c0\u6d4b\u4e0d\u534f\u8c03\u79ef\u6781\u6027\u7c7b\u578b\u3002", "result": "\u53d1\u73b0LLM\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e0b\u66f4\u5bb9\u6613\u901a\u8fc7\u8f7b\u63cf\u6de1\u5199\u548c\u6700\u5c0f\u5316\u8bed\u6c14\u8868\u73b0\u51fa\u4e0d\u73b0\u5b9e\u7684\u79ef\u6781\u6027\uff1b\u5f00\u53d1\u7684\u5206\u7c7b\u5668\u96c6\u6210\u5728\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u4e0d\u534f\u8c03\u79ef\u6781\u6027\u65b9\u9762\u8868\u73b0\u6539\u5584\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u751f\u6210\u901a\u7528\u79ef\u6781\u56de\u5e94\uff0c\u7814\u7a76\u7b26\u5408\u60c5\u5883\u7684\u652f\u6301\u63aa\u65bd\u6765\u5e73\u8861\u79ef\u6781\u60c5\u611f\u4e0e\u60c5\u611f\u8ba4\u540c\uff0c\u4e3a\u6784\u5efa\u60c5\u5883\u611f\u77e5\u548c\u4fdd\u6301\u4fe1\u4efb\u7684\u5728\u7ebf\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2509.10140", "pdf": "https://arxiv.org/pdf/2509.10140", "abs": "https://arxiv.org/abs/2509.10140", "authors": ["Yifan Chang", "Jie Qin", "Limeng Qiao", "Xiaofeng Wang", "Zheng Zhu", "Lin Ma", "Xingang Wang"], "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "categories": ["cs.CV"], "comment": null, "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.", "AI": {"tldr": "VQBridge\u65b9\u6cd5\u89e3\u51b3\u4e86\u5411\u91cf\u91cf\u5316(VQ)\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u901a\u8fc7\u538b\u7f29-\u5904\u7406-\u6062\u590d\u7ba1\u9053\u5b9e\u73b0100%\u7801\u672c\u4f7f\u7528\u7387\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u6027\u80fd", "motivation": "\u4f20\u7edfVQ\u8bad\u7ec3\u5b58\u5728\u76f4\u901a\u4f30\u8ba1\u504f\u5dee\u3001\u6ede\u540e\u66f4\u65b0\u548c\u7a00\u758f\u68af\u5ea6\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7801\u672c\u4f7f\u7528\u7387\u4f4e\u548c\u91cd\u5efa\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5", "method": "\u63d0\u51faVQBridge\u6295\u5f71\u5668\uff0c\u57fa\u4e8e\u6620\u5c04\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7compress-process-recover\u7ba1\u9053\u4f18\u5316\u7801\u5411\u91cf\uff0c\u7ed3\u5408\u5b66\u4e60\u9000\u706b\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3", "result": "\u5728262k\u5927\u7801\u672c\u4e0b\u5b9e\u73b0100%\u4f7f\u7528\u7387\uff0c\u8fbe\u5230SOTA\u91cd\u5efa\u6027\u80fd\uff0c\u4e0eLlamaGen\u96c6\u6210\u540e\u56fe\u50cf\u751f\u6210\u6027\u80fd\u8d85\u8d8aVAR 0.5 rFID\u548cDiT 0.2 rFID", "conclusion": "\u9ad8\u8d28\u91cf\u5206\u8bcd\u5668\u5bf9\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0cFVQ\u65b9\u6cd5\u6709\u6548\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\uff0c\u4e3a\u5927\u89c4\u6a21\u7801\u672c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7a33\u5b9a\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10199", "pdf": "https://arxiv.org/pdf/2509.10199", "abs": "https://arxiv.org/abs/2509.10199", "authors": ["Mikl\u00f3s Seb\u0151k", "Viktor Kov\u00e1cs", "Martin B\u00e1n\u00f3czy", "Daniel M\u00f8ller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "categories": ["cs.CL", "I.7; I.2; J.4"], "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff08\u7279\u522b\u662f\u6cd5\u5f8b\u6587\u4ef6\uff09\u65f6\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4e13\u95e8\u4e3a\u957f\u6587\u672c\u8bbe\u8ba1\u7684Longformer\u6a21\u578b\u5e76\u65e0\u660e\u663e\u4f18\u52bf\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u3001RoBERTa\uff09\u5b58\u5728\u8f93\u5165\u6587\u672c\u957f\u5ea6\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u957f\u8fbe\u6570\u767e\u9875\u7684\u6cd5\u5f8b\u6587\u4ef6\u5206\u7c7b\u4efb\u52a1\uff0c\u9700\u8981\u63a2\u7d22\u9002\u5408\u957f\u6587\u672c\u5904\u7406\u7684\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57285\u79cd\u8bed\u8a00\u4e0a\u4f7f\u7528XLM-RoBERTa\u3001Longformer\u3001GPT-3.5\u3001GPT-4\u7b49\u6a21\u578b\u8fdb\u884c\u591a\u7c7b\u522b\u5206\u7c7b\u5b9e\u9a8c\uff0c\u4f7f\u7528\u6bd4\u8f83\u8bae\u7a0b\u9879\u76ee\u768421\u4e2a\u653f\u7b56\u4e3b\u9898\u6807\u7b7e\u4ee3\u7801\u672c\u3002", "result": "Longformer\u6a21\u578b\u5728\u957f\u6587\u672c\u5904\u7406\u65b9\u9762\u6ca1\u6709\u663e\u793a\u51fa\u7279\u522b\u4f18\u52bf\uff1b\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT\u53d8\u4f53\uff1b\u7c7b\u522b\u95f4\u7684\u652f\u6301\u5ea6\u548c\u5185\u5bb9\u91cd\u53e0\u5ea6\u662f\u5f71\u54cd\u957f\u6587\u672c\u5206\u7c7b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u4e13\u95e8\u4e3a\u957f\u6587\u672c\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u957f\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u7c7b\u522b\u95f4\u7684\u8bed\u4e49\u7279\u5f81\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2509.10156", "pdf": "https://arxiv.org/pdf/2509.10156", "abs": "https://arxiv.org/abs/2509.10156", "authors": ["Goker Erdogan", "Nikhil Parthasarathy", "Catalin Ionescu", "Drew Hudson", "Alexander Lerchner", "Andrew Zisserman", "Mehdi Sajjadi", "Joao Carreira"], "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.", "AI": {"tldr": "LayerLock\u662f\u4e00\u79cd\u901a\u8fc7\u6e10\u8fdb\u5c42\u51bb\u7ed3\u5b9e\u73b0\u4ece\u50cf\u7d20\u9884\u6d4b\u5230\u6f5c\u5728\u9884\u6d4b\u8fc7\u6e21\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u52a0\u901fMAE\u8bad\u7ec3\u5e76\u907f\u514d\u8868\u793a\u5d29\u6e83", "motivation": "\u89c2\u5bdf\u5230\u89c6\u9891\u63a9\u7801\u81ea\u7f16\u7801(MAE)\u8bad\u7ec3\u4e2dViT\u5c42\u6309\u6df1\u5ea6\u987a\u5e8f\u6536\u655b\u7684\u73b0\u8c61\uff0c\u5e0c\u671b\u5229\u7528\u8fd9\u4e00\u53d1\u73b0\u6765\u52a0\u901f\u8bad\u7ec3\u5e76\u6539\u8fdb\u6f5c\u5728\u9884\u6d4b\u65b9\u6cd5", "method": "\u901a\u8fc7\u660e\u786e\u7684\u8fdb\u5ea6\u8868\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u51bb\u7ed3\u6a21\u578b\u5c42\uff0c\u5b9e\u73b0\u4ece\u50cf\u7d20\u9884\u6d4b\u5230\u6f5c\u5728\u9884\u6d4b\u7684\u6e10\u8fdb\u8fc7\u6e21", "result": "\u5728\u9ad8\u8fbe40\u4ebf\u53c2\u6570\u7684\u5927\u578b\u6a21\u578b\u4e0a\u5e94\u7528LayerLock\uff0c\u57284DS\u611f\u77e5\u5957\u4ef6\u4e0a\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u975e\u6f5c\u5728\u63a9\u7801\u9884\u6d4b\u65b9\u6cd5", "conclusion": "LayerLock\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u6a21\u578b\u5e76\u907f\u514d\u8868\u793a\u5d29\u6e83\u95ee\u9898"}}
{"id": "2509.10208", "pdf": "https://arxiv.org/pdf/2509.10208", "abs": "https://arxiv.org/abs/2509.10208", "authors": ["Shengqiang Fu"], "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.", "AI": {"tldr": "\u63d0\u51faSelf Improving Faithfulness Aware Contrastive Tuning\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6307\u5bfc\u673a\u5236\u81ea\u52a8\u751f\u6210\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\uff0c\u63d0\u5347LLM\u5728\u77e5\u8bc6\u51b2\u7a81\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7ecf\u5e38\u751f\u6210\u4e0d\u5fe0\u5b9e\u7684\u54cd\u5e94\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\uff0c\u5b58\u5728\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u6307\u5bfc\u673a\u5236\u8ba9\u57fa\u7840LLM\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ed3\u6784\u5316\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\uff08\u951a\u6837\u672c\u3001\u8bed\u4e49\u7b49\u4ef7\u6b63\u6837\u672c\u3001\u6a21\u62df\u4e0d\u5fe0\u5b9e\u573a\u666f\u7684\u8d1f\u6837\u672c\uff09\uff0c\u7136\u540e\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728ECARE KRE\u548cCOSE KRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eLlama3 8B Instruct\u7684SI FACT\u6a21\u578b\u5c06\u4e0a\u4e0b\u6587\u53ec\u56de\u7387\u63d0\u9ad8\u4e866.2%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5185\u90e8\u8bb0\u5fc6\u7684\u4f9d\u8d56\u3002", "conclusion": "SI FACT\u5728\u589e\u5f3aLLM\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u65b9\u9762\u63d0\u4f9b\u4e86\u5f3a\u6709\u6548\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u4e3b\u52a8\u548c\u53ef\u4fe1\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2509.10241", "pdf": "https://arxiv.org/pdf/2509.10241", "abs": "https://arxiv.org/abs/2509.10241", "authors": ["Elias De Smijter", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints", "categories": ["cs.CV"], "comment": "9 pages, 3 figures, to be presented at ASTRA25,", "summary": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u9690\u5f0f\u548c\u663e\u5f0f\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u5728\u592a\u7a7a3D\u7269\u4f53\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86\u5916\u89c2\u5d4c\u5165\u7684\u4f5c\u7528\u3002\u53d1\u73b0\u5d4c\u5165\u867d\u7136\u80fd\u63d0\u9ad8\u5149\u5ea6\u4fdd\u771f\u5ea6\uff0c\u4f46\u5bf9\u51e0\u4f55\u7cbe\u5ea6\u63d0\u5347\u6709\u9650\uff0c\u800c\u51f8\u9762\u6e85\u5c04\u6bd4\u9ad8\u65af\u6e85\u5c04\u80fd\u63d0\u4f9b\u66f4\u7d27\u51d1\u3001\u65e0\u6742\u4e71\u7684\u8868\u793a\u3002", "motivation": "\u592a\u7a7a\u673a\u5668\u4eba\u5e94\u7528\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u51e0\u4f55\u91cd\u5efa\uff0c\u800c\u73b0\u6709\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4e2d\u5916\u89c2\u5d4c\u5165\u5bf9\u51e0\u4f55\u51c6\u786e\u6027\u7684\u5b9e\u9645\u8d21\u732e\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u7a7a\u95f4\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528SPEED+\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86K-Planes\u3001\u9ad8\u65af\u6e85\u5c04\u548c\u51f8\u9762\u6e85\u5c04\u4e09\u79cd\u65b9\u6cd5\uff0c\u5206\u6790\u5916\u89c2\u5d4c\u5165\u5bf9\u5149\u5ea6\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u8868\u793a\u6548\u7387\u548c\u51e0\u4f55\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "result": "\u5916\u89c2\u5d4c\u5165\u4e3b\u8981\u51cf\u5c11\u663e\u5f0f\u65b9\u6cd5\u6240\u9700\u7684\u56fe\u5143\u6570\u91cf\u800c\u975e\u63d0\u5347\u51e0\u4f55\u4fdd\u771f\u5ea6\uff1b\u51f8\u9762\u6e85\u5c04\u76f8\u6bd4\u9ad8\u65af\u6e85\u5c04\u80fd\u4ea7\u751f\u66f4\u7d27\u51d1\u3001\u65e0\u6742\u4e71\u7684\u8868\u793a\uff0c\u66f4\u9002\u5408\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "conclusion": "\u5916\u89c2\u5d4c\u5165\u5728\u51e0\u4f55\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\u6709\u9650\uff0c\u7814\u7a76\u660e\u786e\u4e86\u7a7a\u95f4\u573a\u666f\u4e2d\u91cd\u5efa\u8d28\u91cf\u4e0e\u8868\u793a\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u592a\u7a7a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u6307\u5bfc\u3002"}}
{"id": "2509.10377", "pdf": "https://arxiv.org/pdf/2509.10377", "abs": "https://arxiv.org/abs/2509.10377", "authors": ["Yixiao Zhou", "Ziyu Zhao", "Dongzhou Cheng", "zhiliang wu", "Jie Gui", "Yi Yang", "Fei Wu", "Yu Cheng", "Hehe Fan"], "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs", "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025", "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.", "AI": {"tldr": "DERN\u662f\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4efb\u52a1\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u526a\u679d\u548c\u795e\u7ecf\u5143\u91cd\u7ec4\u6765\u51cf\u5c11SMoE\u6a21\u578b\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u572850%\u4e13\u5bb6\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u63d0\u53475%\u4ee5\u4e0a", "motivation": "SMoE\u67b6\u6784\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u4ecd\u9700\u52a0\u8f7d\u6240\u6709\u4e13\u5bb6\u53c2\u6570\uff0c\u5bfc\u81f4\u5185\u5b58\u4f7f\u7528\u9ad8\u548c\u90e8\u7f72\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e13\u5bb6\u7ea7\u64cd\u4f5c\uff0c\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u7ea7\u7ed3\u6784", "method": "DERN\u4e09\u6b65\u9aa4\uff1a1) \u4f7f\u7528\u8def\u7531\u7edf\u8ba1\u526a\u679d\u5197\u4f59\u4e13\u5bb6\uff1b2) \u5c06\u4e13\u5bb6\u5206\u89e3\u4e3a\u795e\u7ecf\u5143\u7ea7\u7247\u6bb5\u5e76\u5206\u914d\u5230\u6700\u517c\u5bb9\u7684\u4fdd\u7559\u4e13\u5bb6\uff1b3) \u5728\u4fdd\u7559\u4e13\u5bb6\u5185\u5408\u5e76\u7247\u6bb5\u6784\u5efa\u7d27\u51d1\u8868\u793a", "result": "\u5728Mixtral\u3001Qwen\u548cDeepSeek SMoE\u6a21\u578b\u4e0a\uff0c50%\u4e13\u5bb6\u7a00\u758f\u5ea6\u4e0b\u5e38\u8bc6\u63a8\u7406\u548cMMLU\u57fa\u51c6\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\uff0c\u663e\u8457\u51cf\u5c11\u4e13\u5bb6\u6570\u91cf\u548c\u5185\u5b58\u4f7f\u7528", "conclusion": "DERN\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u91cd\u7ec4\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u95f4\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0SMoE\u6a21\u578b\u7684\u9ad8\u6548\u538b\u7f29\u548c\u90e8\u7f72"}}
{"id": "2509.10250", "pdf": "https://arxiv.org/pdf/2509.10250", "abs": "https://arxiv.org/abs/2509.10250", "authors": ["Haozhen Yan", "Yan Hong", "Suning Lang", "Jiahui Zhan", "Yikun Ji", "Yujie Gao", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.", "AI": {"tldr": "GAMMA\u662f\u4e00\u4e2a\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u9886\u57df\u504f\u5dee\u548c\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\u6765\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728GenImage\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e865.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u5206\u5e03\u5185\u751f\u6210\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u672a\u89c1\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5b83\u4eec\u4f9d\u8d56\u751f\u6210\u7279\u5b9a\u7684\u4f2a\u5f71\uff08\u5982\u98ce\u683c\u5148\u9a8c\u548c\u538b\u7f29\u6a21\u5f0f\uff09\u3002", "method": "\u63d0\u51faGAMMA\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6837\u5316\u64cd\u4f5c\u7b56\u7565\uff08\u57fa\u4e8e\u4fee\u590d\u7684\u64cd\u4f5c\u548c\u8bed\u4e49\u4fdd\u6301\u6270\u52a8\uff09\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u76d1\u7763\uff08\u53cc\u5206\u5272\u5934\u548c\u5206\u7c7b\u5934\uff09\uff0c\u5e76\u5f15\u5165\u53cd\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8ba9\u5206\u5272\u5934\u6307\u5bfc\u5206\u7c7b\u5206\u652f\u3002", "result": "\u5728GenImage\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u51c6\u786e\u7387\u63d0\u53475.8%\uff0c\u5e76\u5bf9\u65b0\u53d1\u5e03\u7684\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4o\uff09\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "GAMMA\u901a\u8fc7\u51cf\u5c11\u9886\u57df\u504f\u5dee\u548c\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10414", "pdf": "https://arxiv.org/pdf/2509.10414", "abs": "https://arxiv.org/abs/2509.10414", "authors": ["Adrian de Wynter"], "title": "Is In-Context Learning Learning?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u786e\u5b9e\u6784\u6210\u5b66\u4e60\u673a\u5236\uff0c\u4f46\u5176\u5b66\u4e60\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u5bf9\u63d0\u793a\u683c\u5f0f\u548c\u5206\u5e03\u53d8\u5316\u654f\u611f\u3002", "motivation": "\u7814\u7a76\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u662f\u5426\u771f\u6b63\u6784\u6210\u5b66\u4e60\u8fc7\u7a0b\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u57fa\u4e8e\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u6f14\u7ece\u63a8\u7406\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u5176\u5b66\u4e60\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21ICL\u5206\u6790\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u63a7\u5236\u8bb0\u5fc6\u6548\u5e94\u3001\u9884\u8bad\u7ec3\u5f71\u54cd\u3001\u5206\u5e03\u504f\u79fb\u3001\u63d0\u793a\u98ce\u683c\u548c\u63aa\u8f9e\u7b49\u56e0\u7d20\uff0c\u8003\u5bdf\u4e0d\u540c\u6761\u4ef6\u4e0bICL\u7684\u8868\u73b0\u3002", "result": "ICL\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u4f46\u5728\u5b66\u4e60\u672a\u89c1\u4efb\u52a1\u548c\u6cdb\u5316\u65b9\u9762\u80fd\u529b\u6709\u9650\uff1b\u5f53\u793a\u4f8b\u6570\u91cf\u8db3\u591f\u591a\u65f6\uff0c\u51c6\u786e\u7387\u5bf9\u793a\u4f8b\u5206\u5e03\u3001\u6a21\u578b\u3001\u63d0\u793a\u98ce\u683c\u548c\u8bed\u8a00\u7279\u5f81\u4e0d\u654f\u611f\uff0c\u4e3b\u8981\u4ece\u63d0\u793a\u4e2d\u7684\u89c4\u5f8b\u6027\u63a8\u65ad\u6a21\u5f0f\u3002", "conclusion": "\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4e34\u65f6\u7f16\u7801\u673a\u5236\u4e0d\u591f\u9c81\u68d2\uff0c\u8868\u660e\u5176\u901a\u7528\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0cICL\u5bf9\u5206\u5e03\u53d8\u5316\uff08\u7279\u522b\u662f\u601d\u7ef4\u94fe\u63d0\u793a\uff09\u654f\u611f\uff0c\u5728\u5f62\u5f0f\u76f8\u4f3c\u4efb\u52a1\u4e0a\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002"}}
{"id": "2509.10257", "pdf": "https://arxiv.org/pdf/2509.10257", "abs": "https://arxiv.org/abs/2509.10257", "authors": ["Ema Masterl", "Tina Vipotnik Vesnaver", "\u017diga \u0160piclin"], "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI", "categories": ["cs.CV"], "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u80ce\u513f\u8111MRI\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5(NiftyMIC\u3001SVRTK\u3001NeSVoR)\u5728140\u4f8b\u626b\u63cf\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0NeSVoR\u91cd\u5efa\u6210\u529f\u7387\u6700\u9ad8(>90%)\uff0c\u867d\u7136\u4e0d\u540c\u65b9\u6cd5\u95f4\u4f53\u79ef\u6d4b\u91cf\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u5bf9\u8111\u5ba4\u6269\u5927\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u65e0\u5f71\u54cd\u3002", "motivation": "\u80ce\u513f\u8111MRI\u901a\u5e38\u91c7\u7528\u5feb\u901f\u591a\u89c6\u89d22D\u5207\u7247\u91c7\u96c6\u4ee5\u51cf\u5c11\u8fd0\u52a8\u4f2a\u5f71\uff0c\u4f46\u8fd9\u4e9b\u56fe\u50cf\u5206\u8fa8\u7387\u4f4e\u4e14\u53ef\u80fd\u53d7\u8fd0\u52a8\u5f71\u54cd\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u63493D\u89e3\u5256\u7ed3\u6784\u3002\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5\u7684\u6bd4\u8f83\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u75c5\u7406\u60c5\u51b5\u4e0b\uff0c\u4ee5\u53ca\u5bf9\u4e0b\u6e38\u4f53\u79ef\u5206\u6790\u548c\u8bca\u65ad\u4efb\u52a1\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5e94\u7528\u4e09\u79cd\u6700\u5148\u8fdb\u7684SRR\u65b9\u6cd5(NiftyMIC\u3001SVRTK\u3001NeSVoR)\u5904\u7406140\u4f8b\u80ce\u513f\u8111MRI\u626b\u63cf(\u5305\u62ec\u5065\u5eb7\u5bf9\u7167\u548c\u8111\u5ba4\u6269\u5927\u75c5\u7406\u75c5\u4f8b)\uff0c\u4f7f\u7528BoUNTi\u7b97\u6cd5\u5206\u5272\u91cd\u5efa\u540e\u7684\u9ad8\u5206\u8fa8\u7387\u4f53\u79ef\u4ee5\u63d0\u53d69\u4e2a\u4e3b\u8981\u8111\u7ed3\u6784\u4f53\u79ef\uff0c\u8bc4\u4f30\u89c6\u89c9\u8d28\u91cf\u3001\u91cd\u5efa\u6210\u529f\u7387\u3001\u4f53\u79ef\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u3002", "result": "NeSVoR\u5728\u5065\u5eb7\u7ec4\u548c\u75c5\u7406\u7ec4\u5747\u8868\u73b0\u51fa\u6700\u9ad8\u4e14\u6700\u4e00\u81f4\u7684\u91cd\u5efa\u6210\u529f\u7387(>90%)\u3002\u867d\u7136\u4e0d\u540cSRR\u65b9\u6cd5\u95f4\u7684\u4f53\u79ef\u4f30\u8ba1\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u8111\u5ba4\u6269\u5927\u7684\u5206\u7c7b\u6027\u80fd\u4e0d\u53d7SRR\u65b9\u6cd5\u9009\u62e9\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86NeSVoR\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5c3d\u7ba1SRR\u5f15\u8d77\u7684\u4f53\u79ef\u53d8\u5f02\u6027\uff0c\u8bca\u65ad\u6027\u80fd\u4ecd\u5177\u6709\u5f39\u6027\u3002"}}
{"id": "2509.10417", "pdf": "https://arxiv.org/pdf/2509.10417", "abs": "https://arxiv.org/abs/2509.10417", "authors": ["Christopher Ormerod", "Gitit Kehat"], "title": "Long Context Automated Essay Scoring with Language Models", "categories": ["cs.CL"], "comment": "8 pages, 2 figures, 2 tables", "summary": "Transformer-based language models are architecturally constrained to process\ntext of a fixed maximum length. Essays written by higher-grade students\nfrequently exceed the maximum allowed length for many popular open-source\nmodels. A common approach to addressing this issue when using these models for\nAutomated Essay Scoring is to truncate the input text. This raises serious\nvalidity concerns as it undermines the model's ability to fully capture and\nevaluate organizational elements of the scoring rubric, which requires long\ncontexts to assess. In this study, we evaluate several models that incorporate\narchitectural modifications of the standard transformer architecture to\novercome these length limitations using the Kaggle ASAP 2.0 dataset. The models\nconsidered in this study include fine-tuned versions of XLNet, Longformer,\nModernBERT, Mamba, and Llama models.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6539\u8fdb\u7684Transformer\u6a21\u578b\u6765\u5904\u7406\u8d85\u957f\u5b66\u751f\u4f5c\u6587\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u56e0\u957f\u5ea6\u9650\u5236\u800c\u9700\u8981\u622a\u65ad\u6587\u672c\u7684\u95ee\u9898\uff0c\u4f7f\u7528Kaggle ASAP 2.0\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\u3002", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u6709\u56fa\u5b9a\u957f\u5ea6\u9650\u5236\uff0c\u800c\u9ad8\u5e74\u7ea7\u5b66\u751f\u7684\u4f5c\u6587\u7ecf\u5e38\u8d85\u8fc7\u8fd9\u4e2a\u9650\u5236\u3002\u622a\u65ad\u6587\u672c\u4f1a\u4e25\u91cd\u5f71\u54cd\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5bf9\u7ec4\u7ec7\u7ed3\u6784\u7b49\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u7684\u8bc4\u5206\u6807\u51c6\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u6539\u8fdb\u67b6\u6784\u7684\u6a21\u578b\uff1aXLNet\u3001Longformer\u3001ModernBERT\u3001Mamba\u548cLlama\u6a21\u578b\u7684\u5fae\u8c03\u7248\u672c\uff0c\u4f7f\u7528Kaggle ASAP 2.0\u6570\u636e\u96c6\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u8fd9\u4e9b\u6539\u8fdb\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u4f5c\u6587\u8bc4\u5206\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4f46\u5177\u4f53\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u8bf4\u660e\u3002", "conclusion": "\u9700\u8981\u91c7\u7528\u80fd\u591f\u5904\u7406\u957f\u6587\u672c\u7684\u6539\u8fdb\u67b6\u6784\u6a21\u578b\u6765\u89e3\u51b3\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u7684\u957f\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u786e\u4fdd\u8bc4\u5206\u7cfb\u7edf\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u4f5c\u6587\u7684\u7ec4\u7ec7\u7ed3\u6784\u7b49\u957f\u4e0a\u4e0b\u6587\u76f8\u5173\u8981\u7d20\u3002"}}
{"id": "2509.10259", "pdf": "https://arxiv.org/pdf/2509.10259", "abs": "https://arxiv.org/abs/2509.10259", "authors": ["Hua Yuan", "Jin Yuan", "Yicheng Jiang", "Yao Zhang", "Xin Geng", "Yong Rui"], "title": "Mask Consistency Regularization in Object Removal", "categories": ["cs.CV"], "comment": null, "summary": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.", "AI": {"tldr": "\u63d0\u51faMask Consistency Regularization (MCR)\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u63a9\u7801\u6269\u5f20\u548c\u91cd\u5851\u6270\u52a8\u6765\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u4e2d\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u63a9\u7801\u5e7b\u89c9\u548c\u63a9\u7801\u5f62\u72b6\u504f\u5dee\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u9762\u4e34\u63a9\u7801\u5e7b\u89c9\uff08\u5728\u63a9\u7801\u533a\u57df\u751f\u6210\u65e0\u5173\u5185\u5bb9\uff09\u548c\u63a9\u7801\u5f62\u72b6\u504f\u5dee\uff08\u586b\u5145\u5185\u5bb9\u6a21\u4eff\u63a9\u7801\u5f62\u72b6\u800c\u975e\u5468\u56f4\u5185\u5bb9\uff09\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faMCR\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e24\u79cd\u63a9\u7801\u6270\u52a8\uff1a\u6269\u5f20\u548c\u91cd\u5851\uff0c\u5f3a\u5236\u8fd9\u4e9b\u6270\u52a8\u5206\u652f\u7684\u8f93\u51fa\u4e0e\u539f\u59cb\u63a9\u7801\u4fdd\u6301\u4e00\u81f4\u3002\u6269\u5f20\u63a9\u7801\u5e2e\u52a9\u5bf9\u9f50\u6a21\u578b\u8f93\u51fa\u4e0e\u5468\u56f4\u5185\u5bb9\uff0c\u91cd\u5851\u63a9\u7801\u9f13\u52b1\u6a21\u578b\u6253\u7834\u63a9\u7801\u5f62\u72b6\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCR\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u63a9\u7801\u5f62\u72b6\u504f\u5dee\uff0c\u5728\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "MCR\u901a\u8fc7\u63a9\u7801\u4e00\u81f4\u6027\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u7269\u4f53\u79fb\u9664\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u9c81\u68d2\u548c\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u56fe\u50cf\u4fee\u590d\u7ed3\u679c\u3002"}}
{"id": "2509.10436", "pdf": "https://arxiv.org/pdf/2509.10436", "abs": "https://arxiv.org/abs/2509.10436", "authors": ["Shadikur Rahman", "Aroosa Hameed", "Gautam Srivastava", "Syed Muhammad Danish"], "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment", "categories": ["cs.CL"], "comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing", "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e91\u8fb9\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u5305\u542bGuideLLM\u3001SolverLLM\u548cJudgeLLM\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u5728RefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523076.84%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u4e91\u8fb9\u534f\u4f5c\u67b6\u6784\uff0c\u5305\u542b\u8fb9\u7f18\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7GuideLLM\uff08\u63d0\u4f9b\u65b9\u6cd5\u6307\u5bfc\uff09\u3001\u4e91\u7aef\u90e8\u7f72\u7684\u5f3a\u5927SolverLLM\uff08\u751f\u6210\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\uff09\u548c\u81ea\u52a8\u8bc4\u4f30\u5668JudgeLLM\u3002", "result": "\u5fae\u8c03\u6a21\u578bRefactorCoder-MoE\u5728RefactorCoderQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523076.84%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u591a\u4e2a\u6280\u672f\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u7cfb\u7edf\u7ea7\u6307\u6807\u4e5f\u663e\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u7279\u5f81\u3002"}}
{"id": "2509.10260", "pdf": "https://arxiv.org/pdf/2509.10260", "abs": "https://arxiv.org/abs/2509.10260", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Yanbing Zeng", "Xiaoming Wei"], "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.", "AI": {"tldr": "MagicMirror\u662f\u4e00\u4e2a\u5168\u9762\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4eba\u5de5\u5236\u54c1\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u9996\u4e2a\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6MagicData340K\u3001\u57fa\u4e8eVLM\u7684MagicAssessor\u8bc4\u4f30\u6a21\u578b\u548c\u81ea\u52a8\u5316\u57fa\u51c6MagicBench\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9876\u7ea7T2I\u6a21\u578b\u4ecd\u5b58\u5728\u4e25\u91cd\u4eba\u5de5\u5236\u54c1\u95ee\u9898", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7269\u7406\u4eba\u5de5\u5236\u54c1\uff08\u5982\u89e3\u5256\u548c\u7ed3\u6784\u7f3a\u9677\uff09\u95ee\u9898\uff0c\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6", "method": "1)\u5efa\u7acb\u4eba\u5de5\u5236\u54c1\u8be6\u7ec6\u5206\u7c7b\u6cd5\uff1b2)\u4eba\u5de5\u6807\u6ce834\u4e07\u5f20\u751f\u6210\u56fe\u50cf\u7684MagicData340K\u6570\u636e\u96c6\uff1b3)\u8bad\u7ec3MagicAssessor\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff1b4)\u8bbe\u8ba1\u65b0\u9896\u6570\u636e\u91c7\u6837\u7b56\u7565\u548c\u591a\u7ea7\u5956\u52b1\u7cfb\u7edf\u7684GRPO\u65b9\u6cd5\uff1b5)\u6784\u5efaMagicBench\u81ea\u52a8\u5316\u57fa\u51c6", "result": "\u8bc4\u4f30\u53d1\u73b0\u5373\u4f7fGPT-image-1\u7b49\u9876\u7ea7\u6a21\u578b\u4e5f\u5b58\u5728\u663e\u8457\u4eba\u5de5\u5236\u54c1\u95ee\u9898\uff0c\u4eba\u5de5\u5236\u54c1\u51cf\u5c11\u662f\u672a\u6765T2I\u53d1\u5c55\u7684\u5173\u952e\u524d\u6cbf", "conclusion": "MagicMirror\u6846\u67b6\u586b\u8865\u4e86T2I\u751f\u6210\u4eba\u5de5\u5236\u54c1\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u57fa\u51c6"}}
{"id": "2509.10446", "pdf": "https://arxiv.org/pdf/2509.10446", "abs": "https://arxiv.org/abs/2509.10446", "authors": ["Rui Lu", "Zhenyu Hou", "Zihan Wang", "Hanchen Zhang", "Xiao Liu", "Yujiang Li", "Shi Feng", "Jie Tang", "Yuxiao Dong"], "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL", "categories": ["cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.", "AI": {"tldr": "DeepDive\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u590d\u6742\u95ee\u9898\u548c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5728BrowseComp\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u5f00\u6e90\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\u548c\u7f3a\u4e4f\u8db3\u591f\u96be\u5ea6\u7684\u76d1\u7763\u6570\u636e\u3002", "method": "1. \u4ece\u5f00\u653e\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u5408\u6210\u590d\u6742\u3001\u56f0\u96be\u4e14\u96be\u4ee5\u627e\u5230\u7684\u95ee\u9898\uff1b2. \u5e94\u7528\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6765\u589e\u5f3aLLM\u7684\u6df1\u5ea6\u641c\u7d22\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\u3002", "result": "DeepDive-32B\u5728BrowseComp\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86WebSailor\u3001DeepSeek-R1-Browse\u548cSearch-o1\uff0c\u591a\u8f6eRL\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u641c\u7d22\u80fd\u529b\uff0c\u5e76\u652f\u6301\u6d4b\u8bd5\u65f6\u7684\u5de5\u5177\u8c03\u7528\u6269\u5c55\u548c\u5e76\u884c\u91c7\u6837\u3002", "conclusion": "DeepDive\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u5408\u6210\u548c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u6e90LLM\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u597d\u7684\u6df1\u5ea6\u641c\u7d22\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.10266", "pdf": "https://arxiv.org/pdf/2509.10266", "abs": "https://arxiv.org/abs/2509.10266", "authors": ["Wenfang Wu", "Tingting Yuan", "Yupeng Li", "Daling Wang", "Xiaoming Fu"], "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.", "AI": {"tldr": "SignClip\u662f\u4e00\u4e2a\u65b0\u7684\u624b\u8bed\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u624b\u52a8\u548c\u975e\u624b\u52a8\u7ebf\u7d22\uff08\u7279\u522b\u662f\u624b\u52bf\u548c\u5507\u90e8\u8fd0\u52a8\u7279\u5f81\uff09\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6765\u63d0\u9ad8\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u624b\u52a8\u4fe1\u53f7\uff08\u624b\u52bf\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u975e\u624b\u52a8\u7ebf\u7d22\u5982\u53e3\u578b\u52a8\u4f5c\u3002\u5b9e\u9645\u4e0a\uff0c\u53e3\u578b\u5728\u624b\u8bed\u4e2d\u4f20\u9012\u91cd\u8981\u8bed\u8a00\u4fe1\u606f\uff0c\u5bf9\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u7684\u624b\u52bf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faSignClip\u6846\u67b6\uff0c\u878d\u5408\u7a7a\u95f4\u624b\u52bf\u548c\u5507\u90e8\u8fd0\u52a8\u7279\u5f81\uff1b\u5f15\u5165\u5206\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u591a\u7ea7\u5bf9\u9f50\u76ee\u6807\uff0c\u786e\u4fdd\u624b\u8bed-\u5507\u90e8\u548c\u89c6\u89c9-\u6587\u672c\u6a21\u6001\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6PHOENIX14T\u548cHow2Sign\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u5728PHOENIX14T\u7684\u65e0\u6ce8\u91ca\u8bbe\u7f6e\u4e0b\uff0cBLEU-4\u4ece24.32\u63d0\u5347\u523024.71\uff0cROUGE\u4ece46.57\u63d0\u5347\u523048.38\u3002", "conclusion": "SignClip\u901a\u8fc7\u6709\u6548\u878d\u5408\u624b\u52a8\u548c\u975e\u624b\u52a8\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u975e\u624b\u52a8\u7ebf\u7d22\u5728\u624b\u8bed\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.10452", "pdf": "https://arxiv.org/pdf/2509.10452", "abs": "https://arxiv.org/abs/2509.10452", "authors": ["Akshat Pandey", "Karun Kumar", "Raphael Tang"], "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers", "categories": ["cs.CL", "cs.LG"], "comment": "5 pages, 2 figures", "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios.", "AI": {"tldr": "WhisTLE\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u9886\u57df\u9002\u5e94\u7684\u6df1\u5ea6\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u7f16\u7801\u5668\u8f93\u51fa\u5e76\u5fae\u8c03\u89e3\u7801\u5668\uff0c\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u5373\u53ef\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5982Whisper\u5728\u5904\u7406\u672a\u89c1\u8bcd\u6c47\u548c\u65b9\u8a00\u65f6\u9700\u8981\u9886\u57df\u9002\u5e94\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u6536\u96c6\u8bed\u97f3\u6570\u636e\u5f80\u5f80\u4e0d\u73b0\u5b9e\uff0c\u56e0\u6b64\u9700\u8981\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWhisTLE\u65b9\u6cd5\uff1a1\uff09\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u4ece\u6587\u672c\u5efa\u6a21\u7f16\u7801\u5668\u8f93\u51fa\uff1b2\uff09\u4f7f\u7528\u5b66\u4e60\u7684\u6587\u672c\u5230\u6f5c\u5728\u7f16\u7801\u5668\u5fae\u8c03\u89e3\u7801\u5668\uff1b3\uff09\u53ef\u9009\u7ed3\u5408\u6587\u672c\u5230\u8bed\u97f3(TTS)\u9002\u5e94\uff1b4\uff09\u63a8\u7406\u65f6\u6062\u590d\u539f\u59cb\u7f16\u7801\u5668\uff0c\u65e0\u989d\u5916\u8fd0\u884c\u65f6\u6210\u672c\u3002", "result": "\u57284\u4e2a\u57df\u5916\u6570\u636e\u96c6\u548c4\u4e2aASR\u6a21\u578b\u4e0a\uff0cWhisTLE\u7ed3\u5408TTS\u76f8\u6bd4\u4ec5\u4f7f\u7528TTS\u9002\u5e94\u76f8\u5bf9\u964d\u4f4e\u8bcd\u9519\u8bef\u738712.3%\uff0c\u572832\u4e2a\u573a\u666f\u4e2d\u768427\u4e2a\u573a\u666f\u4e2d\u4f18\u4e8e\u6240\u6709\u975eWhisTLE\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WhisTLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6587\u672c-only\u9002\u5e94\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3ASR\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u5177\u6709\u5f88\u597d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10278", "pdf": "https://arxiv.org/pdf/2509.10278", "abs": "https://arxiv.org/abs/2509.10278", "authors": ["Vidit Vidit", "Pavel Korshunov", "Amir Mohammadi", "Christophe Ecabert", "Ketan Kotwal", "S\u00e9bastien Marcel"], "title": "Detecting Text Manipulation in Images using Vision Language Models", "categories": ["cs.CV"], "comment": "Accepted in Synthetic Realities and Biometric Security Workshop\n  BMVC-2025. For paper page see https://www.idiap.ch/paper/textvlmdet/", "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u65b9\u9762\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u6b63\u5728\u63a5\u8fd1\u4f46\u4ecd\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\u5982GPT-4o\uff0c\u5e76\u63ed\u793a\u4e86\u4e13\u95e8\u7528\u4e8e\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u7684VLM\u5728\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u4e2d\u5b58\u5728\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4f46\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u7684\u7814\u7a76\u76f8\u5bf9\u7f3a\u5931\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u6587\u672c\u7be1\u6539\u6570\u636e\u96c6\u4e0a\u5206\u6790\u95ed\u6e90\u548c\u5f00\u6e90VLMs\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5bf9\u91ce\u5916\u573a\u666f\u6587\u672c\u548c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6ee5\u7528\u7684\u5e7b\u60f3ID\u5361\u4e0a\u7684\u7be1\u6539\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u65b9\u9762\u6b63\u5728\u8fdb\u6b65\u4f46\u4ecd\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u7684VLM\u5728\u6587\u672c\u7be1\u6539\u4efb\u52a1\u4e0a\u5b58\u5728\u6cdb\u5316\u95ee\u9898\u3002", "conclusion": "\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u662fVLM\u80fd\u529b\u8bc4\u4f30\u7684\u91cd\u8981\u7ef4\u5ea6\uff0c\u5f00\u6e90\u6a21\u578b\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u8fbe\u5230\u95ed\u6e90\u6a21\u578b\u7684\u6c34\u5e73\uff0c\u4e14\u4e13\u95e8\u5316\u6a21\u578b\u9700\u8981\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.09177", "pdf": "https://arxiv.org/pdf/2509.09177", "abs": "https://arxiv.org/abs/2509.09177", "authors": ["Hanyi Mao", "Quanjia Xiao", "Lei Pang", "Haixiao Liu"], "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping\ndirectly in the importance-sampling (IS) weight space. We revisit\nsequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping\nis transplanted to sequences: a fixed clip range systematically reweights short\nvs. long responses, distorting the effective objective. Theoretically, we\nformalize length fairness via a Length Reweighting Error (LRE) and prove that\nsmall LRE yields a directional cosine guarantee between the clipped and true\nupdates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the\nsequence log-IS ratio with a band that applies a KL-corrected drift term and\nscales as $\\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,\nstabilizes training, and outperforms all baselines across multiple evaluation\ndatasets.", "AI": {"tldr": "FSPO\u662f\u4e00\u79cd\u5e8f\u5217\u7ea7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u7a7a\u95f4\u4e2d\u5b9e\u65bd\u957f\u5ea6\u516c\u5e73\u526a\u88c1\u6765\u89e3\u51b3PPO/GRPO\u65b9\u6cd5\u5728\u5e8f\u5217\u957f\u5ea6\u4e0a\u7684\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u7ea7RL\u65b9\u6cd5\u5728\u79fb\u690dPPO/GRPO\u5f0f\u526a\u88c1\u65f6\u5b58\u5728\u56fa\u5b9a\u526a\u88c1\u8303\u56f4\u5bf9\u957f\u77ed\u54cd\u5e94\u7cfb\u7edf\u6027\u91cd\u52a0\u6743\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6709\u6548\u76ee\u6807\u5931\u771f\u3002", "method": "\u63d0\u51faFSPO\u65b9\u6cd5\uff0c\u91c7\u7528\u9ad8\u65af\u542f\u53d1\u5f0f\u89e3\u51b3\u65b9\u6848\uff1a\u7528KL\u6821\u6b63\u6f02\u79fb\u9879\u548c\u221aL\u7f29\u653e\u7684\u5e26\u5bf9\u5e8f\u5217\u5bf9\u6570IS\u6bd4\u7387\u8fdb\u884c\u526a\u88c1\u3002", "result": "FSPO\u5728\u4e0d\u540c\u957f\u5ea6\u533a\u95f4\u5185\u5e73\u5766\u5316\u526a\u88c1\u7387\uff0c\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FSPO\u901a\u8fc7\u7406\u8bba\u5f62\u5f0f\u5316\u7684\u957f\u5ea6\u516c\u5e73\u6027\u6982\u5ff5\u548c\u5b9e\u9645\u6709\u6548\u7684\u526a\u88c1\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5e8f\u5217\u7ea7RL\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.10282", "pdf": "https://arxiv.org/pdf/2509.10282", "abs": "https://arxiv.org/abs/2509.10282", "authors": ["Gang Li", "Tianjiao Chen", "Mingle Zhou", "Min Li", "Delong Han", "Jin Wan"], "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Page 14, 5 pictures", "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.", "AI": {"tldr": "MCL-AD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u3001RGB\u56fe\u50cf\u548c\u6587\u672c\u8bed\u4e49\u7684\u591a\u6a21\u6001\u534f\u4f5c\u5b66\u4e60\u5b9e\u73b0\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u70b9\u4e91\u6570\u636e\uff0c\u5ffd\u7565\u4e86RGB\u56fe\u50cf\u548c\u6587\u672c\u5148\u9a8c\u7b49\u4e92\u8865\u6a21\u6001\u63d0\u4f9b\u7684\u4e30\u5bcc\u8bed\u4e49\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u673a\u5236\uff08MPLM\uff09\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\u80fd\u529b\u548c\u6a21\u6001\u95f4\u534f\u4f5c\u5b66\u4e60\uff0c\u4ee5\u53ca\u534f\u4f5c\u8c03\u5236\u673a\u5236\uff08CMM\uff09\u5145\u5206\u5229\u7528\u70b9\u4e91\u548cRGB\u56fe\u50cf\u7684\u4e92\u8865\u8868\u793a\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eMCL-AD\u6846\u67b6\u5728\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u534f\u4f5c\u5b66\u4e60\u80fd\u591f\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u591a\u79cd\u6a21\u6001\u4fe1\u606f\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.09681", "pdf": "https://arxiv.org/pdf/2509.09681", "abs": "https://arxiv.org/abs/2509.09681", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "title": "DB3 Team's Solution For Meta KDD Cup' 25", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents the db3 team's winning solution for the Meta CRAG-MM\nChallenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,\nmulti-turn question answering benchmark (CRAG-MM), we developed a comprehensive\nframework that integrates tailored retrieval pipelines for different tasks with\na unified LLM-tuning approach for hallucination control. Our solution features\n(1) domain-specific retrieval pipelines handling image-indexed knowledge\ngraphs, web sources, and multi-turn conversations; and (2) advanced refusal\ntraining using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd\nplace in Task 2, and 1st place in Task 3, securing the grand prize for\nexcellence in ego-centric queries through superior handling of first-person\nperspective challenges.", "AI": {"tldr": "db3\u56e2\u961f\u5728KDD Cup'25 Meta CRAG-MM\u6311\u6218\u8d5b\u4e2d\u83b7\u80dc\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u7ba1\u9053\u548cLLM\u5e7b\u89c9\u63a7\u5236\u6280\u672f\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5206\u522b\u83b7\u5f97\u7b2c2\u3001\u7b2c2\u548c\u7b2c1\u540d\u3002", "motivation": "\u89e3\u51b3CRAG-MM\u6311\u6218\u8d5b\u4e2d\u7684\u591a\u6a21\u6001\u3001\u591a\u8f6e\u95ee\u7b54\u95ee\u9898\uff0c\u9700\u8981\u5904\u7406\u56fe\u50cf\u7d22\u5f15\u77e5\u8bc6\u56fe\u8c31\u3001\u7f51\u7edc\u8d44\u6e90\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u63a7\u5236LLM\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u9886\u57df\u7279\u5b9a\u7684\u68c0\u7d22\u7ba1\u9053\uff08\u5904\u7406\u56fe\u50cf\u77e5\u8bc6\u56fe\u8c31\u3001\u7f51\u7edc\u8d44\u6e90\u548c\u591a\u8f6e\u5bf9\u8bdd\uff09\u548c\u5148\u8fdb\u7684\u62d2\u7edd\u8bad\u7ec3\u6280\u672f\uff08SFT\u3001DPO\u548cRL\uff09\u3002", "result": "\u5728Task 1\u548cTask 2\u4e2d\u83b7\u5f97\u7b2c2\u540d\uff0c\u5728Task 3\u4e2d\u83b7\u5f97\u7b2c1\u540d\uff0c\u6700\u7ec8\u8d62\u5f97\u603b\u51a0\u519b\uff0c\u7279\u522b\u5728\u81ea\u6211\u4e2d\u5fc3\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7efc\u5408\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u548cLLM\u8c03\u4f18\uff0c\u6709\u6548\u63a7\u5236\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u590d\u6742\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u6210\u679c\u3002"}}
{"id": "2509.10298", "pdf": "https://arxiv.org/pdf/2509.10298", "abs": "https://arxiv.org/abs/2509.10298", "authors": ["Laith Nayal", "Mahmoud Mousatat", "Bader Rasheed"], "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks", "categories": ["cs.CV"], "comment": "8 pages, 2 tables", "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLipschitz\u5f15\u5bfc\u7684\u968f\u673a\u6df1\u5ea6(DropPath)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u76f8\u5173\u7684\u4e22\u5f03\u6982\u7387\u6765\u63a7\u5236\u7f51\u7edc\u7684\u6709\u6548Lipschitz\u5e38\u6570\uff0c\u5728\u4fdd\u6301\u6e05\u6d01\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548cVision Transformers\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5bf9\u6297\u6270\u52a8\u9ad8\u5ea6\u8106\u5f31\uff0c\u6807\u51c6\u9632\u5fa1\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "method": "Lipschitz\u5f15\u5bfc\u7684\u968f\u673a\u6df1\u5ea6\u65b9\u6cd5\uff0c\u4e22\u5f03\u6982\u7387\u968f\u6df1\u5ea6\u589e\u52a0\u800c\u589e\u52a0\uff0c\u4ee5\u63a7\u5236\u7f51\u7edc\u7684\u6709\u6548Lipschitz\u5e38\u6570\uff0c\u6b63\u5219\u5316\u66f4\u6df1\u5c42\u3002", "result": "\u5728CIFAR-10\u548cViT-Tiny\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u7684\u6e05\u6d01\u7cbe\u5ea6\uff0c\u5728FGSM\u3001PGD-20\u548cAutoAttack\u4e0b\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11FLOPs\u3002", "conclusion": "\u6df1\u5ea6\u76f8\u5173\u7684DropPath\u8c03\u5ea6\u662f\u4e00\u79cd\u6709\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.09684", "pdf": "https://arxiv.org/pdf/2509.09684", "abs": "https://arxiv.org/abs/2509.09684", "authors": ["Bruno Yui Yamate", "Thais Rodrigues Neubauer", "Marcelo Fantinato", "Sarajane Marques Peres"], "title": "Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": "33 pages", "summary": "This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)\nbenchmark dataset designed for the text-to-SQL task in the process mining\ndomain. Text-to-SQL conversion facilitates natural language querying of\ndatabases, increasing accessibility for users without SQL expertise and\nproductivity for those that are experts. The text-2-SQL-4-PM dataset is\ncustomized to address the unique challenges of process mining, including\nspecialized vocabularies and single-table relational structures derived from\nevent logs. The dataset comprises 1,655 natural language utterances, including\nhuman-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods\ninclude manual curation by experts, professional translations, and a detailed\nannotation process to enable nuanced analyses of task complexity. Additionally,\na baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility\nof the dataset for text-to-SQL applications. The results show that\ntext-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering\nbroader applicability for semantic parsing and other natural language\nprocessing tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86text-2-SQL-4-PM\uff0c\u4e00\u4e2a\u7528\u4e8e\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u6587\u672c\u5230SQL\u4efb\u52a1\u7684\u53cc\u8bed\uff08\u8461\u8404\u7259\u8bed-\u82f1\u8bed\uff09\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,655\u4e2a\u81ea\u7136\u8bed\u8a00\u8bed\u53e5\u548c205\u4e2aSQL\u8bed\u53e5\uff0c\u5e76\u8fdb\u884c\u4e86\u57fa\u7ebf\u7814\u7a76\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u4e2d\u6587\u672c\u5230SQL\u8f6c\u6362\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u4e13\u4e1a\u8bcd\u6c47\u548c\u57fa\u4e8e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u5355\u8868\u5173\u7cfb\u7ed3\u6784\uff0c\u63d0\u9ad8\u975eSQL\u4e13\u5bb6\u7528\u6237\u7684\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u548c\u4e13\u5bb6\u7528\u6237\u7684\u751f\u4ea7\u529b\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u624b\u52a8\u6574\u7406\u3001\u4e13\u4e1a\u7ffb\u8bd1\u548c\u8be6\u7ec6\u6807\u6ce8\u8fc7\u7a0b\u7684\u65b9\u6cd5\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528GPT-3.5 Turbo\u8fdb\u884c\u57fa\u7ebf\u7814\u7a76\u6765\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u3002", "result": "text-2-SQL-4-PM\u6570\u636e\u96c6\u652f\u6301\u6587\u672c\u5230SQL\u5b9e\u73b0\u7684\u8bc4\u4f30\uff0c\u5e76\u5728\u8bed\u4e49\u89e3\u6790\u548c\u5176\u4ed6\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u7684\u6587\u672c\u5230SQL\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u57fa\u51c6\u5de5\u5177\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u6570\u636e\u5e93\u67e5\u8be2\u81ea\u7136\u8bed\u8a00\u5316\u65b9\u9762\u7684\u4ef7\u503c\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.10310", "pdf": "https://arxiv.org/pdf/2509.10310", "abs": "https://arxiv.org/abs/2509.10310", "authors": ["Evan Murphy", "Marco Viola", "Vladimir A. Krylov"], "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments", "categories": ["cs.CV"], "comment": "Accepted for publication in the Proceedings of the 27th Irish Machine\n  Vision and Image Processing Conference (IMVIP 2025)", "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u5730\u56fe\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u5b9a\u4f4d\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u8857\u9053\u5bb6\u5177\uff0c\u901a\u8fc7\u968f\u673a\u751f\u6b7b\u4f18\u5316\u7b97\u6cd5\u6574\u5408\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\uff0c\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u73af\u5883\u4e2d\u8857\u9053\u5bb6\u5177\u7684\u7cbe\u786e\u5b9a\u4f4d\u95ee\u9898\uff0c\u8fd9\u5bf9\u516c\u5171\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u76d1\u63a7\u548c\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u5730\u65b9\u5f53\u5c40\u548c\u79c1\u4eba\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u5730\u56fe\u7684\u6982\u7387\u6846\u67b6\uff0c\u5c06\u7a7a\u95f4\u4f4d\u7f6e\u53ef\u80fd\u6027\u7f16\u7801\u4e3a\u5730\u56fe\u683c\u5f0f\uff0c\u6574\u5408GIS\u56fe\u5c42\u3001\u9053\u8def\u5730\u56fe\u7b49\u5916\u90e8\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\uff0c\u91c7\u7528\u968f\u673a\u751f\u6b7b\u4f18\u5316\u7b97\u6cd5\u63a8\u65ad\u6700\u53ef\u80fd\u7684\u8d44\u4ea7\u914d\u7f6e\u3002", "result": "\u901a\u8fc7\u5728\u90fd\u67cf\u6797\u5e02\u4e2d\u5fc3\u8857\u706f\u57fa\u7840\u8bbe\u65bd\u7684\u5730\u7406\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u771f\u5b9e\u6a21\u62df\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u548c\u51c6\u786e\u7684\u57ce\u5e02\u8d44\u4ea7\u6620\u5c04\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u57ce\u5e02\u8857\u9053\u5bb6\u5177\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u7b97\u6cd5\u5b9e\u73b0\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2509.09688", "pdf": "https://arxiv.org/pdf/2509.09688", "abs": "https://arxiv.org/abs/2509.09688", "authors": ["Mohammad Atif", "Vincent Garonne", "Eric Lancon", "Jerome Lauret", "Alexandr Prozorov", "Michal Vranovsky"], "title": "AI-Powered Assistant for Long-Term Access to RHIC Knowledge", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National\nLaboratory concludes 25 years of operation, preserving not only its vast data\nholdings ($\\sim$1 ExaByte) but also the embedded scientific knowledge becomes a\ncritical priority. The RHIC Data and Analysis Preservation Plan (DAPP)\nintroduces an AI-powered assistant system that provides natural language access\nto documentation, workflows, and software, with the aim of supporting\nreproducibility, education, and future discovery. Built upon Large Language\nModels using Retrieval-Augmented Generation and the Model Context Protocol,\nthis assistant indexes structured and unstructured content from RHIC\nexperiments and enables domain-adapted interaction. We report on the\ndeployment, computational performance, ongoing multi-experiment integration,\nand architectural features designed for a sustainable and explainable long-term\nAI access. Our experience illustrates how modern AI/ML tools can transform the\nusability and discoverability of scientific legacy data.", "AI": {"tldr": "RHIC\u63a8\u51faAI\u52a9\u624b\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u4e3a\u5927\u578b\u79d1\u5b66\u5b9e\u9a8c\u6570\u636e\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u8bbf\u95ee\uff0c\u652f\u6301\u6570\u636e\u4fdd\u5b58\u548c\u79d1\u5b66\u77e5\u8bc6\u4f20\u627f\u3002", "motivation": "\u968f\u7740RHIC\u8fd0\u884c25\u5e74\u7ed3\u675f\uff0c\u4fdd\u5b58\u5176\u6d77\u91cf\u6570\u636e\uff08\u7ea61EB\uff09\u548c\u5d4c\u5165\u7684\u79d1\u5b66\u77e5\u8bc6\u6210\u4e3a\u5173\u952e\u4f18\u5148\u4e8b\u9879\uff0c\u9700\u8981\u652f\u6301\u53ef\u91cd\u590d\u6027\u3001\u6559\u80b2\u548c\u672a\u6765\u53d1\u73b0\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u7d22\u5f15RHIC\u5b9e\u9a8c\u7684\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff0c\u5b9e\u73b0\u9886\u57df\u9002\u5e94\u7684\u4ea4\u4e92\u3002", "result": "\u7cfb\u7edf\u5df2\u90e8\u7f72\uff0c\u62a5\u544a\u4e86\u8ba1\u7b97\u6027\u80fd\u3001\u591a\u5b9e\u9a8c\u96c6\u6210\u8fdb\u5c55\uff0c\u4ee5\u53ca\u4e3a\u53ef\u6301\u7eed\u548c\u53ef\u89e3\u91ca\u7684\u957f\u671fAI\u8bbf\u95ee\u8bbe\u8ba1\u7684\u67b6\u6784\u7279\u6027\u3002", "conclusion": "\u73b0\u4ee3AI/ML\u5de5\u5177\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u79d1\u5b66\u9057\u4ea7\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u53ef\u53d1\u73b0\u6027\uff0c\u4e3a\u5927\u578b\u79d1\u5b66\u5b9e\u9a8c\u7684\u6570\u636e\u4fdd\u5b58\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10312", "pdf": "https://arxiv.org/pdf/2509.10312", "abs": "https://arxiv.org/abs/2509.10312", "authors": ["Zhixin Zheng", "Xinyu Wang", "Chang Zou", "Shaobo Wang", "Linfeng Zhang"], "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching", "categories": ["cs.CV"], "comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration", "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.", "AI": {"tldr": "ClusCa\u901a\u8fc7\u7a7a\u95f4\u805a\u7c7b\u548c\u7279\u5f81\u7f13\u5b58\u6280\u672f\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b04.96\u500d\u52a0\u901f\u4e14\u56fe\u50cf\u8d28\u91cf\u63d0\u53470.51%\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u4ec5\u5229\u7528\u65f6\u95f4\u7ef4\u5ea6\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u7ef4\u5ea6\u7684\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u5bf9\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684token\u8fdb\u884c\u7a7a\u95f4\u805a\u7c7b\uff0c\u6bcf\u4e2a\u805a\u7c7b\u53ea\u8ba1\u7b97\u4e00\u4e2a\u4ee3\u8868\u6027token\uff0c\u7136\u540e\u5c06\u4fe1\u606f\u4f20\u64ad\u7ed9\u805a\u7c7b\u5185\u6240\u6709\u5176\u4ed6token\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u7684token\u8ba1\u7b97\u91cf\u3002", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0cFLUX\u5b9e\u73b04.96\u500d\u52a0\u901f\uff0cImageReward\u8fbe\u523099.49%\uff08\u6bd4\u539f\u6a21\u578b\u63d0\u53470.51%\uff09\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u76f4\u63a5\u5e94\u7528\u3002", "conclusion": "ClusCa\u4f5c\u4e3a\u6b63\u4ea4\u4e92\u8865\u7684\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u805a\u7c7b\u6709\u6548\u5229\u7528\u7a7a\u95f4\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u53d8\u6362\u5668\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2509.09689", "pdf": "https://arxiv.org/pdf/2509.09689", "abs": "https://arxiv.org/abs/2509.09689", "authors": ["Himanshu Thakur", "Eshani Agrawal", "Smruthi Mukund"], "title": "Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "A long-standing challenge in developing accurate recommendation models is\nsimulating user behavior, mainly due to the complex and stochastic nature of\nuser interactions. Towards this, one promising line of work has been the use of\nLarge Language Models (LLMs) for simulating user behavior. However, aligning\nthese general-purpose large pre-trained models with user preferences\nnecessitates: (i) effectively and continously parsing large-scale tabular\nuser-item interaction data, (ii) overcoming pre-training-induced inductive\nbiases to accurately learn user specific knowledge, and (iii) achieving the\nformer two at scale for millions of users. While most previous works have\nfocused on complex methods to prompt an LLM or fine-tune it on tabular\ninteraction datasets, our approach shifts the focus to extracting robust\ntextual user representations using a frozen LLM and simulating cost-effective,\nresource-efficient user agents powered by fine-tuned Small Language Models\n(SLMs). Further, we showcase a method for training multiple low-rank adapters\nfor groups of users or \\textit{persona}, striking an optimal balance between\nscalability and performance of user behavior agents. Our experiments provide\ncompelling empirical evidence of the efficacy of our methods, demonstrating\nthat user agents developed using our approach have the potential to bridge the\ngap between offline metrics and real-world performance of recommender systems.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7528\u6237\u6587\u672c\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\u6765\u6784\u5efa\u9ad8\u6548\u7684\u7528\u6237\u884c\u4e3a\u4ee3\u7406\uff0c\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u884c\u4e3a\u6a21\u62df\u7684\u6311\u6218", "motivation": "\u89e3\u51b3\u7528\u6237\u884c\u4e3a\u6a21\u62df\u7684\u590d\u6742\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5904\u7406\u5927\u89c4\u6a21\u8868\u683c\u6570\u636e\u3001\u514b\u670d\u9884\u8bad\u7ec3\u504f\u89c1\uff0c\u5e76\u5728\u767e\u4e07\u7ea7\u7528\u6237\u89c4\u6a21\u4e0a\u5b9e\u73b0\u6709\u6548\u6a21\u62df", "method": "\u4f7f\u7528\u51bb\u7ed3LLM\u63d0\u53d6\u9c81\u68d2\u6587\u672c\u7528\u6237\u8868\u793a\uff0c\u901a\u8fc7\u5fae\u8c03SLMs\u6784\u5efa\u8d44\u6e90\u9ad8\u6548\u7684\u7528\u6237\u4ee3\u7406\uff0c\u91c7\u7528\u591a\u7ec4\u4f4e\u79e9\u9002\u914d\u5668\u8bad\u7ec3\u7528\u6237\u7fa4\u4f53persona", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5f25\u5408\u79bb\u7ebf\u6307\u6807\u4e0e\u771f\u5b9e\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u6700\u4f18\u5e73\u8861\uff0c\u4e3a\u7528\u6237\u884c\u4e3a\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10334", "pdf": "https://arxiv.org/pdf/2509.10334", "abs": "https://arxiv.org/abs/2509.10334", "authors": ["Jordan Sassoon", "Michal Szczepanski", "Martyna Poreba"], "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.", "AI": {"tldr": "I-Segmenter\u662f\u9996\u4e2a\u5b8c\u5168\u6574\u6570\u5316\u7684ViT\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u66ff\u6362\u6d6e\u70b9\u8fd0\u7b97\u4e3a\u6574\u6570\u8fd0\u7b97\uff0c\u5728\u4fdd\u6301\u5408\u7406\u7cbe\u5ea6\u635f\u5931\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387", "motivation": "Vision Transformers\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u53d7\u9650\uff0c\u91cf\u5316\u867d\u7136\u80fd\u63d0\u9ad8\u6548\u7387\u4f46ViT\u5206\u5272\u6a21\u578b\u5728\u4f4e\u7cbe\u5ea6\u4e0b\u8868\u73b0\u8106\u5f31", "method": "\u57fa\u4e8eSegmenter\u67b6\u6784\uff0c\u7cfb\u7edf\u66ff\u6362\u6d6e\u70b9\u8fd0\u7b97\u4e3a\u6574\u6570\u8fd0\u7b97\uff1b\u63d0\u51fa\u03bb-ShiftGELU\u6fc0\u6d3b\u51fd\u6570\u5904\u7406\u957f\u5c3e\u5206\u5e03\uff1b\u79fb\u9664L2\u5f52\u4e00\u5316\u5c42\uff1b\u7528\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u66ff\u6362\u53cc\u7ebf\u6027\u63d2\u503c", "result": "\u5728FP32\u57fa\u7ebf5.1%\u7cbe\u5ea6\u635f\u5931\u5185\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c113.8\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.2\u500d\uff1b\u5355\u5f20\u6821\u51c6\u56fe\u50cf\u7684PTQ\u4e5f\u80fd\u83b7\u5f97\u7ade\u4e89\u6027\u7cbe\u5ea6", "conclusion": "I-Segmenter\u4e3aViT\u5206\u5272\u6a21\u578b\u7684\u73b0\u5b9e\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861"}}
{"id": "2509.09706", "pdf": "https://arxiv.org/pdf/2509.09706", "abs": "https://arxiv.org/abs/2509.09706", "authors": ["Taniya Gidatkar", "Oluwaseun Ajao", "Matthew Shardlow"], "title": "Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks", "categories": ["cs.CR", "cs.AI", "cs.CL", "I.2; H.3.3"], "comment": "8 pages, 4 tables, to appear in proceedings of Recent Advances in\n  Natural Language Processing (RANLP 2025) and ACL Anthology", "summary": "This study evaluates the resilience of large language models (LLMs) against\nadversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.\nUsing systematically designed adversarial tests through TextFooler and\nBERTAttack, we found significant variations in model robustness. RoBERTa-Base\nand FlanT5 demonstrated remarkable resilience, maintaining accuracy even when\nsubjected to sophisticated attacks, with attack success rates of 0%. In\ncontrast. BERT-Base showed considerable vulnerability, with TextFooler\nachieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.\nOur research reveals that while certain LLMs have developed effective defensive\nmechanisms, these safeguards often require substantial computational resources.\nThis study contributes to the understanding of LLM security by identifying\nexisting strengths and weaknesses in current safeguarding approaches and\nproposes practical recommendations for developing more efficient and effective\ndefensive strategies.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u97e7\u6027\uff0c\u53d1\u73b0RoBERTa-Base\u548cFlanT5\u8868\u73b0\u4f18\u5f02\uff08\u653b\u51fb\u6210\u529f\u73870%\uff09\uff0c\u800cBERT-Base\u6613\u53d7\u653b\u51fb\uff08TextFooler\u6210\u529f\u738793.75%\uff09\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u653b\u51fb\u65f6\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8bc6\u522b\u73b0\u6709\u9632\u62a4\u673a\u5236\u7684\u4f18\u7f3a\u70b9\u3002", "method": "\u4f7f\u7528TextFooler\u548cBERTAttack\u5bf9Flan-T5\u3001BERT\u548cRoBERTa-Base\u8fdb\u884c\u7cfb\u7edf\u6027\u5bf9\u6297\u6d4b\u8bd5\u3002", "result": "RoBERTa-Base\u548cFlanT5\u5c55\u73b0\u51fa\u5353\u8d8a\u97e7\u6027\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e3a0%\uff1bBERT-Base\u51c6\u786e\u7387\u4ece48%\u964d\u81f33%\uff0cTextFooler\u653b\u51fb\u6210\u529f\u7387\u8fbe93.75%\u3002", "conclusion": "\u67d0\u4e9bLLM\u5df2\u5177\u5907\u6709\u6548\u9632\u5fa1\u673a\u5236\u4f46\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u5b9e\u8df5\u5efa\u8bae\u3002"}}
{"id": "2509.10341", "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "categories": ["cs.CV"], "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.", "AI": {"tldr": "GARD\u662f\u4e00\u79cd\u57fa\u4e8e\u4f3d\u9a6c\u6269\u6563\u6a21\u578b\u7684OCT\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u51cf\u5c11\u4fdd\u771f\u9879\u548c\u52a0\u901f\u63a8\u7406\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u6709\u6548\u53bb\u9664\u6563\u6591\u566a\u58f0\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "OCT\u56fe\u50cf\u53d7\u6563\u6591\u566a\u58f0\u5f71\u54cd\u4e25\u91cd\uff0c\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u96be\u4ee5\u5728\u566a\u58f0\u53bb\u9664\u548c\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9700\u8981\u66f4\u51c6\u786e\u5730\u6a21\u62df\u6563\u6591\u7edf\u8ba1\u7279\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGARD\u65b9\u6cd5\uff1a\u4f7f\u7528Denoising Diffusion Gamma Model\u66ff\u4ee3\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\uff0c\u5f15\u5165Noise-Reduced Fidelity Term\u5229\u7528\u9884\u5904\u7406\u56fe\u50cf\u6307\u5bfc\u53bb\u566a\uff0c\u91c7\u7528DDIM\u6846\u67b6\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728\u914d\u5bf9\u566a\u58f0-\u4f4e\u566a\u58f0OCT B\u626b\u63cf\u6570\u636e\u96c6\u4e0a\uff0cGARD\u5728PSNR\u3001SSIM\u548cMSE\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9a\u6027\u7ed3\u679c\u663e\u793a\u8fb9\u7f18\u66f4\u6e05\u6670\u3001\u89e3\u5256\u7ec6\u8282\u4fdd\u6301\u66f4\u597d\u3002", "conclusion": "GARD\u901a\u8fc7\u4f3d\u9a6c\u6269\u6563\u6a21\u578b\u548c\u566a\u58f0\u51cf\u5c11\u4fdd\u771f\u9879\uff0c\u6709\u6548\u89e3\u51b3\u4e86OCT\u56fe\u50cf\u53bb\u566a\u4e2d\u566a\u58f0\u53bb\u9664\u4e0e\u7ed3\u6784\u4fdd\u6301\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2509.09707", "pdf": "https://arxiv.org/pdf/2509.09707", "abs": "https://arxiv.org/abs/2509.09707", "authors": ["Camilo Chac\u00f3n Sartori", "Mart\u00edn Isla Pino", "Pedro Pinacho-Davidson", "Christian Blum"], "title": "LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm", "categories": ["cs.NE", "cs.AI", "cs.CL"], "comment": "Submitted to a journal for review", "summary": "Integrating Large Language Models (LLMs) within metaheuristics opens a novel\npath for solving complex combinatorial optimization problems. While most\nexisting approaches leverage LLMs for code generation to create or refine\nspecific heuristics, they often overlook the structural properties of\nindividual problem instances. In this work, we introduce a novel framework that\nintegrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the\nNP-hard Longest Run Subsequence problem. Our approach extends the\ninstance-driven heuristic bias paradigm by introducing a human-LLM\ncollaborative process to co-design and implement a set of computationally\nefficient metrics. The LLM analyzes these instance-specific metrics to generate\na tailored heuristic bias, which steers the BRKGA toward promising areas of the\nsearch space. We conduct a comprehensive experimental evaluation, including\nrigorous statistical tests, convergence and behavioral analyses, and targeted\nablation studies, comparing our method against a standard BRKGA baseline across\n1,050 generated instances of varying complexity. Results show that our\ntop-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically\nsignificant improvements over the baseline, particularly on the most complex\ninstances. Our findings confirm that leveraging an LLM to produce an a priori,\ninstance-driven heuristic bias is a valuable approach for enhancing\nmetaheuristics in complex optimization domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4e0e\u504f\u7f6e\u968f\u673a\u5bc6\u94a5\u9057\u4f20\u7b97\u6cd5(BRKGA)\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u957f\u8fd0\u884c\u5b50\u5e8f\u5217\u95ee\u9898\uff0c\u901a\u8fc7LLM\u5206\u6790\u5b9e\u4f8b\u7279\u5b9a\u6307\u6807\u751f\u6210\u5b9a\u5236\u542f\u53d1\u5f0f\u504f\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5229\u7528LLM\u8fdb\u884c\u4ee3\u7801\u751f\u6210\u6765\u521b\u5efa\u6216\u6539\u8fdb\u7279\u5b9a\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u5355\u4e2a\u95ee\u9898\u5b9e\u4f8b\u7684\u7ed3\u6784\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651\u5b9e\u4f8b\u7279\u5b9a\u7279\u5f81\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4eba\u7c7b-LLM\u534f\u4f5c\u8fc7\u7a0b\u5171\u540c\u8bbe\u8ba1\u548c\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u6307\u6807\uff0cLLM\u5206\u6790\u8fd9\u4e9b\u5b9e\u4f8b\u7279\u5b9a\u6307\u6807\u751f\u6210\u5b9a\u5236\u542f\u53d1\u5f0f\u504f\u7f6e\uff0c\u6307\u5bfcBRKGA\u7b97\u6cd5\u5728\u641c\u7d22\u7a7a\u95f4\u4e2d\u671d\u5411\u6709\u5e0c\u671b\u7684\u533a\u57df\u3002", "result": "\u57281,050\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u5b9e\u4f8b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u4f73\u6df7\u5408\u65b9\u6cd5BRKGA+Llama-4-Maverick\u76f8\u6bd4\u6807\u51c6BRKGA\u57fa\u7ebf\u53d6\u5f97\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6700\u590d\u6742\u5b9e\u4f8b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5229\u7528LLM\u4ea7\u751f\u5148\u9a8c\u7684\u3001\u5b9e\u4f8b\u9a71\u52a8\u7684\u542f\u53d1\u5f0f\u504f\u7f6e\u662f\u589e\u5f3a\u590d\u6742\u4f18\u5316\u9886\u57df\u4e2d\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.10344", "pdf": "https://arxiv.org/pdf/2509.10344", "abs": "https://arxiv.org/abs/2509.10344", "authors": ["Yuexi Du", "Lihui Chen", "Nicha C. Dvornek"], "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by MICCAI 2025", "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.", "AI": {"tldr": "GLAM\u6a21\u578b\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e73\u817aX\u7ebf\u6444\u5f71\u591a\u89c6\u56fe\u5206\u6790\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u4e73\u817aX\u7ebf\u6444\u5f71\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u591a\u89c6\u56fe\u5173\u7cfb\u7279\u6027\uff0c\u65e0\u6cd5\u50cf\u653e\u5c04\u79d1\u533b\u751f\u90a3\u6837\u540c\u65f6\u5206\u6790\u53cc\u4fa7\u89c6\u56fe\uff0c\u5bfc\u81f4\u51e0\u4f55\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u9884\u6d4b\u6548\u679c\u4e0d\u4f73", "method": "\u63d0\u51faGLAM\u6a21\u578b\uff0c\u5229\u7528\u4e73\u817aX\u7ebf\u6444\u5f71\u591a\u89c6\u56fe\u6210\u50cf\u8fc7\u7a0b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u8054\u5408\u5168\u5c40\u548c\u5c40\u90e8\u3001\u89c6\u89c9-\u89c6\u89c9\u3001\u89c6\u89c9-\u8bed\u8a00\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b66\u4e60\u5c40\u90e8\u8de8\u89c6\u56fe\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81", "result": "\u5728EMBED\u6570\u636e\u96c6\uff08\u6700\u5927\u516c\u5f00\u4e73\u817aX\u7ebf\u6444\u5f71\u6570\u636e\u96c6\u4e4b\u4e00\uff09\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "GLAM\u6a21\u578b\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e73\u817aX\u7ebf\u6444\u5f71\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09716", "pdf": "https://arxiv.org/pdf/2509.09716", "abs": "https://arxiv.org/abs/2509.09716", "authors": ["Jun Zhan", "Mingyang Han", "Yuxuan Xie", "Chen Wang", "Dong Zhang", "Kexin Huang", "Haoxiang Shi", "DongXiao Wang", "Tengtao Song", "Qinyuan Cheng", "Shimin Li", "Jun Song", "Xipeng Qiu", "Bo Zheng"], "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\n\\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8bed\u97f3\u98ce\u683c\u9002\u5e94(VSA)\u65b0\u4efb\u52a1\uff0c\u7814\u7a76\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u6839\u636e\u53e3\u8bed\u6307\u4ee4\u8c03\u6574\u8bf4\u8bdd\u98ce\u683c\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u53cc\u8bed\u57fa\u51c6VStyle\u548c\u8bc4\u4f30\u6846\u67b6LALM as a Judge\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u51c6\u786e\u6027\u548c\u6307\u4ee4\u8ddf\u968f\uff0c\u4f46\u5728\u6839\u636e\u53e3\u8bed\u6307\u4ee4\u8c03\u6574\u8bf4\u8bdd\u98ce\u683c(\u5982\u97f3\u8272\u3001\u97f5\u5f8b\u3001\u89d2\u8272\u626e\u6f14)\u65b9\u9762\u7684\u80fd\u529b\u7814\u7a76\u6709\u9650\u3002", "method": "\u63d0\u51faVStyle\u53cc\u8bed\u57fa\u51c6(\u4e2d\u82f1\u6587)\uff0c\u6db5\u76d6\u56db\u4e2a\u8bed\u97f3\u751f\u6210\u7c7b\u522b\uff1b\u5f00\u53d1LALM as a Judge\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u6587\u672c\u5fe0\u5b9e\u5ea6\u3001\u98ce\u683c\u9075\u5faa\u5ea6\u548c\u81ea\u7136\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u6e10\u8fdb\u5f0f\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u5546\u4e1a\u7cfb\u7edf\u548c\u5f00\u6e90SLM\u5728\u53ef\u63a7\u98ce\u683c\u9002\u5e94\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u4efb\u52a1\u7684\u65b0\u9896\u6027\u548c\u6311\u6218\u6027\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03VStyle\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u4e3a\u63a8\u8fdb\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bed\u97f3\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u98ce\u683c\u9002\u5e94\u8fd9\u4e00\u91cd\u8981\u7814\u7a76\u65b9\u5411\u7684\u4ef7\u503c\u548c\u6311\u6218\u3002"}}
{"id": "2509.10345", "pdf": "https://arxiv.org/pdf/2509.10345", "abs": "https://arxiv.org/abs/2509.10345", "authors": ["Georgios Pantazopoulos", "Eda B. \u00d6zyi\u011fit"], "title": "Towards Understanding Visual Grounding in Visual Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u56de\u987e\u4e86\u73b0\u4ee3\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5305\u62ec\u5176\u91cd\u8981\u6027\u3001\u6838\u5fc3\u7ec4\u4ef6\u3001\u5b9e\u9645\u5e94\u7528\u3001\u57fa\u51c6\u8bc4\u4f30\u4ee5\u53ca\u4e0e\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u548c\u63a8\u7406\u7684\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u8bc6\u522b\u89c6\u89c9\u8f93\u5165\u4e2d\u7684\u7279\u5b9a\u533a\u57df\uff0c\u8fd9\u79cd\u80fd\u529b\u5bf9\u4e8e\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u3001\u7ec6\u7c92\u5ea6\u89c6\u89c9\u95ee\u7b54\u3001\u5b9e\u4f53\u5f15\u7528\u5b57\u5e55\u751f\u6210\u4ee5\u53ca\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u63a7\u5236\u7b49\u5e7f\u6cdb\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u9996\u5148\u6982\u8ff0\u4e86\u89c6\u89c9\u5b9a\u4f4d\u5728VLMs\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7136\u540e\u8be6\u7ec6\u9610\u8ff0\u4e86\u5f00\u53d1\u5177\u6709\u5b9a\u4f4d\u80fd\u529b\u7684\u73b0\u4ee3\u6a21\u578b\u7684\u6838\u5fc3\u7ec4\u4ef6\u8303\u5f0f\uff0c\u5e76\u8003\u5bdf\u4e86\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u5305\u62ec\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406\u4ee3\u8868\u6027\u5de5\u4f5c\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u6280\u672f\u53d1\u5c55\u73b0\u72b6\u7684\u5168\u9762\u8ba4\u8bc6\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u9886\u57df\u4e0e\u5176\u4ed6\u76f8\u5173\u6280\u672f(\u5982\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u548c\u63a8\u7406)\u7684\u591a\u65b9\u9762\u76f8\u4e92\u5173\u7cfb\u3002", "conclusion": "\u8bba\u6587\u6700\u540e\u5206\u6790\u4e86\u89c6\u89c9\u5b9a\u4f4d\u9762\u4e34\u7684\u5185\u5728\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684 promising \u65b9\u5411\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.10359", "pdf": "https://arxiv.org/pdf/2509.10359", "abs": "https://arxiv.org/abs/2509.10359", "authors": ["Matteo Trippodo", "Federico Becattini", "Lorenzo Seidenari"], "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention", "categories": ["cs.CV"], "comment": "Accepted as Regular Paper at ACM Multimedia 2025", "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u7684\u89c6\u89c9\u7ec4\u4ef6\u653b\u51fb\u2014\u2014Attention Attack\uff0c\u901a\u8fc7\u4f7f\u7528\u81ea\u52a8\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u4f5c\u4e3a\u4ee3\u7406\u63d0\u793a\u6765\u7834\u574f\u6587\u672c\u63d0\u793a\u4e0e\u89c6\u89c9\u8868\u793a\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u7834\u574f\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7834\u574f\u7f16\u8f91\u8fc7\u7a0b\u800c\u4e0d\u9700\u8981\u4e86\u89e3\u5177\u4f53\u7f16\u8f91\u65b9\u6cd5\u6216\u63d0\u793a\u7684\u653b\u51fb\u624b\u6bb5\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u4f5c\u4e3a\u4ee3\u7406\u63d0\u793a\uff0c\u7834\u574f\u6587\u672c\u63d0\u793a\u4e0e\u89c6\u89c9\u8868\u793a\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6253\u7834\u56fe\u50cf\u5185\u5bb9\u4e0e\u5176\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u5bf9\u9f50\u5173\u7cfb\u3002", "result": "\u5728TEDBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u7f16\u8f91\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u653b\u51fb\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "conclusion": "Attention Attack\u662f\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u7834\u574f\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\u7684\u529f\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u66f4\u597d\u5730\u8861\u91cf\u653b\u51fb\u6548\u679c\u3002"}}
{"id": "2509.09740", "pdf": "https://arxiv.org/pdf/2509.09740", "abs": "https://arxiv.org/abs/2509.09740", "authors": ["Ying Yuan", "Xing-Yue Monica Ge", "Aaron Archer Waterman", "Tommaso Biancalani", "David Richmond", "Yogesh Pandit", "Avtar Singh", "Russell Littman", "Jin Liu", "Jan-Christian Huetter", "Vladimir Ermakov"], "title": "HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large-scale single-cell and Perturb-seq investigations routinely involve\nclustering cells and subsequently annotating each cluster with Gene-Ontology\n(GO) terms to elucidate the underlying biological programs. However, both\nstages, resolution selection and functional annotation, are inherently\nsubjective, relying on heuristics and expert curation. We present\nHYPOGENEAGENT, a large language model (LLM)-driven framework, transforming\ncluster annotation into a quantitatively optimizable task. Initially, an LLM\nfunctioning as a gene-set analyst analyzes the content of each gene program or\nperturbation module and generates a ranked list of GO-based hypotheses,\naccompanied by calibrated confidence scores. Subsequently, we embed every\npredicted description with a sentence-embedding model, compute pair-wise cosine\nsimilarities, and let the agent referee panel score (i) the internal\nconsistency of the predictions, high average similarity within the same\ncluster, termed intra-cluster agreement (ii) their external distinctiveness,\nlow similarity between clusters, termed inter-cluster separation. These two\nquantities are combined to produce an agent-derived resolution score, which is\nmaximized when clusters exhibit simultaneous coherence and mutual exclusivity.\nWhen applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary\ntest, our Resolution Score selects clustering granularities that exhibit\nalignment with known pathway compared to classical metrics such silhouette\nscore, modularity score for gene functional enrichment summary. These findings\nestablish LLM agents as objective adjudicators of cluster resolution and\nfunctional annotation, thereby paving the way for fully automated,\ncontext-aware interpretation pipelines in single-cell multi-omics studies.", "AI": {"tldr": "HYPOGENEAGENT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5c06\u7ec6\u80de\u805a\u7c7b\u6ce8\u91ca\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u4f18\u5316\u7684\u4efb\u52a1\uff0c\u901a\u8fc7LLM\u751f\u6210GO\u5047\u8bbe\u5e76\u8bc4\u4f30\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u533a\u5206\u5ea6\uff0c\u5728Perturb-seq\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u5355\u7ec6\u80de\u548cPerturb-seq\u7814\u7a76\u4e2d\u7684\u805a\u7c7b\u5206\u8fa8\u7387\u9009\u62e9\u548c\u529f\u80fd\u6ce8\u91ca\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u57fa\u56e0\u96c6\u5206\u6790\u5e08\u751f\u6210GO\u5047\u8bbe\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u7136\u540e\u901a\u8fc7\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u8ba1\u7b97\u805a\u7c7b\u5185\u4e00\u81f4\u6027\uff08intra-cluster agreement\uff09\u548c\u805a\u7c7b\u95f4\u533a\u5206\u5ea6\uff08inter-cluster separation\uff09\uff0c\u7ec4\u5408\u5f97\u5230\u5206\u8fa8\u7387\u8bc4\u5206\u3002", "result": "\u5728K562 CRISPRi Perturb-seq\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u9009\u62e9\u7684\u5206\u8fa8\u7387\u4e0e\u5df2\u77e5\u901a\u8def\u5bf9\u9f50\u5ea6\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u5982\u8f6e\u5ed3\u7cfb\u6570\u3001\u6a21\u5757\u5ea6\u8bc4\u5206\u7b49\u3002", "conclusion": "LLM\u4ee3\u7406\u53ef\u4ee5\u4f5c\u4e3a\u805a\u7c7b\u5206\u8fa8\u7387\u548c\u529f\u80fd\u6ce8\u91ca\u7684\u5ba2\u89c2\u88c1\u51b3\u8005\uff0c\u4e3a\u5355\u7ec6\u80de\u591a\u7ec4\u5b66\u7814\u7a76\u4e2d\u7684\u5168\u81ea\u52a8\u5316\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u91ca\u6d41\u7a0b\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.10366", "pdf": "https://arxiv.org/pdf/2509.10366", "abs": "https://arxiv.org/abs/2509.10366", "authors": ["Fabien Allemand", "Attilio Fiandrotti", "Sumanta Chaudhuri", "Alaa Eddine Mazouz"], "title": "Efficient Learned Image Compression Through Knowledge Distillation", "categories": ["cs.CV"], "comment": "19 pages, 21 figures", "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u964d\u4f4e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u6df1\u5ea6\u5b66\u4e60\u538b\u7f29\u65b9\u6cd5\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e0d\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u65f6\u5e94\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4e3b\u6d41\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u8303\u5f0f\uff0c\u8ba9\u8f83\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u90e8\u5206\u5b66\u4e60\u66f4\u5927\u3001\u66f4\u590d\u6742\u6a21\u578b\u7684\u8f93\u51fa\u6765\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\uff0c\u800c\u4e0d\u662f\u72ec\u7acb\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u8868\u660e\u77e5\u8bc6\u84b8\u998f\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u56fe\u50cf\u538b\u7f29\u4efb\u52a1\uff1a\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\u5927\u5c0f\u3001\u5b9e\u73b0\u4e0d\u540c\u7684\u56fe\u50cf\u8d28\u91cf/\u6bd4\u7279\u7387\u6743\u8861\uff0c\u5e76\u8282\u7701\u5904\u7406\u548c\u80fd\u6e90\u8d44\u6e90\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u4e3a\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u7684\u8d44\u6e90\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u672a\u6765\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u4e0d\u540c\u6559\u5e08\u6a21\u578b\u548c\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230\u57fa\u4e8etransformer\u7684\u6a21\u578b\u3002"}}
{"id": "2509.09775", "pdf": "https://arxiv.org/pdf/2509.09775", "abs": "https://arxiv.org/abs/2509.09775", "authors": ["Aleksandr Boldachev"], "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.SE"], "comment": "22 pages, 6 figures", "summary": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework.", "AI": {"tldr": "boldsea\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u4e8b\u4ef6\u7684\u53ef\u6267\u884c\u672c\u4f53\u67b6\u6784\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u52a8\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u4e8b\u4ef6\u8bed\u4e49\u548c\u6570\u636e\u6d41\u67b6\u6784\u89e3\u51b3\u4f20\u7edfBPM\u7cfb\u7edf\u548c\u9762\u5411\u5bf9\u8c61\u8bed\u4e49\u6280\u672f\u7684\u5c40\u9650\u6027", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u7cfb\u7edf\u548c\u9762\u5411\u5bf9\u8c61\u8bed\u4e49\u6280\u672f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u8fd0\u884c\u65f6\u4fee\u6539\u4e8b\u4ef6\u6a21\u578b\u3001\u65f6\u95f4\u900f\u660e\u5ea6\u548c\u6570\u636e\u4e0e\u4e1a\u52a1\u903b\u8f91\u7edf\u4e00\u7684\u80fd\u529b", "method": "\u63d0\u51faboldsea\u8bed\u4e49\u8bed\u8a00(BSL)\u53ca\u5176BNF\u8bed\u6cd5\uff0c\u8bbe\u8ba1boldsea-engine\u67b6\u6784\u76f4\u63a5\u89e3\u91ca\u8bed\u4e49\u6a21\u578b\u4e3a\u53ef\u6267\u884c\u7b97\u6cd5\uff0c\u65e0\u9700\u7f16\u8bd1", "result": "\u5b9e\u73b0\u4e86\u8bed\u4e49\u6a21\u578b\u4f5c\u4e3a\u52a8\u6001\u7ed3\u6784\u76f4\u63a5\u63a7\u5236\u6d41\u7a0b\u6267\u884c\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u6a21\u578b\u4fee\u6539\uff0c\u786e\u4fdd\u65f6\u95f4\u900f\u660e\u5ea6\uff0c\u7edf\u4e00\u6570\u636e\u4e0e\u4e1a\u52a1\u903b\u8f91", "conclusion": "boldsea\u67b6\u6784\u901a\u8fc7\u53ef\u6267\u884c\u672c\u4f53\u548c\u8bed\u4e49\u4e8b\u4ef6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4e0b\u4e00\u4ee3BPM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2509.10388", "pdf": "https://arxiv.org/pdf/2509.10388", "abs": "https://arxiv.org/abs/2509.10388", "authors": ["Zeqing Leo Yuan", "Mani Ramanagopal", "Aswin C. Sankaranarayanan", "Srinivasa G. Narasimhan"], "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u53ef\u89c1\u5149\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u5bf9\uff0c\u901a\u8fc7\u70ed\u6210\u50cf\u68c0\u6d4b\u5438\u6536\u5149\u6765\u63a8\u65ad\u53cd\u5c04\u7387\u548c\u9634\u5f71\u7684\u5e8f\u6570\u5173\u7cfb\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u5206\u89e3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u5730\u9762\u5b9e\u51b5\u6570\u636e\u800c\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u7a00\u758f\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5ba4\u5916\u573a\u666f\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u70ed\u6210\u50cf\u76f8\u673a\u68c0\u6d4b\u88ab\u5438\u6536\u5149\u4ea7\u751f\u7684\u70ed\u91cf\uff0c\u5efa\u7acb\u53ef\u89c1\u5149\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u5f3a\u5ea6\u4e4b\u95f4\u7684\u5e8f\u6570\u5173\u7cfb\uff0c\u8fdb\u800c\u63a8\u5bfc\u51fa\u9634\u5f71\u548c\u53cd\u5c04\u7387\u7684\u5e8f\u6570\u5173\u7cfb\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u89e3\u3002", "result": "\u5728\u81ea\u7136\u5149\u548c\u4eba\u5de5\u5149\u7167\u6761\u4ef6\u4e0b\u5bf9\u5df2\u77e5\u53cd\u5c04\u7387\u548c\u9634\u5f71\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u5728\u591a\u6837\u5316\u5ba4\u5916\u573a\u666f\u8fdb\u884c\u5b9a\u6027\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u4f18\u4e8e\u8fd1\u671f\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u83b7\u53d6\u771f\u5b9e\u4e16\u754c\u5e8f\u6570\u76d1\u7763\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u624b\u52a8\u6807\u6ce8\u4e0d\u53ef\u884c\u7684\u95ee\u9898\uff0c\u5728\u5ba4\u5916\u573a\u666f\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.09864", "pdf": "https://arxiv.org/pdf/2509.09864", "abs": "https://arxiv.org/abs/2509.09864", "authors": ["Jenny Y. Huang", "Mehul Damani", "Yousef El-Kurdi", "Ramon Astudillo", "Wei Sun"], "title": "Latency and Token-Aware Test-Time Compute", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time scaling has emerged as a powerful way to improve large\nlanguage model (LLM) performance by generating multiple candidate responses and\nselecting among them. However, existing work on dynamic allocation for\ntest-time compute typically considers only parallel generation methods such as\nbest-of-N, overlooking incremental decoding methods like beam search, and has\nlargely ignored latency, focusing only on token usage. We formulate\ninference-time scaling as a problem of dynamic compute allocation and method\nselection, where the system must decide which strategy to apply and how much\ncompute to allocate on a per-query basis. Our framework explicitly incorporates\nboth token cost and wall-clock latency, the latter being critical for user\nexperience and particularly for agentic workflows where models must issue\nmultiple queries efficiently. Experiments on reasoning benchmarks show that our\napproach consistently outperforms static strategies, achieving favorable\naccuracy-cost trade-offs while remaining practical for deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u63a8\u7406\u65f6\u8ba1\u7b97\u52a8\u6001\u5206\u914d\u6846\u67b6\uff0c\u540c\u65f6\u8003\u8651token\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u5728\u4fdd\u6301\u90e8\u7f72\u5b9e\u7528\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e76\u884c\u751f\u6210\u6280\u672f\uff08\u5982best-of-N\uff09\uff0c\u5ffd\u7565\u4e86\u589e\u91cf\u89e3\u7801\u65b9\u6cd5\uff08\u5982beam search\uff09\uff0c\u5e76\u4e14\u5927\u591a\u53ea\u5173\u6ce8token\u4f7f\u7528\u800c\u5ffd\u89c6\u4e86\u5ef6\u8fdf\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u7528\u6237\u4f53\u9a8c\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u63a8\u7406\u65f6\u6269\u5c55\u5efa\u6a21\u4e3a\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u548c\u65b9\u6cd5\u9009\u62e9\u95ee\u9898\uff0c\u7cfb\u7edf\u9700\u8981\u57fa\u4e8e\u6bcf\u4e2a\u67e5\u8be2\u51b3\u5b9a\u5e94\u7528\u54ea\u79cd\u7b56\u7565\u4ee5\u53ca\u5206\u914d\u591a\u5c11\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u663e\u5f0f\u5730\u8003\u8651token\u6210\u672c\u548c\u65f6\u949f\u5ef6\u8fdf\u3002", "result": "\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u9759\u6001\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u90e8\u7f72\u5b9e\u7528\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u5229\u7684\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u5904\u7406\u591a\u4e2a\u67e5\u8be2\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u573a\u666f\u3002"}}
{"id": "2509.10407", "pdf": "https://arxiv.org/pdf/2509.10407", "abs": "https://arxiv.org/abs/2509.10407", "authors": ["Xiem HoangVan", "Dang BuiDinh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards", "categories": ["cs.CV"], "comment": null, "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u538b\u7f29\u89c6\u9891\u8d28\u91cf\u589e\u5f3a(CVQE)\u7684\u65b0\u5206\u7c7b\u6cd5\u548c\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7efc\u8ff0\u5728\u7cfb\u7edf\u5206\u7c7b\u3001\u67b6\u6784\u6bd4\u8f83\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u538b\u7f29\u89c6\u9891\u8d28\u91cf\u589e\u5f3a\u7efc\u8ff0\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u65e0\u6cd5\u5c06\u5177\u4f53\u65b9\u6cd5\u4e0e\u7f16\u7801\u6807\u51c6\u548c\u4f2a\u5f71\u7c7b\u578b\u5173\u8054\uff0c\u7f3a\u4e4f\u8de8\u7f16\u7801\u7c7b\u578b\u7684\u67b6\u6784\u8303\u5f0f\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u53ca\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\u4e0d\u5b8c\u5584\u3002", "method": "1) \u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u6309\u67b6\u6784\u8303\u5f0f\u3001\u7f16\u7801\u6807\u51c6\u548c\u538b\u7f29\u57df\u7279\u5f81\u5229\u7528\u5bf9CVQE\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff1b2) \u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6574\u5408\u73b0\u4ee3\u538b\u7f29\u534f\u8bae\u548c\u6807\u51c6\u6d4b\u8bd5\u5e8f\u5217\uff1b3) \u7cfb\u7edf\u5206\u6790\u4e86\u91cd\u5efa\u6027\u80fd\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5efa\u7acb\u4e86CVQE\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u4f9b\u4e86\u516c\u5e73\u7684\u591a\u6807\u51c6\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aCVQE\u7814\u7a76\u548c\u90e8\u7f72\u4e2d\u7684\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u660e\u667a\u6a21\u578b\u9009\u62e9\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2509.09867", "pdf": "https://arxiv.org/pdf/2509.09867", "abs": "https://arxiv.org/abs/2509.09867", "authors": ["Yago Romano Matinez", "Jesse Roberts"], "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLMs promise to assist humans -- not just by answering questions, but by\noffering useful guidance across a wide range of tasks. But how far does that\nassistance go? Can a large language model based agent actually help someone\naccomplish their goal as an active participant? We test this question by\nengaging an LLM in UNO, a turn-based card game, asking it not to win but\ninstead help another player to do so. We built a tool that allows decoder-only\nLLMs to participate as agents within the RLCard game environment. These models\nreceive full game-state information and respond using simple text prompts under\ntwo distinct prompting strategies. We evaluate models ranging from small (1B\nparameters) to large (70B parameters) and explore how model scale impacts\nperformance. We find that while all models were able to successfully outperform\na random baseline when playing UNO, few were able to significantly aid another\nplayer.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u5728UNO\u6e38\u620f\u4e2d\u4f5c\u4e3a\u52a9\u624b\u89d2\u8272\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u867d\u7136\u6240\u6709\u6a21\u578b\u90fd\u80fd\u8d85\u8d8a\u968f\u673a\u57fa\u51c6\uff0c\u4f46\u53ea\u6709\u5c11\u6570\u5927\u6a21\u578b\u80fd\u6709\u6548\u5e2e\u52a9\u5176\u4ed6\u73a9\u5bb6\u83b7\u80dc\u3002", "motivation": "\u6d4b\u8bd5LLM\u4f5c\u4e3a\u4e3b\u52a8\u53c2\u4e0e\u8005\u662f\u5426\u80fd\u771f\u6b63\u5e2e\u52a9\u4eba\u7c7b\u5b8c\u6210\u76ee\u6807\uff0c\u7279\u522b\u662f\u5728\u534f\u4f5c\u6e38\u620f\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u5de5\u5177\u8ba9\u4ec5\u89e3\u7801\u5668LLM\u5728RLCard\u6e38\u620f\u73af\u5883\u4e2d\u4f5c\u4e3a\u4ee3\u7406\u53c2\u4e0eUNO\u6e38\u620f\uff0c\u63a5\u6536\u5b8c\u6574\u6e38\u620f\u72b6\u6001\u4fe1\u606f\uff0c\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\uff0c\u8bc4\u4f30\u4ece1B\u523070B\u53c2\u6570\u7684\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u73a9UNO\u65f6\u90fd\u80fd\u6210\u529f\u8d85\u8d8a\u968f\u673a\u57fa\u51c6\uff0c\u4f46\u53ea\u6709\u5c11\u6570\u6a21\u578b\u80fd\u591f\u663e\u8457\u5e2e\u52a9\u5176\u4ed6\u73a9\u5bb6\u83b7\u80dc\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u6709\u5f71\u54cd\uff0c\u4f46\u5373\u4f7f\u662f\u5927\u6a21\u578b\u5728\u534f\u4f5c\u5e2e\u52a9\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0cLLM\u4f5c\u4e3a\u4e3b\u52a8\u53c2\u4e0e\u8005\u7684\u8f85\u52a9\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2509.10408", "pdf": "https://arxiv.org/pdf/2509.10408", "abs": "https://arxiv.org/abs/2509.10408", "authors": ["Iacopo Curti", "Pierluigi Zama Ramirez", "Alioscia Petrelli", "Luigi Di Stefano"], "title": "Multimodal SAM-adapter for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.", "AI": {"tldr": "MM SAM-adapter\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u914d\u5668\u7f51\u7edc\u5c06\u878d\u5408\u7684\u591a\u6a21\u6001\u7279\u5f81\u6ce8\u5165\u5230Segment Anything Model\u7684RGB\u7279\u5f81\u4e2d\uff0c\u5728\u4fdd\u6301RGB\u7279\u5f81\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u9009\u62e9\u6027\u5229\u7528\u8f85\u52a9\u6a21\u6001\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u6076\u52a3\u5149\u7167\u3001\u906e\u6321\u548c\u6076\u52a3\u5929\u6c14\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8981\u6574\u5408\u8f85\u52a9\u4f20\u611f\u5668\u6570\u636e\uff08\u5982LiDAR\u3001\u7ea2\u5916\uff09\u6765\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faMM SAM-adapter\u6846\u67b6\uff0c\u4f7f\u7528\u9002\u914d\u5668\u7f51\u7edc\u5c06\u878d\u5408\u7684\u591a\u6a21\u6001\u7279\u5f81\u6ce8\u5165\u5230SAM\u7684RGB\u7279\u5f81\u4e2d\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u606f\u7684\u5e73\u8861\u9ad8\u6548\u5229\u7528\uff0c\u4ec5\u5728\u8f85\u52a9\u6a21\u6001\u63d0\u4f9b\u989d\u5916\u7ebf\u7d22\u65f6\u9009\u62e9\u6027\u6574\u5408\u3002", "result": "\u5728DeLiVER\u3001FMB\u548cMUSES\u4e09\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86state-of-the-art\u6027\u80fd\uff0c\u5728RGB-easy\u548cRGB-hard\u5b50\u96c6\u4e0a\u90fd\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u9002\u914d\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u573a\u666f\u7406\u89e3\u7684\u9c81\u68d2\u6027\uff0cMM SAM-adapter\u6846\u67b6\u5728\u6709\u5229\u548c\u4e0d\u5229\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.09870", "pdf": "https://arxiv.org/pdf/2509.09870", "abs": "https://arxiv.org/abs/2509.09870", "authors": ["Hasibur Rahman", "Smit Desai"], "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable conversational agents (CAs) to express\ndistinctive personalities, raising new questions about how such designs shape\nuser perceptions. This study investigates how personality expression levels and\nuser-agent personality alignment influence perceptions in goal-oriented tasks.\nIn a between-subjects experiment (N=150), participants completed travel\nplanning with CAs exhibiting low, medium, or high expression across the Big\nFive traits, controlled via our novel Trait Modulation Keys framework. Results\nrevealed an inverted-U relationship: medium expression produced the most\npositive evaluations across Intelligence, Enjoyment, Anthropomorphism,\nIntention to Adopt, Trust, and Likeability, significantly outperforming both\nextremes. Personality alignment further enhanced outcomes, with Extraversion\nand Emotional Stability emerging as the most influential traits. Cluster\nanalysis identified three distinct compatibility profiles, with \"Well-Aligned\"\nusers reporting substantially positive perceptions. These findings demonstrate\nthat personality expression and strategic trait alignment constitute optimal\ndesign targets for CA personality, offering design implications as LLM-based\nCAs become increasingly prevalent.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u4ee3\u7406\u7684\u4eba\u683c\u8868\u8fbe\u6c34\u5e73\u4e0e\u7528\u6237\u4eba\u683c\u5339\u914d\u5ea6\u5448\u5012U\u578b\u5173\u7cfb\uff0c\u4e2d\u7b49\u8868\u8fbe\u6c34\u5e73\u5728\u591a\u4e2a\u8bc4\u4ef7\u7ef4\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4eba\u683c\u5bf9\u9f50\u53ef\u8fdb\u4e00\u6b65\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u968f\u7740LLM\u4f7f\u5bf9\u8bdd\u4ee3\u7406\u80fd\u591f\u8868\u8fbe\u72ec\u7279\u4e2a\u6027\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u4e2a\u6027\u8868\u8fbe\u6c34\u5e73\u548c\u7528\u6237-\u4ee3\u7406\u4e2a\u6027\u5339\u914d\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5728\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u3002", "method": "\u91c7\u7528150\u540d\u53c2\u4e0e\u8005\u7684\u7ec4\u95f4\u5b9e\u9a8c\uff0c\u4f7f\u7528\u65b0\u9896\u7684Trait Modulation Keys\u6846\u67b6\u63a7\u5236\u5bf9\u8bdd\u4ee3\u7406\u5728\u4e94\u5927\u7279\u8d28\u4e0a\u7684\u4f4e\u3001\u4e2d\u3001\u9ad8\u8868\u8fbe\u6c34\u5e73\uff0c\u8ba9\u53c2\u4e0e\u8005\u5b8c\u6210\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5012U\u578b\u5173\u7cfb\uff1a\u4e2d\u7b49\u8868\u8fbe\u6c34\u5e73\u5728\u667a\u529b\u3001\u6109\u60a6\u5ea6\u3001\u62df\u4eba\u5316\u3001\u91c7\u7528\u610f\u613f\u3001\u4fe1\u4efb\u548c\u559c\u7231\u5ea6\u7b49\u65b9\u9762\u83b7\u5f97\u6700\u79ef\u6781\u8bc4\u4ef7\uff1b\u4e2a\u6027\u5bf9\u9f50\u8fdb\u4e00\u6b65\u6539\u5584\u7ed3\u679c\uff0c\u5916\u5411\u6027\u548c\u60c5\u7eea\u7a33\u5b9a\u6027\u662f\u6700\u6709\u5f71\u54cd\u529b\u7684\u7279\u8d28\uff1b\u805a\u7c7b\u5206\u6790\u8bc6\u522b\u51fa\u4e09\u79cd\u517c\u5bb9\u6027\u7279\u5f81\u3002", "conclusion": "\u4e2a\u6027\u8868\u8fbe\u548c\u7b56\u7565\u6027\u7279\u8d28\u5bf9\u9f50\u6784\u6210\u4e86\u5bf9\u8bdd\u4ee3\u7406\u4e2a\u6027\u7684\u6700\u4f73\u8bbe\u8ba1\u76ee\u6807\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2509.10441", "pdf": "https://arxiv.org/pdf/2509.10441", "abs": "https://arxiv.org/abs/2509.10441", "authors": ["Tao Han", "Wanghan Xu", "Junchao Gong", "Xiaoyu Yue", "Song Guo", "Luping Zhou", "Lei Bai"], "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.", "AI": {"tldr": "InfGen\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7b2c\u4e8c\u4ee3\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u5355\u6b65\u751f\u6210\u5668\u66ff\u6362VAE\u89e3\u7801\u5668\uff0c\u53ef\u4ee5\u4ece\u56fa\u5b9a\u5927\u5c0f\u7684\u6f5c\u5728\u8868\u793a\u751f\u6210\u4efb\u610f\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5c064K\u56fe\u50cf\u751f\u6210\u65f6\u95f4\u4ece100\u591a\u79d2\u51cf\u5c11\u523010\u79d2\u4ee5\u5185\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u8ba1\u7b97\u9700\u6c42\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u5bfc\u81f44K\u56fe\u50cf\u751f\u6210\u5ef6\u8fdf\u8d85\u8fc7100\u79d2\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u5185\u5bb9\u8868\u793a\uff0c\u63d0\u51fa\u4f7f\u7528\u5355\u6b65\u751f\u6210\u5668\u89e3\u7801\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u7528\u65b0\u7684\u751f\u6210\u5668\u66ff\u6362VAE\u89e3\u7801\u5668\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInfGen\u80fd\u591f\u5c06\u591a\u79cd\u6a21\u578b\u5347\u7ea7\u5230\u4efb\u610f\u9ad8\u5206\u8fa8\u7387\u65f6\u4ee3\uff0c\u540c\u65f6\u5c064K\u56fe\u50cf\u751f\u6210\u65f6\u95f4\u7f29\u77ed\u523010\u79d2\u4ee5\u5185\u3002", "conclusion": "InfGen\u7b80\u5316\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u4f7f\u7528\u76f8\u540c\u6f5c\u5728\u7a7a\u95f4\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2509.09987", "pdf": "https://arxiv.org/pdf/2509.09987", "abs": "https://arxiv.org/abs/2509.09987", "authors": ["Sung-Lin Yeh", "Yen Meng", "Hao Tang"], "title": "Whisper Has an Internal Word Aligner", "categories": ["eess.AS", "cs.CL"], "comment": "ASRU 2025", "summary": "There is an increasing interest in obtaining accurate word-level timestamps\nfrom strong automatic speech recognizers, in particular Whisper. Existing\napproaches either require additional training or are simply not competitive.\nThe evaluation in prior work is also relatively loose, typically using a\ntolerance of more than 200 ms. In this work, we discover attention heads in\nWhisper that capture accurate word alignments and are distinctively different\nfrom those that do not. Moreover, we find that using characters produces finer\nand more accurate alignments than using wordpieces. Based on these findings, we\npropose an unsupervised approach to extracting word alignments by filtering\nattention heads while teacher forcing Whisper with characters. Our approach not\nonly does not require training but also produces word alignments that are more\naccurate than prior work under a stricter tolerance between 20 ms and 100 ms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b5b\u9009\u6ce8\u610f\u529b\u5934\u5e76\u4f7f\u7528\u5b57\u7b26\u800c\u975e\u8bcd\u7247\u6bb5\u6765\u4eceWhisper\u4e2d\u63d0\u53d6\u66f4\u7cbe\u786e\u7684\u8bcd\u7ea7\u65f6\u95f4\u6233\uff0c\u572820-100\u6beb\u79d2\u7684\u4e25\u683c\u5bb9\u5dee\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u989d\u5916\u8bad\u7ec3\uff0c\u8981\u4e48\u6027\u80fd\u4e0d\u8db3\uff0c\u4e14\u8bc4\u4f30\u6807\u51c6\u5bbd\u677e\uff08\u901a\u5e38\u4f7f\u7528200\u6beb\u79d2\u4ee5\u4e0a\u7684\u5bb9\u5dee\uff09\u3002\u7814\u7a76\u53d1\u73b0Whisper\u4e2d\u7684\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u80fd\u591f\u6355\u6349\u51c6\u786e\u7684\u8bcd\u5bf9\u9f50\u4fe1\u606f\uff0c\u4e14\u4f7f\u7528\u5b57\u7b26\u6bd4\u8bcd\u7247\u6bb5\u80fd\u4ea7\u751f\u66f4\u7cbe\u7ec6\u7684\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u7b5b\u9009\u6ce8\u610f\u529b\u5934\uff0c\u5728\u6559\u5e08\u5f3a\u5236\u6a21\u5f0f\u4e0b\u4f7f\u7528\u5b57\u7b26\u8f93\u5165Whisper\uff0c\u63d0\u51fa\u65e0\u76d1\u7763\u7684\u8bcd\u5bf9\u9f50\u63d0\u53d6\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u572820-100\u6beb\u79d2\u7684\u4e25\u683c\u5bb9\u5dee\u8303\u56f4\u5185\uff0c\u4ea7\u751f\u7684\u8bcd\u5bf9\u9f50\u6bd4\u73b0\u6709\u5de5\u4f5c\u66f4\u51c6\u786e\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u5bf9\u8bcd\u5bf9\u9f50\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f7f\u7528\u5b57\u7b26\u8f93\u5165\u53ef\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u65f6\u95f4\u6233\uff0c\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u4e25\u683c\u6807\u51c6\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.10453", "pdf": "https://arxiv.org/pdf/2509.10453", "abs": "https://arxiv.org/abs/2509.10453", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u4e09\u79cd\u5148\u8fdb\u7684\u65f6\u5e8f\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e3D\u8111\u90e8MRI\u5206\u6790\uff0c\u901a\u8fc7\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u548c\u5b66\u4e60\u9c81\u68d2\u7a7a\u95f4\u7279\u5f81\uff0c\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u7684\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u3001\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u5bf9\u4e0d\u540c\u626b\u63cf\u6b21\u6570\u548c\u65f6\u95f4\u95f4\u9694\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65f6\u5e8f\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u65f6\u5e8f\u987a\u5e8f\u9884\u6d4b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u5e76\u5b66\u4e60\u9c81\u68d2\u7a7a\u95f4\u7279\u5f81\uff0c\u4f7f\u7528\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u51713,161\u540d\u60a3\u8005\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4e03\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u516d\u4e2a\u4efb\u52a1\u4e0a\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u548c\u4e0d\u540c\u8f93\u5165\u56fe\u50cf\u6570\u91cf\u53ca\u65f6\u95f4\u95f4\u9694\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u65f6\u5e8f\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u5904\u7406\u4e34\u5e8a\u5e94\u7528\u4e2d\u591a\u53d8\u7684\u6570\u636e\u6761\u4ef6\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.10031", "pdf": "https://arxiv.org/pdf/2509.10031", "abs": "https://arxiv.org/abs/2509.10031", "authors": ["Peter Vieting", "Benedikt Hilmes", "Ralf Schl\u00fcter", "Hermann Ney"], "title": "Unified Learnable 2D Convolutional Feature Extraction for ASR", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Neural front-ends represent a promising approach to feature extraction for\nautomatic speech recognition (ASR) systems as they enable to learn specifically\ntailored features for different tasks. Yet, many of the existing techniques\nremain heavily influenced by classical methods. While this inductive bias may\nease the system design, our work aims to develop a more generic front-end for\nfeature extraction. Furthermore, we seek to unify the front-end architecture\ncontrasting with existing approaches that apply a composition of several layer\ntopologies originating from different sources. The experiments systematically\nshow how to reduce the influence of existing techniques to achieve a generic\nfront-end. The resulting 2D convolutional front-end is parameter-efficient and\nsuitable for a scenario with limited computational resources unlike large\nmodels pre-trained on unlabeled audio. The results demonstrate that this\ngeneric unified approach is not only feasible but also matches the performance\nof existing supervised learnable feature extractors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u76842D\u5377\u79ef\u524d\u7aef\u67b6\u6784\uff0c\u7528\u4e8e\u8bed\u97f3\u8bc6\u522b\u7279\u5f81\u63d0\u53d6\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u52a0\u7edf\u4e00\u4e14\u53c2\u6570\u9ad8\u6548\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53", "motivation": "\u73b0\u6709\u795e\u7ecf\u524d\u7aef\u65b9\u6cd5\u4ecd\u53d7\u4f20\u7edf\u65b9\u6cd5\u5f71\u54cd\u8f83\u5927\uff0c\u867d\u7136\u8fd9\u79cd\u5f52\u7eb3\u504f\u7f6e\u6709\u52a9\u4e8e\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4f46\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u66f4\u901a\u7528\u7684\u524d\u7aef\u67b6\u6784\uff0c\u5e76\u7edf\u4e00\u524d\u7aef\u8bbe\u8ba1\u800c\u4e0d\u662f\u7ec4\u5408\u4e0d\u540c\u6765\u6e90\u7684\u5c42\u62d3\u6251", "method": "\u4f7f\u75282D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u524d\u7aef\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u51cf\u5c11\u73b0\u6709\u6280\u672f\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u901a\u7528\u524d\u7aef\u67b6\u6784", "result": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u5668\u76f8\u5f53\uff0c\u8bc1\u660e\u4e86\u901a\u7528\u7edf\u4e00\u65b9\u6cd5\u7684\u53ef\u884c\u6027", "conclusion": "2D\u5377\u79ef\u524d\u7aef\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u5f02\u7684\u901a\u7528\u7279\u5f81\u63d0\u53d6\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\uff0c\u76f8\u6bd4\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u66f4\u5177\u5b9e\u7528\u6027"}}
{"id": "2509.09880", "pdf": "https://arxiv.org/pdf/2509.09880", "abs": "https://arxiv.org/abs/2509.09880", "authors": ["Ya\u015far Utku Al\u00e7alar", "Junno Yun", "Mehmet Ak\u00e7akaya"], "title": "Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Workshop on Computational Advances in Multi-Sensor\n  Adaptive Processing (CAMSAP), 2025", "summary": "Diffusion/score-based models have recently emerged as powerful generative\npriors for solving inverse problems, including accelerated MRI reconstruction.\nWhile their flexibility allows decoupling the measurement model from the\nlearned prior, their performance heavily depends on carefully tuned data\nfidelity weights, especially under fast sampling schedules with few denoising\nsteps. Existing approaches often rely on heuristics or fixed weights, which\nfail to generalize across varying measurement conditions and irregular timestep\nschedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling\n(ZADS), a test-time optimization method that adaptively tunes fidelity weights\nacross arbitrary noise schedules without requiring retraining of the diffusion\nprior. ZADS treats the denoising process as a fixed unrolled sampler and\noptimizes fidelity weights in a self-supervised manner using only undersampled\nmeasurements. Experiments on the fastMRI knee dataset demonstrate that ZADS\nconsistently outperforms both traditional compressed sensing and recent\ndiffusion-based methods, showcasing its ability to deliver high-fidelity\nreconstructions across varying noise schedules and acquisition settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86ZADS\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u81ea\u9002\u5e94\u91c7\u6837\u6280\u672f\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u4f18\u5316\u6570\u636e\u4fdd\u771f\u5ea6\u6743\u91cd\u6765\u63d0\u5347\u52a0\u901fMRI\u91cd\u5efa\u8d28\u91cf", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u4e25\u91cd\u4f9d\u8d56\u7cbe\u5fc3\u8c03\u4f18\u7684\u6570\u636e\u4fdd\u771f\u5ea6\u6743\u91cd\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u91c7\u6837\u8ba1\u5212\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u6743\u91cd\u7684\u65b9\u6cd5", "method": "\u5c06\u53bb\u566a\u8fc7\u7a0b\u89c6\u4e3a\u56fa\u5b9a\u7684\u5c55\u5f00\u91c7\u6837\u5668\uff0c\u4ec5\u4f7f\u7528\u6b20\u91c7\u6837\u6d4b\u91cf\u503c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u4f18\u5316\u4fdd\u771f\u5ea6\u6743\u91cd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6269\u6563\u5148\u9a8c", "result": "\u5728fastMRI\u819d\u76d6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cZADS\u5728\u591a\u79cd\u566a\u58f0\u8ba1\u5212\u548c\u91c7\u96c6\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edf\u538b\u7f29\u611f\u77e5\u548c\u73b0\u6709\u6269\u6563\u65b9\u6cd5", "conclusion": "ZADS\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6743\u91cd\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u4e0d\u540c\u7684\u6d4b\u91cf\u6761\u4ef6\u548c\u65f6\u95f4\u6b65\u8ba1\u5212\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa"}}
{"id": "2509.09926", "pdf": "https://arxiv.org/pdf/2509.09926", "abs": "https://arxiv.org/abs/2509.09926", "authors": ["Jiahao Chen", "Zhiyuan Huang", "Yurou Liu", "Bing Su"], "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.", "AI": {"tldr": "\u63d0\u51faLoFT\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u6765\u89e3\u51b3\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u6269\u5c55\u4e3aLoFT-OW\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u591a\u4ece\u96f6\u8bad\u7ec3\u6a21\u578b\uff0c\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u548c\u4f2a\u6807\u7b7e\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u5229\u7528\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u8303\u5f0f\u6765\u6539\u5584", "method": "\u63d0\u51faLoFT\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u751f\u6210\u66f4\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff1b\u9488\u5bf9\u5f00\u653e\u4e16\u754c\u573a\u666f\u63d0\u51faLoFT-OW\uff0c\u5904\u7406\u672a\u6807\u6ce8\u6570\u636e\u4e2d\u7684\u5206\u5e03\u5916\u6837\u672c", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u5373\u4f7f\u4ec5\u4f7f\u75281%\u7684\u672a\u6807\u6ce8\u6570\u636e\u4e5f\u80fd\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5", "conclusion": "\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u8303\u5f0f\u80fd\u6709\u6548\u63d0\u5347\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u6027\u80fd\uff0cLoFT\u6846\u67b6\u4e3a\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u957f\u5c3e\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2509.10143", "pdf": "https://arxiv.org/pdf/2509.10143", "abs": "https://arxiv.org/abs/2509.10143", "authors": ["Peter Vieting", "Simon Berger", "Thilo von Neumann", "Christoph Boeddeker", "Ralf Schl\u00fcter", "Reinhold Haeb-Umbach"], "title": "Error Analysis in a Modular Meeting Transcription System", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Meeting transcription is a field of high relevance and remarkable progress in\nrecent years. Still, challenges remain that limit its performance. In this\nwork, we extend a previously proposed framework for analyzing leakage in speech\nseparation with proper sensitivity to temporal locality. We show that there is\nsignificant leakage to the cross channel in areas where only the primary\nspeaker is active. At the same time, the results demonstrate that this does not\naffect the final performance much as these leaked parts are largely ignored by\nthe voice activity detection (VAD). Furthermore, different segmentations are\ncompared showing that advanced diarization approaches are able to reduce the\ngap to oracle segmentation by a third compared to a simple energy-based VAD. We\nadditionally reveal what factors contribute to the remaining difference. The\nresults represent state-of-the-art performance on LibriCSS among systems that\ntrain the recognition module on LibriSpeech data only.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4f1a\u8bae\u8f6c\u5f55\u4e2d\u7684\u6cc4\u6f0f\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u4e3b\u8981\u8bf4\u8bdd\u4eba\u6d3b\u52a8\u533a\u57df\u5b58\u5728\u663e\u8457\u7684\u8de8\u901a\u9053\u6cc4\u6f0f\uff0c\u4f46VAD\u4f1a\u5ffd\u7565\u8fd9\u4e9b\u6cc4\u6f0f\u90e8\u5206\uff0c\u56e0\u6b64\u5bf9\u6700\u7ec8\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\u3002\u540c\u65f6\u6bd4\u8f83\u4e86\u4e0d\u540c\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u793a\u5148\u8fdb\u7684\u4e8c\u503c\u5316\u65b9\u6cd5\u80fd\u5c06\u4e0eoracle\u5206\u5272\u7684\u5dee\u8ddd\u7f29\u5c0f\u4e09\u5206\u4e4b\u4e00\u3002", "motivation": "\u4f1a\u8bae\u8f6c\u5f55\u9886\u57df\u8fd1\u5e74\u6765\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u4f46\u4ecd\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u5206\u6790\u8bed\u97f3\u5206\u79bb\u4e2d\u7684\u6cc4\u6f0f\u95ee\u9898\u53ca\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6269\u5c55\u4e86\u5148\u524d\u63d0\u51fa\u7684\u6cc4\u6f0f\u5206\u6790\u6846\u67b6\uff0c\u589e\u52a0\u4e86\u5bf9\u65f6\u95f4\u5c40\u90e8\u6027\u7684\u654f\u611f\u6027\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5206\u5272\u65b9\u6cd5\uff08\u80fd\u91cf\u57faVAD\u4e0e\u5148\u8fdb\u4e8c\u503c\u5316\u65b9\u6cd5\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u4e3b\u8981\u8bf4\u8bdd\u4eba\u6d3b\u52a8\u533a\u57df\u5b58\u5728\u663e\u8457\u8de8\u901a\u9053\u6cc4\u6f0f\uff0c\u4f46VAD\u4f1a\u5ffd\u7565\u8fd9\u4e9b\u6cc4\u6f0f\uff1b\u5148\u8fdb\u4e8c\u503c\u5316\u65b9\u6cd5\u76f8\u6bd4\u7b80\u5355\u80fd\u91cf\u57faVAD\u80fd\u5c06\u4e0eoracle\u5206\u5272\u7684\u5dee\u8ddd\u7f29\u5c0f\u4e09\u5206\u4e4b\u4e00\uff1b\u5728\u4ec5\u4f7f\u7528LibriSpeech\u6570\u636e\u8bad\u7ec3\u7684\u7cfb\u7edf\u4e2d\u8fbe\u5230\u4e86LibriCSS\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u867d\u7136\u5b58\u5728\u8de8\u901a\u9053\u6cc4\u6f0f\u95ee\u9898\uff0c\u4f46\u7531\u4e8eVAD\u7684\u8fc7\u6ee4\u4f5c\u7528\uff0c\u5bf9\u6700\u7ec8\u8f6c\u5f55\u6027\u80fd\u5f71\u54cd\u6709\u9650\uff1b\u5148\u8fdb\u7684\u5206\u5272\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4f46\u4ecd\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2509.09952", "pdf": "https://arxiv.org/pdf/2509.09952", "abs": "https://arxiv.org/abs/2509.09952", "authors": ["Zhi Ying", "Boxiang Rong", "Jingyu Wang", "Maoyuan Xu"], "title": "Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to SIGGRAPH Asia 2025. Project page:\n  https://ubisoft-laforge.github.io/world/chord", "summary": "Material creation and reconstruction are crucial for appearance modeling but\ntraditionally require significant time and expertise from artists. While recent\nmethods leverage visual foundation models to synthesize PBR materials from\nuser-provided inputs, they often fall short in quality, flexibility, and user\ncontrol. We propose a novel two-stage generate-and-estimate framework for PBR\nmaterial generation. In the generation stage, a fine-tuned diffusion model\nsynthesizes shaded, tileable texture images aligned with user input. In the\nestimation stage, we introduce a chained decomposition scheme that sequentially\npredicts SVBRDF channels by passing previously extracted representation as\ninput into a single-step image-conditional diffusion model. Our method is\nefficient, high quality, and enables flexible user control. We evaluate our\napproach against existing material generation and estimation methods,\ndemonstrating superior performance. Our material estimation method shows strong\nrobustness on both generated textures and in-the-wild photographs. Furthermore,\nwe highlight the flexibility of our framework across diverse applications,\nincluding text-to-material, image-to-material, structure-guided generation, and\nmaterial editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u751f\u6210-\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8ePBR\u6750\u8d28\u751f\u6210\uff0c\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u6a21\u578b\u5408\u6210\u7eb9\u7406\u56fe\u50cf\uff0c\u7136\u540e\u4f7f\u7528\u94fe\u5f0f\u5206\u89e3\u65b9\u6848\u9884\u6d4bSVBRDF\u901a\u9053\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u7075\u6d3b\u7684\u7528\u6237\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u6750\u8d28\u521b\u5efa\u548c\u91cd\u5efa\u9700\u8981\u827a\u672f\u5bb6\u5927\u91cf\u65f6\u95f4\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u8d28\u91cf\u3001\u7075\u6d3b\u6027\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u751f\u6210\u9636\u6bb5\u4f7f\u7528\u5fae\u8c03\u6269\u6563\u6a21\u578b\u5408\u6210\u7740\u8272\u3001\u53ef\u5e73\u94fa\u7684\u7eb9\u7406\u56fe\u50cf\uff1b2\uff09\u4f30\u8ba1\u9636\u6bb5\u91c7\u7528\u94fe\u5f0f\u5206\u89e3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u6b65\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u6a21\u578b\u987a\u5e8f\u9884\u6d4bSVBRDF\u901a\u9053\u3002", "result": "\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6750\u8d28\u751f\u6210\u548c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5bf9\u751f\u6210\u7eb9\u7406\u548c\u771f\u5b9e\u7167\u7247\u90fd\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u652f\u6301\u6587\u672c\u5230\u6750\u8d28\u3001\u56fe\u50cf\u5230\u6750\u8d28\u3001\u7ed3\u6784\u5f15\u5bfc\u751f\u6210\u548c\u6750\u8d28\u7f16\u8f91\u7b49\u591a\u79cd\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u4e14\u652f\u6301\u7075\u6d3b\u7528\u6237\u63a7\u5236\uff0c\u4e3a\u6750\u8d28\u521b\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.10401", "pdf": "https://arxiv.org/pdf/2509.10401", "abs": "https://arxiv.org/abs/2509.10401", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Zhen Lin", "Yue Zhang"], "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution.", "AI": {"tldr": "A2P Scaffolding\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u5f52\u56e0\u4ece\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\u8f6c\u53d8\u4e3a\u56e0\u679c\u63a8\u7406\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6b65\u9aa4\u7ea7\u51c6\u786e\u7387\uff082.85\u500d\u63d0\u5347\uff09\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u6b65\u9aa4\u7ea7\u51c6\u786e\u7387\u4f4e\u4e8e17%\uff0c\u65e0\u6cd5\u8fdb\u884c\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u786e\u5b9a\u5355\u4e2a\u52a8\u4f5c\u4fee\u6b63\u662f\u5426\u80fd\u907f\u514d\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u63d0\u51faAbduct-Act-Predict (A2P) Scaffolding\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ed3\u6784\u5316\u6b65\u9aa4\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff1a1)\u6eaf\u56e0\u63a8\u7406\u63a8\u65ad\u9690\u85cf\u7684\u6839\u672c\u539f\u56e0\uff1b2)\u5b9a\u4e49\u6700\u5c0f\u4fee\u6b63\u5e72\u9884\uff1b3)\u6a21\u62df\u540e\u7eed\u8f68\u8ff9\u9a8c\u8bc1\u5e72\u9884\u6548\u679c\u3002", "result": "\u5728Algorithm-Generated\u6570\u636e\u96c6\u4e0a\u8fbe\u523047.46%\u7684\u6b65\u9aa4\u7ea7\u51c6\u786e\u7387\uff08\u76f8\u6bd4\u57fa\u7ebf16.67%\u63d0\u53472.85\u500d\uff09\uff0c\u5728Hand-Crafted\u6570\u636e\u96c6\u4e0a\u8fbe\u523029.31%\u51c6\u786e\u7387\uff08\u76f8\u6bd4\u57fa\u7ebf12.07%\u63d0\u53472.43\u500d\uff09\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6846\u67b6\u91cd\u6784\u95ee\u9898\uff0cA2P Scaffolding\u4e3a\u81ea\u52a8\u5316\u6545\u969c\u5f52\u56e0\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u9a8c\u8bc1\u4e14\u51c6\u786e\u6027\u663e\u8457\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09955", "pdf": "https://arxiv.org/pdf/2509.09955", "abs": "https://arxiv.org/abs/2509.09955", "authors": ["Omar Erak", "Omar Alhussein", "Hatem Abou-Zeid", "Mehdi Bennis", "Sami Muhaidat"], "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "comment": "Submitted to IEEE Journals", "summary": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684token\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5408\u5e76\u8bed\u4e49\u5197\u4f59token\u6765\u538b\u7f29transformer\u8868\u793a\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c", "motivation": "\u5927\u89c4\u6a21transformer\u5728\u8bed\u4e49\u901a\u4fe1\u4e2d\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u8fc7\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a", "method": "\u57fa\u4e8e\u6bcf\u5c42\u76f8\u4f3c\u5ea6\u9608\u503c\u9009\u62e9\u6027\u5408\u5e76\u5197\u4f59token\uff0c\u5c06\u5408\u5e76\u7b56\u7565\u53d1\u73b0\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u5bfb\u627e\u5e15\u7d2f\u6258\u6700\u4f18\u89e3", "result": "\u5728ImageNet\u5206\u7c7b\u4e2d\u51cf\u5c1130% FLOPs\u548c80%\u901a\u4fe1\u6210\u672c\uff0c\u5728VQA\u4efb\u52a1\u4e2d\u4ee51/3\u8ba1\u7b97\u91cf\u548c1/10\u5e26\u5bbd\u8fbe\u5230\u4e0e\u5b8c\u6574\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u667a\u80fd\u573a\u666f\u4e2d\u90e8\u7f72\u5f3a\u5927transformer\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09972", "pdf": "https://arxiv.org/pdf/2509.09972", "abs": "https://arxiv.org/abs/2509.09972", "authors": ["Mohammadreza Narimani", "Alireza Pourreza", "Ali Moghimi", "Mohsen Mesgaran", "Parastoo Farajpoor", "Hamid Jafarbiglu"], "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Author-accepted version (no publisher header/footer). 10 pages +\n  presentation. Published in Proceedings of SPIE Defense + Commercial Sensing\n  2024, Vol. 13053, Paper 1305304. Event: National Harbor, Maryland, USA.\n  Official version: https://doi.org/10.1117/12.3021219", "summary": "This study addresses the escalating threat of branched broomrape (Phelipanche\nramosa) to California's tomato industry, which supplies over 90 percent of U.S.\nprocessing tomatoes. The parasite's largely underground life cycle makes early\ndetection difficult, while conventional chemical controls are costly,\nenvironmentally harmful, and often ineffective. To address this, we combined\ndrone-based multispectral imagery with Long Short-Term Memory (LSTM) deep\nlearning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)\nto handle class imbalance. Research was conducted on a known broomrape-infested\ntomato farm in Woodland, Yolo County, CA, across five key growth stages\ndetermined by growing degree days (GDD). Multispectral images were processed to\nisolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with\n79.09 percent overall accuracy and 70.36 percent recall without integrating\nlater stages. Incorporating sequential growth stages with LSTM improved\ndetection substantially. The best-performing scenario, which integrated all\ngrowth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy\nand 95.37 percent recall. These results demonstrate the strong potential of\ntemporal multispectral analysis and LSTM networks for early broomrape\ndetection. While further real-world data collection is needed for practical\ndeployment, this study shows that UAV-based multispectral sensing coupled with\ndeep learning could provide a powerful precision agriculture tool to reduce\nlosses and improve sustainability in tomato production.", "AI": {"tldr": "\u672c\u7814\u7a76\u7ed3\u5408\u65e0\u4eba\u673a\u591a\u5149\u8c31\u5f71\u50cf\u548cLSTM\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u4f7f\u7528SMOTE\u6280\u672f\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u756a\u8304\u7530\u4e2d\u5206\u679d\u5217\u5f53\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u6700\u4f73\u51c6\u786e\u7387\u8fbe\u523088.37%\uff0c\u53ec\u56de\u738795.37%\u3002", "motivation": "\u5206\u679d\u5217\u5f53\u5bf9\u52a0\u5dde\u756a\u8304\u4ea7\u4e1a\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5176\u5730\u4e0b\u751f\u547d\u5468\u671f\u4f7f\u65e9\u671f\u68c0\u6d4b\u56f0\u96be\uff0c\u4f20\u7edf\u5316\u5b66\u9632\u6cbb\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u73af\u5883\u5371\u5bb3\u5927\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u5728\u5df2\u77e5\u611f\u67d3\u5217\u5f53\u7684\u756a\u8304\u519c\u573a\u8fdb\u884c\u7814\u7a76\uff0c\u4f7f\u7528\u65e0\u4eba\u673a\u591a\u5149\u8c31\u5f71\u50cf\u91c7\u96c6\u6570\u636e\uff0c\u901a\u8fc7LSTM\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u5904\u7406\u65f6\u5e8f\u751f\u957f\u9636\u6bb5\u6570\u636e\uff0c\u5e76\u91c7\u7528SMOTE\u6280\u672f\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728897\u751f\u957f\u5ea6\u65e5\u65f6\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523079.09%\uff0c\u53ec\u56de\u738770.36%\uff1b\u6574\u5408\u6240\u6709\u751f\u957f\u9636\u6bb5\u5e76\u4f7f\u7528SMOTE\u589e\u5f3a\u540e\uff0c\u51c6\u786e\u7387\u63d0\u5347\u81f388.37%\uff0c\u53ec\u56de\u7387\u8fbe\u523095.37%\u3002", "conclusion": "\u65f6\u5e8f\u591a\u5149\u8c31\u5206\u6790\u548cLSTM\u7f51\u7edc\u5728\u65e9\u671f\u5217\u5f53\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u65e0\u4eba\u673a\u591a\u5149\u8c31\u4f20\u611f\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u53ef\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u6709\u529b\u5de5\u5177\uff0c\u51cf\u5c11\u635f\u5931\u5e76\u63d0\u9ad8\u756a\u8304\u751f\u4ea7\u7684\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2509.10096", "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "\u63d0\u51fa\u4e86HHI-Assist\u6570\u636e\u96c6\u548c\u57fa\u4e8eTransformer\u7684\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7269\u7406\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u4ee5\u63d0\u5347\u8f85\u52a9\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u548c\u54cd\u5e94\u80fd\u529b", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u4eba\u53e3\u8001\u9f84\u5316\u9700\u8981\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u4f46\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6765\u786e\u4fdd\u5b89\u5168\u4ea4\u4e92\uff0c\u800c\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u8026\u5408\u52a8\u529b\u5b66\u590d\u6742\u6027\u4f7f\u5f97\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027", "method": "\u6536\u96c6\u4eba\u7c7b-\u4eba\u7c7b\u4ea4\u4e92\u52a8\u4f5c\u6355\u6349\u6570\u636e\u96c6HHI-Assist\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u4ea4\u4e92\u4ee3\u7406\u7684\u59ff\u6001", "result": "\u6a21\u578b\u6709\u6548\u6355\u6349\u4e86\u62a4\u7406\u8005\u548c\u88ab\u62a4\u7406\u8005\u4e4b\u95f4\u7684\u8026\u5408\u52a8\u529b\u5b66\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\uff0c\u5e76\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u63a8\u8fdb\u4ea4\u4e92\u611f\u77e5\u7684\u8fd0\u52a8\u9884\u6d4b\u548c\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u8fd9\u9879\u5de5\u4f5c\u6709\u6f5c\u529b\u663e\u8457\u589e\u5f3a\u673a\u5668\u4eba\u8f85\u52a9\u7b56\u7565\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2509.10098", "pdf": "https://arxiv.org/pdf/2509.10098", "abs": "https://arxiv.org/abs/2509.10098", "authors": ["Muhamad Daniel Ariff Bin Abdul Rahman", "Yusuke Monno", "Masayuki Tanaka", "Masatoshi Okutomi"], "title": "Polarization Denoising and Demosaicking: Dataset and Baseline Method", "categories": ["eess.IV", "cs.CV"], "comment": "Published in ICIP2025; Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/PolarDem/PDD.html", "summary": "A division-of-focal-plane (DoFP) polarimeter enables us to acquire images\nwith multiple polarization orientations in one shot and thus it is valuable for\nmany applications using polarimetric information. The image processing pipeline\nfor a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.\nWhile polarization demosaicking for a noise-free case has increasingly been\nstudied, the research for the joint task of polarization denoising and\ndemosaicking is scarce due to the lack of a suitable evaluation dataset and a\nsolid baseline method. In this paper, we propose a novel dataset and method for\npolarization denoising and demosaicking. Our dataset contains 40 real-world\nscenes and three noise-level conditions, consisting of pairs of noisy mosaic\ninputs and noise-free full images. Our method takes a\ndenoising-then-demosaicking approach based on well-accepted signal processing\ncomponents to offer a reproducible method. Experimental results demonstrate\nthat our method exhibits higher image reconstruction performance than other\nalternative methods, offering a solid baseline.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u504f\u632f\u53bb\u566a\u548c\u53bb\u9a6c\u8d5b\u514b\u7684\u65b0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u5305\u542b40\u4e2a\u771f\u5b9e\u573a\u666f\u548c\u4e09\u79cd\u566a\u58f0\u6761\u4ef6\uff0c\u91c7\u7528\u5148\u53bb\u566a\u540e\u53bb\u9a6c\u8d5b\u514b\u7684\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5408\u9002\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u53ef\u9760\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u504f\u632f\u53bb\u566a\u548c\u53bb\u9a6c\u8d5b\u514b\u7684\u8054\u5408\u4efb\u52a1\u7814\u7a76\u8f83\u5c11\uff0c\u800cDoFP\u504f\u632f\u4eea\u7684\u5355\u6b21\u62cd\u6444\u591a\u504f\u632f\u65b9\u5411\u6210\u50cf\u80fd\u529b\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u5f88\u6709\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u5148\u53bb\u566a\u540e\u53bb\u9a6c\u8d5b\u514b\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u516c\u8ba4\u7684\u4fe1\u53f7\u5904\u7406\u7ec4\u4ef6\u6784\u5efa\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5305\u542b\u566a\u58f0\u9a6c\u8d5b\u514b\u8f93\u5165\u548c\u65e0\u566a\u58f0\u5b8c\u6574\u56fe\u50cf\u5bf9\u7684\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u504f\u632f\u53bb\u566a\u548c\u53bb\u9a6c\u8d5b\u514b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2509.10348", "pdf": "https://arxiv.org/pdf/2509.10348", "abs": "https://arxiv.org/abs/2509.10348", "authors": ["Yehudit Aperstein", "Amit Tzahar", "Alon Gottlib", "Tal Verber", "Ravit Shagan Damti", "Alexander Apartsin"], "title": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 4 figures", "summary": "Overconfidence in deep learning models poses a significant risk in\nhigh-stakes medical imaging tasks, particularly in multi-label classification\nof chest X-rays, where multiple co-occurring pathologies must be detected\nsimultaneously. This study introduces an uncertainty-aware framework for chest\nX-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective\nprediction mechanisms: entropy-based rejection and confidence interval-based\nrejection. Both methods enable the model to abstain from uncertain predictions,\nimproving reliability by deferring ambiguous cases to clinical experts. A\nquantile-based calibration procedure is employed to tune rejection thresholds\nusing either global or class-specific strategies. Experiments conducted on\nthree large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)\ndemonstrate that selective rejection improves the trade-off between diagnostic\naccuracy and coverage, with entropy-based rejection yielding the highest\naverage AUC across all pathologies. These results support the integration of\nselective prediction into AI-assisted diagnostic workflows, providing a\npractical step toward safer, uncertainty-aware deployment of deep learning in\nclinical settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8eDenseNet-121\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u62d2\u7edd\u548c\u7f6e\u4fe1\u533a\u95f4\u62d2\u7edd\u4e24\u79cd\u9009\u62e9\u6027\u9884\u6d4b\u673a\u5236\uff0c\u5728\u80f8\u90e8X\u5149\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\uff0c\u907f\u514d\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u540c\u65f6\u68c0\u6d4b\u591a\u79cd\u5171\u53d1\u75c5\u53d8\u7684\u80f8\u90e8X\u5149\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\uff0c\u4e0d\u786e\u5b9a\u9884\u6d4b\u53ef\u80fd\u5e26\u6765\u9ad8\u98ce\u9669\u3002", "method": "\u4f7f\u7528DenseNet-121\u9aa8\u5e72\u7f51\u7edc\uff0c\u96c6\u6210\u71b5\u57fa\u62d2\u7edd\u548c\u7f6e\u4fe1\u533a\u95f4\u62d2\u7edd\u4e24\u79cd\u9009\u62e9\u6027\u9884\u6d4b\u673a\u5236\uff0c\u91c7\u7528\u5206\u4f4d\u6570\u6821\u51c6\u7a0b\u5e8f\u8c03\u6574\u5168\u5c40\u6216\u7c7b\u7279\u5b9a\u7684\u62d2\u7edd\u9608\u503c\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u6027\u62d2\u7edd\u6539\u5584\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u71b5\u57fa\u62d2\u7edd\u5728\u6240\u6709\u75c5\u7406\u5b66\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747AUC\u3002", "conclusion": "\u8be5\u7814\u7a76\u652f\u6301\u5c06\u9009\u62e9\u6027\u9884\u6d4b\u6574\u5408\u5230AI\u8f85\u52a9\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u66f4\u5b89\u5168\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\u3002"}}
{"id": "2509.10454", "pdf": "https://arxiv.org/pdf/2509.10454", "abs": "https://arxiv.org/abs/2509.10454", "authors": ["Hang Yin", "Haoyu Wei", "Xiuwei Xu", "Wenxuan Guo", "Jie Zhou", "Jiwen Lu"], "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CoRL 2025. Project page: [this https\n  URL](https://bagh2178.github.io/GC-VLN/)", "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bfc\u822a\u6307\u4ee4\u5206\u89e3\u4e3a\u7a7a\u95f4\u7ea6\u675f\u56fe\u5e76\u8fdb\u884c\u7ea6\u675f\u4f18\u5316\u6c42\u89e3\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u65b0\u73af\u5883", "motivation": "\u73b0\u6709\u96f6\u6837\u672cVLN\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u79bb\u6563\u73af\u5883\u6216\u9700\u8981\u5728\u8fde\u7eed\u6a21\u62df\u5668\u73af\u5883\u4e2d\u8fdb\u884c\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u6cdb\u5316\u548c\u90e8\u7f72", "method": "\u6784\u5efa\u7a7a\u95f4\u7ea6\u675f\u5e93\uff0c\u5c06\u4eba\u7c7b\u6307\u4ee4\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08\u5305\u542b\u8def\u5f84\u70b9\u8282\u70b9\u3001\u5bf9\u8c61\u8282\u70b9\u548c\u8fb9\uff09\uff0c\u901a\u8fc7\u7ea6\u675f\u6c42\u89e3\u5668\u8fdb\u884c\u56fe\u7ea6\u675f\u4f18\u5316\u786e\u5b9a\u8def\u5f84\u70b9\u4f4d\u7f6e\uff0c\u5e76\u91c7\u7528\u5bfc\u822a\u6811\u548c\u56de\u6eaf\u673a\u5236\u5904\u7406\u65e0\u89e3\u6216\u591a\u89e3\u60c5\u51b5", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u5bfc\u822a\u6548\u7387\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\u80fd\u6709\u6548\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u6307\u4ee4\u96c6", "conclusion": "\u8be5\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u81ea\u4e3b\u7684\u5bfc\u822a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u901a\u8fc7\u7ea6\u675f\u9a71\u52a8\u8303\u5f0f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u73af\u5883\u9002\u5e94"}}
