<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 65]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 论文提出了一种AI系统，通过Truth Sleuth和Trend Bender两个代理，分别进行视频内容的事实核查和在评论区引导讨论，以对抗YouTube上的错误信息。


<details>
  <summary>Details</summary>
Motivation: 错误信息在数字世界中快速传播，对社会构成威胁，需要创新的解决方案来应对。

Method: 系统采用RAG方法提取并验证视频中的声明，生成报告；Trend Bender利用报告生成有说服力的评论，并通过自评估循环优化输出。

Result: 实验表明系统在事实核查和用户互动方面具有高准确性和潜力。

Conclusion: AI驱动的干预措施能有效对抗错误信息，促进更明智的在线讨论。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [2] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp是一款基于智能手机的离线情感支持应用，利用优化的LLM模型提供心理健康支持，解决了网络连接和数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 数字心理健康平台面临用户可访问性、网络连接和数据隐私等挑战，需要一种离线解决方案。

Method: 开发EmoSApp，使用Torchtune和Executorch优化并部署LLaMA-3.2-1B-Instruct模型，基于14,582个心理健康QA对和多轮对话数据微调。

Result: 定性评估显示EmoSApp能连贯、共情地回应用户问题；定量评估证明模型在低资源环境下有效。

Conclusion: EmoSApp为便携、安全和定制化的AI心理健康解决方案提供了范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [3] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 提出了一种模块化工具链，用于处理敏感、异构的文本数据，支持隐私保护的大规模分析。


<details>
  <summary>Details</summary>
Motivation: 法律、医疗和行政领域的非结构化文本数据资源丰富但未充分利用，主要因敏感信息和语言结构异质性而受限。

Method: 使用开源模型构建工具链，通过LLM提示标准化、总结和翻译文本，结合实体识别和规则方法匿名化。

Result: 在瑞典法院案例中验证工具链有效匿名化并保留语义内容，支持半自动化内容分析。

Conclusion: 该工具链为隐私敏感领域的大规模文本分析提供了新可能。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [4] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 论文提出了一种针对自然语言解释（NLEs）的更新版XAI分类法，以支持透明AI系统的治理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，NLEs成为解释模型行为的关键工具，需要对其特性和治理影响进行系统研究。

Method: 基于可解释AI（XAI）文献，构建了一个针对NLEs的三维分类法，涵盖上下文、生成与呈现、评估三个方面。

Result: 分类法为研究者、审计者和政策制定者提供了描述、设计和改进NLEs的框架。

Conclusion: 该分类法有助于提升AI系统的透明度和治理效果。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [5] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA是一个模块化框架，通过轻量级LoRA适配器和KL正则化训练，减少大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务中表现出色，但存在幻觉问题，影响实际部署的可信度。

Method: 结合自动提示重写、混合检索和低秩适配器调优，通过幻觉检测模块和反馈校正循环提升事实准确性。

Result: AutoRAG-LoRA显著减少了事实漂移，同时保持了模型的效率和模块化。

Conclusion: 该框架有效解决了大语言模型的幻觉问题，提升了生成内容的可信度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [6] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 论文探讨了如何通过语言模型表达不确定性以增强用户信任，提出模仿人类沟通的“拟人化不确定性”概念，并分析了现有研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型输出常过于自信，影响其可信度，需通过不确定性表达提升人机协作效果并减少潜在危害。

Method: 综述人类不确定性沟通研究，分析数据偏见，提出“拟人化不确定性”概念，并分解为未来研究方向。

Result: 揭示了语言模型不确定性表达中的偏见，提出了模仿人类沟通的解决方案。

Conclusion: 未来研究应关注拟人化不确定性，以提升语言模型的不确定性表达效果和用户信任。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [7] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: PLEX是一种无需扰动的高效局部解释方法，通过利用LLM的上下文嵌入和Siamese网络，显著降低了计算开销，同时保持了与LIME和SHAP类似的解释效果。


<details>
  <summary>Details</summary>
Motivation: LLM在文本分类中表现优异，但其复杂性导致解释性差，现有XAI方法（如LIME和SHAP）计算成本高。

Method: PLEX利用LLM的上下文嵌入和Siamese网络，通过一次性训练避免后续扰动，实现高效解释。

Result: 在四个分类任务中，PLEX与LIME和SHAP的一致性超过92%，且在部分情况下表现更优，计算效率提升显著。

Conclusion: PLEX为LLM的文本分类提供了一种高效且解释性强的解决方案。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [8] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 研究分析了大型语言模型（LLM）如何通过情感轮框架建模用户情绪，发现其自然形成与人类心理模型一致的情感层次结构，且模型越大层次越复杂。同时揭示了社会经济角色对情绪识别的系统性偏见，人类研究显示LLM内化了社会感知特征。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在对话代理中的广泛应用，理解其如何建模用户情绪对伦理部署至关重要。

Method: 基于情感轮的心理框架，分析模型输出中情绪状态的概率依赖关系。

Result: LLM自然形成与人类心理模型一致的情感层次结构，且模型越大层次越复杂；同时发现情绪识别中存在社会经济角色的系统性偏见。

Conclusion: 研究不仅揭示了LLM中涌现的情感推理能力，还表明基于认知理论开发更好的模型评估方法具有潜力。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [9] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 本文研究了成人服务网站（ASW）广告文本的语言建模方法，开发了高效的定制Transformer模型，显著提升了文本分析的准确性，并应用于三项具体任务。


<details>
  <summary>Details</summary>
Motivation: ASW广告文本因包含大量表情符号、语法混乱和故意模糊等特点，难以分析，但对其有效分析有助于识别性交易受害者。

Method: 研究了多种语言建模方法，包括信息检索、预训练Transformer和定制Transformer模型，并对比了BERT-base、RoBERTa和ModernBERT等模型。

Result: 定制Transformer模型在准确性、召回率、F1分数和ROC AUC上优于其他模型，并成功应用于ASW数据的三项任务。

Conclusion: 定制Transformer模型为ASW文本分析提供了显著进步，可支持多种下游应用和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [10] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练文本嵌入模型增强标记属性图的语义分析，提升节点分类和关系预测任务的效果。


<details>
  <summary>Details</summary>
Motivation: 标记属性图中丰富的文本属性未被充分利用，希望通过嵌入技术增强语义分析能力。

Method: 将预训练语言模型的嵌入技术应用于图的节点和边属性，不改变图结构。

Result: 文本语义显著提升了属性图分析的准确性和可解释性。

Conclusion: 文本嵌入技术可有效增强属性图的语义分析能力，适用于多种下游任务。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [11] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA是首个评估模型解读科学文献中示意图能力的基准，包含1,500个专家标注示例，测试了18种前沿多模态基础模型，发现其与人类专家存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的示意图包含重要信息，但现有模型在解读此类多模态内容时能力有限，需专门基准评估和改进。

Method: 构建MISS-QA基准，包含465篇科学论文中的1,500个标注示例，测试模型对示意图的解读及回答问题能力。

Result: 18种前沿模型（如o4-mini、Gemini-2.5-Flash）表现显著低于人类专家，错误分析揭示了模型的局限性。

Conclusion: MISS-QA揭示了当前模型在理解科学文献多模态内容上的不足，为未来改进提供了关键方向。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [12] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 研究发现，社交平台上的仇恨言论并不总是因获得社会认同而加剧，某些情况下甚至呈现负相关。


<details>
  <summary>Details</summary>
Motivation: 探讨社会认同理论在仇恨言论中的作用，验证社交认同是否会加剧仇恨言论的传播与极端化。

Method: 分析2018-2021年Parler平台上1.1亿条帖子，观察点赞数与后续仇恨言论的关系。

Result: 点赞数与后续仇恨言论无显著关联，甚至在某些时间区间呈现负相关。

Conclusion: 社会认同对仇恨言论的强化机制在特定平台上可能表现不同，需进一步研究。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [13] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在司法系统中的公平性问题，提出了一个评估框架和数据集JudiFair，并通过实验揭示了LLMs普遍存在的不一致性、偏见和不平衡不准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险领域（如司法）的应用日益广泛，但其公平性及其对社会正义的影响尚未充分研究。确保LLMs作为法官时的公平性是建立其可信度的前提。

Method: 基于司法公平理论，构建了一个包含65个标签和161个值的评估框架，并开发了三个评估指标（不一致性、偏见、不平衡不准确性）来评估LLMs的公平性。

Result: 实验发现16个LLMs普遍存在不公平问题，尤其在人口统计标签上偏见更明显。调整温度参数可影响公平性，但模型规模、发布时间和来源国无显著影响。

Conclusion: 论文提出了一个公开工具包，支持未来研究和改进LLMs的公平性。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [14] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 论文研究了对话生成中用户与系统风格相似性对用户偏好的影响，区分了主观与客观相似性，并发现主观相似性与用户偏好强相关。


<details>
  <summary>Details</summary>
Motivation: 探讨用户与系统风格相似性对用户印象的影响，区分主观与客观相似性的差异。

Method: 构建包含用户偏好、主观风格相似性和客观风格相似性的数据集，并进行分析。

Result: 主观风格相似性与用户偏好呈强正相关，且与客观相似性存在差异。

Conclusion: 需区分主观与客观评价，理解各自在风格相似性与用户偏好关系中的作用。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [15] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 提出HanjaBridge技术，通过注入汉字的语义信息解决韩语中的同音异义词问题，显著提升韩语理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言（如韩语）中因同音异义词导致的语义模糊问题。

Method: 采用HanjaBridge技术，在持续预训练框架中为同音词提供所有可能的汉字候选，结合知识蒸馏防止灾难性遗忘。

Result: 在KoBALT基准上相对提升21%，并观察到中韩跨语言迁移的积极效果。

Conclusion: HanjaBridge在无需推理时额外成本的情况下，显著提升了韩语理解能力。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [16] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在类比推理任务中与人类表现的对比，重点关注语义表示和显式提示的效果。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在类比推理任务中是否能够接近人类表现，并探索其作为人类推理模型的潜力。

Method: 通过故事类比的映射任务，使用句子嵌入评估语义表示，并测试显式提示的效果，比较不同模型大小和架构的表现。

Result: 研究发现LLMs在类比推理中表现不一，模型大小和架构对性能有显著影响，但仍缺乏人类般的稳健推理能力。

Conclusion: LLMs在类比推理任务中显示出潜力，但与人类表现仍有差距，需进一步研究以提升其推理能力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [17] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: DS@GT团队参与eRisk 2025挑战赛，采用提示工程策略，利用LLM进行BDI-II评估，生成结构化JSON输出，评估模型间一致性和内部一致性，最终成绩排名第二。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过大型语言模型（LLM）进行对话式抑郁症检测，探索提示工程策略的有效性。

Method: 采用提示工程策略，多种LLM基于BDI-II标准生成结构化JSON输出，评估模型间一致性和内部一致性。

Result: 最佳提交成绩为DCHR = 0.50, ADODL = 0.89, ASHR = 0.27，在官方排行榜中排名第二。

Conclusion: 提示工程策略有效，模型输出与BDI-II标准一致，能够分析影响症状预测的对话线索。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [18] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign通过微调大语言模型（LLM），将其视为另一种自然语言，解决了手语生成的复杂性和独特规则问题。


<details>
  <summary>Details</summary>
Motivation: 手语生成因复杂性和独特规则，大语言模型的影响有限，需要一种方法弥合手语与口语的差异。

Method: 采用逐步提示策略，提取LLM中的手语知识，微调模型以学习文本与手语的对应关系。

Result: 在How2Sign和Phoenix14T数据集上，TEAM-Sign有效结合LLM的手语知识和推理能力，对齐了手语与口语的分布和语法规则。

Conclusion: TEAM-Sign通过LLM的微调和逐步提示策略，成功提升了手语生成的效果。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [19] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于分层低秩适应（LoRA）的方法，用于检测英语和西班牙语推文中的性别歧视，通过条件适配器路由显式建模标签依赖关系，并在多语言训练中实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本性别歧视检测任务，特别是在多语言环境下，传统方法需要复杂的处理或独立模型，而本文旨在通过参数高效微调简化流程并提升性能。

Method: 采用分层LoRA适配器（rank=16，QLoRA 4-bit）对Llama 3.1 8B的所有线性变换进行适应，显式建模三个子任务的标签依赖关系，并通过多语言训练实现跨语言迁移。

Result: 方法在多语言训练中实现了1.7-2.4%的F1提升，训练时间减少75%，模型存储减少98%，且在三个子任务中表现优异（如二元分类F1为0.6774）。

Conclusion: 通过分层LoRA和多语言训练，本文方法在参数高效性和性能之间取得了平衡，为多语言性别歧视检测提供了高效解决方案。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [20] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2是HUMANE团队为FEVER-25研讨会AVeriTeC共享任务开发的系统，是去年最佳开源模型HerO的增强版，通过文档摘要、答案重构、后训练量化等技术提升了证据质量和验证效率，最终在排行榜上排名第二。


<details>
  <summary>Details</summary>
Motivation: 改进去年最佳开源模型HerO，提升证据质量和验证效率，同时优化计算资源使用。

Method: 采用文档摘要和答案重构提升证据质量，通过后训练量化优化验证预测，并集成更新的语言模型主干。

Result: 在排行榜上排名第二，运行时间最短，展示了高效性和实际应用的潜力。

Conclusion: HerO 2在保持高效的同时显著提升了性能，适用于实际事实验证任务。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [21] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 论文提出了首个韩语新闻立场检测数据集K-News-Stance，并开发了JoA-ICL框架，通过分段立场检测提升长文本立场识别效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有立场检测研究局限于短文本和高资源语言的问题，同时减少新闻推荐系统中的过滤气泡和政治极化。

Method: 引入K-News-Stance数据集，并提出JoA-ICL框架，利用语言模型代理检测关键段落立场并聚合为全文立场。

Result: JoA-ICL优于现有方法，能有效识别长新闻立场，并在新闻推荐和媒体偏见分析中展现应用潜力。

Conclusion: 分段立场检测方法能提升长文本立场识别效果，有助于新闻多样性和媒体偏见分析。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [22] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 该研究提出了一种基于大型语言模型（LLM）的临床NLP流程，用于从非结构化临床笔记中提取心血管疾病（CVD）的早期指标，并通过领域适应和提示工程优化性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病的及时识别和准确风险分层对降低全球死亡率至关重要，但现有预测模型主要依赖结构化数据，忽略了非结构化临床笔记中的有价值信息。

Method: 研究采用了领域适应的大型语言模型，结合心血管特定微调、提示推理和实体感知推理，从自由文本报告中提取症状并进行上下文关联。

Result: 在MIMIC-III和CARDIO-NLP数据集上的评估显示，该方法在精确率、召回率、F1分数和AUROC方面表现优异，临床相关性高（kappa = 0.82）。

Conclusion: 该研究展示了LLM在临床决策支持系统（CDSS）中的潜力，能够提升早期预警系统，并将患者叙述转化为可操作的风险评估。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [23] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 论文提出了一种基于混合Transformer的情感分析框架，用于分析孟加拉国七月革命期间社交媒体上的公众情绪，结合多种预训练模型和机器学习分类器，取得了83.7%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过情感分析技术解码孟加拉语社交媒体评论中的公众情绪，以理解七月革命期间的社会动态。

Method: 采用混合Transformer框架（包括BanglaBERT、mBERT、XLM-RoBERTa和提出的XMB-BERT），结合PCA降维和多种机器学习分类器进行情感分析。

Result: 提出的XMB-BERT与投票分类器组合达到83.7%的准确率，优于其他模型。

Conclusion: 研究表明机器学习技术能有效分析低资源语言（如孟加拉语）的社会情绪，为类似研究提供了新思路。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [24] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 论文探讨了在跨境金融活动中使用大型语言模型（LLMs）改进实体匹配的准确性，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 跨境金融活动增加，需要准确识别和分类外国实体，以支持风险管理、合规和防止金融不当行为。传统方法在语言变体和法律形式变化下效果不佳。

Method: 评估了传统方法（如Jaccard、余弦和Levenshtein距离）、基于Hugging Face的LLMs和接口型LLMs（如Microsoft Copilot、阿里巴巴的Qwen 2.5），使用65个葡萄牙公司案例数据集。

Result: 传统方法准确率超92%，但假阳性率高（20-40%）；接口型LLMs表现更优，准确率超93%，F1分数超96%，假阳性率更低（40-80%）。

Conclusion: LLMs在实体匹配任务中优于传统方法，尤其在处理语言变体和法律变化时表现更佳。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [25] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: DIJA是一种针对扩散式大语言模型（dLLMs）的越狱攻击框架，揭示了其独特的安全弱点，导致标准对齐机制失效。


<details>
  <summary>Details</summary>
Motivation: 现有对齐机制无法保护dLLMs免受上下文感知的掩码输入对抗性提示攻击，暴露了新的安全漏洞。

Method: DIJA通过构造对抗性交错掩码文本提示，利用dLLMs的双向建模和平行解码机制，绕过对齐机制。

Result: DIJA在多种评估指标上显著优于现有越狱方法，最高可达100%的关键词攻击成功率。

Conclusion: 研究强调了重新设计dLLMs安全对齐机制的紧迫性。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [26] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）中的数据中毒攻击，揭示了多个触发器可以共存且互不干扰的现象，并提出了一种基于分层权重差异分析的后处理恢复方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注攻击的有效性，对触发器机制及多触发器交互的理解有限，本文旨在填补这一空白。

Method: 提出一个框架研究LLMs中的中毒问题，展示多触发器共存现象，并通过分层权重差异分析设计后处理恢复方法。

Result: 实验表明，多触发器可以共存且具有鲁棒性，后处理方法能高效移除触发器行为。

Conclusion: LLMs存在更广泛且持久的中毒漏洞，提出的后处理方法为多触发器中毒提供了实用防御方案。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [27] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 提出了一种基于集成系统的多语言多模态推理方法，在ImageCLEF 2025 EXAMS V挑战中表现优异，通过精心设计的提示词和跨语言增强，轻量级OCR-VLM集成系统优于端到端模型。


<details>
  <summary>Details</summary>
Motivation: 解决多语言教育场景中的高精度推理问题，探索轻量级集成系统与提示词设计的有效性。

Method: 集成Gemini系列模型（Gemini 2.5 Flash、Gemini 1.5 Pro、Gemini 2.5 Pro），通过少样本和零样本提示协调，并进行广泛的消融研究和跨语言数据增强。

Result: 在官方排行榜中，系统（Team MSA）在多语言赛道以81.4%准确率排名第一，并在13个语言赛道中11个领先，最高达95.07%（克罗地亚语）和92.12%（意大利语）。

Conclusion: 轻量级OCR-VLM集成系统结合精确提示策略和跨语言增强，在高风险多语言教育场景中优于端到端模型。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [28] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）如何记忆并泄露个人信息，提出了一种量化模型中个人-事实关联的方法，以支持机器遗忘和GDPR合规。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLMs在记忆个人数据时可能违反GDPR的“被遗忘权”问题，现有方法无法有效识别模型中存储的个体级数据。

Method: 方法包括构建WikiMem数据集（含5000+自然语言测试样本）和一种模型无关的度量标准，通过校准负对数似然对真实值与反事实进行排序。

Result: 评估了15种LLMs（参数范围410M-70B），发现记忆程度与个体网络存在感和模型规模相关。

Conclusion: 研究为识别LLMs中个体级记忆数据提供了基础，支持动态构建遗忘集以满足机器遗忘和GDPR要求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [29] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 研究探讨了多代理系统（MAS）在定性研究中的表现，发现温度和代理角色对共识构建有显著影响，但MAS在编码准确性上并未优于单代理系统。


<details>
  <summary>Details</summary>
Motivation: 探索多代理系统（MAS）在定性研究中的潜力，尤其是其在编码和数据标注方面的表现，以验证其是否优于单代理系统。

Method: 通过实验研究，使用6种开源LLM和18种实验配置，分析了77,000多个编码决策，比较了不同温度和代理角色对共识构建和编码准确性的影响。

Result: 温度和代理角色显著影响共识构建，但MAS在编码准确性上未表现出明显优势，仅在某些特定条件下（如特定模型和代码类别）有所提升。

Conclusion: 研究揭示了LLM在定性研究中的局限性，挑战了多样MAS角色能带来更好结果的假设，并开源了实验代码。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [30] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 本文介绍了西班牙语和加泰罗尼亚语的社会偏见评估数据集EsBBQ和CaBBQ，用于评估LLMs在西班牙社会背景下的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 现有资源主要针对英语和美国社会背景，缺乏其他语言和地区的社会偏见评估工具。

Method: 基于BBQ数据集，设计了西班牙语和加泰罗尼亚语的平行数据集，采用多选题QA形式评估10类社会偏见。

Result: LLMs在模糊场景中倾向于选择错误答案，且高QA准确率常与依赖社会偏见相关。

Conclusion: EsBBQ和CaBBQ填补了非英语社会偏见评估的空白，揭示了LLMs在非英语环境中的偏见问题。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [31] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM是一种基于LLM的框架，通过提示链和思维链推理从RFC文档中提取精确的FSM，解决了现有技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有FSM提取技术在可扩展性、覆盖范围和自然语言规范模糊性方面存在不足，需要更高效的解决方案。

Method: FlowFSM结合LLM、提示链和思维链推理，系统处理协议规范，识别状态转换并构建结构化规则书。

Result: 在FTP和RTSP协议上的实验表明，FlowFSM提取精度高，且减少了虚假转换。

Conclusion: FlowFSM展示了基于代理的LLM系统在协议分析和FSM推断中的潜力，适用于网络安全和逆向工程。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [32] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 论文提出SAE-LAPE方法，通过稀疏自编码器识别大语言模型中的语言特定特征，发现这些特征主要位于模型中后层，并影响多语言性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以从多语言表征中分离语言特定单元，因此探索稀疏自编码器以学习跨语言的具体和抽象概念。

Method: 使用基于特征激活概率的SAE-LAPE方法，在LLM的前馈网络中识别语言特定特征。

Result: 发现语言特定特征主要位于模型中后层，可解释性强，且对多语言性能和语言输出有影响，语言识别性能与fastText相当。

Conclusion: SAE-LAPE方法有效识别语言特定特征，为理解LLM的多语言机制提供了新视角。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [33] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: KV-Latent通过降采样Key-Value向量维度至潜在空间，显著减少KV Cache占用并提升推理速度，仅需少量额外训练。


<details>
  <summary>Details</summary>
Motivation: Decoder架构的KV Cache在推理过程中逐渐增加，成为内存消耗和数据传输带宽的主要效率瓶颈。

Method: 提出KV-Latent范式，降采样Key-Value向量至潜在空间，并改进Rotary Positional Embedding的频率采样机制以增强稳定性。

Result: 实验表明，该方法在减少KV Cache占用和提升推理速度方面效果显著，且对模型性能影响较小。

Conclusion: KV-Latent为构建高效语言模型系统提供了新思路，并开辟了KV Cache节省和高效LLMs的新可能性。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [34] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的自动形式化管道，利用错误反馈实现无需训练的形式化方法，并构建了一个高质量的数学问题数据集。


<details>
  <summary>Details</summary>
Motivation: 推动形式化数学推理的发展，解决自然语言数学问题与形式化语言之间的转换难题。

Method: 基于大语言模型的自动形式化管道，结合错误反馈和少样本学习，构建并评估数据集。

Result: 创建了包含3,922个自然语言问题和9,787个Lean形式化问题的数据集，64.46%质量较高；实验证明错误反馈和增加采样数能提升形式化效果。

Conclusion: 该数据集适合作为自动定理证明器的基准，且提出的方法在形式化任务中表现优异。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [35] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 论文提出首个中文细粒度仇恨言论数据集（STATE ToxiCN），并研究了编码仇恨术语及大语言模型的解释能力，提出整合标注词典的方法以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 中文仇恨言论检测研究滞后，且缺乏细粒度标注数据和编码仇恨术语的解释性研究。

Method: 引入STATE ToxiCN数据集，研究编码仇恨术语及大语言模型解释能力，提出整合标注词典的方法。

Result: 显著提升仇恨言论检测性能，为中文仇恨言论检测的可解释性研究提供资源。

Conclusion: 该研究填补了中文仇恨言论检测的资源空白，并提升了模型的可解释性和性能。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [36] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot是一个多代理大型语言模型系统，旨在提升罗马尼亚语医生在文本远程医疗中的沟通质量，而非临床准确性。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中医生书面回复的沟通质量问题，而非医学准确性。

Method: 使用三个LLM代理，通过DSPy自动优化提示，基于低资源罗马尼亚语数据设计，部署开放权重模型。

Result: 实证评估和41名医生的实际部署显示用户评价和回复质量显著提升。

Conclusion: Dr.Copilot是罗马尼亚医疗环境中首批实际部署的LLM系统之一，有效提升了沟通质量。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [37] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 论文提出了一种名为ConVA的方法，通过控制LLM内部价值向量的激活，直接对齐模型与人类价值观，确保一致性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，如何使其与人类价值观对齐变得至关重要，以提供清晰、透明且能适应变化的模型行为。

Method: 提出上下文控制的价值向量识别方法，以及门控价值向量激活方法，确保准确、无偏且不影响模型性能的价值控制。

Result: 实验表明，该方法在10种基本价值观上实现了最高的控制成功率，同时保持模型性能和流畅性。

Conclusion: ConVA方法有效实现了LLMs与人类价值观的对齐，且能抵御恶意输入的影响。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [38] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 该论文提出了一种结合人类专家和大型语言模型（LLM）的方法，用于评估学术论文的方法新颖性，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性评估方法（如专家判断或引用组合）存在知识有限和有效性不确定的问题，因此需要结合人类和LLM的优势。

Method: 从同行评审报告中提取新颖性相关句子，利用LLM总结论文方法部分，并设计了一个基于稀疏注意力的文本引导融合模块来整合人类和LLM知识。

Result: 实验表明，该方法在预测论文方法新颖性方面优于大量基线模型。

Conclusion: 结合人类和LLM知识的方法能有效提升新颖性评估的准确性。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [39] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次系统比较了不同流程模型表示（PMRs）在大型语言模型（LLMs）应用于流程建模（PMo）任务中的表现，并提出了PMo数据集。


<details>
  <summary>Details</summary>
Motivation: 现有PMRs在结构、复杂性和可用性上差异大，且缺乏系统比较，导致LLM在PMo任务中的应用难以评估。

Method: 通过PMo数据集（含55个流程描述及9种PMRs模型），从LLM适用性和流程模型生成（PMG）性能两个维度评估PMRs。

Result: Mermaid在六项PMo标准中综合得分最高，BPMN text在流程元素相似性上表现最佳。

Conclusion: 研究为LLM在PMo任务中的PMR选择提供了实证依据，Mermaid和BPMN text分别在不同场景下表现突出。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [40] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 论文探讨了在Transformer模型中应用加权损失函数以解决多标签情感检测中的数据不平衡问题，结果显示对高频情感类别有效，但对少数类别影响有限。


<details>
  <summary>Details</summary>
Motivation: 解决多标签情感检测中的数据不平衡问题，避免传统重采样方法的计算负担。

Method: 使用BERT、RoBERTa和BART模型，在BRIGHTER数据集上应用动态调整类权重的加权损失函数。

Result: 加权损失函数提高了高频情感类别的性能，但对少数类别影响有限。

Conclusion: 该方法在多标签情感检测中有效，但仍需进一步解决少数类别的性能问题。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [41] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 论文提出DCR框架，用于检测和量化大语言模型（LLMs）在评估数据中的污染问题，并通过模糊推理系统生成统一的DCR因子，调整性能指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展导致评估数据污染（BDC）问题加剧，影响性能指标的真实性，需要一种轻量级、可解释的方法来检测和量化污染。

Method: 提出数据污染风险（DCR）框架，通过四个粒度级别（语义、信息、数据和标签）检测污染，并使用模糊推理系统生成DCR因子。

Result: 在9个LLMs（0.5B-72B）上验证，DCR框架能准确诊断污染严重程度，调整后的准确率误差在4%以内。

Conclusion: DCR框架高效透明，可集成到常规评估中，促进公平比较并提升LLM基准测试的可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [42] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0结合了非推理模式和推理模式，提升了可用性和推理能力，支持多语言（英语、韩语、西班牙语），并推出两种规模模型（32B和1.2B），性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为迎接AI代理时代，EXAONE 4.0旨在提供更强大的推理能力和多语言支持，同时保持高可用性。

Method: 整合非推理和推理模式，扩展多语言支持（新增西班牙语），并推出两种规模的模型（32B和1.2B）。

Result: EXAONE 4.0在同类开源模型中表现优异，甚至可与前沿模型竞争。

Conclusion: EXAONE 4.0通过多语言支持和高效模型设计，为AI代理时代提供了强大工具，模型已公开供研究使用。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [43] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 论文引入因果CoT图（CCGs）以揭示链式思维（CoT）提升语言模型推理性能的机制，并通过数据集KisMATH验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维（CoT）如何提升语言模型在推理任务中的表现，缺乏共识机制。

Method: 提出因果CoT图（CCGs），自动从推理轨迹中提取有向无环图，建模语言模型输出的细粒度因果依赖。

Result: 通过KisMATH数据集验证，CCGs中的推理节点是最终答案的中介，且语言模型内部结构与CCGs相似。

Conclusion: KisMATH为链式思维在语言模型推理中的作用提供了新研究途径。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [44] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 论文介绍了Ettin模型套件，比较了编码器-解码器架构在分类、检索和生成任务中的表现，发现各自擅长的领域不同，且直接转换架构效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究编码器和解码器架构在不同任务中的表现差异，并提供公平比较的基础。

Method: 使用相同训练方法训练不同规模的编码器和解码器模型，并进行任务适配实验。

Result: 编码器在分类和检索任务中表现更好，解码器在生成任务中更优；直接转换架构效果不佳。

Conclusion: 编码器和解码器架构各有优势，直接转换效果不理想，建议根据任务选择合适架构。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [45] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 研究发现，通过提示可以控制大型语言模型的推理策略，但单一策略无法持续提升准确性。自适应选择最优策略可能提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通常倾向于单一推理策略，可能限制其在多样化推理任务中的表现。研究旨在探索提示是否能控制LLMs的推理策略及其对逻辑问题解决的影响。

Method: 通过实验评估不同提示对LLMs推理策略的影响，并提出方法指导模型自适应选择最优策略。

Result: 实验表明，单一策略无法持续提升准确性，但自适应策略选择可能改善性能。

Conclusion: 研究提出了指导LLMs选择推理策略的新方法，为优化其推理能力提供了新思路。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [46] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 论文介绍了HKGAI-V1，一个为香港定制的多语言大语言模型，通过深度调整和检索增强生成技术，实现了对本地文化和法律的高效支持。


<details>
  <summary>Details</summary>
Motivation: 解决香港多语言环境、独特的社会法律框架及文化价值观需求，建立符合本地特色的AI基础设施。

Method: 基于DeepSeek架构，通过全参数微调和检索增强生成（RAG）技术，开发了HKGAI-V1模型，并设计了对抗性香港价值观基准测试。

Result: HKGAI-V1在处理香港特定文化敏感查询上优于通用模型，并提供了数字主权治理框架。

Conclusion: 论文不仅提供了技术成果，还为其他地区开发本地化AI系统提供了可复制的蓝图。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [47] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 研究发现，在酒店亮点总结任务中，简单的词重叠指标与人类判断相关性高（0.63），优于复杂方法；LLM生成总结质量高，但评估不可靠；错误信息风险最大。


<details>
  <summary>Details</summary>
Motivation: 探讨在酒店亮点总结任务中，如何评估生成内容对输入数据的忠实性，并比较不同评估方法的有效性。

Method: 通过人类评估活动（分类错误评估和细粒度标注），比较传统指标、可训练方法和LLM作为评估者的表现。

Result: 词重叠指标与人类判断相关性高（0.63），LLM评估不可靠（易过度或不足标注），错误信息风险最大。

Conclusion: 简单指标在跨域数据中表现优异，LLM生成总结可用但评估需谨慎，错误信息是主要风险。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet提出了一种基于小波变换和因果推理的低光图像增强方法，通过全局和局部因果分析优化图像增强效果。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽视实例级语义信息和特征特性，CWNet旨在通过因果推理解决这一问题。

Method: CWNet结合因果推理和小波变换，包括全局度量学习和局部CLIP语义损失，优化频率信息恢复。

Result: 实验表明CWNet在多个数据集上显著优于现有方法。

Conclusion: CWNet通过因果分析和小波变换实现了更精确的低光图像增强。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [49] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 提出一种结合外部生物知识的新框架，通过解耦扰动特异性和细胞系特异性表征，提升显微镜图像分析模型在新细胞系中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有高通量筛选技术在新细胞系中表现不佳，主要由于细胞形态和生物学异质性。

Method: 整合外部生物知识（如蛋白互作和转录组数据），构建知识图谱，解耦扰动特异性和细胞系特异性表征。

Result: 在RxRx数据库上验证，通过少样本微调显著提升新细胞系的图像分析性能。

Conclusion: 该方法在真实药物发现场景中表现出色，为表型筛选提供了有效工具。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [50] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该研究审计了两个先进的FER数据集，发现其中许多图像是摆拍而非自然表情，且模型对非白人或深肤色人群存在负面情绪预测偏差。


<details>
  <summary>Details</summary>
Motivation: 解决FER算法在自然表情识别和跨种族肤色表现上的性能下降问题，揭示数据集收集实践的影响。

Method: 随机抽样审计数据集中的图像，区分自然与摆拍表情，并测试模型在不同肤色人群中的表现。

Result: 发现数据集包含大量摆拍图像，且模型对非白人或深肤色人群存在负面情绪预测偏差。

Conclusion: 数据集质量和模型偏见可能影响FER在真实场景中的性能，需改进数据收集和模型训练以减少偏见。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [51] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 提出一种无需描述符的兴趣点匹配方法，显著降低内存使用，但匹配精度略低于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统兴趣点匹配需要计算和存储描述符，导致内存占用高，本文旨在消除这一需求。

Method: 在兴趣点检测阶段直接关联匹配点，无需生成或匹配描述符。

Result: 匹配精度略低于传统方法，但内存使用大幅减少。

Conclusion: 该方法在内存效率上有显著优势，适用于对内存敏感的场景。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [52] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一种新的航天器图像数据集，用于训练和评估实时图像分割模型，以支持太空中的自主检测系统。


<details>
  <summary>Details</summary>
Motivation: 太空中的航天器易受环境损害，人工或机器人维修成本高且风险大，因此需要可靠的自主检测系统。

Method: 使用真实航天器模型和合成背景创建了64k标注图像数据集，并添加噪声和失真模拟真实环境。微调YOLOv8和YOLOv11模型进行性能测试。

Result: 模型在模拟实时约束下达到Dice分数0.92，Hausdorff距离0.69，推理时间约0.5秒。

Conclusion: 数据集和模型为太空实时图像分割提供了有效基准，支持自主检测系统的开发。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [53] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种数据高效的方法，通过LLM代理系统增强空间推理能力，解决复杂室内仓库场景中的空间问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间理解任务上表现不足，需通过大规模微调提升能力，本文旨在提出更高效的方法。

Method: 构建LLM代理系统，整合多种工具进行空间推理和API交互，以解决复杂空间问题。

Result: 在AI City Challenge数据集上验证，系统在对象检索、计数和距离估计等任务中表现高精度和高效。

Conclusion: 该方法在数据效率和任务性能上优于传统大规模微调方法，适用于复杂空间推理场景。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [54] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT是一种嵌套ViT架构，通过动态调整计算资源提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有嵌套Transformer模型对所有输入分配相同计算资源，导致效率低下。

Method: 采用渐进式思考阶段和Token Recycling机制，动态调整注意力头。

Result: 在相同计算量下，准确率提升2.0-2.9个百分点。

Conclusion: ThinkingViT高效且兼容现有ViT，可作为插件升级。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [55] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）的自主目标检测框架（LAOD），通过动态生成场景特定对象名称，实现无需标签的零样本检测。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖固定类别集，灵活性不足；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。

Method: 利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，提出新指标CAAP和SNAP分别评估定位和命名能力。

Result: 在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，能够高效检测和命名新对象。

Conclusion: LAOD框架提升了开放世界理解的自主性和适应性。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [56] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM是一种改进的Grad-CAM方法，通过跨卷积层聚合信息并应用Winsorization技术，生成更鲁棒和连贯的显著图。


<details>
  <summary>Details</summary>
Motivation: 现有Grad-CAM方法仅关注最终卷积层或简单平均多层信息，可能掩盖重要语义或放大噪声，需改进以提升模型解释性。

Method: 提出Winsor-CAM，利用Winsorization技术衰减极端值，并通过用户可调阈值实现语义级调控。

Result: 在PASCAL VOC 2012数据集上，Winsor-CAM显著优于Grad-CAM和均匀层平均基线，定位指标更优。

Conclusion: Winsor-CAM通过多层解释和人机交互控制，推动了可信AI的发展。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [57] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来更新表示，提升模型适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法难以解释参数更新的贡献，稀疏编码框架旨在解决这一问题。

Method: 利用稀疏编码表示微调特征，特征字典原子作为基本构建块，稀疏系数指示原子重要性。

Result: 方法在图像编辑和文本到图像概念定制任务中表现优于基线微调方法。

Conclusion: 稀疏编码框架为微调提供了更高的可解释性和适应性。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [58] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 提出了一种结合LOF算法和YOLO-v11n的轻量级结直肠息肉检测框架，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠息肉及时准确检测对预防结直肠癌至关重要，现有方法需改进。

Method: 使用LOF算法过滤噪声数据，结合YOLO-v11n模型，通过5折交叉验证和增强策略训练。

Result: 精度95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%，性能优于先前方法。

Conclusion: 该方法适合临床实时应用，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [59] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super是一种改进的3D医学图像中心线追踪模型，解决了原版Trexplorer的重复分支和过早终止问题，并在多个数据集上表现优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 准确追踪管状树结构（如血管和气道）在医学图像中至关重要，但现有模型存在重复分支和过早终止的问题。

Method: 提出Trexplorer Super，通过新方法改进原版模型，并开发了三个不同难度的中心线数据集（一个合成，两个真实）用于评估。

Result: Trexplorer Super在所有数据集上均优于现有SOTA模型，且合成数据表现与真实数据表现不一致。

Conclusion: Trexplorer Super显著提升了中心线追踪性能，同时公开的代码和数据集为未来研究提供了支持。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [60] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 本文介绍了一种基于CNN的轻量级全球天气预报模型KAI-a，其性能与最先进模型相当，但计算需求显著降低。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的AI天气预报模型计算复杂度高，资源需求大，因此需要一种更高效的替代方案。

Method: KAI-a采用尺度不变架构和InceptionNeXt模块，结合地球系统数据的特性设计，训练于ERA5数据集。

Result: KAI-a在中等范围天气预报中表现优异，参数仅700万，训练时间短，且能有效捕捉极端天气事件。

Conclusion: KAI-a展示了CNN在天气预报中的潜力，为高效、轻量级的AI天气预报模型提供了新方向。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [61] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文提出两种新正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度标签不一致问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度标签不一致（TsDLI）问题，提升模型泛化性和可解释性。

Method: 提出Local Variation Loss (LVL)和Local-Global Consistency Loss (LGCL)两种正则化方法，结合数学原理和图论框架。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL表现优于现有方法，LVL综合排名最佳。

Conclusion: 新方法有效解决了标签不一致问题，平衡了可解释性和预测性能。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [62] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill是一种几何引导的弱监督自蒸馏框架，通过教师-学生学习和基于视场的掩码提升跨视图定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完全监督学习，需要昂贵的真实姿态标注，GeoDistill旨在减少标注需求并提升定位鲁棒性。

Method: 教师模型定位全景图像，学生模型通过基于视场的掩码预测有限视场图像的位置，通过对齐预测结果优化特征学习。

Result: 实验表明GeoDistill显著提升定位性能，并引入无需精确平面位置真实值的朝向估计网络。

Conclusion: GeoDistill为跨视图定位提供了可扩展且高效的解决方案。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [63] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为GAPL-SCD的图聚合原型学习方法，用于解决语义变化检测中的多任务优化问题，通过自适应权重分配和梯度旋转提升性能。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）需要同时优化多个任务，容易导致负迁移问题，影响模型性能。

Method: 设计了多任务联合优化框架，结合语义分割、变化检测和图聚合原型学习，采用自适应权重分配和梯度旋转方法缓解任务冲突。

Result: 在SECOND和Landsat-SCD数据集上实现了最先进的性能，显著提升了SCD任务的准确性和鲁棒性。

Conclusion: GAPL-SCD框架有效解决了多任务优化问题，为语义变化检测提供了更优的解决方案。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [64] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的新型ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，解决了身份模糊输入和随机生成过程带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复技术在视觉质量上有显著提升，但身份不确定性仍未解决，RIDFR旨在解决这一问题。

Method: RIDFR利用预训练扩散模型，结合内容注入模块和身份注入模块，并通过对齐学习抑制ID无关语义的干扰。

Result: 实验表明，RIDFR优于现有方法，能重建高质量且身份保真的结果，具有强鲁棒性。

Conclusion: RIDFR通过结合扩散模型和身份对齐，显著提升了人脸修复的身份保真度和鲁棒性。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [65] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 本文提出一个新的女性运动动作数据集WomenSports，并设计了一种结合通道注意力的CNN方法，用于小规模训练数据的运动分类，取得了89.15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有女性运动动作数据集缺乏足够的多样性和规模，限制了相关研究的发展。

Method: 提出WomenSports数据集，并设计了一种基于CNN的深度特征提取方法，结合通道注意力机制优化特征表示。

Result: 在WomenSports数据集上，使用ResNet-50实现了89.15%的分类准确率。

Conclusion: WomenSports数据集和提出的方法为女性运动动作分类提供了有效工具，数据集已公开供研究使用。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [66] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波注意力机制和射线编码器的新型HOI检测架构，旨在解决现有方法效率低和资源消耗大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测器在高效性和可靠性上表现不足，亟需一种更高效的架构来提升性能。

Method: 设计了小波注意力机制的主干网络和射线编码器，分别用于提取多阶交互特征和优化注意力区域。

Result: 在ImageNet和HICO-DET等基准数据集上验证了架构的有效性。

Conclusion: 提出的架构显著提升了HOI检测的效率和准确性，代码已开源。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [67] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait提出了一种通过残差学习解决步态识别中遮挡问题的方法，同时保持对完整步态的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前步态识别方法未解决遮挡问题，且现有方法需成对遮挡与完整序列，不实用。RG-Gait旨在改进遮挡识别而不影响完整步态性能。

Method: 将遮挡步态建模为完整步态表示的残差偏差，通过残差学习网络自适应整合残差。

Result: 在Gait3D、GREW和BRIAR数据集上验证，RG-Gait显著提升遮挡步态识别性能且不降低完整步态准确性。

Conclusion: 残差学习是解决遮挡步态识别并保留完整步态性能的有效方法。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [68] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN是一种轻量级架构设计，通过多阶空间特征和通道聚合模块提升性能，在ImageNet和COCO上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer中存在的简单性偏差和信息冗余问题，提升空间和通道信息处理效率。

Method: 采用可变感受野的核和波基通道聚合模块，动态捕获多阶空间特征并减少冗余。

Result: 在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数），性能优于基准。

Conclusion: SpaRTAN通过高效设计实现了高性能和参数效率，为视觉任务提供了新思路。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [69] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP利用CLIP模型进行零样本异常检测，通过特征匹配和跨模态对齐，结合批次内图像作为参考，并利用文本信息过滤噪声特征，显著提升了异常分类和分割性能。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测（ZSAD）在工业应用中至关重要，但现有方法在测试时缺乏标签信息，导致性能受限。FiSeCLIP旨在通过结合CLIP的特征匹配和跨模态对齐能力，解决这一问题。

Method: FiSeCLIP利用批次内其他图像作为参考，通过文本信息过滤噪声特征，并恢复CLIP的局部语义相关性，以适配细粒度异常检测任务。

Result: 在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，AU-ROC和F1-max指标分别比SOTA方法AdaCLIP提升4.6%和5.7%。

Conclusion: FiSeCLIP为ZSAD提供了一个强大的基线方法，展示了CLIP在零样本异常检测中的潜力，尤其是在工业应用中。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [70] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于语义显著区域的放射学报告生成方法（SISRNet），通过聚焦医学关键区域，解决现有方法因数据偏差导致的报告不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法生成的放射学报告虽然流畅，但因数据偏差（异常稀疏且细微）导致医学准确性不足，限制了临床应用。

Method: SISRNet通过细粒度跨模态语义识别医学关键显著区域，并在图像建模和报告生成中系统关注这些区域，从而捕捉细微异常。

Result: 在IU-Xray和MIMIC-CXR数据集上，SISRNet表现优于同类方法。

Conclusion: SISRNet通过语义显著区域引导，有效提升放射学报告的临床准确性，解决了数据偏差问题。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [71] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge的CBCT-to-MDCT翻译框架，结合GAN先验和人类引导的条件扩散，确保解剖保真度和感知可控性。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN或扩散模型在医学图像翻译中边界一致性和临床偏好对齐的不足。

Method: 结合GAN先验与人类反馈（通过CFG），通过迭代优化和锦标赛选择内化人类偏好。

Result: 在临床数据集上，RMSE、SSIM、LPIPS和Dice指标表现优异，仅需10步采样。

Conclusion: 该框架高效且有效，适用于实时、偏好对齐的医学图像翻译。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [72] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本提示调优的插件方法，用于个性化开放词汇语义分割任务，通过减少错误预测和增强文本提示表示来提升性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割（OVSS）无法理解个性化文本（如“我的杯子”）以分割用户感兴趣的特定区域，本文旨在解决这一问题。

Method: 提出了一种文本提示调优的插件方法，结合负掩模提议和视觉嵌入注入，以减少错误预测并增强文本提示表示。

Result: 在FSS$^\text{per}$、CUB$^\text{per}$和ADE$^\text{per}$等新基准上验证了方法的优越性。

Conclusion: 该方法在不影响原始OVSS性能的前提下，显著提升了个性化开放词汇语义分割的效果。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [73] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了一种双域去雾网络DGFDNet，结合空间域和频域特征，通过物理引导的退化对齐提升去雾性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂雾霾条件下性能不足，且计算成本高，频域与空间域特征耦合较弱。

Method: 设计了HAFM模块生成雾霾置信图，MGAM模块融合多尺度特征，PCGB分支迭代优化先验。

Result: 在四个基准数据集上达到最优性能，兼具鲁棒性和实时性。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [74] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D是一个新型多视角高分辨率踝足表面点云数据集，专注于自然步态下的踝足区域建模，用于评估3D点云补全方法。


<details>
  <summary>Details</summary>
Motivation: 步态中踝足复合体的运动学分析对生物力学研究和临床评估至关重要，但动态步态条件下的数据采集因遮挡和视角限制而具有挑战性。

Method: FootGait3D包含46名受试者的8,403帧点云数据，通过五相机深度传感系统采集，提供完整和部分视角的点云以评估补全方法。

Result: 数据集支持单模态和多模态补全网络的基准测试，为生物力学、临床步态分析和机器人应用提供详细3D模型。

Conclusion: FootGait3D推动了踝足建模研究，为步态分析和相关应用提供了宝贵资源。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [75] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Swin Transformer的端到端目标检测架构，用于高分辨率卫星图像，通过UpConvMixer和Fusion Blocks提升性能，在xView数据集上表现优于现有方法11.46%。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率卫星图像中目标检测的挑战，如多尺度对象和计算效率问题。

Method: 采用Swin Transformer替代CNN骨干网络，结合UpConvMixer块和Fusion Blocks，引入CBAM注意力机制和多路径头设计。

Result: 在xView数据集上达到32.95%的准确率，超越现有方法11.46%。

Conclusion: GLOD通过创新的架构设计，显著提升了卫星图像目标检测的性能和效率。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [76] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn提出了一种原型驱动的学习框架，用于语言引导的分割，通过原型驱动的语义近似模块减少对文本输入的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有医学语言引导分割方法依赖成对的图像-文本输入，导致许多缺乏配对报告的图像数据无法充分利用，且限制了临床应用。

Method: ProLearn引入原型驱动的语义近似（PSA）模块，从文本报告中提取语义信息并初始化原型空间，支持无文本输入的语义引导近似。

Result: 在QaTa-COV19、MosMedData+和Kvasir-SEG数据集上的实验表明，ProLearn在文本有限的情况下优于现有方法。

Conclusion: ProLearn通过减少对文本的依赖，提升了语言引导分割的适用性和性能。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [77] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP是一种新的局部3D高斯编辑框架，通过3D-GALP和正则化SDS损失实现精确和大幅度的部分级修改。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经表示和实例级编辑模型在多视图2D部分分割和SDS损失的模糊性方面存在局限性，难以实现精确的局部3D编辑。

Method: 提出3D-GALP模块生成鲁棒的3D掩码，并结合正则化SDS损失（包括L1锚定损失和高斯先验去除等）实现精确编辑。

Result: 实验表明，RoMaP在重建和生成的3D高斯场景中实现了最先进的局部编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更鲁棒和灵活的部分级编辑能力。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [78] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于关节角度的标记自由人体姿态估计（HPE）优化方法，通过傅里叶级数近似关节角度变化，设计双向循环网络优化HRNet，显著提升了姿态估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有HPE方法在关键点识别和轨迹平滑性上存在误差，且训练数据标注不准确限制了深度学习模型的性能。

Method: 1. 提出基于关节角度的人体姿态模型；2. 用高阶傅里叶级数近似关节角度变化生成高质量数据；3. 设计双向循环网络作为后处理模块优化HRNet。

Result: 在花样滑冰和霹雳舞等挑战性场景中，JAR方法优于现有HPE优化网络。

Conclusion: 基于关节角度的优化方法显著提升了HPE的准确性和鲁棒性。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [79] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了结构对称性和部分遮挡问题，并发布了一个新的数据集SKD。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器的单目姿态估计在在轨服务任务中至关重要，但现有关键点检测器对结构对称性和部分遮挡敏感。

Method: 提出GKNet，利用关键点图的几何约束，并发布SKD数据集用于验证。

Result: 实验表明GKNet在精度和有效性上优于现有方法。

Conclusion: GKNet为航天器姿态估计提供了高精度解决方案，代码和数据集已开源。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [80] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于交叉验证策略的深度学习模型，用于从GPR图像中自动识别道路地下病害（RSD），显著提高了识别准确率并减少了人工工作量。


<details>
  <summary>Details</summary>
Motivation: GPR图像中的RSD识别依赖人工且效率低，现有深度学习方法受限于数据质量和网络能力。

Method: 构建了高质量3D GPR数据集，提出交叉验证策略优化YOLO模型，提升RSD识别能力。

Result: 在实地测试中召回率超过98.6%，检测系统可减少约90%的人工工作量。

Conclusion: 该方法为RSD自动识别提供了高效解决方案，具有实际应用价值。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [81] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 提出首个3D大气基准Atmos-Bench和新型FourCastX网络，用于恢复卫星LiDAR数据中的大气结构，无需辅助输入且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助输入和简化物理近似，缺乏标准化3D基准，可能引入不确定性且无法充分捕捉真实大气辐射传输效应。

Method: 通过耦合WRF和增强的COSP模拟器生成高质量3D散射体积数据，构建Atmos-Bench基准；设计FourCastX网络嵌入物理约束，提升恢复质量。

Result: 在Atmos-Bench数据集上，FourCastX在355 nm和532 nm波段均优于现有基线模型。

Conclusion: Atmos-Bench为卫星3D大气结构恢复设定了新标准，有助于更深入的气候研究。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [82] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统综述了视觉识别模型的可解释性研究，并提出了一种以人为中心的分类法，总结了评估指标需求并探讨了新技术带来的机遇。


<details>
  <summary>Details</summary>
Motivation: 随着视觉识别模型在关键领域（如自动驾驶和医疗诊断）的应用增加，理解模型机制和诊断失败的需求推动了可解释性研究的发展。

Method: 通过系统综述现有研究，提出了一种基于意图、对象、呈现和方法的分类法，并总结了评估指标需求。

Result: 建立了一套系统且一致的XAI方法分类标准，并探讨了新技术（如多模态大模型）带来的新机遇。

Conclusion: 本文旨在组织现有研究并启发未来对视觉识别模型可解释性的进一步探索。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [83] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++是一种新型多模态大语言模型，专注于通用关键点理解，通过用户指令整合多模态输入，实现高精度关键点检测。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在捕捉细粒度语义信息（如关键点）方面表现不足，而关键点对图像分析、对象检索和行为识别至关重要。

Method: 采用“识别-检测”范式，先解释关键点语义，再通过结构化思维链机制定位其精确位置，并扩展训练数据集至50万样本。

Result: 在多个关键点检测基准测试中表现优异，展示了卓越的准确性和泛化能力。

Conclusion: KptLLM++为细粒度图像理解提供了统一解决方案，对提升人机交互具有变革性意义。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [84] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于水母物种检测与分类，结合多种特征提取技术和分类器，最高准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 水母对海洋生态系统至关重要，但其快速增殖和生态影响给生物多样性保护带来挑战，准确识别物种对生态监测和管理至关重要。

Method: 整合MobileNetV3、ResNet50等特征提取技术，结合传统机器学习分类器和前馈神经网络，使用softmax函数直接分类。

Result: MobileNetV3与人工神经网络的组合表现最佳，准确率达98%。

Conclusion: 深度学习与混合框架能有效解决生物多样性挑战，推动海洋物种检测技术发展。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [85] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出了一种多模态引导的硬样本生成与学习框架（HSGL），通过结合文本和视觉模态，显式定义、生成和优化硬样本，提升衣物变化行人重识别（CC-ReID）的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 硬样本在衣物变化行人重识别任务中存在定义模糊和生成困难的问题，限制了模型在衣物或视角变化下的鲁棒性。

Method: HSGL包含双粒度硬样本生成（DGHSG）和硬样本自适应学习（HSAL），前者利用多模态信息生成语义一致的硬样本，后者通过硬度感知优化策略调整特征距离。

Result: 在多个CC-ReID基准测试中表现优异，显著加速学习收敛，并在PRCC和LTCC数据集上达到最先进性能。

Conclusion: 多模态引导的硬样本生成与学习框架有效提升了CC-ReID任务的性能，展示了其在处理硬样本问题上的潜力。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [86] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 论文提出了一种多模态场景表示框架MMOne，通过模态建模模块和多模态分解机制解决模态冲突问题，提升各模态的表征能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过多模态感知世界，但模态间的差异（如属性差异和粒度差异）带来了挑战。研究旨在解决这些差异，实现更高效的多模态场景表示。

Method: 提出MMOne框架，包括模态建模模块（带模态指示器）和多模态分解机制，将多模态高斯分布分解为单模态高斯分布，并分离共享与模态特定信息。

Result: 实验表明，该方法显著提升各模态的表征能力，且可扩展至更多模态。

Conclusion: MMOne框架有效解决了模态冲突问题，实现了紧凑高效的多模态场景表示。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [87] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，并在多个数据集上取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 近年来滑坡灾害频发，但传统观测方法难以覆盖大范围复杂地形，因此需要自动化解决方案。

Method: 设计了一种新型神经网络架构，用于滑坡检测和分割任务，输入为遥感图像。

Result: 在LandSlide4Sense、Bijie和Nepal数据集上，检测任务的F1分数分别为98.23和93.83，分割任务的mIoU分数为63.74和76.88。

Conclusion: 该模型具有实际应用潜力，可集成到滑坡观测系统中。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [88] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文探讨了大型视觉语言模型的色彩视觉能力，构建了一个多类别、多难度级别的测试数据集，并提出了微调策略以提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的色彩视觉能力尚未被充分研究，填补这一空白是本文的主要动机。

Method: 定义色彩视觉测试任务，构建多类别、多难度级别的数据集，分析模型错误类型并提出微调策略。

Result: 通过实验验证了微调策略对提升模型色彩视觉能力的有效性。

Conclusion: 本文为大型视觉语言模型的色彩视觉能力研究提供了数据集和方法，未来可进一步优化模型性能。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [89] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 提出一种基于自监督对比学习的柑橘病害分类方法CMCRL，利用无标注数据优化模型，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 柑橘病害严重影响产量，传统深度学习方法依赖大量标注数据，成本高。本文旨在减少标注需求，提升分类效果。

Method: 引入聚类中心和多层对比训练范式，提出CMCRL算法，利用无标注样本优化模型，适应病害症状相似性。

Result: 在公开数据集CDD上，准确率提升4.5%-30.1%，接近全监督方法，且在F1分数等指标上表现优异。

Conclusion: CMCRL方法显著减少标注依赖，性能优越，适用于类不平衡问题，为病害检测提供高效解决方案。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [90] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 论文全面评估了开源通用和医学专用视觉语言模型在医疗任务中的表现，发现通用大模型在某些任务上已超越医学专用模型，但推理能力仍是瓶颈，且临床可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在医疗任务中的能力，填补现有研究空白。

Method: 评估多种规模的视觉语言模型（3B至72B参数）在八个医疗基准测试中的表现，分为理解和推理两部分。

Result: 1. 通用大模型在部分任务上优于医学专用模型；2. 推理能力普遍较弱；3. 不同基准测试表现差异显著。

Conclusion: 当前模型尚未达到临床可靠性要求，需加强多模态对齐和更严格的评估协议。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [91] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: 提出了一种名为MCULoRA的新方法，通过解耦模态组合的共享信息和动态调整训练比例，解决了多模态情感识别中模态不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 实际应用中多模态数据常因传感器故障或隐私保护而不完整，现有方法因模态组合训练梯度冲突导致性能下降。

Method: MCULoRA包含两个模块：模态组合感知的低秩适应（MCLA）解耦共享信息，动态参数微调（DPFT）优化训练比例。

Result: 在多个基准数据集上的实验表明，MCULoRA在下游任务准确率上显著优于现有方法。

Conclusion: MCULoRA为不完整多模态学习提供了一种参数高效的训练框架，显著提升了性能。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [92] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 论文提出了首个专门用于评估长视频生成模型叙事表达能力的基准NarrLV，通过引入时间叙事原子（TNA）和基于电影叙事理论的自动提示生成管道，结合多级评估指标，揭示了当前模型在叙事表达上的能力边界。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成模型缺乏专门评估叙事表达能力的基准，现有评估主要依赖简单叙事提示的基准（如VBench），无法全面衡量模型的叙事丰富性。

Method: 1. 引入时间叙事原子（TNA）作为基本叙事单元，定量衡量叙事丰富性；2. 基于电影叙事理论构建自动提示生成管道；3. 设计基于多级叙事表达水平的评估指标，利用MLLM框架生成和回答问题。

Result: 实验表明，NarrLV的评估指标与人类判断高度一致，揭示了当前视频生成模型在叙事表达上的详细能力边界。

Conclusion: NarrLV填补了长视频生成模型叙事评估的空白，为未来研究提供了有效的评估工具，并揭示了模型在叙事表达上的局限性。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [93] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出一种基于公平性的连续敏感属性分组方法，通过最大化组间歧视差异的新标准，识别关键子群体，并在实验中验证其有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将连续敏感属性（如肤色）划分为预定义组可能忽略少数群体的歧视问题，需更精细的分组方法。

Method: 提出基于歧视水平的公平性分组方法，通过最大化组间歧视差异的新标准划分数据，并在合成数据集和真实数据集（CelebA、FFHQ）上验证。

Result: 方法能揭示更细微的歧视模式，且结果在不同数据集中稳定；用于去偏时，公平性提升且对准确性影响小。

Conclusion: 该方法为连续敏感属性的公平性评估和去偏提供了有效工具，适合工业应用。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [94] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的框架，通过预训练模型获取烟雾掩码和图像描述，改进修复模型，并引入新的损失函数和过滤工具，生成高质量烟雾图像以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 森林火灾烟雾图像数据稀缺，现有修复模型生成质量不足，导致烟雾与背景不一致。

Method: 使用预训练分割和多模态模型获取烟雾掩码和描述；提出基于掩码和掩码图像特征的网络架构；引入掩码随机差异损失函数；利用多模态大语言模型过滤图像。

Result: 生成的烟雾图像真实多样，有效提升了森林火灾烟雾检测模型的性能。

Conclusion: 提出的框架解决了烟雾图像生成的质量问题，为烟雾检测任务提供了高质量数据集。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [95] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD框架通过结构化多视角分解解决3D视觉定位中复杂多锚点查询和空间描述不一致问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂多锚点查询和视角变化导致的空间描述不一致问题。

Method: 提出ViewSRD框架，包含Simple Relation Decoupling（SRD）模块、Multi-view Textual-Scene Interaction（Multi-TSI）模块和Textual-Scene Reasoning模块，通过多视角分解和跨模态一致性视图令牌（CCVTs）整合文本与场景特征。

Result: 在3D视觉定位数据集上表现优异，尤其在需要精确空间区分的复杂查询中。

Conclusion: ViewSRD通过结构化多视角分解和跨模态特征整合，显著提升了3D视觉定位的性能。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [96] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测与识别，解决了现有深度学习模型在该领域的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在国防和监控领域的目标检测与识别任务面临诸多挑战，如数据集有限、硬件限制、天气影响等，导致现有SOTA模型表现不佳。

Method: 基于改进的YOLOv5s，优化了检测头、特征融合和自定义数据增强策略，提出了YOLOatr模型。

Result: 在DSIAC MWIR数据集上测试，YOLOatr在实时目标识别任务中达到了99.6%的SOTA性能。

Conclusion: YOLOatr在热红外图像目标识别任务中表现出色，为复杂环境下的实时检测提供了有效解决方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [97] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP是一个基于IoT的番茄植物表型数据集，包含64,464张RGB图像和3,616张高分辨率图像，用于精细表型分析。通过深度学习框架验证，其准确性接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法存在观察者偏见和不一致性问题，影响分析的准确性和可重复性。

Method: 开发了TomatoMAP数据集，采用IoT成像系统和标准化数据采集协议，结合深度学习框架（MobileNetv3、YOLOv11、MaskRCNN）进行验证。

Result: 模型在精细表型分析中的准确性和速度与专家相当，Cohen's Kappa和热图验证了方法的可靠性。

Conclusion: TomatoMAP为植物表型研究提供了高精度、可重复的解决方案，自动化方法接近专家水平。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [98] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 本文提出了一种任务导向的人体抓取合成方法，通过任务感知接触图实现场景和任务的综合建模，显著提升了抓取质量和任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统抓取合成方法仅考虑物体与手的关系，缺乏对场景和任务的综合理解，导致抓取姿势与任务需求不匹配。

Method: 采用两阶段流程：首先生成任务感知接触图，随后基于该图合成任务导向的抓取姿势。

Result: 实验验证了场景和任务建模的重要性，新方法在抓取质量和任务性能上显著优于现有方法。

Conclusion: 任务感知接触图是实现高效任务导向抓取合成的关键，新方法为相关领域提供了有效解决方案。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [99] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于人工智能的方法，通过YOLOv11模型自动识别侵蚀区域并估算其面积，开发了交互式网页应用EROSCAN。


<details>
  <summary>Details</summary>
Motivation: 传统的河流侵蚀监测方法依赖专业知识和手动处理，效率低且耗时。

Method: 使用YOLOv11模型，结合照片和LiDAR图像进行微调和训练，通过Roboflow平台进行数据标注和分割。

Result: 模型准确率为70%，能精确识别侵蚀区域并计算其面积，开发了EROSCAN系统。

Conclusion: 该方法优化了侵蚀检测和量化，为风险管理和土地规划提供了便利。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [100] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新型高斯泼溅框架，首次支持在表面重建过程中使用多种几何基元，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一基元（椭圆或椭球）表示复杂物体表面，限制了重建质量。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的高斯泼溅。

Result: 实验证明框架有效，能实现高精度的表面重建。

Conclusion: 多基元高斯泼溅框架显著提升了表面重建质量。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [101] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet结合单目深度估计与多视角立体视觉，通过注意力机制和动态深度候选更新，提升纹理缺失和反射区域的深度预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有MVS方法在纹理缺失和反射区域表现不佳，而单目深度估计无需特征匹配，能在这些区域提供稳健的相对深度估计。

Method: 提出MonoMVSNet，通过注意力机制整合单目特征，动态更新深度候选，并设计相对一致性损失监督深度预测。

Result: 在DTU和Tanks-and-Temples数据集上达到SOTA性能，在Tanks-and-Temples Intermediate和Advanced基准中排名第一。

Conclusion: MonoMVSNet通过结合单目深度估计与多视角几何，显著提升了MVS在挑战性区域的性能。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [102] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 论文提出了UGC-VideoCap，一个专注于音频视觉融合的短视频字幕基准和模型框架，填补了现有视觉中心化方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕基准和模型过于视觉中心化，忽视了音频在场景动态、说话者意图和叙事背景中的关键作用。

Method: 提出了UGC-VideoCap基准和UGC-VideoCaptioner(3B)模型，采用三阶段人工标注和两阶段训练策略（监督微调+GRPO）。

Result: 基准包含1000个TikTok视频和4000个QA对，模型在有限数据下表现高效且具有竞争力。

Conclusion: UGC-VideoCap为无约束真实场景下的多模态视频字幕提供了高质量基准和数据高效解决方案。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [103] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 该论文提出了一种几何方法，用于分析人脸识别模型中嵌入空间的多尺度几何结构，并引入了一种物理启发的对齐度量，揭示了模型对不同属性的不变性程度。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在人脸识别任务中取得了显著进展，但现有方法主要关注身份信息，忽略了嵌入空间中由可解释的面部和图像属性（如发色、对比度）形成的几何结构。

Method: 提出了一种几何方法，结合物理启发的对齐度量，评估人脸识别模型对属性的依赖性或不变性，并在合成数据增强的模型上进行验证。

Result: 研究发现，模型对不同属性表现出不同程度的不变性，揭示了其优势和局限性，增强了模型的可解释性。

Conclusion: 该研究为理解人脸识别模型的嵌入空间提供了新视角，有助于改进模型设计并提升其可解释性。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [104] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR模型在非差分隐私适应任务中表现优于DM，但在差分隐私适应中表现不佳，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探索VAR模型在特定下游任务（如医疗数据生成）中的适应方法，并填补其在差分隐私适应领域的空白。

Method: 实现并对比多种VAR适应策略，与现有DM适应策略进行基准测试。

Result: VAR在非差分隐私适应中优于DM，但在差分隐私适应中表现较差。

Conclusion: VAR在非隐私任务中表现优异，但需进一步研究差分隐私适应方法。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [105] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI框架通过NeRV和INR技术，解决了大图像压缩中的速度慢和压缩比低问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化性差，INR技术虽有潜力但存在速度慢和压缩比低的问题。

Method: COLI利用NeRV技术，采用预训练-微调范式、混合精度训练和并行化目标加速收敛，并通过Hyper-Compression技术提升压缩比。

Result: 在两个医学影像数据集上，COLI在PSNR和SSIM指标上表现优异，同时显著降低bpp，并加速NeRV训练达4倍。

Conclusion: COLI框架有效解决了INR在大图像压缩中的局限性，为高效图像压缩提供了新思路。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [106] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和扩散生成模型的血管几何合成方法，用于生成高保真度的主动脉几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法受限于线性假设，难以处理复杂血管拓扑结构。HUG-VAS旨在解决这一问题。

Method: 结合NURBS表面参数化和扩散生成模型，采用分层架构生成中心线和径向轮廓。

Result: 生成的主动脉几何结构与原始数据集高度一致，支持零样本条件生成。

Conclusion: HUG-VAS首次将图像先验与生成形状建模统一结合，具有广泛的应用潜力。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [107] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI算法通过结合组合边缘像素采样和卷积参数空间密度估计，实现了在模糊图像中高精度的圆检测与拟合，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在退化成像条件下鲁棒的圆检测与拟合问题，填补圆检测与精确参数拟合之间的技术空白。

Method: 结合组合边缘像素采样和卷积参数空间密度估计的3C-FBI算法。

Result: 在真实医学数据和合成数据中，3C-FBI实现了高精度（Jaccard指数0.896）和实时性能（40.3 fps），优于传统方法。

Conclusion: 3C-FBI在精度、速度和鲁棒性上的优异表现，使其适用于医疗影像、机器人和工业检测等挑战性场景。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [108] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类感知的模糊颜色模型COLIBRI，通过模糊集和逻辑构建颜色分类框架，实验验证其优于传统颜色模型。


<details>
  <summary>Details</summary>
Motivation: 计算机难以模仿人类颜色感知，需一种更贴近人类视觉感知的计算颜色表示方法。

Method: 采用三阶段实验：初步实验确定可区分颜色刺激，大规模人类分类调查提取模糊分区，生成反映感知不确定性的隶属函数。

Result: 模型在人类感知对齐上优于RGB、HSV和LAB等传统模型，适用于设计、AI等领域。

Conclusion: COLIBRI模型填补了计算颜色表示与人类感知之间的空白，具有广泛的应用潜力。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [109] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出了一种5阶段框架，用于从EEG信号解码视觉表征，并通过跨模态对齐和重新排序实现上下文感知的EEG到图像生成。


<details>
  <summary>Details</summary>
Motivation: EEG信号复杂且噪声多，解码视觉表征具有挑战性，因此需要一种新方法。

Method: 5阶段框架：EEG编码器、跨模态对齐、标题重新排序、加权插值和图像生成。

Result: 生成高质量图像，分类准确率提升13.43%，生成准确率提升15.21%，Fréchet Inception Distance降低36.61%。

Conclusion: 该方法在语义对齐和图像质量上优于现有技术。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [110] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist提出了一种解决文本到图像生成中一致性问题的方法，通过点跟踪注意力和自适应令牌合并，实现前景和背景的细粒度一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持背景细节和应对大动作变化时存在一致性问题，限制了实际应用。

Method: 采用点跟踪注意力和自适应令牌合并技术，解耦控制前景和背景。

Result: 能够生成连续或离散场景中一致的高质量图像，适用于更广泛的实际场景。

Conclusion: CharaConsist是首个针对DiT模型的一致性生成方法，提升了生成质量和适用性。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [111] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种流式4D视觉几何变换器，用于实时感知和重建视频中的4D时空几何，采用因果变换器架构和隐式记忆机制，支持高效在线处理。


<details>
  <summary>Details</summary>
Motivation: 解决视频中4D时空几何感知与重建的实时性和交互性挑战。

Method: 采用因果变换器架构，利用时间因果注意力和历史键值缓存作为隐式记忆，实现高效流式长期4D重建。训练时通过知识蒸馏从密集双向视觉几何变换器（VGGT）中提取知识。

Result: 在多个4D几何感知基准测试中表现优异，显著提升在线推理速度，同时保持高质量空间一致性。

Conclusion: 该模型为可扩展和交互式4D视觉系统提供了可行方案，代码已开源。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [112] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 该论文综述了深度估计在3D计算机视觉中的重要性，探讨了传统硬件方法的局限性及基于视觉方法的挑战，并提出了深度基础模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统深度估计方法成本高、分辨率低且对环境敏感，而基于视觉的方法在泛化和稳定性上面临挑战。深度基础模型通过大规模数据和零样本泛化能力提供了解决方案。

Method: 论文综述了单目、立体、多视图和单目视频设置下的深度学习架构和范式，并探讨了大规模数据集的作用。

Result: 通过分析关键架构和训练策略，论文提出了构建鲁棒深度基础模型的路径。

Conclusion: 深度基础模型有望解决现有挑战，并为未来研究和应用提供方向。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [113] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: q-bio.QM

TL;DR: 提出了一种新型的AGFS-Tractometry方法，用于增强白质纤维束的精细统计分析，通过图谱引导和置换测试提高敏感性和特异性。


<details>
  <summary>Details</summary>
Motivation: 研究白质纤维束的局部差异，尤其是在健康和疾病群体之间，需要更精细和一致的分析方法。

Method: 结合图谱引导的纤维束分割和置换测试，实现精细尺度的统计分析，并与其他先进方法（AFQ和BUAN）进行比较。

Result: 在合成和真实数据中，AGFS-Tractometry表现出更高的敏感性和特异性，能识别更多解剖学一致的显著差异区域。

Conclusion: AGFS-Tractometry能够有效检测白质纤维束的局部差异，为研究提供了新工具。

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 论文提出了一种模块化的多智能体AI视觉分类框架，结合了通用多模态智能体、非视觉推理协调器和RAG模块，用于零样本场景下的可信分类，并在苹果叶病诊断中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI在零样本场景下的可信度问题，尤其是在无需微调的情况下如何确保其可靠性。

Method: 引入模块化框架，包括多模态智能体、协调器和RAG模块，通过置信度校准和图像检索提升信任度。

Result: 零样本场景下准确率提升77.94%，总体准确率达85.63%，且图像RAG能纠正智能体的过度自信。

Conclusion: 该框架将感知与元推理分离，具有可扩展性和可解释性，适用于诊断和生物学等信任关键领域。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [115] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文提出了首个全面的Web of Agents (WoA)进化概述，揭示了现代协议（如A2A和MCP）是对早期标准（如FIPA和OWL）局限性的直接进化响应，并通过四轴分类法统一分析各代代理架构。


<details>
  <summary>Details</summary>
Motivation: 研究WoA领域的碎片化问题，整合多智能体系统（MAS）和语义Web的历史，以揭示现代系统的知识传承并促进对该领域发展的整体理解。

Method: 引入四轴分类法（语义基础、通信范式、智能位置、发现机制），系统比较各代代理架构，分析智能位置的范式转变。

Result: 发现智能位置从外部数据（语义Web）或平台（MAS）转向代理核心模型（LLM），为现代Agentic AI奠定了基础。

Conclusion: 新协议虽必要但不足以构建稳健开放的WoA生态系统，未来研究应聚焦去中心化身份、经济模型、安全与治理等社会技术挑战。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [116] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在主题分析中的可行性，发现GPT-4o在少量示例提示下表现最佳，可作为定性研究的补充工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要深度解释和领域专业知识的归纳主题分析中面临挑战，研究旨在评估其替代专家驱动的主题分析的可行性。

Method: 使用两个Reddit数据集（n=286和n=686）和专家定义的12个主题，将任务建模为一系列二元分类，采用零、单和少量示例提示策略，评估五个LLMs的性能。

Result: GPT-4o在少量示例提示下表现最佳（准确率90.9%，F1分数0.71），高流行主题的分布与专家分类接近。

Conclusion: 少量示例提示的LLM方法可自动化主题分析，为定性研究提供可扩展的补充工具。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [117] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer是一个自动生成高质量导航指令的框架，结合NavInstrCritic进行无标注评估，提升语言导航研究的可扩展性和通用性。


<details>
  <summary>Details</summary>
Motivation: 专家提供的导航指令数量有限，合成标注质量不足，限制了大规模研究。

Method: NavComposer通过分解语义实体（动作、场景、对象）并重组为自然语言指令，支持数据无关的适应性；NavInstrCritic从对比匹配、语义一致性和语言多样性三个维度评估指令质量。

Result: 实验证明方法有效，支持更广泛的研究应用。

Conclusion: NavComposer和NavInstrCritic为语言导航研究提供了高质量指令生成和评估的解决方案。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [118] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 提出一种基于扩散模型的CT重建方法CLS-DM，通过跨模态特征对比学习实现2D X射线到3D CT的潜在空间对齐，显著提升重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统CT重建方法存在高辐射和时间成本问题，稀疏视图X射线重建成为研究热点，但2D与3D模态潜在空间差异大，需解决对齐问题。

Method: 提出CLS-DM模型，结合跨模态特征对比学习，从2D X射线图像中提取3D潜在信息并实现模态间潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM指标上优于经典和前沿生成模型。

Conclusion: CLS-DM不仅提升稀疏X射线重建CT的效果和经济性，还可推广至其他跨模态任务，如文本到图像合成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [119] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 论文提出了一种名为3D MIR的新方法，结合深度学习和物理约束，从磁场图像中恢复半导体封装中的3D电流信息。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，准确恢复3D信息对于非破坏性测试（NDT）定位电路缺陷至关重要。

Method: 3D MIR通过三个阶段实现：1）CNN处理磁场图像预测参数；2）结合空间物理约束提供初始估计；3）优化器调整参数以最小化误差。

Result: 3D MIR方法能够高精度恢复3D信息，为半导体封装中的磁场图像重建设定了新标准。

Conclusion: 该方法展示了深度学习和物理驱动优化在实际应用中的潜力。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [120] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net是一种新型分割框架，结合双曲卷积、多尺度纹理学习和生物启发的突触可塑性机制，显著提升了腹部CT图像中肝脏和肿瘤分割的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 由于复杂的解剖结构、肿瘤外观的多样性和有限的标注数据，肝脏和肿瘤分割在腹部CT图像中仍然具有挑战性。

Method: HANS-Net结合了双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性感知的蒙特卡洛dropout和轻量级时间注意力机制。

Result: 在LiTS数据集上，HANS-Net的平均Dice得分为93.26%，IoU为88.09%，ASSD为0.72 mm，VOE为11.91%。跨数据集验证也表现出色。

Conclusion: HANS-Net在肝脏和肿瘤分割中表现出高效性、鲁棒性和泛化能力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [121] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Main category: eess.IV

TL;DR: 该研究比较了传统机器学习和深度学习在肺炎检测中的表现，发现Vision Transformers（尤其是Cross-ViT）在准确率和召回率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 肺炎（如COVID-19引发的）是全球健康挑战，需要快速准确的诊断方法。

Method: 比较了传统机器学习（PCA聚类、逻辑回归、支持向量分类）和深度学习（CNN、ViT）在5,856张儿童胸片上的表现。

Result: Cross-ViT表现最佳，准确率88.25%，召回率99.42%，且模型架构比大小更重要。

Conclusion: Vision Transformers在肺炎检测中具有潜力，可提升诊断速度和准确性。

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [122] [Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification](https://arxiv.org/abs/2507.10869)
*Chetan Madan,Aarjav Satia,Soumen Basu,Pankaj Gupta,Usha Dutta,Chetan Arora*

Main category: eess.IV

TL;DR: GLCM-MAE是一种基于GLCM损失的新型MAE预训练框架，用于医学图像的自监督表示学习，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统MAE在医学图像中因纹理信息的重要性而表现不佳，GLCM-MAE通过捕捉强度和空间关系改进重建效果。

Method: 提出基于GLCM的损失函数，将GLCM矩阵匹配转化为可微分损失，用于MAE预训练。

Result: 在四个医学图像任务中，GLCM-MAE性能超越现有方法，提升幅度为0.5%至3.1%。

Conclusion: GLCM-MAE通过保留形态特征，有效提升医学图像表示学习性能。

Abstract: Masked Autoencoders (MAEs) have emerged as a dominant strategy for
self-supervised representation learning in natural images, where models are
pre-trained to reconstruct masked patches with a pixel-wise mean squared error
(MSE) between original and reconstructed RGB values as the loss. We observe
that MSE encourages blurred image re-construction, but still works for natural
images as it preserves dominant edges. However, in medical imaging, when the
texture cues are more important for classification of a visual abnormality, the
strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)
feature in Radiomics studies, we propose a novel MAE based pre-training
framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM
captures intensity and spatial relationships in an image, hence proposed loss
helps preserve morphological features. Further, we propose a novel formulation
to convert matching GLCM matrices into a differentiable loss function. We
demonstrate that unsupervised pre-training on medical images with the proposed
GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms
the current state-of-the-art across four tasks - gallbladder cancer detection
from ultrasound images by 2.1%, breast cancer detection from ultrasound by
3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by
0.6%. Source code and pre-trained models are available at:
https://github.com/ChetanMadan/GLCM-MAE.

</details>


### [123] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Main category: eess.IV

TL;DR: U-RWKV是一种基于RWKV架构的新型医疗图像分割框架，通过DARM和SASE模块解决了U-Net类方法全局感受野不足的问题，实现了高效的长距离建模和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中实现医疗图像分割的公平性，需要轻量级高性能的解决方案。现有方法如U-Net因全局感受野有限，难以捕捉长距离依赖关系。

Method: 提出U-RWKV框架，结合DARM模块（双RWKV和QuadScan机制）和SASE模块（动态适应不同特征提取阶段），实现高效长距离建模。

Result: 实验表明，U-RWKV在计算高效的同时，达到了最先进的分割性能。

Conclusion: U-RWKV为资源受限环境提供了实用的高级医疗影像技术解决方案，代码已开源。

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [124] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 比较强化学习（RL）和监督微调（SFT）在数学问题上的表现，发现RL在领域内略有提升，而SFT在知识密集型任务上表现更差。


<details>
  <summary>Details</summary>
Motivation: 理解RL和SFT在LLM后训练中的动态差异及其对模型能力的影响。

Method: 在同一模型和超参数下，比较RL和SFT在数学问题上的表现，并分析模型参数变化。

Result: RL在数学任务上略有提升，但SFT在知识密集型任务上表现更差；SFT对模型参数的修改更显著。

Conclusion: RL可能放大现有能力，而SFT可能用新技能替代旧技能，但冻结部分模型的效果尚不明确。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [125] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 本文提出了一种基于编码器-解码器架构的领域自适应小型语言模型（SLM），用于增强产品和服务税码的预测。


<details>
  <summary>Details</summary>
Motivation: 跨国企业每天处理大量交易，需遵守不同司法管辖区的复杂税务规定，准确预测税码（如HSN或SAC）对避免税务处罚至关重要。

Method: 采用编码器-解码器架构的SLM，通过处理非结构化产品和服务数据，生成层次化税码序列。

Result: 实验表明，该模型在结构化税码序列预测任务中表现优于扁平分类器、仅解码器和仅编码器架构。

Conclusion: 该方法可扩展至其他政府规定的税码（如UNSPSC或NCM），为税务合规领域提供高效解决方案。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [126] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM是一种新型的后训练量化方法，通过显式结合一阶梯度项改进量化误差补偿，显著提升大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有补偿式权重校准方法假设一阶项可忽略，但实际中渐进补偿过程会引入累积的一阶偏差，导致假设不成立。

Method: FOEM通过直接计算潜在权重与全精度权重的差异近似梯度，避免高成本的反向传播，并利用预计算的Cholesky因子实时恢复Hessian子矩阵的逆。

Result: 在3比特权重量化中，FOEM将Llama3-8B的困惑度降低89.6%，并将Llama3-70B的5-shot MMLU准确率从51.7%提升至74.9%，接近全精度性能。

Conclusion: FOEM不仅性能优于现有方法，还能与先进技术无缝集成，进一步缩小与全精度基线的差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [127] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM提出了一种基于分层扩散策略的通信感知LoRA适配框架，通过强化学习和扩散模型优化LoRA的秩配置，显著减少传输成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上运行大型语言模型（LLMs）面临通信带宽和计算资源限制，现有LoRA方法的固定秩配置和参数传输效率低。

Method: AirLLM将秩配置建模为结构化动作向量，结合PPO生成粗粒度决策和DDIM细化，实现任务和信道自适应的秩向量。

Result: 实验表明，AirLLM在不同信噪比下均能提升微调性能并显著降低传输成本。

Conclusion: AirLLM通过强化学习和扩散模型的结合，为远程微调提供了高效且可扩展的解决方案。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [128] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA是一种新颖的联邦学习框架，通过全局样本选择器和客户端自适应调整机制，有效解决医学图像分类中的标签噪声问题，提升模型鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 医学联邦学习中标签噪声和类别不平衡问题导致模型性能下降，现有方法难以应对噪声异质性和数据不平衡。

Method: FedGSCA结合全局样本选择器和客户端自适应调整机制，动态处理噪声标签和类别分布。

Result: 在真实和合成医学数据集上，FedGSCA在极端和异质噪声场景下表现优于现有方法，显著提升模型稳定性。

Conclusion: FedGSCA为医学联邦学习提供了一种高效且鲁棒的解决方案，适用于复杂噪声环境。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [129] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 该论文将扩散和流式生成模型扩展到权重空间学习，通过梯度流匹配统一轨迹推断技术，优化权重生成和初始化。


<details>
  <summary>Details</summary>
Motivation: 扩散和流式生成模型在图像合成等领域已取得成功，但尚未充分应用于权重空间学习。本文旨在利用优化动态的结构先验，提升权重生成和初始化效果。

Method: 提出梯度流匹配框架，将梯度下降轨迹建模为轨迹推断问题，结合自编码器、任务上下文数据等技术优化权重生成。

Result: 实验表明，该方法在生成分布内权重、改进下游训练初始化及微调性能方面优于基线。

Conclusion: 该方法不仅在权重生成和初始化上表现优异，还在安全关键系统中展示了检测有害协变量偏移的实用价值。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [130] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP是一种简单、低开销的持续学习方法，结合ReLUDown和Decreasing Backpropagation，平衡了模型的塑性和稳定性，在Continual ImageNet基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，模型需要在适应新任务的同时保留旧知识，现有方法往往在塑性和稳定性之间失衡。

Method: RDBP结合了ReLUDown（防止神经元休眠的轻量激活修改）和Decreasing Backpropagation（生物启发的梯度调度方案，保护早期层免受灾难性更新）。

Result: 在Continual ImageNet基准测试中，RDBP在塑性和稳定性上达到或超越现有方法，同时降低计算成本。

Conclusion: RDBP为实际持续学习提供了实用解决方案，并为未来方法设定了明确基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [131] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个用于通过生成去噪模型进行空间推理的软件框架，旨在简化研究流程。


<details>
  <summary>Details</summary>
Motivation: 生成去噪模型在图像生成中表现优异，但在多连续变量推理中的应用尚需基础设施支持。

Method: 提供易于使用的接口，支持变量映射、生成模型范式和推理策略。

Result: 框架开源，支持多种去噪模型和推理方法。

Conclusion: Spatial Reasoners为生成推理研究提供了高效工具。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [132] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LoRA和适配器的高效微调方法，用于检测大规模日志数据中的异常序列，相比传统方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于日志数据量大且复杂，传统规则或深度学习方法难以有效检测异常日志序列，因此需要更高效的解决方案。

Method: 采用参数高效的微调方法，特别是LoRA和适配器技术，结合小型大语言模型（LLMs）在Thunderbird数据集上进行实验。

Result: LoRA微调方法比LogBert全微调方法性能提升18-19%，准确率达到97.76%-98.83%，而后者仅为79.37%。

Conclusion: LoRA微调方法在日志异常检测中表现出色，为系统维护和开发提供了更高效的解决方案。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [133] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: 论文介绍了SWE-MERA，一个动态更新的基准测试，旨在解决SWE-bench数据集中的数据污染问题，通过自动化收集GitHub问题并严格验证质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SWE-bench）存在严重的数据污染问题，例如解决方案泄漏和测试用例不足，影响了评估的准确性。

Method: 采用自动化管道从GitHub收集真实问题，并进行严格质量验证，生成约10,000个潜在任务（目前300个样本可用）。

Result: 使用Aider编码代理评估，SWE-MERA在最新LLMs中表现出强大的区分能力，并报告了2024年9月至2025年6月收集的任务性能。

Conclusion: SWE-MERA通过动态更新和严格验证，有效解决了数据污染问题，为LLMs在软件工程中的评估提供了更可靠的基准。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [134] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: 提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）实现未见物体的连续分割，无需依赖学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有未见物体实例分割（UOIS）方法因依赖静态视觉特征而泛化能力差的问题，强调视觉的交互性和时间性。

Method: 利用机器人交互产生的相对旋转和线性速度，通过体帧不变特征（BFIF）实时分割物体，无需等待动作完成。

Result: 平均分割准确率比现有UOIS方法高27.5%，且生成的掩膜可作为视觉基础模型的提示进一步提升性能。

Conclusion: rt-RISeg通过交互感知显著提升了未见物体的分割性能，且具有独立性和扩展性。

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [135] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer的多任务学习框架，用于提升社交机器人在多用户环境中的决策能力，通过两种新的损失函数和真实数据集验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多用户环境中，社交机器人需要理解上下文并决定何时及向谁回应，而现有研究主要关注单用户交互。

Method: 采用Transformer多任务学习框架，引入两种新损失函数：一种约束主动说话者以改进场景建模，另一种引导机器人选择对其直接说话的回应。

Result: 实验表明，该模型在回应决策上优于现有启发式和单任务方法，达到最先进性能。

Conclusion: 该研究为开发具有社交智能的机器人提供了新方法，使其能在多用户环境中进行自然且上下文感知的交互。

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [136] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav是一种基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，提升机器人在动态环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在动态和非结构化环境中表现不佳，强化学习方法又因泛化能力差和仿真多样性不足而难以实际应用。

Method: 利用多模态大语言模型推理和条件变分自编码器，结合单样本示例和思维链提示策略，实现零样本场景理解和专家级超参数调整。

Result: 实验表明，LE-Nav能生成人类水平的超参数，在真实导航试验和用户研究中表现优于现有方法，成功率和主观评价均更高。

Conclusion: LE-Nav通过自适应超参数调整，显著提升了导航性能和社会接受度，适用于动态和非结构化环境。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [137] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景、优化高斯分布及物理模拟，显著提升了稀疏视图和动态环境下的3D几何重建效果。


<details>
  <summary>Details</summary>
Motivation: 透明物体的反射和折射特性使其3D几何重建困难，尤其在稀疏视图和动态环境中。TRAN-D旨在解决这些问题。

Method: TRAN-D通过分离透明物体与背景，优化高斯分布，并使用物体感知损失和物理模拟减少伪影。

Result: 在合成和真实场景中，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度提升1.5倍。

Conclusion: TRAN-D在透明物体深度重建中表现出色，显著优于现有方法，适用于动态和稀疏视图场景。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [138] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: 首次提出仅依赖视觉的飞行控制方法，通过事件相机和神经网络替代传统惯性传感器。


<details>
  <summary>Details</summary>
Motivation: 飞行机器人通常依赖惯性传感器，而许多飞行生物仅依赖视觉。研究旨在探索仅视觉控制的可行性。

Method: 使用向下事件相机和循环卷积神经网络，通过监督学习训练，实现姿态和转速估计。

Result: 实验证明该方法可替代传统惯性测量单元，且网络在不同环境中有一定泛化能力。

Conclusion: 仅视觉飞行控制是未来小型自主飞行机器人的可行方案。

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [139] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 论文提出LiLM-RDB-SFC方法，结合轻量级语言模型和关系数据库，优化SFC管理和VNF放置，FLAN-T5表现优于BART和SQLCoder。


<details>
  <summary>Details</summary>
Motivation: 现代SDN和NFV环境中，SFC管理和VNF放置是关键挑战，传统DRL方法因依赖结构化数据和固定规则而受限。

Method: 结合LiLM（BART和FLAN-T5）与RDB，通过语言模型解析网络状态查询，指导DRL模型优化SFC配置。

Result: FLAN-T5在测试损失（0.00161 vs 0.00734）、准确率（94.79% vs 80.2%）和处理时间（2h 2min vs 2h 38min）上优于BART；与SQLCoder相比，准确率相当但处理时间减少96%。

Conclusion: LiLM-RDB-SFC方法显著提升SFC管理效率，FLAN-T5是更优的语言模型选择。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [140] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 论文探讨了使用Spiker+框架为MNIST数据集生成优化的SNN加速器，分析了边缘计算中的权衡。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器对边缘应用的低延迟和高效能推理至关重要，SNN因其事件驱动和稀疏特性适合低功耗FPGA部署。

Method: 利用开源的Spiker+框架，支持高级网络拓扑、神经元模型和量化规范，自动生成可部署的HDL。

Result: 评估了多种配置，分析了与边缘计算约束相关的权衡。

Conclusion: Spiker+框架能有效生成优化的SNN加速器，适用于边缘计算场景。

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [141] [Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent](https://arxiv.org/abs/2507.11461)
*Christian Daniele,Silvia Villa,Samuel Vaiter,Luca Calatroni*

Main category: math.OC

TL;DR: 本文提出了一种基于Mirror Descent的新型DEQ模型，用于解决Poisson逆问题，通过非欧几何适应数据项结构，提升了性能并减少了超参数依赖。


<details>
  <summary>Details</summary>
Motivation: 传统DEQ模型在Gaussian保真度下表现良好，但在Poisson逆问题中效果有限。本文旨在扩展DEQ的应用范围，解决Poisson数据保真度问题。

Method: 提出基于Mirror Descent的DEQ模型，利用非欧几何适应数据项结构，确保收敛性，并提供高效训练和无参数推理策略。

Result: 实验表明，该方法优于传统模型，性能接近Bregman Plug-and-Play方法，同时减少了初始化和超参数调优的敏感性。

Conclusion: 新型DEQ模型在Poisson逆问题中表现出色，为学习神经正则化器提供了理论框架和实用工具。

Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed
points, which have recently gained attention for learning image regularization
functionals, particularly in settings involving Gaussian fidelities, where
assumptions on the forward operator ensure contractiveness of standard
(proximal) Gradient Descent operators. In this work, we extend the application
of DEQs to Poisson inverse problems, where the data fidelity term is more
appropriately modeled by the Kullback-Leibler divergence. To this end, we
introduce a novel DEQ formulation based on Mirror Descent defined in terms of a
tailored non-Euclidean geometry that naturally adapts with the structure of the
data term. This enables the learning of neural regularizers within a principled
training framework. We derive sufficient conditions to guarantee the
convergence of the learned reconstruction scheme and propose computational
strategies that enable both efficient training and fully parameter-free
inference. Numerical experiments show that our method outperforms traditional
model-based approaches and it is comparable to the performance of Bregman
Plug-and-Play methods, while mitigating their typical drawbacks - namely,
sensitivity to initialization and careful tuning of hyperparameters. The code
is publicly available at https://github.com/christiandaniele/DEQ-MD.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [142] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: 探讨了自我表露在对话式用户界面（CUIs）中的重要性，以及如何通过表达不确定性和透明化CUI的推理过程来促进自我表露。


<details>
  <summary>Details</summary>
Motivation: 自我表露对心理健康有益，但人们常因担心他人反应而难以实现。研究旨在通过CUI设计缓解这一问题。

Method: 通过分析社会线索，探讨CUI表达不确定性和透明化推理过程对自我表露的影响。

Result: 透明化的CUI推理和不确定性表达可能增强用户自我表露的意愿。

Conclusion: 通过改进CUI的设计，可以促进用户更自然的自我表露行为。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [143] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: MultiVox是一个新的语音助手基准测试，旨在评估模型如何整合语音和视觉线索，包括副语言特征，以实现真正的多模态理解。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试无法全面评估语音助手在生成上下文感知响应时的表现，尤其是在理解细粒度语音特征和视觉线索方面存在不足。

Method: 引入MultiVox基准测试，包含1000条人工标注和录制的语音对话，涵盖多样化的副语言特征和视觉线索。

Result: 对9个先进模型的评估显示，尽管人类在这些任务中表现出色，当前模型仍难以生成上下文相关的响应。

Conclusion: MultiVox填补了现有基准测试的不足，为评估多模态语音助手的能力提供了新工具。

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [144] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Main category: quant-ph

TL;DR: 提出了一种随机纠缠配置方法，用于优化变分量子电路的纠缠拓扑，显著提升量子机器学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前固定纠缠拓扑无法适应任务需求，限制了量子模型超越经典模型的潜力。

Method: 通过随机二进制矩阵编码纠缠拓扑，利用纠缠密度和单量子比特约束探索候选拓扑空间。

Result: 在心脏MRI疾病分类任务中，16%的配置优于经典基线，最高准确率提升20%。

Conclusion: 随机纠缠配置方法具有鲁棒性和泛化性，显著提升量子模型性能。

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [145] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: TREC Deep Learning track第四年研究，利用MS MARCO数据集扩展了数据规模，专注于构建更完整的测试集，深度神经网络模型继续优于传统方法，但今年结果有意外。


<details>
  <summary>Details</summary>
Motivation: 利用MS MARCO数据集扩展数据规模，构建更完整的测试集以提升检索任务质量。

Method: 使用MS MARCO数据集，扩展了文档和段落集合规模，专注于段落检索任务，文档任务为次要任务。

Result: 深度神经网络模型仍优于传统方法，但今年密集检索表现不如去年，部分顶级模型未采用密集检索。

Conclusion: 今年测试集质量更高，但结果出现意外，密集检索表现下降，未来需进一步分析原因。

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [146] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: 论文提出建议，帮助研究人员与公众有效沟通大语言模型（LLMs）的能力与局限，以促进公众理解和支持。


<details>
  <summary>Details</summary>
Motivation: 当前公众对自然语言处理（NLP）兴趣高涨，但存在术语模糊、期望过高和伦理问题等障碍，影响公众理解和研究支持。

Method: 通过分析NLP研究和新闻报道，提出沟通建议，涵盖术语清晰化、合理期望和伦理透明三个方面。

Result: 建议旨在提升公众对NLP的理解，避免误解，并促进研究的可持续发展。

Conclusion: 透明、有效的沟通是增强公众理解和支持NLP研究的关键。

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [147] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: 论文评估了多种开源和专有大型语言模型（LLMs）在欧洲专利律师资格考试（EQE）上的表现，发现即使表现最佳的模型也未达到专业标准，揭示了模型在逻辑一致性、多模态能力和提示适应性等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化LLMs在法律领域的实际表现，并分析其局限性，以纠正公众对模型性能的高估。

Method: 通过在欧洲专利律师资格考试的部分题目上测试多种LLMs（如GPT系列、Anthropic、Deepseek和Llama-3等），结合人类专家对答案的评估，分析模型的性能。

Result: OpenAI o1表现最佳（准确率0.82），但所有模型均未达到专业标准（阈值0.90）。模型对温度和提示词敏感，且专家更重视答案的法律逻辑而非单纯正确性。

Conclusion: 尽管LLMs表现突出，但仍需改进逻辑一致性、多模态能力和提示适应性，才能接近人类专利律师水平。

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [148] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: 该共享任务评估了基于大语言模型（LLM）的AI导师的教学能力，重点关注其在教育对话中纠正学生错误的响应质量。任务分为五个维度，吸引了50多个国际团队参与，结果显示仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估AI导师在教学对话中的表现，尤其是纠正学生错误的能力，以推动教育领域AI技术的发展。

Method: 设计了五个评估维度（如错误识别、指导提供等），通过国际团队提交的模型与人工标注的黄金标准对比进行评估。

Result: 最佳模型在四个教学能力评估维度上的F1分数为58.34至71.81，导师身份识别任务的F1分数为96.98。

Conclusion: 任务结果显示了AI导师在教育领域的潜力，但仍需进一步改进，相关资源已公开以支持未来研究。

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [149] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D是一个将低质量3D资产提升为高质量的新框架，通过HFS-SDEdit方法增强纹理，并结合几何优化，显著提升3D模型质量。


<details>
  <summary>Details</summary>
Motivation: 高质量3D资产稀缺且获取成本高，Elevate3D旨在解决这一问题。

Method: 采用HFS-SDEdit增强纹理，结合单目几何预测器优化几何，逐视图交替进行纹理和几何细化。

Result: Elevate3D在3D模型细化中达到最先进质量，优于现有方法。

Conclusion: Elevate3D有效缓解了高质量开源3D资产的短缺问题。

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>
