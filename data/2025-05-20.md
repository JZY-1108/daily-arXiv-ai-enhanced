<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 193]
- [cs.CV](#cs.CV) [Total: 199]
- [eess.IV](#eess.IV) [Total: 19]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 41]
- [cs.AI](#cs.AI) [Total: 25]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)
*Jinqiang Wang,Huansheng Ning,Tao Zhu,Jianguo Ding*

Main category: cs.CL

TL;DR: SynPT is an LLM-driven method for mining implicit user intentions in tourism, addressing dataset scarcity and domain-specific challenges. It outperforms existing methods in evaluations.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with ambiguous tourist inquiries and lack proactive guidance due to poor-quality datasets. Existing methods fail to adapt to tourism-specific needs.

Method: SynPT uses LLM-driven user and assistant agents to simulate dialogues from seed data, generating a high-quality dataset (SynPT-Dialog) for fine-tuning LLMs.

Result: SynPT outperforms existing methods in human and LLM evaluations, with practical applicability shown in case studies.

Conclusion: SynPT effectively addresses tourism-specific challenges in implicit intention mining and proactive guidance, with potential adaptability to English scenarios.

Abstract: In the tourism domain, Large Language Models (LLMs) often struggle to mine
implicit user intentions from tourists' ambiguous inquiries and lack the
capacity to proactively guide users toward clarifying their needs. A critical
bottleneck is the scarcity of high-quality training datasets that facilitate
proactive questioning and implicit intention mining. While recent advances
leverage LLM-driven data synthesis to generate such datasets and transfer
specialized knowledge to downstream models, existing approaches suffer from
several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed
distributions of detail levels in initial inquiries, (3) contextual redundancy
in the implicit intention mining module, and (4) lack of explicit thinking
about tourists' emotions and intention values. Therefore, we propose SynPT (A
Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User
Intentions in the Tourism), which constructs an LLM-driven user agent and
assistant agent to simulate dialogues based on seed data collected from Chinese
tourism websites. This approach addresses the aforementioned limitations and
generates SynPT-Dialog, a training dataset containing explicit reasoning. The
dataset is utilized to fine-tune a general LLM, enabling it to proactively mine
implicit user intentions. Experimental evaluations, conducted from both human
and LLM perspectives, demonstrate the superiority of SynPT compared to existing
methods. Furthermore, we analyze key hyperparameters and present case studies
to illustrate the practical applicability of our method, including discussions
on its adaptability to English-language scenarios. All code and data are
publicly available.

</details>


### [2] [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)
*Harika Abburi,Sanmitra Bhattacharya,Edward Bowen,Nirmala Pudota*

Main category: cs.CL

TL;DR: The paper addresses the detection of AI-generated text and model attribution, proposing neural architectures for two tasks: distinguishing human vs. AI text (Task A) and identifying the source model (Task B). The models achieved competitive F1 scores.


<details>
  <summary>Details</summary>
Motivation: The misuse of LLMs (e.g., fake news, spam) necessitates accurate detection of AI-generated text and model attribution to ensure responsible use.

Method: Two neural architectures (optimized and simpler variants) were proposed for each task: Task A (human vs. AI) and Task B (model attribution).

Result: For Task A, the optimized model ranked fifth with an F1 score of 0.994. For Task B, the simpler model ranked fifth with an F1 score of 0.627.

Conclusion: The proposed architectures effectively address AI-generated text detection and model attribution, contributing to responsible LLM use.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating text that closely resembles human writing across a wide range of
styles and genres. However, such capabilities are prone to potential misuse,
such as fake news generation, spam email creation, and misuse in academic
assignments. As a result, accurate detection of AI-generated text and
identification of the model that generated it are crucial for maintaining the
responsible use of LLMs. In this work, we addressed two sub-tasks put forward
by the Defactify workshop under AI-Generated Text Detection shared task at the
Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A
involved distinguishing between human-authored or AI-generated text, while Task
B focused on attributing text to its originating language model. For each task,
we proposed two neural architectures: an optimized model and a simpler variant.
For Task A, the optimized neural architecture achieved fifth place with $F1$
score of 0.994, and for Task B, the simpler neural architecture also ranked
fifth place with $F1$ score of 0.627.

</details>


### [3] [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
*Yuxuan Li,Aoi Naito,Hirokazu Shirado*

Main category: cs.CL

TL;DR: The paper introduces the Hidden Profile paradigm to evaluate multi-agent LLM systems, revealing collective reasoning failures and behavioral nuances compared to human groups.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a theory-grounded benchmark for assessing collective reasoning failures in multi-agent LLM systems.

Method: The Hidden Profile paradigm is formalized and instantiated as a benchmark with nine tasks. Experiments are conducted with GPT-4.1 and five other LLMs.

Result: Multi-agent systems fail to match single-agent accuracy, showing behavioral differences like sensitivity to social desirability. A cooperation-contradiction trade-off is identified.

Conclusion: The work provides a reproducible framework for evaluating multi-agent LLM systems and highlights future research directions in collective intelligence and human-AI interaction.

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving through distributed information integration, but also risk
replicating collective reasoning failures observed in human groups. Yet, no
theory-grounded benchmark exists to systematically evaluate such failures. In
this paper, we introduce the Hidden Profile paradigm from social psychology as
a diagnostic testbed for multi-agent LLM systems. By distributing critical
information asymmetrically across agents, the paradigm reveals how inter-agent
dynamics support or hinder collective reasoning. We first formalize the
paradigm for multi-agent decision-making under distributed knowledge and
instantiate it as a benchmark with nine tasks spanning diverse scenarios,
including adaptations from prior human studies. We then conduct experiments
with GPT-4.1 and five other leading LLMs, including reasoning-enhanced
variants, showing that multi-agent systems across all models fail to match the
accuracy of single agents given complete information. While agents' collective
performance is broadly comparable to that of human groups, nuanced behavioral
differences emerge, such as increased sensitivity to social desirability.
Finally, we demonstrate the paradigm's diagnostic utility by exploring a
cooperation-contradiction trade-off in multi-agent LLM systems. We find that
while cooperative agents are prone to over-coordination in collective settings,
increased contradiction impairs group convergence. This work contributes a
reproducible framework for evaluating multi-agent LLM systems and motivates
future research on artificial collective intelligence and human-AI interaction.

</details>


### [4] [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)
*Kyudan Jung,Hojun Cho,Jooyeol Yun,Jaehyeok Jang,Jagul Choo*

Main category: cs.CL

TL;DR: A system called Talk-to-Your-Slides uses LLMs to edit PowerPoint slides in real-time via COM communication, outperforming baselines in success rate and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM research for PowerPoint focuses on slide generation, neglecting the tedious task of editing existing slides.

Method: A two-level approach: high-level LLM agent interprets instructions and plans edits, while low-level Python scripts manipulate PowerPoint objects.

Result: Outperforms baselines in execution success rate, instruction fidelity, and editing efficiency, supported by TSBench dataset.

Conclusion: Talk-to-Your-Slides offers flexible, context-aware slide editing, advancing LLM applications in productivity tools.

Abstract: Existing research on large language models (LLMs) for PowerPoint
predominantly focuses on slide generation, overlooking the common yet tedious
task of editing existing slides. We introduce Talk-to-Your-Slides, an
LLM-powered agent that directly edits slides within active PowerPoint sessions
through COM communication. Our system employs a two-level approach: (1)
high-level processing where an LLM agent interprets instructions and formulates
editing plans, and (2) low-level execution where Python scripts directly
manipulate PowerPoint objects. Unlike previous methods relying on predefined
operations, our approach enables more flexible and contextually-aware editing.
To facilitate evaluation, we present TSBench, a human-annotated dataset of 379
diverse editing instructions with corresponding slide variations. Experimental
results demonstrate that Talk-to-Your-Slides significantly outperforms baseline
methods in execution success rate, instruction fidelity, and editing
efficiency. Our code and benchmark are available at
https://anonymous.4open.science/r/talk-to-your-slides/

</details>


### [5] [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)
*Xiaomin Li,Mingye Gao,Yuexing Hao,Taoran Li,Guangya Wan,Zihan Wang,Yijun Wang*

Main category: cs.CL

TL;DR: MedGUIDE is a benchmark evaluating LLMs' ability to follow clinical guidelines, revealing their limitations in structured decision-making despite domain specialization.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can reliably adhere to structured clinical guidelines, crucial for safe medical diagnostics.

Method: Constructed MedGUIDE using 55 NCCN decision trees and LLM-generated scenarios, applying a two-stage quality selection process. Evaluated 25 LLMs.

Result: Domain-specific LLMs often underperform in guideline adherence; performance improvements via in-context guidelines or pretraining were tested.

Conclusion: MedGUIDE highlights LLMs' challenges in clinical guideline adherence, emphasizing the need for safe integration in medical settings.

Abstract: Clinical guidelines, typically structured as decision trees, are central to
evidence-based medical practice and critical for ensuring safe and accurate
diagnostic decision-making. However, it remains unclear whether Large Language
Models (LLMs) can reliably follow such structured protocols. In this work, we
introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to
make guideline-consistent clinical decisions. MedGUIDE is constructed from 55
curated NCCN decision trees across 17 cancer types and uses clinical scenarios
generated by LLMs to create a large pool of multiple-choice diagnostic
questions. We apply a two-stage quality selection process, combining
expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical
and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25
LLMs spanning general-purpose, open-source, and medically specialized models,
and find that even domain-specific LLMs often underperform on tasks requiring
structured guideline adherence. We also test whether performance can be
improved via in-context guideline inclusion or continued pretraining. Our
findings underscore the importance of MedGUIDE in assessing whether LLMs can
operate safely within the procedural frameworks expected in real-world clinical
settings.

</details>


### [6] [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)
*Jian-Qiao Zhu,Haijiang Yan,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: The paper introduces a method to identify steering vectors for modifying LLM behavior without retraining, using alignment of latent representations from behavioral and neural data.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic way to influence LLM behavior without costly retraining or fine-tuning.

Method: Align latent representations from behavioral methods (MCMC with LLMs) with neural activations to derive steering vectors.

Result: Steering vectors effectively modulate LLM outputs to align with targeted behaviors, such as risk preferences.

Conclusion: The approach offers a reliable and targeted way to steer LLM behavior using internal representations.

Abstract: Changing the behavior of large language models (LLMs) can be as
straightforward as editing the Transformer's residual streams using
appropriately constructed "steering vectors." These modifications to internal
neural activations, a form of representation engineering, offer an effective
and targeted means of influencing model behavior without retraining or
fine-tuning the model. But how can such steering vectors be systematically
identified? We propose a principled approach for uncovering steering vectors by
aligning latent representations elicited through behavioral methods
(specifically, Markov chain Monte Carlo with LLMs) with their neural
counterparts. To evaluate this approach, we focus on extracting latent risk
preferences from LLMs and steering their risk-related outputs using the aligned
representations as steering vectors. We show that the resulting steering
vectors successfully and reliably modulate LLM outputs in line with the
targeted behavior.

</details>


### [7] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
*Udita Patel,Rutu Mulkar,Jay Roberts,Cibi Chakravarthy Senthilkumar,Sujay Gandhi,Xiaofei Zheng,Naumaan Nayyar,Rafael Castrillo*

Main category: cs.CL

TL;DR: THELMA is a reference-free framework with six metrics for evaluating RAG-based QA applications holistically, aiding developers in improving pipelines without labeled data.


<details>
  <summary>Details</summary>
Motivation: To provide a holistic and fine-grained evaluation method for RAG QA applications without needing labeled sources or reference responses.

Method: Proposes THELMA, a framework with six interdependent metrics designed for evaluating RAG QA applications.

Result: THELMA helps identify specific RAG components needing improvement in QA applications.

Conclusion: THELMA offers a practical solution for developers to evaluate and enhance RAG QA pipelines effectively.

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>


### [8] [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Chia-Hsuan Lee,Sambit Sahu*

Main category: cs.CL

TL;DR: CGD is a multi-stage framework that improves SFT by integrating teacher critiques and refined responses, reducing imitation issues and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Addresses the imitation problem in SFT where models reproduce responses without understanding.

Method: CGD uses teacher critiques and refined responses to train a student model, incorporating rationale.

Result: Achieves significant gains on math (+17.5%) and language tasks (+6.3%), mitigating format drift.

Conclusion: CGD effectively enhances understanding and performance in SFT by leveraging critiques and refined responses.

Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from
the imitation problem, where the model learns to reproduce the correct
responses without \emph{understanding} the underlying rationale. To address
this limitation, we propose \textsc{Critique-Guided Distillation (CGD)}, a
novel multi-stage framework that integrates teacher model generated
\emph{explanatory critiques} and \emph{refined responses} into the SFT process.
A student model is then trained to map the triplet of prompt, teacher critique,
and its own initial response to the corresponding refined teacher response,
thereby learning both \emph{what} to imitate and \emph{why}. Using
entropy-based analysis, we show that \textsc{CGD} reduces refinement
uncertainty and can be interpreted as a Bayesian posterior update. We perform
extensive empirical evaluation of \textsc{CGD}, on variety of benchmark tasks,
and demonstrate significant gains on both math (AMC23 +17.5%) and language
understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format
drift issues observed in previous critique fine-tuning (CFT) techniques.

</details>


### [9] [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)
*Xiang Fu*

Main category: cs.CL

TL;DR: A developmentally ordered curriculum improves reasoning transparency and efficiency in small language models (SLMs), as shown by Cognivolve, a 124M-parameter GPT-2 model.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning transparency and sample-efficiency in SLMs by using a structured curriculum.

Method: Train Cognivolve on a four-stage syllabus (lexical matching to multi-step symbolic inference) without task-specific fine-tuning.

Result: Cognivolve achieves target accuracy faster, activates more reasoning heads, and improves attention balance. Disordered curriculum or optimizer resets fail to replicate gains.

Conclusion: Ordered progression drives improvements, but challenges remain, such as lower final-answer success and under-detection of verbal-knowledge heads.

Abstract: We demonstrate that a developmentally ordered curriculum markedly improves
reasoning transparency and sample-efficiency in small language models (SLMs).
Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage
syllabus that ascends from lexical matching to multi-step symbolic inference
and then evaluate it without any task-specific fine-tuning. Cognivolve reaches
target accuracy in half the optimization steps of a single-phase baseline,
activates an order-of-magnitude more gradient-salient reasoning heads, and
shifts those heads toward deeper layers, yielding higher-entropy attention that
balances local and long-range context. The same curriculum applied out of order
or with optimizer resets fails to reproduce these gains, confirming that
progression--not extra compute--drives the effect. We also identify open
challenges: final-answer success still lags a conventional run by about 30%,
and our saliency probe under-detects verbal-knowledge heads in the hardest
stage, suggesting directions for mixed-stage fine-tuning and probe expansion.

</details>


### [10] [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.CL

TL;DR: The paper surveys multilingual prompt engineering techniques for LLMs, categorizing methods, analyzing performance across 250 languages, and deriving insights on language families and resource levels.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of ensuring LLM effectiveness across multiple languages without extensive re-training, focusing on multilingual prompt engineering.

Method: Survey and categorization of 39 multilingual prompting techniques from 36 papers, applied to 30 NLP tasks, with analysis of datasets spanning 250 languages.

Result: Identified SoTA methods, insights on language families and resource levels, and distribution of NLP tasks by resource type.

Conclusion: Multilingual prompt engineering is a viable approach to enhance LLM performance across diverse languages, accessible even to non-experts.

Abstract: Large language models (LLMs) have demonstrated impressive performance across
a wide range of Natural Language Processing (NLP) tasks. However, ensuring
their effectiveness across multiple languages presents unique challenges.
Multilingual prompt engineering has emerged as a key approach to enhance LLMs'
capabilities in diverse linguistic settings without requiring extensive
parameter re-training or fine-tuning. With growing interest in multilingual
prompt engineering over the past two to three years, researchers have explored
various strategies to improve LLMs' performance across languages and NLP tasks.
By crafting structured natural language prompts, researchers have successfully
extracted knowledge from LLMs across different languages, making these
techniques an accessible pathway for a broader audience, including those
without deep expertise in machine learning, to harness the capabilities of
LLMs. In this paper, we survey and categorize different multilingual prompting
techniques based on the NLP tasks they address across a diverse set of datasets
that collectively span around 250 languages. We further highlight the LLMs
employed, present a taxonomy of approaches and discuss potential
state-of-the-art (SoTA) methods for specific multilingual datasets.
Additionally, we derive a range of insights across language families and
resource levels (high-resource vs. low-resource), including analyses such as
the distribution of NLP tasks by language resource type and the frequency of
prompting methods across different language families. Our survey reviews 36
research papers covering 39 prompting techniques applied to 30 multilingual NLP
tasks, with the majority of these studies published in the last two years.

</details>


### [11] [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
*Zhibo Hu,Chen Wang,Yanfeng Shu,Hye-Young Paik,Liming Zhu*

Main category: cs.CL

TL;DR: The paper addresses ambiguity in natural language for LLMs by analyzing latent space representation differences and proposing a new distance metric to detect ambiguity, improving tool calling performance.


<details>
  <summary>Details</summary>
Motivation: Ambiguity in natural language hinders accurate text-to-structured data mapping in LLMs, affecting tasks like tool calling and text-to-SQL. Existing methods rely on trial-and-error or biased mappings, prompting a need for a better approach.

Method: The study characterizes ambiguity by analyzing latent space differences, using a new distance metric (path kernel from SAE gradients) to detect ambiguity. It identifies patterns and proposes a framework for missing concept prediction.

Result: The proposed method detects ambiguity effectively and improves LLM performance on ambiguous agentic tool calling tasks.

Conclusion: The framework leverages latent space analysis to address ambiguity, offering a novel solution for improving LLM accuracy in structured data mapping.

Abstract: Ambiguity in natural language is a significant obstacle for achieving
accurate text to structured data mapping through large language models (LLMs),
which affects the performance of tasks such as mapping text to agentic tool
calling and text-to-SQL queries. Existing methods of ambiguity handling either
exploit ReACT framework to produce the correct mapping through trial and error,
or supervised fine tuning to guide models to produce a biased mapping to
improve certain tasks. In this paper, we adopt a different approach that
characterizes the representation difference of ambiguous text in the latent
space and leverage the difference to identify ambiguity before mapping them to
structured data. To detect ambiguity of a sentence, we focused on the
relationship between ambiguous questions and their interpretations and what
cause the LLM ignore multiple interpretations. Different to the distance
calculated by dense embedding vectors, we utilize the observation that
ambiguity is caused by concept missing in latent space of LLM to design a new
distance measurement, computed through the path kernel by the integral of
gradient values for each concepts from sparse-autoencoder (SAE) under each
state. We identify patterns to distinguish ambiguous questions with this
measurement. Based on our observation, We propose a new framework to improve
the performance of LLMs on ambiguous agentic tool calling through missing
concepts prediction.

</details>


### [12] [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)
*Susanna Rücker,Alan Akbik*

Main category: cs.CL

TL;DR: The paper evaluates design choices for Dual Encoder-based Entity Disambiguation (ED), introduces VerbalizED with contextual label verbalizations and hard negative sampling, and achieves State-of-the-Art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve Entity Disambiguation by analyzing key design decisions in Dual Encoder models, such as loss functions and negative sampling strategies.

Method: Proposes VerbalizED, a document-level Dual Encoder model with contextual label verbalizations and efficient hard negative sampling, plus an iterative prediction variant.

Result: Validated on AIDA-Yago, the approach achieves State-of-the-Art performance on the ZELDA benchmark.

Conclusion: The study highlights impactful design choices and presents VerbalizED as an effective solution for ED, advancing the field.

Abstract: Entity disambiguation (ED) is the task of linking mentions in text to
corresponding entries in a knowledge base. Dual Encoders address this by
embedding mentions and label candidates in a shared embedding space and
applying a similarity metric to predict the correct label. In this work, we
focus on evaluating key design decisions for Dual Encoder-based ED, such as its
loss function, similarity metric, label verbalization format, and negative
sampling strategy. We present the resulting model VerbalizED, a document-level
Dual Encoder model that includes contextual label verbalizations and efficient
hard negative sampling. Additionally, we explore an iterative prediction
variant that aims to improve the disambiguation of challenging data points.
Comprehensive experiments on AIDA-Yago validate the effectiveness of our
approach, offering insights into impactful design choices that result in a new
State-of-the-Art system on the ZELDA benchmark.

</details>


### [13] [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)
*Sukairaj Hafiz Imam,Babangida Sani,Dawit Ketema Gete,Bedru Yimam Ahamed,Ibrahim Said Ahmad,Idris Abdulmumin,Seid Muhie Yimam,Muhammad Yahuza Bello,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: The paper examines challenges in developing ASR for low-resource African languages, proposing strategies like community-driven data collection and lightweight models to address issues like data scarcity and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Low-resource African languages are underrepresented in ASR research, hindering digital accessibility and socioeconomic participation.

Method: Analyzes barriers (data scarcity, linguistic complexity, etc.) and evaluates strategies (community-driven data, multilingual learning, privacy techniques) through case studies.

Result: Pilot projects demonstrate feasibility of tailored solutions, emphasizing interdisciplinary collaboration and investment.

Conclusion: A roadmap for ethical, inclusive ASR systems is proposed to enhance linguistic diversity and digital accessibility in Africa.

Abstract: Automatic Speech Recognition (ASR) technologies have transformed
human-computer interaction; however, low-resource languages in Africa remain
significantly underrepresented in both research and practical applications.
This study investigates the major challenges hindering the development of ASR
systems for these languages, which include data scarcity, linguistic
complexity, limited computational resources, acoustic variability, and ethical
concerns surrounding bias and privacy. The primary goal is to critically
analyze these barriers and identify practical, inclusive strategies to advance
ASR technologies within the African context. Recent advances and case studies
emphasize promising strategies such as community-driven data collection,
self-supervised and multilingual learning, lightweight model architectures, and
techniques that prioritize privacy. Evidence from pilot projects involving
various African languages showcases the feasibility and impact of customized
solutions, which encompass morpheme-based modeling and domain-specific ASR
applications in sectors like healthcare and education. The findings highlight
the importance of interdisciplinary collaboration and sustained investment to
tackle the distinct linguistic and infrastructural challenges faced by the
continent. This study offers a progressive roadmap for creating ethical,
efficient, and inclusive ASR systems that not only safeguard linguistic
diversity but also improve digital accessibility and promote socioeconomic
participation for speakers of African languages.

</details>


### [14] [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
*Ana Ezquerro,David Vilares,Anssi Yli-Jyrä,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: A family of hierarchical bracketing encodings for sequence labeling dependency parsing is introduced, proving the 4-bit projective encoding is suboptimal. An optimal encoding using 12 labels is derived and extended for non-projectivity, achieving competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and compactness of encodings for dependency parsing by minimizing the number of labels used while maintaining accuracy.

Method: Developed a family of hierarchical bracketing encodings, derived an optimal version using 12 labels, and extended it for non-projectivity.

Result: The new encodings outperform the 4-bit projective encoding in compactness and achieve competitive accuracy across diverse treebanks.

Conclusion: The proposed hierarchical bracketing encodings offer a more efficient and compact solution for dependency parsing, with practical benefits for sequence labeling tasks.

Abstract: We present a family of encodings for sequence labeling dependency parsing,
based on the concept of hierarchical bracketing. We prove that the existing
4-bit projective encoding belongs to this family, but it is suboptimal in the
number of labels used to encode a tree. We derive an optimal hierarchical
bracketing, which minimizes the number of symbols used and encodes projective
trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also
extend optimal hierarchical bracketing to support arbitrary non-projectivity in
a more compact way than previous encodings. Our new encodings yield competitive
accuracy on a diverse set of treebanks.

</details>


### [15] [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
*Shun Inadumi,Nobuhiro Ueda,Koichiro Yoshino*

Main category: cs.CL

TL;DR: The paper introduces a framework unifying textual and multimodal reference resolution to address ambiguities in dialogue, showing improved performance in pronoun phrase grounding.


<details>
  <summary>Details</summary>
Motivation: To integrate textual and multimodal reference resolution for better handling of ambiguities like pronouns and ellipses in dialogue.

Method: A framework mapping mention embeddings to object embeddings and selecting based on similarity, incorporating textual reference resolution.

Result: The model outperforms MDETR and GLIP in pronoun phrase grounding, with textual reference resolution improving confidence scores.

Conclusion: Incorporating textual reference relations reduces ambiguities in visually grounded dialogues.

Abstract: Multimodal reference resolution, including phrase grounding, aims to
understand the semantic relations between mentions and real-world objects.
Phrase grounding between images and their captions is a well-established task.
In contrast, for real-world applications, it is essential to integrate textual
and multimodal reference resolution to unravel the reference relations within
dialogue, especially in handling ambiguities caused by pronouns and ellipses.
This paper presents a framework that unifies textual and multimodal reference
resolution by mapping mention embeddings to object embeddings and selecting
mentions or objects based on their similarity. Our experiments show that
learning textual reference resolution, such as coreference resolution and
predicate-argument structure analysis, positively affects performance in
multimodal reference resolution. In particular, our model with coreference
resolution performs better in pronoun phrase grounding than representative
models for this task, MDETR and GLIP. Our qualitative analysis demonstrates
that incorporating textual reference relations strengthens the confidence
scores between mentions, including pronouns and predicates, and objects, which
can reduce the ambiguities that arise in visually grounded dialogues.

</details>


### [16] [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)
*Kevin Wu,Eric Wu,Rahul Thapa,Kevin Wei,Angela Zhang,Arvind Suresh,Jacqueline J. Tao,Min Woo Sun,Alejandro Lozano,James Zou*

Main category: cs.CL

TL;DR: The paper introduces MedCaseReasoning, a dataset to evaluate LLMs' clinical reasoning, showing current models' shortcomings and improvements via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks (e.g., MedQA, MMLU) focus only on diagnostic accuracy, ignoring reasoning quality, which is critical in medicine.

Method: Created MedCaseReasoning, a dataset of 14,489 diagnostic cases with clinician-authored reasoning, and evaluated LLMs' performance on it.

Result: Top models like DeepSeek-R1 scored only 48% accuracy and 64% recall on reasoning; fine-tuning improved accuracy by 29% and recall by 41%.

Conclusion: Fine-tuning LLMs on reasoning traces enhances diagnostic and reasoning performance, highlighting the need for reasoning-focused benchmarks.

Abstract: Doctors and patients alike increasingly use Large Language Models (LLMs) to
diagnose clinical cases. However, unlike domains such as math or coding, where
correctness can be objectively defined by the final answer, medical diagnosis
requires both the outcome and the reasoning process to be accurate. Currently,
widely used medical benchmarks like MedQA and MMLU assess only accuracy in the
final answer, overlooking the quality and faithfulness of the clinical
reasoning process. To address this limitation, we introduce MedCaseReasoning,
the first open-access dataset for evaluating LLMs on their ability to align
with clinician-authored diagnostic reasoning. The dataset includes 14,489
diagnostic question-and-answer cases, each paired with detailed reasoning
statements derived from open-access medical case reports. We evaluate
state-of-the-art reasoning LLMs on MedCaseReasoning and find significant
shortcomings in their diagnoses and reasoning: for instance, the top-performing
open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy
and mentions only 64% of the clinician reasoning statements (recall). However,
we demonstrate that fine-tuning LLMs on the reasoning traces derived from
MedCaseReasoning significantly improves diagnostic accuracy and clinical
reasoning recall by an average relative gain of 29% and 41%, respectively. The
open-source dataset, code, and models are available at
https://github.com/kevinwu23/Stanford-MedCaseReasoning.

</details>


### [17] [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
*Feijiang Han,Xiaodong Yu,Jianheng Tang,Lyle Ungar*

Main category: cs.CL

TL;DR: Training-free method ZeroTuning improves LLMs by tuning attention on the initial empty token, outperforming task-specific token tuning and showing robustness across tasks and models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving LLMs rely on auxiliary mechanisms, introducing bias and limiting applicability. This paper explores the untapped potential of the initial empty token as a control point.

Method: ZeroTuning adjusts head-specific attention on the semantically empty initial token, leveraging its role as an attention sink to sharpen or flatten attention distributions.

Result: ZeroTuning boosts performance on text classification, QA, and multi-turn tasks (e.g., 11.71% improvement for Llama-3.1-8B) and is robust to various conditions.

Conclusion: The initial empty token is a powerful, overlooked control point, offering insights for inference-time tuning and model interpretability.

Abstract: Recently, training-free methods for improving large language models (LLMs)
have attracted growing interest, with token-level attention tuning emerging as
a promising and interpretable direction. However, existing methods typically
rely on auxiliary mechanisms to identify important or irrelevant task-specific
tokens, introducing potential bias and limiting applicability. In this paper,
we uncover a surprising and elegant alternative: the semantically empty initial
token is a powerful and underexplored control point for optimizing model
behavior. Through theoretical analysis, we show that tuning the initial token's
attention sharpens or flattens the attention distribution over subsequent
tokens, and its role as an attention sink amplifies this effect. Empirically,
we find that: (1) tuning its attention improves LLM performance more
effectively than tuning other task-specific tokens; (2) the effect follows a
consistent trend across layers, with earlier layers having greater impact, but
varies across attention heads, with different heads showing distinct
preferences in how they attend to this token. Based on these findings, we
propose ZeroTuning, a training-free approach that improves LLM performance by
applying head-specific attention adjustments to this special token. Despite
tuning only one token, ZeroTuning achieves higher performance on text
classification, multiple-choice, and multi-turn conversation tasks across
models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves
Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its
multi-turn score from 7.804 to 7.966. The method is also robust to limited
resources, few-shot settings, long contexts, quantization, decoding strategies,
and prompt variations. Our work sheds light on a previously overlooked control
point in LLMs, offering new insights into both inference-time tuning and model
interpretability.

</details>


### [18] [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)
*Xianglong Xu,John Bowen,Rojin Taheri*

Main category: cs.CL

TL;DR: Token masking regularization improves transformer-based text classification by randomly replacing input tokens with [MASK], enhancing model robustness and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance transformer models' effectiveness by introducing stochastic perturbations during training to capture deeper inter-token dependencies.

Method: Proposes token masking regularization, randomly replacing input tokens with [MASK] at probability p, introducing gradient averaging.

Result: Consistent improvements in language identification and sentiment analysis across models, with p=0.1 as a general default.

Conclusion: Token masking regularization reduces overfitting and acts as implicit ensembling, improving model performance.

Abstract: While transformer-based models achieve strong performance on text
classification, we explore whether masking input tokens can further enhance
their effectiveness. We propose token masking regularization, a simple yet
theoretically motivated method that randomly replaces input tokens with a
special [MASK] token at probability p. This introduces stochastic perturbations
during training, leading to implicit gradient averaging that encourages the
model to capture deeper inter-token dependencies. Experiments on language
identification and sentiment analysis -- across diverse models (mBERT,
Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard
regularization techniques. We identify task-specific optimal masking rates,
with p = 0.1 as a strong general default. We attribute the gains to two key
effects: (1) input perturbation reduces overfitting, and (2) gradient-level
smoothing acts as implicit ensembling.

</details>


### [19] [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)
*Wenyu Huang,Pavlos Vougiouklis,Mirella Lapata,Jeff Z. Pan*

Main category: cs.CL

TL;DR: The paper investigates how Language Models (LMs) handle multi-hop question answering (MHQA), revealing that encoder-decoder models outperform causal decoder-only LMs, document order impacts performance, and modifying the causal mask can enhance results. Attention weight analysis further aids performance improvement.


<details>
  <summary>Details</summary>
Motivation: To understand and improve LMs' performance on MHQA tasks, given the challenges posed by causal masks and multi-hop reasoning.

Method: Permuting search results (documents) under various configurations, comparing encoder-decoder and decoder-only LMs, modifying causal masks, and analyzing attention weights.

Result: Encoder-decoder models (e.g., Flan-T5) outperform decoder-only LMs; document order alignment with reasoning chain improves performance; bi-directional attention boosts decoder-only models. Attention weights correlate with answer correctness.

Conclusion: Encoder-decoder models are more effective for MHQA, document order matters, and attention weight analysis can heuristically improve LM performance. The findings offer practical insights for enhancing MHQA systems.

Abstract: Multi-hop Question Answering (MHQA) adds layers of complexity to question
answering, making it more challenging. When Language Models (LMs) are prompted
with multiple search results, they are tasked not only with retrieving relevant
information but also employing multi-hop reasoning across the information
sources. Although LMs perform well on traditional question-answering tasks, the
causal mask can hinder their capacity to reason across complex contexts. In
this paper, we explore how LMs respond to multi-hop questions by permuting
search results (retrieved documents) under various configurations. Our study
reveals interesting findings as follows: 1) Encoder-decoder models, such as the
ones in the Flan-T5 family, generally outperform causal decoder-only LMs in
MHQA tasks, despite being significantly smaller in size; 2) altering the order
of gold documents reveals distinct trends in both Flan T5 models and fine-tuned
decoder-only models, with optimal performance observed when the document order
aligns with the reasoning chain order; 3) enhancing causal decoder-only models
with bi-directional attention by modifying the causal mask can effectively
boost their end performance. In addition to the above, we conduct a thorough
investigation of the distribution of LM attention weights in the context of
MHQA. Our experiments reveal that attention weights tend to peak at higher
values when the resulting answer is correct. We leverage this finding to
heuristically improve LMs' performance on this task. Our code is publicly
available at https://github.com/hwy9855/MultiHopQA-Reasoning.

</details>


### [20] [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
*Raymond Baartmans,Matthew Raffel,Rahul Vikram,Aiden Deringer,Lizhong Chen*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) to generate Natural Semantic Metalanguage (NSM) explications, improving accuracy and cross-translatability over manual methods.


<details>
  <summary>Details</summary>
Motivation: To automate the slow, manual process of creating NSM explications for NLP tasks.

Method: Uses LLMs (1B and 8B models) with tailored datasets and automatic evaluation methods.

Result: The models outperform GPT-4o in generating accurate, cross-translatable explications.

Conclusion: This advances universal semantic representation with LLMs, enabling new NLP applications.

Abstract: The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a
universal set of semantic primes: simple, primitive word-meanings that have
been shown to exist in most, if not all, languages of the world. According to
this framework, any word, regardless of complexity, can be paraphrased using
these primes, revealing a clear and universally translatable meaning. These
paraphrases, known as explications, can offer valuable applications for many
natural language processing (NLP) tasks, but producing them has traditionally
been a slow, manual process. In this work, we present the first study of using
large language models (LLMs) to generate NSM explications. We introduce
automatic evaluation methods, a tailored dataset for training and evaluation,
and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in
producing accurate, cross-translatable explications, marking a significant step
toward universal semantic representation with LLMs and opening up new
possibilities for applications in semantic analysis, translation, and beyond.

</details>


### [21] [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
*Yufei Xiang,Yiqun Shen,Yeqin Zhang,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: Retrospex is a new LLM-based agent framework that leverages past experiences through a Reinforcement Learning Critic and dynamic action rescoring, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent frameworks underutilize past experiences for improvement, limiting their potential.

Method: Retrospex combines LLM action likelihood with RL Critic values from offline retrospection and uses dynamic action rescoring for environment interaction.

Result: Retrospex outperforms contemporary baselines in ScienceWorld, ALFWorld, and Webshop environments.

Conclusion: Retrospex effectively utilizes past experiences to enhance LLM agent performance, demonstrating its superiority over existing frameworks.

Abstract: Large Language Models (LLMs) possess extensive knowledge and commonsense
reasoning capabilities, making them valuable for creating powerful agents.
However, existing LLM agent frameworks have not fully utilized past experiences
for improvement. This work introduces a new LLM-based agent framework called
Retrospex, which addresses this challenge by analyzing past experiences in
depth. Unlike previous approaches, Retrospex does not directly integrate
experiences into the LLM's context. Instead, it combines the LLM's action
likelihood with action values estimated by a Reinforcement Learning (RL)
Critic, which is trained on past experiences through an offline
''retrospection'' process. Additionally, Retrospex employs a dynamic action
rescoring mechanism that increases the importance of experience-based values
for tasks that require more interaction with the environment. We evaluate
Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its
advantages over strong, contemporary baselines.

</details>


### [22] [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
*Shen Li,Renfen Hu,Lijun Wang*

Main category: cs.CL

TL;DR: The paper introduces AI Taiyan, a specialized large language model for Classical Chinese, outperforming general and traditional models in tasks like punctuation, translation, and word explanation with only 1.8B parameters.


<details>
  <summary>Details</summary>
Motivation: General-purpose models underperform in specialized domains like Classical Chinese, and fine-tuning struggles with domain-specific knowledge.

Method: Developed AI Taiyan with tailored model design, data processing, foundational training, and fine-tuning.

Result: AI Taiyan excels in Classical Chinese tasks, matching or surpassing human performance and outperforming other models.

Conclusion: The study offers a blueprint for efficient domain-specific model development and highlights applications in ancient text collation and language research.

Abstract: General-purpose large language models demonstrate notable capabilities in
language comprehension and generation, achieving results that are comparable
to, or even surpass, human performance in many language information processing
tasks. Nevertheless, when general models are applied to some specific domains,
e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and
fine-tuning open-source foundational models similarly struggles to adequately
incorporate domain-specific knowledge. To address this challenge, this study
developed a large language model, AI Taiyan, specifically designed for
understanding and generating Classical Chinese. Experiments show that with a
reasonable model design, data processing, foundational training, and
fine-tuning, satisfactory results can be achieved with only 1.8 billion
parameters. In key tasks related to Classical Chinese information processing
such as punctuation, identification of allusions, explanation of word meanings,
and translation between ancient and modern Chinese, this model exhibits a clear
advantage over both general-purpose large models and domain-specific
traditional models, achieving levels close to or surpassing human baselines.
This research provides a reference for the efficient construction of
specialized domain-specific large language models. Furthermore, the paper
discusses the application of this model in fields such as the collation of
ancient texts, dictionary editing, and language research, combined with case
studies.

</details>


### [23] [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)
*Taolin Zhang,Dongyang Li,Qizhou Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.CL

TL;DR: The paper introduces BELLE, a framework for multi-hop QA that matches question types with specific methods, using a bi-level debate system to optimize reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing multi-hop QA methods lack adaptability to question types, prompting the need for a tailored approach.

Method: BELLE uses a bi-level debate system with multiple agents to select and combine methods ('operators') for multi-hop QA, leveraging fast and slow debaters for reasoning validation.

Result: BELLE outperforms baselines in various datasets and offers higher cost-effectiveness in complex scenarios.

Conclusion: BELLE effectively addresses multi-hop QA by aligning methods with question types and optimizing reasoning through debate.

Abstract: Multi-hop question answering (QA) involves finding multiple relevant passages
and performing step-by-step reasoning to answer complex questions. Previous
works on multi-hop QA employ specific methods from different modeling
perspectives based on large language models (LLMs), regardless of the question
types. In this paper, we first conduct an in-depth analysis of public multi-hop
QA benchmarks, dividing the questions into four types and evaluating five types
of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,
Iterative-step, Sub-step, and Adaptive-step. We find that different types of
multi-hop questions have varying degrees of sensitivity to different types of
methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to
address multi-hop QA by specifically focusing on the correspondence between
question types and methods, where each type of method is regarded as an
''operator'' by prompting LLMs differently. The first level of BELLE includes
multiple agents that debate to obtain an executive plan of combined
''operators'' to address the multi-hop QA task comprehensively. During the
debate, in addition to the basic roles of affirmative debater, negative
debater, and judge, at the second level, we further leverage fast and slow
debaters to monitor whether changes in viewpoints are reasonable. Extensive
experiments demonstrate that BELLE significantly outperforms strong baselines
in various datasets. Additionally, the model consumption of BELLE is higher
cost-effectiveness than that of single models in more complex multi-hop QA
scenarios.

</details>


### [24] [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
*Kaitao Song,Xiaohua Wang,Xu Tan,Huiqiang Jiang,Chengruidong Zhang,Yongliang Shen,Cen LU,Zihao Li,Zifan Song,Caihua Shan,Yansen Wang,Kan Ren,Xiaoqing Zheng,Tao Qin,Yuqing Yang,Dongsheng Li,Lili Qiu*

Main category: cs.CL

TL;DR: The paper introduces Chain-of-Model (CoM), a learning paradigm that embeds causal relationships in hidden states for efficient scaling and flexible inference. It proposes Chain-of-Representation (CoR) and applies it to Transformers as Chain-of-Language-Model (CoLM), with CoLM-Air adding KV sharing for extensibility. Results show comparable performance to standard Transformers with added flexibility.


<details>
  <summary>Details</summary>
Motivation: To improve model training efficiency and inference flexibility by incorporating causal relationships into hidden states, enabling progressive scaling and elastic inference.

Method: Introduces CoM and CoR, applies them to Transformers as CoLM, and extends with KV sharing in CoLM-Air.

Result: CoLM achieves performance comparable to standard Transformers while offering progressive scaling and multiple model sizes for flexible inference.

Conclusion: CoM and CoLM provide a novel approach to building efficient and flexible language models, with potential for further applications.

Abstract: In this paper, we propose a novel learning paradigm, termed Chain-of-Model
(CoM), which incorporates the causal relationship into the hidden states of
each layer as a chain style, thereby introducing great scaling efficiency in
model training and inference flexibility in deployment. We introduce the
concept of Chain-of-Representation (CoR), which formulates the hidden states at
each layer as a combination of multiple sub-representations (i.e., chains) at
the hidden dimension level. In each layer, each chain from the output
representations can only view all of its preceding chains in the input
representations. Consequently, the model built upon CoM framework can
progressively scale up the model size by increasing the chains based on the
previous models (i.e., chains), and offer multiple sub-models at varying sizes
for elastic inference by using different chain numbers. Based on this
principle, we devise Chain-of-Language-Model (CoLM), which incorporates the
idea of CoM into each layer of Transformer architecture. Based on CoLM, we
further introduce CoLM-Air by introducing a KV sharing mechanism, that computes
all keys and values within the first chain and then shares across all chains.
This design demonstrates additional extensibility, such as enabling seamless LM
switching, prefilling acceleration and so on. Experimental results demonstrate
our CoLM family can achieve comparable performance to the standard Transformer,
while simultaneously enabling greater flexiblity, such as progressive scaling
to improve training efficiency and offer multiple varying model sizes for
elastic inference, paving a a new way toward building language models. Our code
will be released in the future at: https://github.com/microsoft/CoLM.

</details>


### [25] [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
*Yansong Ning,Wei Li,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.CL

TL;DR: The paper proposes Long⊗Short, a framework for compressing long CoT in LLMs by distinguishing between important and less important thoughts, improving reasoning efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods compress all thoughts equally in long CoT, limiting conciseness and effectiveness. The study aims to optimize reasoning by differentiating thought importance.

Method: The approach involves: 1) analyzing thought importance via automatic chunking and Monte Carlo rollouts, 2) proposing a metric for effectiveness/efficiency, 3) fine-tuning LLMs for long/short-thought reasoning, and 4) using multi-turn reinforcement learning for collaboration.

Result: The method reduces token length by over 80% while maintaining performance on benchmarks like MATH500 and GPQA Diamond.

Conclusion: Long⊗Short enhances reasoning efficiency by leveraging collaborative LLMs, achieving significant compression without performance loss.

Abstract: Compressing long chain-of-thought (CoT) from large language models (LLMs) is
an emerging strategy to improve the reasoning efficiency of LLMs. Despite its
promising benefits, existing studies equally compress all thoughts within a
long CoT, hindering more concise and effective reasoning. To this end, we first
investigate the importance of different thoughts by examining their
effectiveness and efficiency in contributing to reasoning through automatic
long CoT chunking and Monte Carlo rollouts. Building upon the insights, we
propose a theoretically bounded metric to jointly measure the effectiveness and
efficiency of different thoughts. We then propose Long$\otimes$Short, an
efficient reasoning framework that enables two LLMs to collaboratively solve
the problem: a long-thought LLM for more effectively generating important
thoughts, while a short-thought LLM for efficiently generating remaining
thoughts. Specifically, we begin by synthesizing a small amount of cold-start
data to fine-tune LLMs for long-thought and short-thought reasoning styles,
respectively. Furthermore, we propose a synergizing-oriented multi-turn
reinforcement learning, focusing on the model self-evolution and collaboration
between long-thought and short-thought LLMs. Experimental results show that our
method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance
compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while
reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and
GPQA Diamond benchmarks. Our data and code are available at
https://github.com/yasNing/Long-otimes-Short/.

</details>


### [26] [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)
*Chenlu Wang,Weimin Lyu,Ritwik Banerjee*

Main category: cs.CL

TL;DR: ClaD (Class Distillation) is a new training method for detecting nuanced or deviant language (e.g., sexism, metaphors, sarcasm) efficiently, outperforming larger models with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Improving online discourse safety and clarity by detecting nuanced language, while reducing computational costs and data demands.

Method: Uses Mahalanobis distance-based loss and an interpretable decision algorithm for class separation.

Result: Outperforms baselines in sexism, metaphor, and sarcasm detection, matching LLM performance with smaller models.

Conclusion: ClaD is an efficient solution for extracting small target classes from diverse backgrounds in language tasks.

Abstract: Detecting deviant language such as sexism, or nuanced language such as
metaphors or sarcasm, is crucial for enhancing the safety, clarity, and
interpretation of online social discourse. While existing classifiers deliver
strong results on these tasks, they often come with significant computational
cost and high data demands. In this work, we propose \textbf{Cla}ss
\textbf{D}istillation (ClaD), a novel training paradigm that targets the core
challenge: distilling a small, well-defined target class from a highly diverse
and heterogeneous background. ClaD integrates two key innovations: (i) a loss
function informed by the structural properties of class distributions, based on
Mahalanobis distance, and (ii) an interpretable decision algorithm optimized
for class separation. Across three benchmark detection tasks -- sexism,
metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with
smaller language models and orders of magnitude fewer parameters, achieves
performance comparable to several large language models (LLMs). These results
demonstrate ClaD as an efficient tool for pragmatic language understanding
tasks that require gleaning a small target class from a larger heterogeneous
background.

</details>


### [27] [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
*Hongliang Li,Jinan Xu,Gengping Cui,Changhao Guan,Fengran Mo,Kaiyu Huang*

Main category: cs.CL

TL;DR: The paper introduces Multilingual Collaborative Defense (MCD), a method to enhance LLM safeguards against multilingual jailbreak attacks, showing superior performance and language transferability.


<details>
  <summary>Details</summary>
Motivation: Addressing the vulnerability of LLMs to jailbreak attacks via rare or underrepresented languages, highlighting the lack of multilingual safeguarding research.

Method: Proposes MCD, a learning method optimizing continuous safety prompts for multilingual safeguarding, tested on manually constructed multilingual jailbreak benchmarks.

Result: MCD outperforms existing methods in safeguarding and demonstrates strong language transfer capabilities.

Conclusion: MCD effectively enhances multilingual LLM safety, generalizes well, and mitigates language safety misalignment.

Abstract: The robustness and security of large language models (LLMs) has become a
prominent research area. One notable vulnerability is the ability to bypass LLM
safeguards by translating harmful queries into rare or underrepresented
languages, a simple yet effective method of "jailbreaking" these models.
Despite the growing concern, there has been limited research addressing the
safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to
enhance multilingual safety. In this work, we investigate the correlation
between various attack features across different languages and propose
Multilingual Collaborative Defense (MCD), a novel learning method that
optimizes a continuous, soft safety prompt automatically to facilitate
multilingual safeguarding of LLMs. The MCD approach offers three advantages:
First, it effectively improves safeguarding performance across multiple
languages. Second, MCD maintains strong generalization capabilities while
minimizing false refusal rates. Third, MCD mitigates the language safety
misalignment caused by imbalances in LLM training corpora. To evaluate the
effectiveness of MCD, we manually construct multilingual versions of commonly
used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess
various safeguarding methods. Additionally, we introduce these datasets in
underrepresented (zero-shot) languages to verify the language transferability
of MCD. The results demonstrate that MCD outperforms existing approaches in
safeguarding against multilingual jailbreak attempts while also exhibiting
strong language transfer capabilities. Our code is available at
https://github.com/HLiang-Lee/MCD.

</details>


### [28] [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)
*Guijin Son,Jiwoo Hong,Honglu Fan,Heejeong Nam,Hyunwoo Ko,Seungwon Lim,Jinyeop Song,Jinha Choi,Gonçalo Paulo,Youngjae Yu,Stella Biderman*

Main category: cs.CL

TL;DR: The paper explores using LLMs for verifying scientific manuscripts, introducing the SPOT dataset. Current LLMs perform poorly, with low recall and precision, and lack reliability for academic verification.


<details>
  <summary>Details</summary>
Motivation: To investigate the feasibility of using LLMs for automating the verification of scientific manuscripts, addressing a gap in prior work focused on generative tasks.

Method: Introduces SPOT, a dataset of 83 papers with 91 verified errors, and evaluates state-of-the-art LLMs on their ability to detect these errors.

Result: LLMs perform poorly (max 21.1% recall, 6.1% precision), lack consistency across runs, and make student-level errors, indicating unreliability.

Conclusion: Current LLMs fall short of dependable academic verification, highlighting a significant capability gap.

Abstract: Recent advances in large language models (LLMs) have fueled the vision of
automated scientific discovery, often called AI Co-Scientists. To date, prior
work casts these systems as generative co-authors responsible for crafting
hypotheses, synthesizing code, or drafting manuscripts. In this work, we
explore a complementary application: using LLMs as verifiers to automate the
\textbf{academic verification of scientific manuscripts}. To that end, we
introduce SPOT, a dataset of 83 published papers paired with 91 errors
significant enough to prompt errata or retraction, cross-validated with actual
authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find
that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best
scores, with all others near zero). Furthermore, confidence estimates are
uniformly low, and across eight independent runs, models rarely rediscover the
same errors, undermining their reliability. Finally, qualitative analysis with
domain experts reveals that even the strongest models make mistakes resembling
student-level misconceptions derived from misunderstandings. These findings
highlight the substantial gap between current LLM capabilities and the
requirements for dependable AI-assisted academic verification.

</details>


### [29] [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Shuai Wang*

Main category: cs.CL

TL;DR: NAMET, a noise-aware model editing method, improves reliability in large-scale knowledge updates for LLMs by addressing embedding collisions.


<details>
  <summary>Details</summary>
Motivation: Existing model editing techniques degrade in massive editing scenarios due to embedding collisions, limiting their practical effectiveness.

Method: NAMET introduces noise during memory extraction in MEMIT, a simple one-line modification, to enhance editing reliability.

Result: Experiments on six LLMs and three datasets show NAMET outperforms existing methods for editing thousands of facts.

Conclusion: NAMET is a simple yet effective solution for scalable and reliable knowledge updates in LLMs.

Abstract: Model editing techniques are essential for efficiently updating knowledge in
large language models (LLMs). However, the effectiveness of existing approaches
degrades in massive editing scenarios, particularly when evaluated with
practical metrics or in context-rich settings. We attribute these failures to
embedding collisions among knowledge items, which undermine editing reliability
at scale. To address this, we propose NAMET (Noise-aware Model Editing in
Transformers), a simple yet effective method that introduces noise during
memory extraction via a one-line modification to MEMIT. Extensive experiments
across six LLMs and three datasets demonstrate that NAMET consistently
outperforms existing methods when editing thousands of facts.

</details>


### [30] [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)
*Xiechi Zhang,Zetian Ouyang,Linlin Wang,Gerard de Melo,Zhu Cao,Xiaoling Wang,Ya Zhang,Yanfeng Wang,Liang He*

Main category: cs.CL

TL;DR: AutoMedEval is an open-sourced 13B-parameter model designed to evaluate medical LLMs' question-answering skills, reducing reliance on human evaluation by using hierarchical training and iterative knowledge introspection.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics and human evaluation for medical LLMs are inadequate due to token-overlap limitations, high costs, and expertise gaps. Proprietary LLM-based methods also lack usability in medicine.

Method: AutoMedEval employs hierarchical training with curriculum instruction tuning and iterative knowledge introspection to develop medical assessment capabilities with limited data.

Result: AutoMedEval outperforms baselines in correlating with human judgments, demonstrating its effectiveness.

Conclusion: AutoMedEval offers a reliable, automatic alternative to human evaluation for assessing medical LLMs, addressing existing limitations in the field.

Abstract: With the proliferation of large language models (LLMs) in the medical domain,
there is increasing demand for improved evaluation techniques to assess their
capabilities. However, traditional metrics like F1 and ROUGE, which rely on
token overlaps to measure quality, significantly overlook the importance of
medical terminology. While human evaluation tends to be more reliable, it can
be very costly and may as well suffer from inaccuracies due to limits in human
expertise and motivation. Although there are some evaluation methods based on
LLMs, their usability in the medical field is limited due to their proprietary
nature or lack of expertise. To tackle these challenges, we present
AutoMedEval, an open-sourced automatic evaluation model with 13B parameters
specifically engineered to measure the question-answering proficiency of
medical LLMs. The overarching objective of AutoMedEval is to assess the quality
of responses produced by diverse models, aspiring to significantly reduce the
dependence on human evaluation. Specifically, we propose a hierarchical
training method involving curriculum instruction tuning and an iterative
knowledge introspection mechanism, enabling AutoMedEval to acquire professional
medical assessment capabilities with limited instructional data. Human
evaluations indicate that AutoMedEval surpasses other baselines in terms of
correlation with human judgments.

</details>


### [31] [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
*Weikai Xu,Zhizheng Jiang,Yuxuan Liu,Wei Liu,Jian Luan,Yuanchun Li,Yunxin Liu,Bin Wang,Bo An*

Main category: cs.CL

TL;DR: Mobile-Bench-v2 is introduced to address limitations in existing benchmarks for VLM-based mobile agents, offering multi-path evaluation, noisy environments, and ambiguous instructions for proactive interaction assessment.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack stable reward signals, multi-solution evaluation, and realistic noisy or proactive interaction scenarios.

Method: A slot-based instruction generation method is used to create Mobile-Bench-v2, featuring multi-path offline evaluation, noisy splits, and ambiguous instruction splits.

Result: Mobile-Bench-v2 evaluates agents like AppAgent-v1 and Mobile-Agent-v2, providing a more realistic and comprehensive benchmark.

Conclusion: Mobile-Bench-v2 improves evaluation of mobile agents by addressing prior shortcomings and introducing diverse testing scenarios.

Abstract: VLM-based mobile agents are increasingly popular due to their capabilities to
interact with smartphone GUIs and XML-structured texts and to complete daily
tasks. However, existing online benchmarks struggle with obtaining stable
reward signals due to dynamic environmental changes. Offline benchmarks
evaluate the agents through single-path trajectories, which stands in contrast
to the inherently multi-solution characteristics of GUI tasks. Additionally,
both types of benchmarks fail to assess whether mobile agents can handle noise
or engage in proactive interactions due to a lack of noisy apps or overly full
instructions during the evaluation process. To address these limitations, we
use a slot-based instruction generation method to construct a more realistic
and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a
common task split, with offline multi-path evaluation to assess the agent's
ability to obtain step rewards during task execution. It contains a noisy split
based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to
formulate a real noisy environment. Furthermore, an ambiguous instruction split
with preset Q\&A interactions is released to evaluate the agent's proactive
interaction capabilities. We conduct evaluations on these splits using the
single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,
as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are
available at https://huggingface.co/datasets/xwk123/MobileBench-v2.

</details>


### [32] [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)
*Zepeng Ding,Dixuan Wang,Ziqin Luo,Guochao Jiang,Deqing Yang,Jiaqing Liang*

Main category: cs.CL

TL;DR: RLAP is a Reinforcement Learning enhanced Adaptive Planning framework for multi-step NLP tasks, leveraging linguistic features and LLMs for optimal subtask ordering.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-step planning in NLP tasks overlook linguistic features and rely on LLMs' intrinsic capabilities, leading to suboptimal results.

Method: RLAP models NLP tasks as a Markov decision process (MDP), using a lightweight Actor model trained via reinforcement learning to estimate Q-values for natural language sequences.

Result: RLAP is applied to three NLP tasks, showing effectiveness and robustness in experiments.

Conclusion: RLAP improves multi-step NLP task performance by adaptively planning subtask order based on linguistic features and reinforcement learning.

Abstract: Multi-step planning has been widely employed to enhance the performance of
large language models (LLMs) on downstream natural language processing (NLP)
tasks, which decomposes the original task into multiple subtasks and guide LLMs
to solve them sequentially without additional training. When addressing task
instances, existing methods either preset the order of steps or attempt
multiple paths at each step. However, these methods overlook instances'
linguistic features and rely on the intrinsic planning capabilities of LLMs to
evaluate intermediate feedback and then select subtasks, resulting in
suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this
paper we propose a Reinforcement Learning enhanced Adaptive Planning framework
(RLAP). In our framework, we model an NLP task as a Markov decision process
(MDP) and employ an LLM directly into the environment. In particular, a
lightweight Actor model is trained to estimate Q-values for natural language
sequences consisting of states and actions through reinforcement learning.
Therefore, during sequential planning, the linguistic features of each sequence
in the MDP can be taken into account, and the Actor model interacts with the
LLM to determine the optimal order of subtasks for each task instance. We apply
RLAP on three different types of NLP tasks and conduct extensive experiments on
multiple datasets to verify RLAP's effectiveness and robustness.

</details>


### [33] [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP is a method for answering questions over mixed personal data sources by creating executable operator trees via recursive decomposition, ensuring data stays on-device.


<details>
  <summary>Details</summary>
Motivation: To provide convenient, traceable access to personal data (e.g., calendars, workouts) while keeping it on user devices.

Method: Recursive decomposition to create executable operator trees, integrating structured and unstructured sources.

Result: ReQAP enables traceable answers over mixed data. The PerQA benchmark is released for testing.

Conclusion: ReQAP offers a practical solution for on-device personal data QA, validated by the PerQA benchmark.

Abstract: Question answering over mixed sources, like text and tables, has been
advanced by verbalizing all contents and encoding it with a language model. A
prominent case of such heterogeneous data is personal information: user devices
log vast amounts of data every day, such as calendar entries, workout
statistics, shopping records, streaming history, and more. Information needs
range from simple look-ups to queries of analytical nature. The challenge is to
provide humans with convenient access with small footprint, so that all
personal data stays on the user devices. We present ReQAP, a novel method that
creates an executable operator tree for a given question, via recursive
decomposition. Operators are designed to enable seamless integration of
structured and unstructured sources, and the execution of the operator tree
yields a traceable answer. We further release the PerQA benchmark, with
persona-based data and questions, covering a diverse spectrum of realistic user
needs.

</details>


### [34] [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)
*Zhangyu Wang,Siyuan Gao,Rong Zhou,Hao Wang,Li Ning*

Main category: cs.CL

TL;DR: The paper proposes an embedding-free retrieval framework for LLMs to improve long-term context retention, outperforming baselines while reducing storage and runtime.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems rely on embedding-based retrieval, which can retrieve semantically similar but misaligned content, and recent variants introduce high computation and storage overhead.

Method: The framework leverages LLMs' logical inferencing ability for retrieval, using iterative search space refinement and a novel importance measure, avoiding explicit graph construction.

Result: Experiments on long-context QA benchmarks (NovelQA, Marathon) show superior performance and significant reductions in storage and runtime.

Conclusion: The proposed embedding-free retrieval framework effectively addresses limitations of existing RAG systems, offering improved accuracy and efficiency.

Abstract: Large Language Models (LLMs) have achieved impressive progress in natural
language processing, but their limited ability to retain long-term context
constrains performance on document-level or multi-turn tasks.
Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant
information from an external corpus. However, existing RAG systems often rely
on embedding-based retrieval trained on corpus-level semantic similarity, which
can lead to retrieving content that is semantically similar in form but
misaligned with the question's true intent. Furthermore, recent RAG variants
construct graph- or hierarchy-based structures to improve retrieval accuracy,
resulting in significant computation and storage overhead. In this paper, we
propose an embedding-free retrieval framework. Our method leverages the logical
inferencing ability of LLMs in retrieval using iterative search space
refinement guided by our novel importance measure and extend our retrieval
results with logically related information without explicit graph construction.
Experiments on long-context QA benchmarks, including NovelQA and Marathon, show
that our approach outperforms strong baselines while reducing storage and
runtime by over an order of magnitude.

</details>


### [35] [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)
*Yuheng Lu,ZiMeng Bai,Caixia Yuan,Huixing Jiang,Xiaojie Wang*

Main category: cs.CL

TL;DR: The paper introduces MISO, a method to enhance LLMs' ability to follow complex instructions by transforming sequential inputs into parallel sub-contexts and jointly optimizing alignment.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex instructions involving multiple constraints, and existing SFT methods often overlook sub-contexts, reducing effectiveness.

Method: Proposes MISO, a mixture-of-contexts paradigm, to process parallel sub-contexts and improve SFT by jointly optimizing overall alignment and sub-context influence.

Result: MISO outperforms standard SFT in complex instruction-following and shows potential for training efficiency.

Conclusion: MISO is an effective fine-tuning method for LLMs, particularly for complex instructions, and offers efficiency benefits.

Abstract: Large language models (LLMs) exhibit remarkable capabilities in handling
natural language tasks; however, they may struggle to consistently follow
complex instructions including those involve multiple constraints.
Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to
improve their ability to follow instructions. In addressing complex instruction
following, existing efforts primarily focus on data-driven methods that
synthesize complex instruction-output pairs for SFT. However, insufficient
attention allocated to crucial sub-contexts may reduce the effectiveness of
SFT. In this work, we propose transforming sequentially structured input
instruction into multiple parallel instructions containing subcontexts. To
support processing this multi-input, we propose MISO (Multi-Input
Single-Output), an extension to currently dominant decoder-only
transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that
jointly considers the overall instruction-output alignment and the influence of
individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning
to complex instructionfollowing datasets and evaluate it with standard LLM
inference. Empirical results demonstrate the superiority of MISO as a
fine-tuning method for LLMs, both in terms of effectiveness in complex
instruction-following scenarios and its potential for training efficiency.

</details>


### [36] [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)
*Yu-Ting Lee,Hui-Ying Shih,Fu-Chieh Chang,Pei-Yuan Wu*

Main category: cs.CL

TL;DR: The paper explains how intrinsic self-correction in language models improves performance by analyzing prompt-induced changes in hidden states and output distributions. It provides a mathematical framework and experimental results showing enhanced latent concept recognition.


<details>
  <summary>Details</summary>
Motivation: To understand the performance gains of intrinsic self-correction in language models and how prompting affects hidden states and output distributions.

Method: Mathematical formulation of self-correction, analysis of prompt-induced shifts in hidden states, and experiments on text detoxification using zephyr-7b-sft.

Result: Substantial gap in inner products of prompt-induced shifts between toxic and non-toxic tokens, indicating enhanced latent concept recognition.

Conclusion: Self-correction prompts improve a model's ability to recognize latent concepts, offering explainable insights into the mechanism of prompting.

Abstract: We provide an explanation for the performance gains of intrinsic
self-correction, a process where a language model iteratively refines its
outputs without external feedback. More precisely, we investigate how prompting
induces interpretable changes in hidden states and thus affects the output
distributions. We hypothesize that each prompt-induced shift lies in a linear
span of some linear representation vectors, naturally separating tokens based
on individual concept alignment. Building around this idea, we give a
mathematical formulation of self-correction and derive a concentration result
for output tokens based on alignment magnitudes. Our experiments on text
detoxification with zephyr-7b-sft reveal a substantial gap in the inner
products of the prompt-induced shifts and the unembeddings of the top-100 most
toxic tokens vs. those of the unembeddings of the bottom-100 least toxic
tokens, under toxic instructions. This suggests that self-correction prompts
enhance a language model's capability of latent concept recognition. Our
analysis offers insights into the underlying mechanism of self-correction by
characterizing how prompting works explainably. For reproducibility, our code
is available.

</details>


### [37] [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)
*Yuyao Zhang,Zhicheng Dou,Xiaoxi Li,Jiajie Jin,Yongkang Wu,Zhonghua Li,Qi Ye,Ji-Rong Wen*

Main category: cs.CL

TL;DR: QCompiler, a neuro-symbolic framework, formalizes complex queries using a minimal BNF grammar, improving RAG systems' precision for nested and dependent queries.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of precise search intent recognition in RAG systems, especially for complex queries under resource constraints.

Method: QCompiler uses a BNF grammar $G[q]$, a Query Expression Translator, Lexical Syntax Parser, and Recursive Descent Processor to compile queries into ASTs.

Result: Atomic sub-queries in leaf nodes enhance document retrieval and response generation, improving RAG system performance for complex queries.

Conclusion: QCompiler bridges the gap in handling complex queries in RAG systems by combining linguistic grammar rules and compiler design.

Abstract: Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.

</details>


### [38] [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
*Xuanle Zhao,Xuexin Liu,Haoyue Yang,Xianzhen Luo,Fanhu Zeng,Jianling Li,Qi Shi,Chi Chen*

Main category: cs.CL

TL;DR: ChartEdit is a new benchmark for evaluating MLLMs on chart editing tasks, revealing their limitations in precise intent interpretation and accurate edits.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack robust evaluation for chart editing, a complex task requiring understanding, reasoning, and intent interpretation.

Method: Proposed ChartEdit, a benchmark with 1,405 diverse editing instructions on 233 real-world charts, manually annotated. Evaluated 10 MLLMs at code and chart levels.

Result: Large-scale models partially match reference images but struggle with precise edits (SOTA score: 59.96). Small-scale models perform poorly overall.

Conclusion: Chart editing remains challenging for MLLMs, highlighting the need for further development and better evaluation frameworks.

Abstract: Although multimodal large language models (MLLMs) show promise in generating
chart rendering code, chart editing presents a greater challenge. This
difficulty stems from its nature as a labor-intensive task for humans that also
demands MLLMs to integrate chart understanding, complex reasoning, and precise
intent interpretation. While many MLLMs claim such editing capabilities,
current assessments typically rely on limited case studies rather than robust
evaluation methodologies, highlighting the urgent need for a comprehensive
evaluation framework. In this work, we propose ChartEdit, a new high-quality
benchmark designed for chart editing tasks. This benchmark comprises $1,405$
diverse editing instructions applied to $233$ real-world charts, with each
instruction-chart instance having been manually annotated and validated for
accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream
MLLMs across two types of experiments, assessing them at both the code and
chart levels. The results suggest that large-scale models can generate code to
produce images that partially match the reference images. However, their
ability to generate accurate edits according to the instructions remains
limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,
highlighting significant challenges in precise modification. In contrast,
small-scale models, including chart-domain models, struggle both with following
editing instructions and generating overall chart images, underscoring the need
for further development in this area. Code is available at
https://github.com/xxlllz/ChartEdit.

</details>


### [39] [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
*Aswini Kumar Padhi,Anil Bandhakavi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: HiPPrO, a two-stage framework, improves counterspeech generation by hierarchically optimizing multiple attributes and preference optimization, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing counterspeech methods focus on single attributes, limiting nuance and effectiveness. A multi-attribute approach can enhance counterspeech quality.

Method: HiPPrO uses hierarchical prefix embedding spaces and preference optimization (reference and reward-free) to generate constructive counterspeech, leveraging annotated emotion labels.

Result: HiPPrO improves intent conformity by ~38% and Rouge scores by ~3%, ~2%, ~3% (Rouge-1, Rouge-2, Rouge-L) over baselines, with human evaluations confirming superiority.

Conclusion: Multi-attribute conditioning, as demonstrated by HiPPrO, significantly advances counterspeech generation efficacy.

Abstract: Counterspeech has proven to be a powerful tool to combat hate speech online.
Previous studies have focused on generating counterspeech conditioned only on
specific intents (single attributed). However, a holistic approach considering
multiple attributes simultaneously can yield more nuanced and effective
responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with
Preference Optimization, a novel two-stage framework that utilizes the
effectiveness of attribute-specific prefix embedding spaces hierarchically
optimized during the counterspeech generation process in the first phase.
Thereafter, we incorporate both reference and reward-free preference
optimization to generate more constructive counterspeech. Furthermore, we
extend IntentCONANv2 by annotating all 13,973 counterspeech instances with
emotion labels by five annotators. HiPPrO leverages hierarchical prefix
optimization to integrate these dual attributes effectively. An extensive
evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent
conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,
respectively, compared to several baseline models. Human evaluations further
substantiate the superiority of our approach, highlighting the enhanced
relevance and appropriateness of the generated counterspeech. This work
underscores the potential of multi-attribute conditioning in advancing the
efficacy of counterspeech generation systems.

</details>


### [40] [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)
*Md. Rafiul Biswas,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: A bilingual Arabic-English dataset with emotion and hope speech annotations was created to address dataset scarcity, validated by high annotator agreement and baseline model performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of multi-emotion datasets for underrepresented languages like Arabic and English, enabling better NLP and cross-linguistic analysis.

Method: Created a dataset with 23,456 Arabic and 10,036 English entries, annotated for emotions and hope speech. Used Fleiss' Kappa for reliability and evaluated with a baseline machine learning model.

Result: High annotator agreement (0.75-0.85 Fleiss' Kappa) and baseline model micro-F1-Score of 0.67 validate the dataset's quality.

Conclusion: The dataset is a valuable resource for advancing NLP in underrepresented languages and cross-linguistic emotion and hope speech analysis.

Abstract: This research introduces a bilingual dataset comprising 23,456 entries for
Arabic and 10,036 entries for English, annotated for emotions and hope speech,
addressing the scarcity of multi-emotion (Emotion and hope) datasets. The
dataset provides comprehensive annotations capturing emotion intensity,
complexity, and causes, alongside detailed classifications and subcategories
for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,
revealing 0.75-0.85 agreement among annotators both for Arabic and English
language. The evaluation metrics (micro-F1-Score=0.67) obtained from the
baseline model (i.e., using a machine learning model) validate that the data
annotations are worthy. This dataset offers a valuable resource for advancing
natural language processing in underrepresented languages, fostering better
cross-linguistic analysis of emotions and hope speech.

</details>


### [41] [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)
*Xu Liu,Guanyi Chen*

Main category: cs.CL

TL;DR: The CCNU team developed a system for the Mu-SHROOM task, using multiple LLMs to identify hallucinations in QA systems across 14 languages, achieving top results in Hindi and Top-5 in seven others.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying hallucinations in multilingual question-answering systems by leveraging diverse LLMs.

Method: Employed parallel LLMs with distinct expertise, integrating internal and external knowledge, using DeepSeek-V3 for annotation.

Result: Achieved #1 ranking for Hindi and Top-5 in seven other languages.

Conclusion: The approach effectively simulates crowdsourcing and highlights insights from unsuccessful methods.

Abstract: We present the system developed by the Central China Normal University (CCNU)
team for the Mu-SHROOM shared task, which focuses on identifying hallucinations
in question-answering systems across 14 different languages. Our approach
leverages multiple Large Language Models (LLMs) with distinct areas of
expertise, employing them in parallel to annotate hallucinations, effectively
simulating a crowdsourcing annotation process. Furthermore, each LLM-based
annotator integrates both internal and external knowledge related to the input
during the annotation process. Using the open-source LLM DeepSeek-V3, our
system achieves the top ranking (\#1) for Hindi data and secures a Top-5
position in seven other languages. In this paper, we also discuss unsuccessful
approaches explored during our development process and share key insights
gained from participating in this shared task.

</details>


### [42] [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
*Md. Rafiul Biswas,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: The paper introduces a multilabel Arabic hate speech dataset of 10,000 tweets, annotated for offensive content and hate speech targets, with high inter-annotator agreement. AraBERTv2 achieved the best performance in evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of identifying hate speech in Arabic due to dialectal variations.

Method: Collected and annotated 10,000 Arabic tweets for offensive content and hate speech targets, involving multiple annotators and measuring inter-annotator agreement. Evaluated using transformers-based models.

Result: Inter-annotator agreement was 0.86 for offensive content and 0.71 for hate speech targets. AraBERTv2 achieved a micro-F1 score of 0.7865 and accuracy of 0.786.

Conclusion: The dataset and AraBERTv2 model are effective for Arabic hate speech detection, despite dialectal challenges.

Abstract: Identifying hate speech content in the Arabic language is challenging due to
the rich quality of dialectal variations. This study introduces a multilabel
hate speech dataset in the Arabic language. We have collected 10000 Arabic
tweets and annotated each tweet, whether it contains offensive content or not.
If a text contains offensive content, we further classify it into different
hate speech targets such as religion, gender, politics, ethnicity, origin, and
others. A text can contain either single or multiple targets. Multiple
annotators are involved in the data annotation task. We calculated the
inter-annotator agreement, which was reported to be 0.86 for offensive content
and 0.71 for multiple hate speech targets. Finally, we evaluated the data
annotation task by employing a different transformers-based model in which
AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of
0.786.

</details>


### [43] [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Wayne Xin Zhao,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: The paper investigates how LLMs integrate internal and external knowledge in RAG, identifying four knowledge utilization stages and introducing KAPE for neuron analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the underexplored mechanisms of knowledge utilization in LLM-based RAG for improved interpretability and reliability.

Method: Macroscopic knowledge stream analysis and microscopic module-level investigation using KAPE for neuron identification.

Result: Identified four knowledge utilization stages and demonstrated how passage relevance guides knowledge flow. Achieved targeted shifts in knowledge reliance via neuron deactivation.

Conclusion: Provides insights for enhancing interpretability and reliability in RAG-based LLMs, supporting robust generative solutions.

Abstract: Considering the inherent limitations of parametric knowledge in large
language models (LLMs), retrieval-augmented generation (RAG) is widely employed
to expand their knowledge scope. Since RAG has shown promise in
knowledge-intensive tasks like open-domain question answering, its broader
application to complex tasks and intelligent assistants has further advanced
its utility. Despite this progress, the underlying knowledge utilization
mechanisms of LLM-based RAG remain underexplored. In this paper, we present a
systematic investigation of the intrinsic mechanisms by which LLMs integrate
internal (parametric) and external (retrieved) knowledge in RAG scenarios.
Specially, we employ knowledge stream analysis at the macroscopic level, and
investigate the function of individual modules at the microscopic level.
Drawing on knowledge streaming analyses, we decompose the knowledge utilization
process into four distinct stages within LLM layers: knowledge refinement,
knowledge elicitation, knowledge expression, and knowledge contestation. We
further demonstrate that the relevance of passages guides the streaming of
knowledge through these stages. At the module level, we introduce a new method,
knowledge activation probability entropy (KAPE) for neuron identification
associated with either internal or external knowledge. By selectively
deactivating these neurons, we achieve targeted shifts in the LLM's reliance on
one knowledge source over the other. Moreover, we discern complementary roles
for multi-head attention and multi-layer perceptron layers during knowledge
formation. These insights offer a foundation for improving interpretability and
reliability in retrieval-augmented LLMs, paving the way for more robust and
transparent generative solutions in knowledge-intensive domains.

</details>


### [44] [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)
*Yupei Ren,Xinyi Zhou,Ning Zhang,Shangqing Zhao,Man Lan,Xiaopeng Bai*

Main category: cs.CL

TL;DR: The paper proposes 14 fine-grained argument relation types to address the limitations of simplistic current models, enhancing the understanding of complex argument structures. It validates these through experiments on tasks like argument component detection and relation prediction, emphasizing their role in writing quality assessment.


<details>
  <summary>Details</summary>
Motivation: Current argument relations are too simplistic and fail to capture complex real-world argument structures, limiting their effectiveness.

Method: Proposes 14 fine-grained relation types (vertical and horizontal) and tests them on tasks like argument component detection, relation prediction, and automated essay grading. Also examines writing quality's impact.

Result: Findings show the importance of fine-grained annotations for argumentative writing quality and advocate for multi-dimensional analysis.

Conclusion: Fine-grained argument relations improve understanding of complex structures and are valuable for writing assessment, encouraging broader argument analysis.

Abstract: Argument mining has garnered increasing attention over the years, with the
recent advancement of Large Language Models (LLMs) further propelling this
trend. However, current argument relations remain relatively simplistic and
foundational, struggling to capture the full scope of argument information,
particularly when it comes to representing complex argument structures in
real-world scenarios. To address this limitation, we propose 14 fine-grained
relation types from both vertical and horizontal dimensions, thereby capturing
the intricate interplay between argument components for a thorough
understanding of argument structure. On this basis, we conducted extensive
experiments on three tasks: argument component detection, relation prediction,
and automated essay grading. Additionally, we explored the impact of writing
quality on argument component detection and relation prediction, as well as the
connections between discourse relations and argumentative features. The
findings highlight the importance of fine-grained argumentative annotations for
argumentative writing quality assessment and encourage multi-dimensional
argument analysis.

</details>


### [45] [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)
*Jingxue Chen,Qingkun Tang,Qianchun Lu,Siyuan Fang*

Main category: cs.CL

TL;DR: The paper proposes Mixture of Losses (MoL), a framework to balance domain-specific and general language training in LLMs, avoiding degradation in general skills while improving domain accuracy.


<details>
  <summary>Details</summary>
Motivation: Domain-specific LLMs face hallucinations and accuracy issues, and traditional CPT methods degrade general language skills due to biased data and improper corpus ratios.

Method: MoL uses cross-entropy loss for domain data and KL divergence for general corpus training, preserving foundational skills while enhancing domain expertise.

Result: A 1:1 domain-to-general corpus ratio works best. The model outperforms traditional CPT, with 27.9% higher accuracy on Math-500 and 83.3% improvement on AIME25.

Conclusion: MoL effectively balances domain and general training, avoiding catastrophic forgetting and significantly improving performance.

Abstract: Although LLMs perform well in general tasks, domain-specific applications
suffer from hallucinations and accuracy limitations. CPT approaches encounter
two key issues: (1) domain-biased data degrades general language skills, and
(2) improper corpus-mixture ratios limit effective adaptation. To address
these, we propose a novel framework, Mixture of Losses (MoL), which decouples
optimization objectives for domain-specific and general corpora. Specifically,
cross-entropy (CE) loss is applied to domain data to ensure knowledge
acquisition, while Kullback-Leibler (KL) divergence aligns general-corpus
training with the base model's foundational capabilities. This dual-loss
architecture preserves universal skills while enhancing domain expertise,
avoiding catastrophic forgetting. Empirically, we validate that a 1:1
domain-to-general corpus ratio optimally balances training and overfitting
without the need for extensive tuning or resource-intensive experiments.
Furthermore, our experiments demonstrate significant performance gains compared
to traditional CPT approaches, which often suffer from degradation in general
language capabilities; our model achieves 27.9% higher accuracy on the Math-500
benchmark in the non-think reasoning mode, and an impressive 83.3% improvement
on the challenging AIME25 subset in the think mode, underscoring the
effectiveness of our approach.

</details>


### [46] [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
*Vinod Raman,Hilal Asi,Satyen Kale*

Main category: cs.CL

TL;DR: A prompt-adaptive strategy for Best-of-N alignment improves computational efficiency by dynamically allocating inference-time compute based on alignment difficulty.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational expense of uniform Best-of-N sampling by accounting for differences in alignment difficulty across prompts.

Method: A two-stage algorithm: initial exploratory phase estimates reward distributions, followed by adaptive budget allocation.

Result: Outperforms uniform allocation with the same budget, remains competitive with larger budgets, and improves with batch size.

Conclusion: The adaptive strategy is efficient, practical, and compatible with any LM/RM combination.

Abstract: Recent advances in test-time alignment methods, such as Best-of-N sampling,
offer a simple and effective way to steer language models (LMs) toward
preferred behaviors using reward models (RM). However, these approaches can be
computationally expensive, especially when applied uniformly across prompts
without accounting for differences in alignment difficulty. In this work, we
propose a prompt-adaptive strategy for Best-of-N alignment that allocates
inference-time compute more efficiently. Motivated by latency concerns, we
develop a two-stage algorithm: an initial exploratory phase estimates the
reward distribution for each prompt using a small exploration budget, and a
second stage adaptively allocates the remaining budget using these estimates.
Our method is simple, practical, and compatible with any LM/RM combination.
Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different
batches of prompts show that our adaptive strategy consistently outperforms the
uniform allocation with the same inference budget. Moreover, our experiments
show that our adaptive strategy remains competitive against uniform allocations
with 20% larger inference budgets and even improves in performance as the batch
size grows.

</details>


### [47] [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)
*Matúš Pikuliak*

Main category: cs.CL

TL;DR: GenderBench is an open-source evaluation suite for measuring gender biases in LLMs, revealing consistent issues like stereotypical reasoning and discriminatory behavior.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive tools for evaluating gender biases in LLMs and improve benchmarking reproducibility.

Method: Developed GenderBench with 14 probes to quantify 19 gender-related harmful behaviors, evaluated 12 LLMs.

Result: LLMs exhibit issues like stereotypical reasoning, inequitable gender representation, and discriminatory behavior in high-stakes scenarios.

Conclusion: GenderBench provides a robust tool for identifying and mitigating gender biases in LLMs, highlighting the need for further improvements.

Abstract: We present GenderBench -- a comprehensive evaluation suite designed to
measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19
gender-related harmful behaviors exhibited by LLMs. We release GenderBench as
an open-source and extensible library to improve the reproducibility and
robustness of benchmarking across the field. We also publish our evaluation of
12 LLMs. Our measurements reveal consistent patterns in their behavior. We show
that LLMs struggle with stereotypical reasoning, equitable gender
representation in generated texts, and occasionally also with discriminatory
behavior in high-stakes scenarios, such as hiring.

</details>


### [48] [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)
*Peng Ding,Jun Kuang,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: SAGE is a training-free defense strategy for LLMs to enhance safety against jailbreak attacks by aligning detection and response capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs detect jailbreak prompts but often produce unsafe responses, revealing a safety gap.

Method: SAGE uses a Discriminative Analysis Module and a Discriminative Response Module to improve safety.

Result: SAGE achieves a 99% defense success rate against jailbreak attacks while maintaining general helpfulness.

Conclusion: SAGE contributes to coherent safety awareness in LLMs, with code and datasets publicly available.

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
various tasks but remain vulnerable to meticulously crafted jailbreak attacks.
In this paper, we identify a critical safety gap: while LLMs are adept at
detecting jailbreak prompts, they often produce unsafe responses when directly
processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware
Guard Enhancement), a training-free defense strategy designed to align LLMs'
strong safety discrimination performance with their relatively weaker safety
generation ability. SAGE consists of two core components: a Discriminative
Analysis Module and a Discriminative Response Module, enhancing resilience
against sophisticated jailbreak attempts through flexible safety discrimination
instructions. Extensive experiments demonstrate SAGE's effectiveness and
robustness across various open-source and closed-source LLMs of different sizes
and architectures, achieving an average 99% defense success rate against
numerous complex and covert jailbreak methods while maintaining helpfulness on
general benchmarks. We further conduct mechanistic interpretability analysis
through hidden states and attention distributions, revealing the underlying
mechanisms of this detection-generation discrepancy. Our work thus contributes
to developing future LLMs with coherent safety awareness and generation
behavior. Our code and datasets are publicly available at
https://github.com/NJUNLP/SAGE.

</details>


### [49] [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)
*Harald Baayen,Kristian Berg,Maziyah Mohamed*

Main category: cs.CL

TL;DR: The study explores morphological productivity using a cognitive-computational model (DLM) and a diachronic analysis of Thomas Mann's writing. It finds systematicities in form-meaning mappings and low novel word production in Mann's output.


<details>
  <summary>Details</summary>
Motivation: To understand morphological productivity by combining computational modeling (DLM) with a real-world case study (Thomas Mann's writing).

Method: Uses the Discriminative Lexicon Model (DLM) for computational analysis of Finnish, Malay, and English data, and examines Thomas Mann's reading and writing patterns.

Result: DLM links affix-like units to semantic centroids; Mann's novel word production is low compared to input.

Conclusion: Productivity depends on systematic form-meaning relations; speaker-specific embeddings pose challenges for novel words.

Abstract: In this study, we approach morphological productivity from two perspectives:
a cognitive-computational perspective, and a diachronic perspective zooming in
on an actual speaker, Thomas Mann. For developing the first perspective, we
make use of a cognitive computational model of the mental lexicon, the
discriminative lexicon model. For computational mappings between form and
meaning to be productive, in the sense that novel, previously unencountered
words, can be understood and produced, there must be systematicities between
the form space and the semantic space. If the relation between form and meaning
would be truly arbitrary, a model could memorize form and meaning pairings, but
there is no way in which the model would be able to generalize to novel test
data. For Finnish nominal inflection, Malay derivation, and English
compounding, we explore, using the Discriminative Lexicon Model as a
computational tool, to trace differences in the degree to which inflectional
and word formation patterns are productive. We show that the DLM tends to
associate affix-like sublexical units with the centroids of the embeddings of
the words with a given affix. For developing the second perspective, we study
how the intake and output of one prolific writer, Thomas Mann, changes over
time. We show by means of an examination of what Thomas Mann is likely to have
read, and what he wrote, that the rate at which Mann produces novel derived
words is extremely low. There are far more novel words in his input than in his
output. We show that Thomas Mann is less likely to produce a novel derived word
with a given suffix the greater the average distance is of the embeddings of
all derived words to the corresponding centroid, and discuss the challenges of
using speaker-specific embeddings for low-frequency and novel words.

</details>


### [50] [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)
*Guy Davidson,Todd M. Gureckis,Brenden M. Lake,Adina Williams*

Main category: cs.CL

TL;DR: The paper explores whether different prompting methods (demonstrations vs. instructions) in language models lead to similar task representations, using function vectors to analyze and compare their mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how task representations vary with different prompting methods, aiming to improve interpretability and control over model behavior.

Method: Generalizes function vectors to analyze instruction prompts, comparing them with demonstration-based vectors and dissecting their contributions to task performance.

Result: Demonstration- and instruction-based function vectors use different model components, suggesting distinct but overlapping task representations.

Conclusion: Combining instructions and demonstrations is beneficial, but monitoring task inference universally is challenging due to differing mechanisms.

Abstract: Demonstrations and instructions are two primary approaches for prompting
language models to perform in-context learning (ICL) tasks. Do identical tasks
elicited in different ways result in similar representations of the task? An
improved understanding of task representation mechanisms would offer
interpretability insights and may aid in steering models. We study this through
function vectors, recently proposed as a mechanism to extract few-shot ICL task
representations. We generalize function vectors to alternative task
presentations, focusing on short textual instruction prompts, and successfully
extract instruction function vectors that promote zero-shot task accuracy. We
find evidence that demonstration- and instruction-based function vectors
leverage different model components, and offer several controls to dissociate
their contributions to task performance. Our results suggest that different
task presentations do not induce a common task representation but elicit
different, partly overlapping mechanisms. Our findings offer principled support
to the practice of combining textual instructions and task demonstrations,
imply challenges in universally monitoring task inference across presentation
forms, and encourage further examinations of LLM task inference mechanisms.

</details>


### [51] [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
*Yunshui Li,Yiyuan Ma,Shen Yan,Chaoyi Zhang,Jing Liu,Jianqiao Lu,Ziwen Xu,Mengzhao Chen,Minrui Wang,Shiyi Zhan,Jin Ma,Xunhao Lai,Yao Luo,Xingyan Bin,Hongbin Ren,Mingji Han,Wenhao Hao,Bairen Yi,LingJun Liu,Bole Ma,Xiaoying Jia,Zhou Xun,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Model merging in large-scale pre-training improves performance and reduces costs, with insights on strategies and hyperparameters.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of model merging in enhancing large language models during pre-training, which remains understudied.

Method: Extensive experiments with dense and MoE architectures, merging checkpoints trained with constant learning rates.

Result: Significant performance improvements, accurate prediction of annealing behavior, and lower training costs.

Conclusion: Provides practical guidelines for effective model merging in pre-training, benefiting the open-source community.

Abstract: Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.

</details>


### [52] [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)
*Mohammad Shokri,Sarah Ita Levitan,Rivka Levitan*

Main category: cs.CL

TL;DR: The paper examines LLMs' ability to obscure authorship through paraphrasing and style alteration, finding variable effectiveness across authors. A personalized prompting method is proposed to improve results.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs perform in obfuscating authorship and address variability in effectiveness across individual authors.

Method: Focuses on user-wise performance analysis and introduces a personalized prompting technique.

Result: LLMs show a bimodal distribution of efficacy; personalized prompting outperforms standard methods.

Conclusion: Personalized prompting mitigates some variability in LLM performance for authorship obfuscation.

Abstract: In this paper, we investigate the efficacy of large language models (LLMs) in
obfuscating authorship by paraphrasing and altering writing styles. Rather than
adopting a holistic approach that evaluates performance across the entire
dataset, we focus on user-wise performance to analyze how obfuscation
effectiveness varies across individual authors. While LLMs are generally
effective, we observe a bimodal distribution of efficacy, with performance
varying significantly across users. To address this, we propose a personalized
prompting method that outperforms standard prompting techniques and partially
mitigates the bimodality issue.

</details>


### [53] [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)
*Isabela Pereira Gregio,Ian Pons,Anna Helena Reali Costa,Artur Jordão*

Main category: cs.CL

TL;DR: A method to mitigate bias in LLMs by evaluating prediction inconsistencies without training or fine-tuning, improving fairness metrics by up to 27 percentage points.


<details>
  <summary>Details</summary>
Motivation: Address the pervasive issue of bias in LLM responses, hindering ethical and responsible applications.

Method: Generates sentence variations by modifying attributes, comparing predictions to detect bias without training or prior data knowledge.

Result: Improves fairness in Llama3 by up to 27 percentage points, enhancing reliability without parameter tuning.

Conclusion: The method advances LLM use in ethically sensitive tasks by improving fairness and trustworthiness.

Abstract: Large Language Models (LLMs) push the bound-aries in natural language
processing and generative AI, driving progress across various aspects of modern
society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,
predictions) poses a significant and open challenge, hindering their
application in tasks involving ethical sensitivity and responsible
decision-making. In this work, we propose a straightforward, user-friendly and
practical method to mitigate such biases, enhancing the reliability and
trustworthiness of LLMs. Our method creates multiple variations of a given
sentence by modifying specific attributes and evaluates the corresponding
prediction behavior compared to the original, unaltered, prediction/sentence.
The idea behind this process is that critical ethical predictions often exhibit
notable inconsistencies, indicating the presence of bias. Unlike previous
approaches, our method relies solely on forward passes (i.e., testing-time
adversaries), eliminating the need for training, fine-tuning, or prior
knowledge of the training data distribution. Through extensive experiments on
the popular Llama family, we demonstrate the effectiveness of our method in
improving various fairness metrics, focusing on the reduction of disparities in
how the model treats individuals from different racial groups. Specifically,
using standard metrics, we improve the fairness in Llama3 in up to 27
percentage points. Overall, our approach significantly enhances fairness,
equity, and reliability in LLM-generated results without parameter tuning or
training data modifications, confirming its effectiveness in practical
scenarios. We believe our work establishes an important step toward enabling
the use of LLMs in tasks that require ethical considerations and responsible
decision-making.

</details>


### [54] [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)
*Fitsum Gaim,Hoyun Song,Huije Lee,Changgeon Ko,Eui Jun Hwang,Jong C. Park*

Main category: cs.CL

TL;DR: A large-scale, multi-task benchmark dataset for abusive language detection in Tigrinya social media is introduced, addressing resource gaps for low-resource languages. It includes annotations for abusiveness, sentiment, and topic classification, with strong baselines and outperforming models in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for abusive language detection in low-resource languages like Tigrinya, leaving vulnerable users exposed to online hostility.

Method: Developed a dataset of 13,717 YouTube comments annotated by native speakers, using an iterative term clustering approach for data selection. Accommodates both Romanized and Ge'ez scripts.

Result: Specialized multi-task models outperformed frontier models, achieving up to 86% accuracy (+7 points) in abusiveness detection.

Conclusion: The dataset and findings promote research on online safety for low-resource languages, with resources made publicly available.

Abstract: Content moderation research has recently made significant advances, but still
fails to serve the majority of the world's languages due to the lack of
resources, leaving millions of vulnerable users to online hostility. This work
presents a large-scale human-annotated multi-task benchmark dataset for abusive
language detection in Tigrinya social media with joint annotations for three
tasks: abusiveness, sentiment, and topic classification. The dataset comprises
13,717 YouTube comments annotated by nine native speakers, collected from 7,373
videos with a total of over 1.2 billion views across 51 channels. We developed
an iterative term clustering approach for effective data selection. Recognizing
that around 64% of Tigrinya social media content uses Romanized
transliterations rather than native Ge'ez script, our dataset accommodates both
writing systems to reflect actual language use. We establish strong baselines
across the tasks in the benchmark, while leaving significant challenges for
future contributions. Our experiments reveal that small, specialized multi-task
models outperform the current frontier models in the low-resource setting,
achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the
resources publicly available to promote research on online safety.

</details>


### [55] [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
*Elisa Bassignana,Amanda Cercas Curry,Dirk Hovy*

Main category: cs.CL

TL;DR: The study examines how socioeconomic status (SES) affects interactions with LLMs, revealing differences in usage, style, and topics between SES groups. Higher SES users employ abstract language and concise requests, while lower SES users anthropomorphize LLMs and use concrete language.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding how SES influences interactions with language technologies, moving beyond proxy metrics and synthetic data.

Method: Surveyed 1,000 individuals from diverse SES backgrounds and analyzed 6,482 prompts from their LLM interactions.

Result: Systematic SES-based differences in language technology usage, interaction styles, and topics were found, highlighting a digital divide.

Conclusion: SES must be considered in language technology development to mitigate linguistic disparities and reduce the AI Gap across socioeconomic groups.

Abstract: Socioeconomic status (SES) fundamentally influences how people interact with
each other and more recently, with digital technologies like Large Language
Models (LLMs). While previous research has highlighted the interaction between
SES and language technology, it was limited by reliance on proxy metrics and
synthetic data. We survey 1,000 individuals from diverse socioeconomic
backgrounds about their use of language technologies and generative AI, and
collect 6,482 prompts from their previous interactions with LLMs. We find
systematic differences across SES groups in language technology usage (i.e.,
frequency, performed tasks), interaction styles, and topics. Higher SES entails
a higher level of abstraction, convey requests more concisely, and topics like
'inclusivity' and 'travel'. Lower SES correlates with higher
anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more
concrete language. Our findings suggest that while generative language
technologies are becoming more accessible to everyone, socioeconomic linguistic
differences still stratify their use to exacerbate the digital divide. These
differences underscore the importance of considering SES in developing language
technologies to accommodate varying linguistic needs rooted in socioeconomic
factors and limit the AI Gap across SES groups.

</details>


### [56] [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)
*Darmawan Wicaksono,Hasri Akbar Awal Rozaq,Nevfel Boz*

Main category: cs.CL

TL;DR: The study analyzes anti-refugee sentiment in Turkish social media using a BERTurk-based Emotion Recognition Model (ERM), achieving 92.62% accuracy. It highlights the importance of localized NLP tools for Turkish discourse.


<details>
  <summary>Details</summary>
Motivation: To understand emotional nuances in Turkish social media discourse, especially anti-refugee sentiment, and advance sentiment analysis for underrepresented languages.

Method: Developed an ERM using BERTurk and the TREMO dataset to categorize emotions (happiness, fear, anger, sadness, disgust, surprise) in Turkish X (Twitter) data.

Result: Achieved 92.62% accuracy in emotion categorization, uncovering emotional trends in Turkish discourse.

Conclusion: Localized NLP tools like the ERM model are transformative for Turkish sentiment analysis, with applications in marketing, PR, and crisis management, emphasizing the need for region-specific research.

Abstract: Social media platforms like X (formerly Twitter) play a crucial role in
shaping public discourse and societal norms. This study examines the term
Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise
of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and
the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)
tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such
as happiness, fear, anger, sadness, disgust, and surprise. By applying this
model to large-scale X data, the study uncovers emotional nuances in Turkish
discourse, contributing to computational social science by advancing sentiment
analysis in underrepresented languages and enhancing our understanding of
global digital discourse and the unique linguistic challenges of Turkish. The
findings underscore the transformative potential of localized NLP tools, with
our ERM model offering practical applications for real-time sentiment analysis
in Turkish-language contexts. By addressing critical areas, including
marketing, public relations, and crisis management, these models facilitate
improved decision-making through timely and accurate sentiment tracking. This
highlights the significance of advancing research that accounts for regional
and linguistic nuances.

</details>


### [57] [Truth Neurons](https://arxiv.org/abs/2505.12182)
*Haohang Li,Yupeng Cao,Yangyang Yu,Jordan W. Suchow,Zining Zhu*

Main category: cs.CL

TL;DR: The paper identifies 'truth neurons' in language models that encode truthfulness independently of subject matter, validating their existence across models and showing their importance for model reliability.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and reliability of language models by mechanistically identifying how truthfulness is encoded at the neuron level.

Method: Proposes a method to identify truth neurons, validates their existence across models, and tests their impact by suppressing activations.

Result: Truth neurons exist across models, and suppressing them degrades performance on truthfulness benchmarks, indicating their general importance.

Conclusion: The findings provide insights into truthfulness mechanisms in language models and suggest ways to enhance their trustworthiness.

Abstract: Despite their remarkable success and deployment across diverse workflows,
language models sometimes produce untruthful responses. Our limited
understanding of how truthfulness is mechanistically encoded within these
models jeopardizes their reliability and safety. In this paper, we propose a
method for identifying representations of truthfulness at the neuron level. We
show that language models contain truth neurons, which encode truthfulness in a
subject-agnostic manner. Experiments conducted across models of varying scales
validate the existence of truth neurons, confirming that the encoding of
truthfulness at the neuron level is a property shared by many language models.
The distribution patterns of truth neurons over layers align with prior
findings on the geometry of truthfulness. Selectively suppressing the
activations of truth neurons found through the TruthfulQA dataset degrades
performance both on TruthfulQA and on other benchmarks, showing that the
truthfulness mechanisms are not tied to a specific dataset. Our results offer
novel insights into the mechanisms underlying truthfulness in language models
and highlight potential directions toward improving their trustworthiness and
reliability.

</details>


### [58] [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)
*Manari Hirose,Masato Uchida*

Main category: cs.CL

TL;DR: A framework evaluates LLMs for ideological biases using 436 binary-choice questions, revealing inconsistencies and problematic biases in ChatGPT and Gemini, emphasizing the need for ethical considerations in AI development.


<details>
  <summary>Details</summary>
Motivation: To understand biases and societal implications of LLMs for ethical and effective use.

Method: Quantitative analysis of 436 binary-choice questions with no definitive answers, applied to ChatGPT and Gemini.

Result: LLMs show ideological differences across models and languages; ChatGPT adapts opinions to questioners, and both models exhibit unethical biases.

Conclusion: Addressing ideological and ethical biases is crucial for socially aligned AI; the framework provides a flexible evaluation method.

Abstract: The widespread integration of Large Language Models (LLMs) across various
sectors has highlighted the need for empirical research to understand their
biases, thought patterns, and societal implications to ensure ethical and
effective use. In this study, we propose a novel framework for evaluating LLMs,
focusing on uncovering their ideological biases through a quantitative analysis
of 436 binary-choice questions, many of which have no definitive answer. By
applying our framework to ChatGPT and Gemini, findings revealed that while LLMs
generally maintain consistent opinions on many topics, their ideologies differ
across models and languages. Notably, ChatGPT exhibits a tendency to change
their opinion to match the questioner's opinion. Both models also exhibited
problematic biases, unethical or unfair claims, which might have negative
societal impacts. These results underscore the importance of addressing both
ideological and ethical considerations when evaluating LLMs. The proposed
framework offers a flexible, quantitative method for assessing LLM behavior,
providing valuable insights for the development of more socially aligned AI
systems.

</details>


### [59] [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)
*Yi-Chien Lin,Hongao Zhu,William Schuler*

Main category: cs.CL

TL;DR: The study examines whether larger language models (LLMs) better predict human sentence processing, finding inverse scaling beyond a certain size, suggesting misalignment with human cognition.


<details>
  <summary>Details</summary>
Motivation: To test the 'quality-power' relationship in LLMs as models of human sentence processing and address conflicting findings about scaling effects.

Method: Evaluates LLM scaling using entire vectors while controlling for the number of predictors in larger models.

Result: Inverse scaling occurs, indicating larger LLMs may worsen alignment with human sentence processing.

Conclusion: LLM inadequacies in predicting human data may stem from misalignment, not insufficient model size.

Abstract: The impressive linguistic abilities of large language models (LLMs) have
recommended them as models of human sentence processing, with some conjecturing
a positive 'quality-power' relationship (Wilcox et al., 2023), in which
language models' (LMs') fit to psychometric data continues to improve as their
ability to predict words in context increases. This is important because it
suggests that elements of LLM architecture, such as veridical attention to
context and a unique objective of predicting upcoming words, reflect the
architecture of the human sentence processing faculty, and that any
inadequacies in predicting human reading time and brain imaging data may be
attributed to insufficient model complexity, which recedes as larger models
become available. Recent studies (Oh and Schuler, 2023) have shown this scaling
inverts after a point, as LMs become excessively large and accurate, when word
prediction probability (as information-theoretic surprisal) is used as a
predictor. Other studies propose the use of entire vectors from differently
sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting
doubt on the value of surprisal as a predictor, but do not control for the
larger number of predictors in vectors from larger LMs. This study evaluates
LLM scaling using entire LLM vectors, while controlling for the larger number
of predictors in vectors from larger LLMs. Results show that inverse scaling
obtains, suggesting that inadequacies in predicting human reading time and
brain imaging data may be due to substantial misalignment between LLMs and
human sentence processing, which worsens as larger models are used.

</details>


### [60] [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)
*Xiyan Fu,Wei Liu*

Main category: cs.CL

TL;DR: LLM-as-a-Judge is unreliable for multilingual evaluation, showing inconsistency across languages, especially low-resource ones. Training or scaling models doesn't help, but an ensemble strategy improves consistency.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLM-as-a-Judge in multilingual evaluation, given its popularity as an alternative to human annotators.

Method: Evaluated five models across five tasks in 25 languages, analyzing factors like language diversity and model scale.

Result: LLMs show inconsistent judgments (average Fleiss' Kappa ~0.3), with poor performance in low-resource languages. Training or scaling doesn't improve consistency.

Conclusion: LLMs are unreliable for multilingual evaluation; an ensemble strategy offers a practical improvement.

Abstract: LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced
large language models assess generation results in alignment with human
instructions. While these models serve as a promising alternative to human
annotators, their reliability in multilingual evaluation remains uncertain. To
bridge this gap, we conduct a comprehensive analysis of multilingual
LLM-as-a-Judge. Specifically, we evaluate five models from different model
families across five diverse tasks involving 25 languages. Our findings reveal
that LLMs struggle to achieve consistent judgment results across languages,
with an average Fleiss' Kappa of approximately 0.3, and some models performing
even worse. To investigate the cause of inconsistency, we analyze various
influencing factors. We observe that consistency varies significantly across
languages, with particularly poor performance in low-resource languages.
Additionally, we find that neither training on multilingual data nor increasing
model scale directly improves judgment consistency. These findings suggest that
LLMs are not yet reliable for evaluating multilingual predictions. We finally
propose an ensemble strategy which improves the consistency of the multilingual
judge in real-world applications.

</details>


### [61] [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
*Shaobo Wang,Ziming Wang,Xiangqi Jin,Jize Wang,Jiajun Zhang,Kaixin Li,Zichen Wen,Zhong Li,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CL

TL;DR: Data Whisperer is a training-free, attention-based method for selecting optimal subsets of task-specific data for fine-tuning LLMs, outperforming traditional methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Efficiently selecting subsets of growing datasets for fine-tuning LLMs is crucial to balance performance and computational costs, but existing methods are either resource-intensive or heuristic-based.

Method: Data Whisperer uses few-shot in-context learning with the target model to select data subsets without additional training, leveraging attention mechanisms.

Result: Outperforms full dataset training (e.g., 10% of GSM8K data on Llama-3-8B-Instruct) and existing methods with a 3.1-point improvement and 7.4× speedup.

Conclusion: Data Whisperer offers a scalable, efficient solution for data selection in LLM fine-tuning, reducing computational overhead while maintaining performance.

Abstract: Fine-tuning large language models (LLMs) on task-specific data is essential
for their effective deployment. As dataset sizes grow, efficiently selecting
optimal subsets for training becomes crucial to balancing performance and
computational costs. Traditional data selection methods often require
fine-tuning a scoring model on the target dataset, which is time-consuming and
resource-intensive, or rely on heuristics that fail to fully leverage the
model's predictive capabilities. To address these challenges, we propose Data
Whisperer, an efficient, training-free, attention-based method that leverages
few-shot in-context learning with the model to be fine-tuned. Comprehensive
evaluations were conducted on both raw and synthetic datasets across diverse
tasks and models. Notably, Data Whisperer achieves superior performance
compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just
10% of the data, and outperforms existing methods with a 3.1-point improvement
and a 7.4$\times$ speedup.

</details>


### [62] [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)
*Jiwei Tang,Zhicheng Zhang,Shunlong Wu,Jingheng Ye,Lichen Bai,Zitai Wang,Tingwei Lu,Jiaqi Chen,Lin Hai,Hai-Tao Zheng,Hong-Gee Kim*

Main category: cs.CL

TL;DR: GMSA is a context compression framework for LLMs, improving efficiency and reducing redundancy in long-context scenarios. It uses Group Merging and Layer Semantic Alignment, achieving faster inference and better performance in QA tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with low efficiency and redundant information in long-context scenarios. GMSA aims to address these challenges by compressing context and aligning semantics.

Method: GMSA employs Group Merging to extract summary vectors and Layer Semantic Alignment to bridge semantic gaps. It uses autoencoder training and Knowledge Extraction Fine-tuning (KEFT) for downstream tasks.

Result: GMSA outperforms traditional methods in context restoration, achieves faster convergence, and provides a 2x speedup in QA tasks while surpassing SOTA methods.

Conclusion: GMSA effectively improves LLM performance in long-context scenarios by compressing input and aligning semantics, demonstrating superior efficiency and accuracy.

Abstract: Large language models (LLMs) have achieved impressive performance in a
variety of natural language processing (NLP) tasks. However, when applied to
long-context scenarios, they face two challenges, i.e., low computational
efficiency and much redundant information. This paper introduces GMSA, a
context compression framework based on the encoder-decoder architecture, which
addresses these challenges by reducing input sequence length and redundant
information. Structurally, GMSA has two key components: Group Merging and Layer
Semantic Alignment (LSA). Group merging is used to effectively and efficiently
extract summary vectors from the original context. Layer semantic alignment, on
the other hand, aligns the high-level summary vectors with the low-level
primary input semantics, thus bridging the semantic gap between different
layers. In the training process, GMSA first learns soft tokens that contain
complete semantics through autoencoder training. To furtherly adapt GMSA to
downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract
knowledge from the soft tokens for downstream tasks. We train GMSA by randomly
sampling the compression rate for each sample in the dataset. Under this
condition, GMSA not only significantly outperforms the traditional compression
paradigm in context restoration but also achieves stable and significantly
faster convergence with only a few encoder layers. In downstream
question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in
end-to-end inference while outperforming both the original input prompts and
various state-of-the-art (SOTA) methods by a large margin.

</details>


### [63] [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
*Rongguang Ye,Ming Tang*

Main category: cs.CL

TL;DR: UniCuCo introduces a universal model for efficient pruning of LLMs, using StratNet and Gaussian processes to handle multiple requests faster while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for LLMs are inefficient for multiple simultaneous requests due to linear processing time growth.

Method: Proposes UniCuCo with StratNet, leveraging Gaussian processes to approximate pruning strategy evaluation and enable gradient-based updates.

Result: UniCuCo is 28 times faster than baselines for 64 requests with comparable accuracy.

Conclusion: UniCuCo addresses inefficiency in multi-request pruning, offering a scalable solution for real-world LLM compression.

Abstract: Existing pruning methods for large language models (LLMs) focus on achieving
high compression rates while maintaining model performance. Although these
methods have demonstrated satisfactory performance in handling a single user's
compression request, their processing time increases linearly with the number
of requests, making them inefficient for real-world scenarios with multiple
simultaneous requests. To address this limitation, we propose a Univeral Model
for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that
learns to map arbitrary requests to their optimal pruning strategy. The
challenge in training StratNet lies in the high computational cost of
evaluating pruning strategies and the non-differentiable nature of the pruning
process, which hinders gradient backpropagation for StratNet updates. To
overcome these challenges, we leverage a Gaussian process to approximate the
evaluation process. Since the gradient of the Gaussian process is computable,
we can use it to approximate the gradient of the non-differentiable pruning
process, thereby enabling StratNet updates. Experimental results show that
UniCuCo is 28 times faster than baselines in processing 64 requests, while
maintaining comparable accuracy to baselines.

</details>


### [64] [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)
*Tong Bao,Yi Zhao,Jin Mao,Chengzhi Zhang*

Main category: cs.CL

TL;DR: The study analyzes the impact of LLMs (e.g., ChatGPT) on academic writing, revealing increased LLM-preferred words, lexical complexity, and sentiment, but decreased syntactic complexity, cohesion, and readability. Scholars with weaker English proficiency used LLMs more, and Computer Science showed the most style changes.


<details>
  <summary>Details</summary>
Motivation: To systematically examine the linguistic impact of LLMs on academic writing, addressing a gap in existing quantitative-focused research.

Method: A large-scale linguistic analysis of 823,798 arXiv abstracts over a decade, focusing on features like word frequency, lexical/syntactic complexity, cohesion, readability, and sentiment.

Result: LLMs increased LLM-preferred words, lexical complexity, and sentiment, but reduced syntactic complexity, cohesion, and readability. Scholars with weaker English proficiency relied more on LLMs, and Computer Science exhibited the most style changes.

Conclusion: LLMs significantly influence academic writing, simplifying sentence structure but reducing readability. Discipline and English proficiency affect adoption and impact.

Abstract: Large Language Models (LLMs), such as ChatGPT, have prompted academic
concerns about their impact on academic writing. Existing studies have
primarily examined LLM usage in academic writing through quantitative
approaches, such as word frequency statistics and probability-based analyses.
However, few have systematically examined the potential impact of LLMs on the
linguistic characteristics of academic writing. To address this gap, we
conducted a large-scale analysis across 823,798 abstracts published in last
decade from arXiv dataset. Through the linguistic analysis of features such as
the frequency of LLM-preferred words, lexical complexity, syntactic complexity,
cohesion, readability and sentiment, the results indicate a significant
increase in the proportion of LLM-preferred words in abstracts, revealing the
widespread influence of LLMs on academic writing. Additionally, we observed an
increase in lexical complexity and sentiment in the abstracts, but a decrease
in syntactic complexity, suggesting that LLMs introduce more new vocabulary and
simplify sentence structure. However, the significant decrease in cohesion and
readability indicates that abstracts have fewer connecting words and are
becoming more difficult to read. Moreover, our analysis reveals that scholars
with weaker English proficiency were more likely to use the LLMs for academic
writing, and focused on improving the overall logic and fluency of the
abstracts. Finally, at discipline level, we found that scholars in Computer
Science showed more pronounced changes in writing style, while the changes in
Mathematics were minimal.

</details>


### [65] [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)
*Quanjiang Guo,Jinchuan Zhang,Sijie Wang,Ling Tian,Zhao Kang,Bin Yan,Weidong Xiao*

Main category: cs.CL

TL;DR: TKRE is a novel framework combining LLMs and traditional models for Few-Shot Relation Extraction, using explanation-driven knowledge and a two-stage pre-training strategy to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and limited generalization in FSRE by integrating LLMs with traditional models.

Method: TKRE uses LLMs to generate synthetic data and employs a two-stage pre-training strategy (MSLM and SCL) for enhanced relational reasoning.

Result: TKRE achieves state-of-the-art performance on benchmark datasets for FSRE.

Conclusion: TKRE effectively tackles FSRE challenges and shows promise for low-resource scenarios.

Abstract: Few-Shot Relation Extraction (FSRE) remains a challenging task due to the
scarcity of annotated data and the limited generalization capabilities of
existing models. Although large language models (LLMs) have demonstrated
potential in FSRE through in-context learning (ICL), their general-purpose
training objectives often result in suboptimal performance for task-specific
relation extraction. To overcome these challenges, we propose TKRE (Two-Stage
Knowledge-Guided Pre-training for Relation Extraction), a novel framework that
synergistically integrates LLMs with traditional relation extraction models,
bridging generative and discriminative learning paradigms. TKRE introduces two
key innovations: (1) leveraging LLMs to generate explanation-driven knowledge
and schema-constrained synthetic data, addressing the issue of data scarcity;
and (2) a two-stage pre-training strategy combining Masked Span Language
Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational
reasoning and generalization. Together, these components enable TKRE to
effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets
demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in
FSRE and underscoring its potential for broader application in low-resource
scenarios. \footnote{The code and data are released on
https://github.com/UESTC-GQJ/TKRE.

</details>


### [66] [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)
*Sriram Selvam,Anneswa Ghosh*

Main category: cs.CL

TL;DR: PANORAMA is a synthetic dataset designed to study PII memorization in LLMs, offering diverse, realistic PII samples for privacy risk assessment.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of comprehensive datasets to study PII memorization in LLMs, which is critical for privacy risk mitigation.

Method: Created 384,789 synthetic samples from 9,674 profiles using constrained selection and AI-generated content to mimic real-world PII distribution.

Result: Fine-tuning Mistral-7B showed increased PII memorization with data repetition, varying by content type.

Conclusion: PANORAMA provides a valuable resource for privacy risk assessment and developing privacy-preserving LLMs.

Abstract: The memorization of sensitive and personally identifiable information (PII)
by large language models (LLMs) poses growing privacy risks as models scale and
are increasingly deployed in real-world applications. Existing efforts to study
sensitive and PII data memorization and develop mitigation strategies are
hampered by the absence of comprehensive, realistic, and ethically sourced
datasets reflecting the diversity of sensitive information found on the web. We
introduce PANORAMA - Profile-based Assemblage for Naturalistic Online
Representation and Attribute Memorization Analysis, a large-scale synthetic
corpus of 384,789 samples derived from 9,674 synthetic profiles designed to
closely emulate the distribution, variety, and context of PII and sensitive
data as it naturally occurs in online environments. Our data generation
pipeline begins with the construction of internally consistent, multi-attribute
human profiles using constrained selection to reflect real-world demographics
such as education, health attributes, financial status, etc. Using a
combination of zero-shot prompting and OpenAI o3-mini, we generate diverse
content types - including wiki-style articles, social media posts, forum
discussions, online reviews, comments, and marketplace listings - each
embedding realistic, contextually appropriate PII and other sensitive
information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B
model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and
measure PII memorization rates - revealing not only consistent increases with
repetition but also variation across content types, highlighting PANORAMA's
ability to model how memorization risks differ by context. Our dataset and code
are publicly available, providing a much-needed resource for privacy risk
assessment, model auditing, and the development of privacy-preserving LLMs.

</details>


### [67] [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
*Haojin Wang,Zining Zhu,Freda Shi*

Main category: cs.CL

TL;DR: The paper investigates the expressiveness of autoregressive LMs by analyzing their ability to approximate target token distributions, finding that entropy and outlier tokens influence approximation difficulty.


<details>
  <summary>Details</summary>
Motivation: To systematically understand the probability distributions LMs can produce and identify which distributions are harder to elicit.

Method: Uses gradient-based prompt tuning (soft or hard) to find prompts that induce LMs to approximate target distributions.

Result: Low/high-entropy distributions and those with outlier tokens are easier to approximate; LM-generated targets are also easier than random ones.

Conclusion: The findings reveal insights into LM expressiveness and challenges in using them as probability distribution proposers.

Abstract: Autoregressive neural language models (LMs) generate a probability
distribution over tokens at each time step given a prompt. In this work, we
attempt to systematically understand the probability distributions that LMs can
produce, showing that some distributions are significantly harder to elicit
than others. Specifically, for any target next-token distribution over the
vocabulary, we attempt to find a prompt that induces the LM to output a
distribution as close as possible to the target, using either soft or hard
gradient-based prompt tuning. We find that (1) in general, distributions with
very low or very high entropy are easier to approximate than those with
moderate entropy; (2) among distributions with the same entropy, those
containing ''outlier tokens'' are easier to approximate; (3) target
distributions generated by LMs -- even LMs with different tokenizers -- are
easier to approximate than randomly chosen targets. These results offer
insights into the expressiveness of LMs and the challenges of using them as
probability distribution proposers.

</details>


### [68] [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)
*Chi Zhang,Huaping Zhong,Hongtao Li,Chengliang Chai,Jiawei Hong,Yuhao Deng,Jiacheng Wang,Tian Tan,Yizhou Yan,Jiantao Qiu,Ye Yuan,Guoren Wang,Conghui He,Lei Cao*

Main category: cs.CL

TL;DR: EQUAL is a framework for extracting diverse instruction tuning data from web corpora, reducing costs and improving model performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for synthesizing instruction data lack diversity and are costly, limiting real-world applicability.

Method: EQUAL uses document clustering and a multi-armed bandit strategy to iteratively select and extract high-quality QA pairs.

Result: EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5% on tested models.

Conclusion: EQUAL effectively addresses the challenges of cost and quality in instruction tuning data extraction.

Abstract: Instruction tuning improves the performance of large language models (LLMs),
but it heavily relies on high-quality training data. Recently, LLMs have been
used to synthesize instruction data using seed question-answer (QA) pairs.
However, these synthesized instructions often lack diversity and tend to be
similar to the input seeds, limiting their applicability in real-world
scenarios. To address this, we propose extracting instruction tuning data from
web corpora that contain rich and diverse knowledge. A naive solution is to
retrieve domain-specific documents and extract all QA pairs from them, but this
faces two key challenges: (1) extracting all QA pairs using LLMs is
prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to
the downstream tasks, potentially degrading model performance. To tackle these
issues, we introduce EQUAL, an effective and scalable data extraction framework
that iteratively alternates between document selection and high-quality QA pair
extraction to enhance instruction tuning. EQUAL first clusters the document
corpus based on embeddings derived from contrastive learning, then uses a
multi-armed bandit strategy to efficiently identify clusters that are likely to
contain valuable QA pairs. This iterative approach significantly reduces
computational cost while boosting model performance. Experiments on
AutoMathText and StackOverflow across four downstream tasks show that EQUAL
reduces computational costs by 5-10x and improves accuracy by 2.5 percent on
LLaMA-3.1-8B and Mistral-7B

</details>


### [69] [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)
*Yuhang Zhou,Xutian Chen,Yixin Cao,Yuchen Ni,Yu He,Siyu Tian,Xiang Liu,Jian Zhang,Chuanjun Ji,Guangnan Ye,Xipeng Qiu*

Main category: cs.CL

TL;DR: Teach2Eval is an indirect evaluation framework for LLMs that assesses their teaching abilities to weaker models, avoiding traditional benchmark pitfalls.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarks for LLMs face fairness, scalability, and contamination issues, necessitating a more robust evaluation method.

Method: Teach2Eval evaluates LLMs by their ability to teach weaker models, converting tasks into MCQs via teacher-generated feedback for scalable, automated assessment.

Result: Tests on 26 LLMs show alignment with human and model-based rankings, providing interpretability for training.

Conclusion: Teach2Eval offers a scalable, multi-dimensional, and interpretable alternative to traditional LLM evaluation methods.

Abstract: Recent progress in large language models (LLMs) has outpaced the development
of effective evaluation methods. Traditional benchmarks rely on task-specific
metrics and static datasets, which often suffer from fairness issues, limited
scalability, and contamination risks. In this paper, we introduce Teach2Eval,
an indirect evaluation framework inspired by the Feynman Technique. Instead of
directly testing LLMs on predefined tasks, our method evaluates a model's
multiple abilities to teach weaker student models to perform tasks effectively.
By converting open-ended tasks into standardized multiple-choice questions
(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,
automated, and multi-dimensional assessment. Our approach not only avoids data
leakage and memorization but also captures a broad range of cognitive abilities
that are orthogonal to current benchmarks. Experimental results across 26
leading LLMs show strong alignment with existing human and model-based dynamic
rankings, while offering additional interpretability for training guidance.

</details>


### [70] [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)
*Chengwei Qin,Wenxuan Zhou,Karthik Abinav Sankararaman,Nanshu Wang,Tengyu Xu,Alexander Radovic,Eryk Helenowski,Arya Talebzadeh,Aditya Tayade,Sinong Wang,Shafiq Joty,Han Fang,Hao Ma*

Main category: cs.CL

TL;DR: The paper addresses hallucination in LLMs, proposing RATE-FT, a fine-tuning method with an auxiliary task, improving detection accuracy by 3% over general fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs is a major issue, especially in open-domain long-form tasks, and existing detection methods are limited or rely on external tools.

Method: The study explores internal states, prompting, probing, and fine-tuning, with RATE-FT (fine-tuning with an auxiliary task) introduced as the most effective approach.

Result: RATE-FT outperforms general fine-tuning by 3% on LongFact, demonstrating effectiveness and generalizability across models and datasets.

Conclusion: RATE-FT enhances hallucination detection in LLMs, offering a reliable reference-free solution for open-domain long-form tasks.

Abstract: Hallucination, the generation of factually incorrect information, remains a
significant challenge for large language models (LLMs), especially in
open-domain long-form generation. Existing approaches for detecting
hallucination in long-form tasks either focus on limited domains or rely
heavily on external fact-checking tools, which may not always be available.
  In this work, we systematically investigate reference-free hallucination
detection in open-domain long-form responses. Our findings reveal that internal
states (e.g., model's output probability and entropy) alone are insufficient
for reliably (i.e., better than random guessing) distinguishing between factual
and hallucinated content. To enhance detection, we explore various existing
approaches, including prompting-based methods, probing, and fine-tuning, with
fine-tuning proving the most effective. To further improve the accuracy, we
introduce a new paradigm, named RATE-FT, that augments fine-tuning with an
auxiliary task for the model to jointly learn with the main task of
hallucination detection. With extensive experiments and analysis using a
variety of model families & datasets, we demonstrate the effectiveness and
generalizability of our method, e.g., +3% over general fine-tuning methods on
LongFact.

</details>


### [71] [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
*Pratim Chowdhary*

Main category: cs.CL

TL;DR: The paper introduces $K$-MSHC and Search-K-MSHC to identify minimal attention head sets in mid-sized language models, revealing task-specific circuits and non-linear overlap patterns.


<details>
  <summary>Details</summary>
Motivation: To understand which neural components drive specific capabilities in mid-sized language models (≤10B parameters).

Method: Introduces $K$-MSHC and Search-K-MSHC to identify minimal attention head sets for classification tasks, applied to Gemma-9B on syntactic tasks.

Result: Task-specific head circuits were found: grammar uses early layers, word problems use shallow and deep regions, arithmetic verification is distributed. Non-linear overlap patterns and dedicated 'super-heads' were discovered.

Conclusion: Syntactic and numerical competencies arise from specialized yet partially reusable head circuits, with minimal cross-task overlap for critical heads.

Abstract: Understanding which neural components drive specific capabilities in
mid-sized language models ($\leq$10B parameters) remains a key challenge. We
introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),
a methodology to identify minimal sets of attention heads crucial for
classification tasks as well as Search-K-MSHC, an efficient algorithm for
discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,
we analyze three syntactic task families: grammar acceptability, arithmetic
verification, and arithmetic word problems. Our findings reveal distinct
task-specific head circuits, with grammar tasks predominantly utilizing early
layers, word problems showing pronounced activity in both shallow and deep
regions, and arithmetic verification demonstrating a more distributed pattern
across the network. We discover non-linear circuit overlap patterns, where
different task pairs share computational components at varying levels of
importance. While grammar and arithmetic share many "weak" heads, arithmetic
and word problems share more consistently critical "strong" heads. Importantly,
we find that each task maintains dedicated "super-heads" with minimal
cross-task overlap, suggesting that syntactic and numerical competencies emerge
from specialized yet partially reusable head circuits.

</details>


### [72] [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)
*Md. Atiqur Rahman,Sabrina Islam,Mushfiqul Haque Omi*

Main category: cs.CL

TL;DR: A framework for enhancing LLM-based MT evaluation for low-resource languages using dialect-guided methods, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating MT for low-resource languages with multiple dialects due to limited reference translations and dialect-specific context.

Method: Extends the ONUBAD dataset with Sylheti-English pairs, augments tokenizer vocabulary, introduces a regression head, and designs dialect-guided prompting.

Result: Outperforms existing methods with a +0.1083 gain in Spearman correlation and improvements in other metrics.

Conclusion: The proposed dialect-guided framework effectively enhances MT evaluation for low-resource languages, with publicly available dataset and code.

Abstract: Evaluating machine translation (MT) for low-resource languages poses a
persistent challenge, primarily due to the limited availability of high quality
reference translations. This issue is further exacerbated in languages with
multiple dialects, where linguistic diversity and data scarcity hinder robust
evaluation. Large Language Models (LLMs) present a promising solution through
reference-free evaluation techniques; however, their effectiveness diminishes
in the absence of dialect-specific context and tailored guidance. In this work,
we propose a comprehensive framework that enhances LLM-based MT evaluation
using a dialect guided approach. We extend the ONUBAD dataset by incorporating
Sylheti-English sentence pairs, corresponding machine translations, and Direct
Assessment (DA) scores annotated by native speakers. To address the vocabulary
gap, we augment the tokenizer vocabulary with dialect-specific terms. We
further introduce a regression head to enable scalar score prediction and
design a dialect-guided (DG) prompting strategy. Our evaluation across multiple
LLMs shows that the proposed pipeline consistently outperforms existing
methods, achieving the highest gain of +0.1083 in Spearman correlation, along
with improvements across other evaluation settings. The dataset and the code
are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.

</details>


### [73] [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)
*Linghan Huang,Haolin Jin,Zhaoge Bi,Pengyue Yang,Peizhou Zhao,Taozhao Chen,Xiongfei Wu,Lei Ma,Huaming Chen*

Main category: cs.CL

TL;DR: The paper evaluates closed-source LLMs' vulnerability to multilingual adversarial attacks, finding Qwen-Max most vulnerable and GPT-4o most resilient. Chinese prompts yield higher attack success rates, and a novel Two-Sides attack technique is most effective.


<details>
  <summary>Details</summary>
Motivation: To assess the security of closed-source LLMs under multilingual adversarial attacks, addressing gaps in existing research focused on open-source models.

Method: An integrated adversarial framework tests 32 jailbreak attack types on models like GPT-4o and Qwen-Max, using attack success rate (ASR) across prompt design, model architecture, and language environment.

Result: Qwen-Max is most vulnerable; GPT-4o is most resilient. Chinese prompts have higher ASRs, and the Two-Sides attack technique is most effective.

Conclusion: The study underscores the need for language-aware alignment and cross-lingual defenses in LLMs, advocating for more robust and inclusive AI systems.

Abstract: Large language models (LLMs) have seen widespread applications across various
domains, yet remain vulnerable to adversarial prompt injections. While most
existing research on jailbreak attacks and hallucination phenomena has focused
primarily on open-source models, we investigate the frontier of closed-source
LLMs under multilingual attack scenarios. We present a first-of-its-kind
integrated adversarial framework that leverages diverse attack techniques to
systematically evaluate frontier proprietary solutions, including GPT-4o,
DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories
of security contents in both English and Chinese, generating 38,400 responses
across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as
the quantitative metric to assess performance from three dimensions: prompt
design, model architecture, and language environment. Our findings suggest that
Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.
Notably, prompts in Chinese consistently yield higher ASRs than their English
counterparts, and our novel Two-Sides attack technique proves to be the most
effective across all models. This work highlights a dire need for
language-aware alignment and robust cross-lingual defenses in LLMs, and we hope
it will inspire researchers, developers, and policymakers toward more robust
and inclusive AI systems.

</details>


### [74] [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
*Kun Huang,Weikai Xu,Yuxuan Liu,Quandong Wang,Pengzhi Gao,Wei Liu,Jian Luan,Bin Wang,Bo An*

Main category: cs.CL

TL;DR: The paper introduces Iterative Preference Learning (IPL) to enhance VLM-based mobile agents' reasoning in GUI tasks by addressing data scarcity and improving generalization without costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving reasoning in GUI tasks lack diverse data or rely on expensive annotations, limiting agent performance.

Method: Proposes IPL, which constructs a CoaT-tree via iterative sampling, scores nodes with rule-based rewards, and uses T-DPO pairs. Includes a three-stage instruction evolution for diverse training data.

Result: MobileIPL outperforms baselines like OS-ATLAS and UI-TARS, achieving SOTA on three benchmarks and strong out-of-domain generalization.

Conclusion: IPL effectively addresses data scarcity and improves reasoning in GUI tasks, demonstrating superior performance and generalization.

Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.

</details>


### [75] [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: HBO is a novel method for fine-tuning LLMs that addresses data imbalance and heterogeneity both globally (across datasets) and locally (within datasets) using a bilevel optimization strategy with Global and Local Actors.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook imbalance and heterogeneity within individual datasets, limiting their effectiveness in fine-tuning LLMs.

Method: HBO employs a bilevel optimization strategy with a Global Actor for cross-dataset balancing and Local Actors for intra-dataset optimization, guided by reward functions based on the LLM's training state.

Result: HBO outperforms baselines, achieving significant accuracy gains across nine diverse tasks in multilingual and multitask setups.

Conclusion: HBO provides a comprehensive solution to data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training.

Abstract: Fine-tuning large language models (LLMs) on a mixture of diverse datasets
poses challenges due to data imbalance and heterogeneity. Existing methods
often address these issues across datasets (globally) but overlook the
imbalance and heterogeneity within individual datasets (locally), which limits
their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a
novel method that enables LLMs to autonomously adjust data allocation during
fine-tuning both across datasets (globally) and within each individual dataset
(locally). HBO employs a bilevel optimization strategy with two types of
actors: a Global Actor, which balances data sampling across different subsets
of the training mixture, and several Local Actors, which optimizes data usage
within each subset based on difficulty levels. These actors are guided by
reward functions derived from the LLM's training state, which measure learning
progress and relative performance improvement. We evaluate HBO on three LLM
backbones across nine diverse tasks in multilingual and multitask setups.
Results show that HBO consistently outperforms existing baselines, achieving
significant accuracy gains. Our in-depth analysis further demonstrates that
both the global actor and local actors of HBO effectively adjust data usage
during fine-tuning. HBO provides a comprehensive solution to the challenges of
data imbalance and heterogeneity in LLM fine-tuning, enabling more effective
training across diverse datasets.

</details>


### [76] [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)
*Yuwei Zhang,Wenhao Yu,Shangbin Feng,Yifan Zhu,Letian Peng,Jayanth Srinivasa,Gaowen Liu,Jingbo Shang*

Main category: cs.CL

TL;DR: The paper introduces WikiDYK, a benchmark for testing LLMs' knowledge memorization, revealing CLMs underperform BiLMs by 23%. A collaborative framework improves accuracy by 29.1%.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks for evaluating LLMs' knowledge memorization capabilities.

Method: Proposes WikiDYK, a benchmark using Wikipedia's 'Did You Know' entries, converted into diverse question-answer pairs. Tests CLMs and BiLMs via continued pre-training.

Result: CLMs show 23% lower accuracy than BiLMs. A collaborative framework using BiLM ensembles boosts reliability by 29.1%.

Conclusion: WikiDYK is a scalable benchmark; BiLMs outperform CLMs, and collaborative frameworks enhance LLM knowledge integration.

Abstract: Despite significant advances in large language models (LLMs), their knowledge
memorization capabilities remain underexplored, due to the lack of standardized
and high-quality test ground. In this paper, we introduce a novel, real-world
and large-scale knowledge injection benchmark that evolves continuously over
time without requiring human intervention. Specifically, we propose WikiDYK,
which leverages recently-added and human-written facts from Wikipedia's "Did
You Know..." entries. These entries are carefully selected by expert Wikipedia
editors based on criteria such as verifiability and clarity. Each entry is
converted into multiple question-answer pairs spanning diverse task formats
from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290
facts and 77,180 questions, which is also seamlessly extensible with future
updates from Wikipedia editors. Extensive experiments using continued
pre-training reveal a surprising insight: despite their prevalence in modern
LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge
memorization capabilities compared to Bidirectional Language Models (BiLMs),
exhibiting a 23% lower accuracy in terms of reliability. To compensate for the
smaller scales of current BiLMs, we introduce a modular collaborative framework
utilizing ensembles of BiLMs as external knowledge repositories to integrate
with LLMs. Experiment shows that our framework further improves the reliability
accuracy by up to 29.1%.

</details>


### [77] [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: ExpertSteer is a novel method using external expert models to generate steering vectors for controlling LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing activation steering methods rely on the model's own vectors, limiting effectiveness and excluding external expert models.

Method: ExpertSteer aligns dimensions with auto-encoders, identifies intervention layers, generates vectors with Recursive Feature Machines, and applies them during inference.

Result: ExpertSteer significantly outperforms baselines on 15 benchmarks across four domains.

Conclusion: ExpertSteer enables effective LLM control using external experts, offering a scalable and efficient solution.

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across various
tasks, yet guiding them to follow desired behaviours during inference remains a
significant challenge. Activation steering offers a promising method to control
the generation process of LLMs by modifying their internal activations.
However, existing methods commonly intervene in the model's behaviour using
steering vectors generated by the model itself, which constrains their
effectiveness to that specific model and excludes the possibility of leveraging
powerful external expert models for steering. To address these limitations, we
propose ExpertSteer, a novel approach that leverages arbitrary specialized
expert models to generate steering vectors, enabling intervention in any LLMs.
ExpertSteer transfers the knowledge from an expert model to a target LLM
through a cohesive four-step process: first aligning representation dimensions
with auto-encoders to enable cross-model transfer, then identifying
intervention layer pairs based on mutual information analysis, next generating
steering vectors from the expert model using Recursive Feature Machines, and
finally applying these vectors on the identified layers during inference to
selectively guide the target LLM without updating model parameters. We conduct
comprehensive experiments using three LLMs on 15 popular benchmarks across four
distinct domains. Experiments demonstrate that ExpertSteer significantly
outperforms established baselines across diverse tasks at minimal cost.

</details>


### [78] [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)
*Xinye Li,Mingqi Wan,Dianbo Sui*

Main category: cs.CL

TL;DR: Team asdfo123's submission to LLMSR@XLLM25 uses Meta-Llama-3-8B-Instruct for fine-grained reasoning tasks, achieving competitive results without fine-tuning or complex pipelines.


<details>
  <summary>Details</summary>
Motivation: To evaluate large language models on producing controllable and interpretable reasoning processes for structural reasoning tasks.

Method: A few-shot, multi-turn prompt guides the model to enumerate conditions, label reasoning steps, and verify logic, followed by lightweight post-processing.

Result: Ranked 5th overall with macro F1 scores comparable to more resource-intensive methods.

Conclusion: The approach shows promise for structural reasoning with LLMs, with identified strengths and limitations guiding future research.

Abstract: We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which
evaluates large language models on producing fine-grained, controllable, and
interpretable reasoning processes. Systems must extract all problem conditions,
decompose a chain of thought into statement-evidence pairs, and verify the
logical validity of each pair. Leveraging only the off-the-shelf
Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that
first enumerates all conditions and then guides the model to label, cite, and
adjudicate every reasoning step. A lightweight post-processor based on regular
expressions normalises spans and enforces the official JSON schema. Without
fine-tuning, external retrieval, or ensembling, our method ranks 5th overall,
achieving macro F1 scores on par with substantially more complex and
resource-consuming pipelines. We conclude by analysing the strengths and
limitations of our approach and outlining directions for future research in
structural reasoning with LLMs. Our code is available at
https://github.com/asdfo123/LLMSR-asdfo123.

</details>


### [79] [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
*Qizhou Chen,Dakan Wang,Taolin Zhang,Zaoming Yan,Chengsong You,Chengyu Wang,Xiaofeng He*

Main category: cs.CL

TL;DR: UniEdit is a unified benchmark for LLM editing, addressing limitations of narrow datasets by covering diverse domains and ripple effects.


<details>
  <summary>Details</summary>
Motivation: Current LLM editing datasets are limited in scope and overlook broad editing demands and ripple effects.

Method: Constructs editing samples from 25 domains, uses NMCS for ripple effect sampling, and converts knowledge subgraphs to natural language.

Result: UniEdit is confirmed as comprehensive and diverse, with experiments revealing LLM editing strengths and weaknesses.

Conclusion: UniEdit provides valuable insights for future LLM editing research across open knowledge domains.

Abstract: Model editing aims to enhance the accuracy and reliability of large language
models (LLMs) by efficiently adjusting their internal parameters. Currently,
most LLM editing datasets are confined to narrow knowledge domains and cover a
limited range of editing evaluation. They often overlook the broad scope of
editing demands and the diversity of ripple effects resulting from edits. In
this context, we introduce UniEdit, a unified benchmark for LLM editing
grounded in open-domain knowledge. First, we construct editing samples by
selecting entities from 25 common domains across five major categories,
utilizing the extensive triple knowledge available in open-domain knowledge
graphs to ensure comprehensive coverage of the knowledge domains. To address
the issues of generality and locality in editing, we design an Neighborhood
Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given
knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we
employ proprietary LLMs to convert the sampled knowledge subgraphs into natural
language text, guaranteeing grammatical accuracy and syntactical diversity.
Extensive statistical analysis confirms the scale, comprehensiveness, and
diversity of our UniEdit benchmark. We conduct comprehensive experiments across
multiple LLMs and editors, analyzing their performance to highlight strengths
and weaknesses in editing across open knowledge domains and various evaluation
criteria, thereby offering valuable insights for future research endeavors.

</details>


### [80] [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)
*Axel Abels,Tom Lenaerts*

Main category: cs.CL

TL;DR: The paper explores bias in LLMs, showing they mirror human biases. It tests crowd-based mitigation strategies, finding simple averaging worsens biases, while weighted aggregation and hybrid human-LLM crowds reduce biases and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the inadvertent perpetuation of biases in LLMs by analyzing their responses and developing effective mitigation strategies.

Method: Analyzed LLM responses to bias-eliciting headlines, tested crowd-based strategies (averaging, weighted aggregation), and evaluated hybrid human-LLM crowds.

Result: Simple averaging exacerbates biases; weighted aggregation and hybrid crowds reduce biases and improve accuracy.

Conclusion: Hybrid human-LLM crowds are most effective for bias mitigation and accuracy, leveraging the strengths of both.

Abstract: Despite their performance, large language models (LLMs) can inadvertently
perpetuate biases found in the data they are trained on. By analyzing LLM
responses to bias-eliciting headlines, we find that these models often mirror
human biases. To address this, we explore crowd-based strategies for mitigating
bias through response aggregation. We first demonstrate that simply averaging
responses from multiple LLMs, intended to leverage the "wisdom of the crowd",
can exacerbate existing biases due to the limited diversity within LLM crowds.
In contrast, we show that locally weighted aggregation methods more effectively
leverage the wisdom of the LLM crowd, achieving both bias mitigation and
improved accuracy. Finally, recognizing the complementary strengths of LLMs
(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing
both significantly enhance performance and further reduce biases across ethnic
and gender-related contexts.

</details>


### [81] [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: CAPTURE is a new benchmark for evaluating context-aware prompt injection guardrails, revealing high false negatives in attacks and excessive false positives in benign cases.


<details>
  <summary>Details</summary>
Motivation: Existing guardrail models for prompt injection lack context-awareness and suffer from over-defense, necessitating a better evaluation framework.

Method: Introduces CAPTURE, a context-aware benchmark, to assess attack detection and over-defense with minimal in-domain examples.

Result: Current guardrail models show high false negatives in adversarial cases and excessive false positives in benign scenarios.

Conclusion: CAPTURE highlights critical limitations in existing prompt injection defenses, urging improvements in context-aware guardrails.

Abstract: Prompt injection remains a major security risk for large language models.
However, the efficacy of existing guardrail models in context-aware settings
remains underexplored, as they often rely on static attack benchmarks.
Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel
context-aware benchmark assessing both attack detection and over-defense
tendencies with minimal in-domain examples. Our experiments reveal that current
prompt injection guardrail models suffer from high false negatives in
adversarial cases and excessive false positives in benign scenarios,
highlighting critical limitations.

</details>


### [82] [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
*Mohsinul Kabir,Tasfia Tahsin,Sophia Ananiadou*

Main category: cs.CL

TL;DR: The paper investigates bias origins in LMs, emphasizing data-model interactions and temporal influences, revealing architectural robustness in transformers and data provenance's impact on bias.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic research on bias origins in LMs, focusing on data quality, model architecture, and temporal dynamics.

Method: Comparative behavioral theory and evaluation of data, model design, and temporal dynamics in bias propagation, comparing transformers and n-gram LMs.

Result: Transformers show architectural robustness; n-gram LMs are context-window sensitive. Data provenance affects bias, and certain biases are disproportionately amplified.

Conclusion: A holistic approach is needed to trace bias origins across data and model dimensions to mitigate harm.

Abstract: Current research on bias in language models (LMs) predominantly focuses on
data quality, with significantly less attention paid to model architecture and
temporal influences of data. Even more critically, few studies systematically
investigate the origins of bias. We propose a methodology grounded in
comparative behavioral theory to interpret the complex interaction between
training data and model architecture in bias propagation during language
modeling. Building on recent work that relates transformers to n-gram LMs, we
evaluate how data, model design choices, and temporal dynamics affect bias
propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to
context window size in bias propagation, while transformers demonstrate
architectural robustness; (2) the temporal provenance of training data
significantly affects bias; and (3) different model architectures respond
differentially to controlled bias injection, with certain biases (e.g. sexual
orientation) being disproportionately amplified. As language models become
ubiquitous, our findings highlight the need for a holistic approach -- tracing
bias to its origins across both data and model dimensions, not just symptoms,
to mitigate harm.

</details>


### [83] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Main category: cs.CL

TL;DR: SLOT is a test-time inference method that optimizes language models for individual prompts, improving accuracy by updating a lightweight parameter vector.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with complex or underrepresented instructions, leading to poor performance.

Method: SLOT updates a sample-specific parameter vector at test-time, minimizing cross-entropy loss on the input prompt.

Result: SLOT outperforms benchmarks, e.g., Qwen2.5-7B gains 8.6% accuracy on GSM8K, and DeepSeek-R1-Distill-Llama-70B achieves SOTA on GPQA.

Conclusion: SLOT enhances LLM performance by efficiently adapting to individual prompts, demonstrating significant accuracy improvements.

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [84] [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)
*Yepeng Weng,Qiao Hu,Xujie Chen,Li Liu,Dianwen Mei,Huishi Qiu,Jiang Tian,Zhongchao Shi*

Main category: cs.CL

TL;DR: Traversal Verification improves speculative decoding by using leaf-to-root traversal, enhancing acceptance rates and throughput compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods suffer from suboptimal acceptance length and inefficient candidate utilization due to token-level verification.

Method: Introduces Traversal Verification, a novel algorithm that verifies token sequences from leaf-to-root, preserving valid subsequences.

Result: Theoretical proof shows identical probability distribution to the target model, with experimental results confirming improved acceptance length and throughput.

Conclusion: Traversal Verification offers a lossless, efficient alternative to current speculative decoding methods, achieving significant acceleration.

Abstract: Speculative decoding is a promising approach for accelerating large language
models. The primary idea is to use a lightweight draft model to speculate the
output of the target model for multiple subsequent timesteps, and then verify
them in parallel to determine whether the drafted tokens should be accepted or
rejected. To enhance acceptance rates, existing frameworks typically construct
token trees containing multiple candidates in each timestep. However, their
reliance on token-level verification mechanisms introduces two critical
limitations: First, the probability distribution of a sequence differs from
that of individual tokens, leading to suboptimal acceptance length. Second,
current verification schemes begin from the root node and proceed layer by
layer in a top-down manner. Once a parent node is rejected, all its child nodes
should be discarded, resulting in inefficient utilization of speculative
candidates. This paper introduces Traversal Verification, a novel speculative
decoding algorithm that fundamentally rethinks the verification paradigm
through leaf-to-root traversal. Our approach considers the acceptance of the
entire token sequence from the current node to the root, and preserves
potentially valid subsequences that would be prematurely discarded by existing
methods. We theoretically prove that the probability distribution obtained
through Traversal Verification is identical to that of the target model,
guaranteeing lossless inference while achieving substantial acceleration gains.
Experimental results across different large language models and multiple tasks
show that our method consistently improves acceptance length and throughput
over existing methods

</details>


### [85] [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)
*Konstantinos Xylogiannopoulos,Petros Xanthopoulos,Panagiotis Karampelas,Georgios Bakamitsos*

Main category: cs.CL

TL;DR: The paper addresses the threat of AI-paraphrased content for copyright infringement, proposing a pattern-based method to detect ChatGPT-generated paraphrases with high accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI for malicious paraphrasing threatens original content creators' revenue, yet lacks academic research.

Method: A pattern-based similarity detection algorithm is proposed to identify ChatGPT-paraphrased news articles, tested on a benchmark dataset of 2,224 real and paraphrased articles.

Result: The method achieves high performance metrics (96.23% accuracy, 96.25% precision, 96.21% sensitivity, 96.25% specificity, 96.23% F1 score).

Conclusion: The pattern-based approach effectively detects AI-paraphrased content without deep learning, offering a solution to mitigate copyright infringement risks.

Abstract: Generative AI paraphrased text can be used for copyright infringement and the
AI paraphrased content can deprive substantial revenue from original content
creators. Despite this recent surge of malicious use of generative AI, there
are few academic publications that research this threat. In this article, we
demonstrate the ability of pattern-based similarity detection for AI
paraphrased news recognition. We propose an algorithmic scheme, which is not
limited to detect whether an article is an AI paraphrase, but, more
importantly, to identify that the source of infringement is the ChatGPT. The
proposed method is tested with a benchmark dataset specifically created for
this task that incorporates real articles from BBC, incorporating a total of
2,224 articles across five different news categories, as well as 2,224
paraphrased articles created with ChatGPT. Results show that our pattern
similarity-based method, that makes no use of deep learning, can detect ChatGPT
assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for
precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1
score.

</details>


### [86] [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
*Zhenhe Wu,Jian Yang,Jiaheng Liu,Xianjie Wu,Changzai Pan,Jie Zhang,Yu Zhao,Shuangyong Song,Yongxiang Li,Zhoujun Li*

Main category: cs.CL

TL;DR: Table-R1, a reinforcement learning method, improves LLM table understanding by integrating region evidence and dynamic rewards, achieving significant performance gains and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLM performance in table question answering, which is underexplored despite the potential of techniques like CoT and PoT.

Method: Uses Region-Enhanced Supervised Fine-Tuning (RE-SFT) and Table-Aware Group Relative Policy Optimization (TARPO) to integrate region evidence and balance rewards.

Result: Achieves 14.36-point average improvement on benchmarks, outperforms larger models, and reduces token use by 67.5%.

Conclusion: Table-R1 and TARPO significantly advance LLM tabular reasoning efficiency and accuracy.

Abstract: Tables present unique challenges for language models due to their structured
row-column interactions, necessitating specialized approaches for effective
comprehension. While large language models (LLMs) have demonstrated potential
in table reasoning through prompting and techniques like chain-of-thought (CoT)
and program-of-thought (PoT), optimizing their performance for table question
answering remains underexplored. In this paper, we introduce region-based
Table-R1, a novel reinforcement learning approach that enhances LLM table
understanding by integrating region evidence into reasoning steps. Our method
employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in
identifying relevant table regions before generating answers, incorporating
textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group
Relative Policy Optimization (TARPO) introduces a mixed reward system to
dynamically balance region accuracy and answer correctness, with decaying
region rewards and consistency penalties to align reasoning steps. Experiments
show that Table-R1 achieves an average performance improvement of 14.36 points
across multiple base models on three benchmark datasets, even outperforming
baseline models with ten times the parameters, while TARPO reduces response
token consumption by 67.5% compared to GRPO, significantly advancing LLM
capabilities in efficient tabular reasoning.

</details>


### [87] [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)
*Wenqiao Zhu,Chao Xu,Lulu Wang,Jun Wu*

Main category: cs.CL

TL;DR: PSC (Phase Shift Calibration) enhances existing RoPE-based methods by calibrating predefined frequencies, improving performance across varying context window sizes and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to optimize RoPE's base frequencies due to exponential search space, prompting the need for a calibration module like PSC.

Method: Introduces PSC to calibrate predefined frequencies in RoPE-based methods (e.g., PI, YaRN, LongRoPE), tested on models with context windows from 16k to 64k.

Result: PSC reduces perplexity as context window size increases (16k to 64k) and shows robustness across models and tasks.

Conclusion: PSC effectively enhances RoPE-based methods, offering broad applicability and improved performance for large context windows.

Abstract: Rotary Position Embedding (RoPE) is an efficient position encoding approach
and is widely utilized in numerous large language models (LLMs). Recently, a
lot of methods have been put forward to further expand the context window based
on RoPE. The core concept of those methods is to predefine or search for a set
of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a
challenge for existing methods to predefine an optimal factor due to the
exponential search space. In view of this, we introduce PSC (Phase Shift
Calibration), a small module for calibrating the frequencies predefined by
existing methods. With the employment of PSC, we demonstrate that many existing
methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted
extensive experiments across multiple models and tasks. The results demonstrate
that (1) when PSC is enabled, the comparative reductions in perplexity increase
as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our
approach is broadly applicable and exhibits robustness across a variety of
models and tasks. The code can be found at https://github.com/WNQzhu/PSC.

</details>


### [88] [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)
*Jinming Zhang,Yunfei Long*

Main category: cs.CL

TL;DR: The paper introduces the LPLH framework, a cognitively inspired approach for LLMs to play IF games more like humans by focusing on narrative context and gameplay logic.


<details>
  <summary>Details</summary>
Motivation: Existing AI approaches for IF games prioritize task-specific metrics over human-like comprehension, lacking narrative and commonsense alignment.

Method: LPLH integrates structured map building, action learning, and feedback-driven experience analysis to guide LLMs systematically.

Result: The framework aligns LLM behavior with narrative intent and commonsense, improving interpretability and human-like performance.

Conclusion: LPLH reframes IF games as a learning problem for LLMs, advancing context-aware gameplay in text-based environments.

Abstract: Interactive Fiction games (IF games) are where players interact through
natural language commands. While recent advances in Artificial Intelligence
agents have reignited interest in IF games as a domain for studying
decision-making, existing approaches prioritize task-specific performance
metrics over human-like comprehension of narrative context and gameplay logic.
This work presents a cognitively inspired framework that guides Large Language
Models (LLMs) to learn and play IF games systematically. Our proposed
**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three
key components: (1) structured map building to capture spatial and narrative
relationships, (2) action learning to identify context-appropriate commands,
and (3) feedback-driven experience analysis to refine decision-making over
time. By aligning LLMs-based agents' behavior with narrative intent and
commonsense constraints, LPLH moves beyond purely exploratory strategies to
deliver more interpretable, human-like performance. Crucially, this approach
draws on cognitive science principles to more closely simulate how human
players read, interpret, and respond within narrative worlds. As a result, LPLH
reframes the IF games challenge as a learning problem for LLMs-based agents,
offering a new path toward robust, context-aware gameplay in complex text-based
environments.

</details>


### [89] [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
*Siyang Wu,Honglin Bao,Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: Self-questioning improves LLMs' understanding by activating latent knowledge, tested on a challenging patent benchmark.


<details>
  <summary>Details</summary>
Motivation: LLMs have latent knowledge that is hard to access; self-questioning aims to improve understanding, especially in technical domains.

Method: Proposed self-questioning strategy, evaluated on 1.3M computer science patent pairs with pairwise differentiation tasks.

Result: Self-questioning significantly improves performance; external retrieval further enhances results. Smaller models generate better-aligned questions for mid-sized models.

Conclusion: Self-questioning is effective for improving LLM comprehension and serves as a diagnostic tool for knowledge organization.

Abstract: Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that prompting
LLMs to generate and answer their own questions - targeting the background
knowledge required for the task - significantly improves performance. These
self-generated questions and answers activate otherwise underutilized internal
knowledge. Allowing LLMs to retrieve answers from external scientific texts
further enhances performance, suggesting that model knowledge is compressed and
lacks the full richness of the training data. We also find that
chain-of-thought prompting and self-questioning converge, though
self-questioning remains more effective for improving understanding of
technical concepts. Notably, we uncover an asymmetry in prompting: smaller
models often generate more fundamental, more open-ended, better-aligned
questions for mid-sized models than large models with better understanding do,
revealing a new strategy for cross-model collaboration. Altogether, our
findings establish self-questioning as both a practical mechanism for
automatically improving LLM comprehension, especially in domains with sparse
and underrepresented knowledge, and a diagnostic probe of how internal and
external knowledge are organized.

</details>


### [90] [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)
*Yuyang Ding,Dan Qiao,Juntao Li,Jiajie Xu,Pingfu Chao,Xiaofang Zhou,Min Zhang*

Main category: cs.CL

TL;DR: The paper explores the effectiveness and robustness of distantly supervised named entity recognition (DS-NER) by comparing annotation techniques and introducing a novel noise assessment framework.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on latent noise distribution between different distant annotation methods in DS-NER.

Method: The study evaluates traditional rule-based methods and large language model supervision, and introduces a framework categorizing noise into unlabeled-entity (UEP) and noisy-entity problems (NEP) with specialized solutions.

Result: The proposed method significantly improves performance on eight real-world datasets, outperforming state-of-the-art methods.

Conclusion: The framework effectively addresses noise challenges in DS-NER, demonstrating superior results across diverse datasets and annotation techniques.

Abstract: Distantly supervised named entity recognition (DS-NER) has emerged as a cheap
and convenient alternative to traditional human annotation methods, enabling
the automatic generation of training data by aligning text with external
resources. Despite the many efforts in noise measurement methods, few works
focus on the latent noise distribution between different distant annotation
methods. In this work, we explore the effectiveness and robustness of DS-NER by
two aspects: (1) distant annotation techniques, which encompasses both
traditional rule-based methods and the innovative large language model
supervision approach, and (2) noise assessment, for which we introduce a novel
framework. This framework addresses the challenges by distinctly categorizing
them into the unlabeled-entity problem (UEP) and the noisy-entity problem
(NEP), subsequently providing specialized solutions for each. Our proposed
method achieves significant improvements on eight real-world distant
supervision datasets originating from three different data sources and
involving four distinct annotation techniques, confirming its superiority over
current state-of-the-art methods.

</details>


### [91] [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
*Weixiao Zhou,Junnan Zhu,Gengyao Li,Xianfu Cheng,Xinnian Liang,Feifei Zhai,Zhoujun Li*

Main category: cs.CL

TL;DR: The paper investigates LLMs' performance on a task combining discussion and background knowledge for summarization, revealing limitations in background retrieval, opinion integration, and self-correction.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of outside observer confusion in dialogue summarization systems by incorporating background knowledge.

Method: Modeling the task as background and opinion summaries, defining two summarization patterns, and evaluating 12 LLMs under structured-prompt and self-reflection paradigms.

Result: LLMs struggle with background retrieval and opinion integration, with top models achieving <69% average performance. They also lack self-evaluation and correction capabilities.

Conclusion: Current LLMs are inadequate for tasks requiring combined discussion and background knowledge, highlighting the need for improved self-correction and evaluation.

Abstract: In this work, we investigate the performance of LLMs on a new task that
requires combining discussion with background knowledge for summarization. This
aims to address the limitation of outside observer confusion in existing
dialogue summarization systems due to their reliance solely on discussion
information. To achieve this, we model the task output as background and
opinion summaries and define two standardized summarization patterns. To
support assessment, we introduce the first benchmark comprising high-quality
samples consistently annotated by human experts and propose a novel
hierarchical evaluation framework with fine-grained, interpretable metrics. We
evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our
findings reveal: (1) LLMs struggle with background summary retrieval,
generation, and opinion summary integration. (2) Even top LLMs achieve less
than 69% average performance across both patterns. (3) Current LLMs lack
adequate self-evaluation and self-correction capabilities for this task.

</details>


### [92] [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)
*Xiao Long,Liansheng Zhuang,Chen Shen,Shaotian Yan,Yifei Li,Shafei Wang*

Main category: cs.CL

TL;DR: The paper introduces RTSoG, a training-free framework for KGQA tasks that decomposes complex questions into sub-questions and uses reward-guided tree search to improve reasoning path retrieval and answer generation.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs-based KGQA methods focus on exploring new reasoning paths but neglect historical paths, leading to sub-optimal results. Complex question semantics also cause inaccurate path retrieval.

Method: RTSoG decomposes questions into simpler sub-questions, uses SC-MCTS to retrieve weighted reasoning paths, and stacks them to generate answers.

Result: RTSoG improves performance by 8.7% on GrailQA and 7.0% on WebQSP over state-of-the-art methods.

Conclusion: RTSoG effectively addresses the limitations of current KGQA methods by leveraging historical reasoning paths and handling complex semantics, achieving significant performance gains.

Abstract: Recently, large language models (LLMs) have demonstrated impressive
performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to
find answers based on knowledge graphs (KGs) for natural language questions.
Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented
Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the
large KGs, and then generates the answers based on them. However, these methods
emphasize the exploration of new optimal reasoning paths in KGs while ignoring
the exploitation of historical reasoning paths, which may lead to sub-optimal
reasoning paths. Additionally, the complex semantics contained in questions may
lead to the retrieval of inaccurate reasoning paths. To address these issues,
this paper proposes a novel and training-free framework for KGQA tasks called
Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original
question into a series of simpler and well-defined sub-questions to handle the
complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided
by a reward model is introduced to iteratively retrieve weighted reasoning
paths as contextual knowledge. Finally, it stacks the weighted reasoning paths
according to their weights to generate the final answers. Extensive experiments
on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves
8.7\% and 7.0\% performance improvement over the state-of-the-art method on the
GrailQA and the WebQSP respectively.

</details>


### [93] [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)
*Nikita Tatarinov,Vidhyakshaya Kannan,Haricharana Srinivasa,Arnav Raj,Harpreet Singh Anand,Varun Singh,Aditya Luthra,Ravij Lade,Agam Shah,Sudheer Chava*

Main category: cs.CL

TL;DR: KG-QAGen is a framework for generating QA pairs at varying complexity levels to evaluate long-context language models, revealing their struggles with multi-hop inference and set operations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of structured benchmarks for assessing long-context language models' ability to retrieve and process information across extensive documents.

Method: KG-QAGen extracts QA pairs from financial agreements, varying complexity along multi-hop retrieval, set operations, and answer plurality dimensions.

Result: A dataset of 20,139 QA pairs was created, showing even top models struggle with set-based comparisons and multi-hop inference.

Conclusion: The framework highlights systematic failure modes in models, such as semantic misinterpretation and handling implicit relations.

Abstract: The increasing context length of modern language models has created a need
for evaluating their ability to retrieve and process information across
extensive documents. While existing benchmarks test long-context capabilities,
they often lack a structured way to systematically vary question complexity. We
introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a
framework that (1) extracts QA pairs at multiple complexity levels (2) by
leveraging structured representations of financial agreements (3) along three
key dimensions -- multi-hop retrieval, set operations, and answer plurality --
enabling fine-grained assessment of model performance across controlled
difficulty levels. Using this framework, we construct a dataset of 20,139 QA
pairs (the largest number among the long-context benchmarks) and open-source a
part of it. We evaluate 13 proprietary and open-source LLMs and observe that
even the best-performing models are struggling with set-based comparisons and
multi-hop logical inference. Our analysis reveals systematic failure modes tied
to semantic misinterpretation and inability to handle implicit relations.

</details>


### [94] [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)
*Xu Zheng,Zhuomin Chen,Esteban Schafir,Sipeng Chen,Hojat Allah Salehi,Haifeng Chen,Farhad Shirani,Wei Cheng,Dongsheng Luo*

Main category: cs.CL

TL;DR: LM$^2$otifs is an explainable framework for detecting machine-generated texts using graph neural networks, offering interpretable motifs for differentiation.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of explainability in existing methods for distinguishing machine-generated and human-generated texts.

Method: LM$^2$otifs transforms text into graphs, uses graph neural networks for prediction, and extracts interpretable motifs for explanations.

Result: Demonstrates comparable performance on benchmarks and confirms the effectiveness of extracted motifs in differentiation.

Conclusion: LM$^2$otifs provides both accurate detection and interpretability, revealing distinct linguistic fingerprints of machine-generated texts.

Abstract: The impressive ability of large language models to generate natural text
across various tasks has led to critical challenges in authorship
authentication. Although numerous detection methods have been developed to
differentiate between machine-generated texts (MGT) and human-generated texts
(HGT), the explainability of these methods remains a significant gap.
Traditional explainability techniques often fall short in capturing the complex
word relationships that distinguish HGT from MGT. To address this limitation,
we present LM$^2$otifs, a novel explainable framework for MGT detection.
Inspired by probabilistic graphical models, we provide a theoretical rationale
for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks
to achieve both accurate detection and interpretability. The LM$^2$otifs
pipeline operates in three key stages: first, it transforms text into graphs
based on word co-occurrence to represent lexical dependencies; second, graph
neural networks are used for prediction; and third, a post-hoc explainability
method extracts interpretable motifs, offering multi-level explanations from
individual words to sentence structures. Extensive experiments on multiple
benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The
empirical evaluation of the extracted explainable motifs confirms their
effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis
reveals distinct and visible linguistic fingerprints characteristic of MGT.

</details>


### [95] [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)
*Yanting Li,Jiyue Jiang,Zikang Wang,Ziqian Lin,Dongchen He,Yuheng Shan,Yanruisheng Shao,Jiayi Li,Xiangyu Shi,Jiuming Wang,Yanyu Chen,Yimin Fan,Han Li,Yu Li*

Main category: cs.CL

TL;DR: DS-ProGen, a dual-structure deep language model, integrates backbone geometry and surface features for precise protein sequence design, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Inverse Protein Folding (IPF) rely on limited structural features, restricting their accuracy in sequence prediction.

Method: DS-ProGen combines backbone coordinates and surface-level representations in a next-amino-acid prediction paradigm.

Result: Achieves a 61.47% recovery rate on the PRIDE dataset and excels in predicting interactions with biological partners.

Conclusion: DS-ProGen demonstrates the advantage of multi-modal structural encoding for functional and stable protein design.

Abstract: Inverse Protein Folding (IPF) is a critical subtask in the field of protein
design, aiming to engineer amino acid sequences capable of folding correctly
into a specified three-dimensional (3D) conformation. Although substantial
progress has been achieved in recent years, existing methods generally rely on
either backbone coordinates or molecular surface features alone, which
restricts their ability to fully capture the complex chemical and geometric
constraints necessary for precise sequence prediction. To address this
limitation, we present DS-ProGen, a dual-structure deep language model for
functional protein design, which integrates both backbone geometry and
surface-level representations. By incorporating backbone coordinates as well as
surface chemical and geometric descriptors into a next-amino-acid prediction
paradigm, DS-ProGen is able to generate functionally relevant and structurally
stable sequences while satisfying both global and local conformational
constraints. On the PRIDE dataset, DS-ProGen attains the current
state-of-the-art recovery rate of 61.47%, demonstrating the synergistic
advantage of multi-modal structural encoding in protein design. Furthermore,
DS-ProGen excels in predicting interactions with a variety of biological
partners, including ligands, ions, and RNA, confirming its robust functional
retention capabilities.

</details>


### [96] [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)
*Navid Madani,Rohini Srihari*

Main category: cs.CL

TL;DR: ESC-Judge is an automated framework for evaluating emotional-support LLMs, grounded in Hill's counseling model, matching human annotators' reliability at lower cost.


<details>
  <summary>Details</summary>
Motivation: The lack of a scalable, theory-grounded method to compare emotional-support LLMs motivates the development of ESC-Judge.

Method: ESC-Judge synthesizes realistic help-seeker roles, isolates model strategies via dual-agent sessions, and uses a judge LLM for pairwise comparisons.

Result: ESC-Judge matched PhD-level annotators on 85% (Exploration), 83% (Insight), and 86% (Action) decisions.

Conclusion: ESC-Judge offers a reliable, cost-effective solution for evaluating emotional-support LLMs, with all resources released for transparency.

Abstract: Large language models (LLMs) increasingly power mental-health chatbots, yet
the field still lacks a scalable, theory-grounded way to decide which model is
most effective to deploy. We present ESC-Judge, the first end-to-end evaluation
framework that (i) grounds head-to-head comparisons of emotional-support LLMs
in Clara Hill's established Exploration-Insight-Action counseling model,
providing a structured and interpretable view of performance, and (ii) fully
automates the evaluation pipeline at scale. ESC-Judge operates in three stages:
first, it synthesizes realistic help-seeker roles by sampling empirically
salient attributes such as stressors, personality, and life history; second, it
has two candidate support agents conduct separate sessions with the same role,
isolating model-specific strategies; and third, it asks a specialized judge LLM
to express pairwise preferences across rubric-anchored skills that span the
Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched
PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and
86 percent of Action decisions, demonstrating human-level reliability at a
fraction of the cost. All code, prompts, synthetic roles, transcripts, and
judgment scripts are released to promote transparent progress in emotionally
supportive AI.

</details>


### [97] [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)
*Varvara Arzt,Allan Hanbury,Michael Wiegand,Gábor Recski,Terra Blevins*

Main category: cs.CL

TL;DR: RE models struggle with unseen data, showing poor generalization. Data quality, not lexical similarity, is key for transferability. Fine-tuning works best with high-quality data, while few-shot ICL excels with noisy data. Zero-shot baselines sometimes outperform cross-dataset results. Benchmark structural issues also hinder transferability.


<details>
  <summary>Details</summary>
Motivation: To assess whether RE models learn robust relational patterns or rely on spurious correlations by analyzing their generalization capabilities.

Method: Cross-dataset experiments evaluating RE models' performance on unseen data, comparing fine-tuning and few-shot in-context learning (ICL) strategies.

Result: Higher intra-dataset performance doesn't guarantee transferability, often indicating overfitting. Data quality is crucial for robust transfer, with fine-tuning best for high-quality data and few-shot ICL for noisy data. Zero-shot baselines occasionally outperform.

Conclusion: RE models face generalization challenges due to dataset-specific artefacts and benchmark structural issues. Data quality and adaptation strategy choice are critical for improving transferability.

Abstract: Analysing the generalisation capabilities of relation extraction (RE) models
is crucial for assessing whether they learn robust relational patterns or rely
on spurious correlations. Our cross-dataset experiments find that RE models
struggle with unseen data, even within similar domains. Notably, higher
intra-dataset performance does not indicate better transferability, instead
often signaling overfitting to dataset-specific artefacts. Our results also
show that data quality, rather than lexical similarity, is key to robust
transfer, and the choice of optimal adaptation strategy depends on the quality
of data available: while fine-tuning yields the best cross-dataset performance
with high-quality data, few-shot in-context learning (ICL) is more effective
with noisier data. However, even in these cases, zero-shot baselines
occasionally outperform all cross-dataset results. Structural issues in RE
benchmarks, such as single-relation per sample constraints and non-standardised
negative class definitions, further hinder model transferability.

</details>


### [98] [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)
*Md Mehrab Tanjim,Yeonjun In,Xiang Chen,Victor S. Bursztyn,Ryan A. Rossi,Sungchul Kim,Guang-Jie Ren,Vaishnavi Muppala,Shun Jiang,Yongsung Kim,Chanyoung Park*

Main category: cs.CL

TL;DR: The paper explores ambiguity in NLP, focusing on LLMs in CQA, categorizes disambiguation methods, compares their pros and cons, reviews datasets, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: Addressing ambiguity in NLP is critical, especially with LLMs, to improve language-driven systems like CQA.

Method: Defines key terms, categorizes disambiguation approaches, compares methods, and reviews datasets.

Result: Provides a comparative analysis of disambiguation techniques and identifies open problems.

Conclusion: Aims to contribute to robust language systems by reviewing ambiguity and disambiguation research with LLMs.

Abstract: Ambiguity remains a fundamental challenge in Natural Language Processing
(NLP) due to the inherent complexity and flexibility of human language. With
the advent of Large Language Models (LLMs), addressing ambiguity has become
even more critical due to their expanded capabilities and applications. In the
context of Conversational Question Answering (CQA), this paper explores the
definition, forms, and implications of ambiguity for language driven systems,
particularly in the context of LLMs. We define key terms and concepts,
categorize various disambiguation approaches enabled by LLMs, and provide a
comparative analysis of their advantages and disadvantages. We also explore
publicly available datasets for benchmarking ambiguity detection and resolution
techniques and highlight their relevance for ongoing research. Finally, we
identify open problems and future research directions, proposing areas for
further investigation. By offering a comprehensive review of current research
on ambiguities and disambiguation with LLMs, we aim to contribute to the
development of more robust and reliable language systems.

</details>


### [99] [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)
*Yang Zhao,Pu Wang,Yibo Zhao,Hongru Du,Hao,Yang*

Main category: cs.CL

TL;DR: TrafficSafe, an LLM-based framework, improves crash prediction by 42% F1-score and identifies key risk factors like alcohol-impaired driving.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to interpret complex crash data interactions, limiting risk factor identification.

Method: TrafficSafe uses multi-modal data (58,903 reports) and fine-tunes LLMs for text-based reasoning and feature attribution.

Result: 42% F1-score improvement; alcohol-impaired driving is the top severe crash factor.

Conclusion: TrafficSafe advances traffic safety research by leveraging AI for actionable insights.

Abstract: Predicting crash events is crucial for understanding crash distributions and
their contributing factors, thereby enabling the design of proactive traffic
safety policy interventions. However, existing methods struggle to interpret
the complex interplay among various sources of traffic crash data, including
numeric characteristics, textual reports, crash imagery, environmental
conditions, and driver behavior records. As a result, they often fail to
capture the rich semantic information and intricate interrelationships embedded
in these diverse data sources, limiting their ability to identify critical
crash risk factors. In this research, we propose TrafficSafe, a framework that
adapts LLMs to reframe crash prediction and feature attribution as text-based
reasoning. A multi-modal crash dataset including 58,903 real-world reports
together with belonged infrastructure, environmental, driver, and vehicle
information is collected and textualized into TrafficSafe Event Dataset. By
customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves
a 42% average improvement in F1-score over baselines. To interpret these
predictions and uncover contributing factors, we introduce TrafficSafe
Attribution, a sentence-level feature attribution framework enabling
conditional risk analysis. Findings show that alcohol-impaired driving is the
leading factor in severe crashes, with aggressive and impairment-related
behaviors having nearly twice the contribution for severe crashes compared to
other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal
features during model training, guiding strategic crash data collection for
iterative performance improvements. The proposed TrafficSafe offers a
transformative leap in traffic safety research, providing a blueprint for
translating advanced AI technologies into responsible, actionable, and
life-saving outcomes.

</details>


### [100] [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
*A. Feder Cooper,Aaron Gokaslan,Amy B. Cyphert,Christopher De Sa,Mark A. Lemley,Daniel E. Ho,Percy Liang*

Main category: cs.CL

TL;DR: The paper examines memorization in LLMs and its implications for copyright lawsuits, showing varied memorization levels across models and books.


<details>
  <summary>Details</summary>
Motivation: To clarify the polarized claims about memorization in LLMs and its relevance to copyright law.

Method: Used probabilistic extraction to analyze memorization in 13 open-weight LLMs, focusing on the Books3 dataset.

Result: Memorization varies by model and book; some books are almost entirely memorized (e.g., Harry Potter in Llama 3.1 70B), while most books aren't memorized in larger LLMs.

Conclusion: Findings complicate copyright cases, not clearly favoring plaintiffs or defendants, but highlight nuanced memorization patterns.

Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make
sweeping, opposing claims about the extent to which large language models
(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial
ML and copyright law, we show that these polarized positions dramatically
oversimplify the relationship between memorization and copyright. To do so, we
leverage a recent probabilistic extraction technique to extract pieces of the
Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show
that it's possible to extract substantial parts of at least some books from
different LLMs. This is evidence that the LLMs have memorized the extracted
text; this memorized content is copied inside the model parameters. But the
results are complicated: the extent of memorization varies both by model and by
book. With our specific experiments, we find that the largest LLMs don't
memorize most books -- either in whole or in part. However, we also find that
Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost
entirely. We discuss why our results have significant implications for
copyright cases, though not ones that unambiguously favor either side.

</details>


### [101] [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
*Hiram Ring*

Main category: cs.CL

TL;DR: The paper introduces taggedPBC, a large automatically tagged parallel dataset for crosslinguistic research, addressing limitations of existing datasets by covering 1,500+ languages with 1,800+ sentences. It shows high accuracy and introduces a novel N1 ratio for word order analysis.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for crosslinguistic studies are limited—either covering few languages extensively or many languages minimally. This restricts insights into universal language properties. The paper aims to bridge this gap.

Method: Developed taggedPBC, an automatically tagged parallel dataset with 1,800+ sentences from 1,500+ languages. Evaluated tag accuracy against SOTA taggers and hand-tagged corpora. Introduced the N1 ratio for word order analysis.

Result: TaggedPBC outperforms previous resources in scale and accuracy. The N1 ratio correlates with expert word order classifications, enabling accurate predictions for languages not in existing databases.

Conclusion: TaggedPBC is a significant advancement for corpus-based crosslinguistic research, though further expansion is needed. It is publicly available on GitHub for collaboration.

Abstract: Existing datasets available for crosslinguistic investigations have tended to
focus on large amounts of data for a small group of languages or a small amount
of data for a large number of languages. This means that claims based on these
datasets are limited in what they reveal about universal properties of the
human language faculty. While this has begun to change through the efforts of
projects seeking to develop tagged corpora for a large number of languages,
such efforts are still constrained by limits on resources. The current paper
reports on a large automatically tagged parallel dataset which has been
developed to partially address this issue. The taggedPBC contains more than
1,800 sentences of pos-tagged parallel text data from over 1,500 languages,
representing 133 language families and 111 isolates, dwarfing previously
available resources. The accuracy of tags in this dataset is shown to correlate
well with both existing SOTA taggers for high-resource languages (SpaCy,
Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).
Additionally, a novel measure derived from this dataset, the N1 ratio,
correlates with expert determinations of word order in three typological
databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier
trained on this feature can accurately identify basic word order for languages
not in those databases. While much work is still needed to expand and develop
this dataset, the taggedPBC is an important step to enable corpus-based
crosslinguistic investigations, and is made available for research and
collaboration via GitHub.

</details>


### [102] [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)
*Lekang Jiang,Chengzu Li,Stephan Goetz*

Main category: cs.CL

TL;DR: The paper introduces EPD, a European patent dataset, to address limitations of existing USPTO-based datasets, improving claim generation tasks with jurisdictional diversity, quality, and real-world simulation.


<details>
  <summary>Details</summary>
Motivation: Existing patent claim datasets are limited to USPTO, lacking diversity in legal standards and drafting conventions. EPD aims to fill this gap for European patents.

Method: EPD provides rich textual data and structured metadata for tasks like claim generation. It includes high-quality granted patents and a challenging subset for real-world simulation.

Result: LLMs fine-tuned on EPD outperform those trained on previous datasets and GPT-4o in claim quality and cross-domain generalization. Performance drops on challenging samples.

Conclusion: EPD enhances patent research with jurisdictional diversity and quality, highlighting the need for further work on real-world claim generation challenges.

Abstract: Drafting patent claims is time-intensive, costly, and requires professional
skill. Therefore, researchers have investigated large language models (LLMs) to
assist inventors in writing claims. However, existing work has largely relied
on datasets from the United States Patent and Trademark Office (USPTO). To
enlarge research scope regarding various jurisdictions, drafting conventions,
and legal standards, we introduce EPD, a European patent dataset. EPD presents
rich textual data and structured metadata to support multiple patent-related
tasks, including claim generation. This dataset enriches the field in three
critical aspects: (1) Jurisdictional diversity: Patents from different offices
vary in legal and drafting conventions. EPD fills a critical gap by providing a
benchmark for European patents to enable more comprehensive evaluation. (2)
Quality improvement: EPD offers high-quality granted patents with finalized and
legally approved texts, whereas others consist of patent applications that are
unexamined or provisional. Experiments show that LLMs fine-tuned on EPD
significantly outperform those trained on previous datasets and even GPT-4o in
claim quality and cross-domain generalization. (3) Real-world simulation: We
propose a difficult subset of EPD to better reflect real-world challenges of
claim generation. Results reveal that all tested LLMs perform substantially
worse on these challenging samples, which highlights the need for future
research.

</details>


### [103] [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
*Hanwen Shen,Ting Ying*

Main category: cs.CL

TL;DR: The paper explores how much human-authored outline is needed for LLMs to generate high-quality million-word novels, introducing a two-stage hierarchical pipeline to reduce semantic distortion.


<details>
  <summary>Details</summary>
Motivation: To address the gap in ultra-long novel generation by LLMs, quantifying distortion and optimizing human effort in outlining.

Method: Information-theoretic analysis and a hierarchical two-stage pipeline (outline -> detailed outline -> manuscript) tested on Chinese novels.

Result: A two-stage approach significantly reduces semantic distortion compared to single-stage methods, with an optimal outline length identified.

Conclusion: Provides empirical guidance for authors and researchers using LLMs to create million-word novels.

Abstract: Writing novels with Large Language Models (LLMs) raises a critical question:
how much human-authored outline is necessary to generate high-quality
million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer
have improved stylistic coherence and logical consistency, they primarily
target shorter novels (10k--100k words), leaving ultra-long generation largely
unexplored. Drawing on insights from recent text compression methods like
LLMZip and LLM2Vec, we conduct an information-theoretic analysis that
quantifies distortion occurring when LLMs compress and reconstruct ultra-long
novels under varying compression-expansion ratios. We introduce a hierarchical
two-stage generation pipeline (outline -> detailed outline -> manuscript) and
find an optimal outline length that balances information preservation with
human effort. Through extensive experimentation with Chinese novels, we
establish that a two-stage hierarchical outline approach significantly reduces
semantic distortion compared to single-stage methods. Our findings provide
empirically-grounded guidance for authors and researchers collaborating with
LLMs to create million-word novels.

</details>


### [104] [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
*Omar Mahmoud,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

TL;DR: Steering a single layer in LLMs improves non-English token processing, matching translation baselines and outperforming prompt optimization. Advanced techniques like SFT and RLHF enhance multilingual capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs process non-English tokens and improve their performance in multilingual contexts.

Method: Using representation steering by adding a learned vector to a single layer's activations, combined with SFT and RLHF.

Result: Achieves results comparable to translation baselines and surpasses prompt optimization methods.

Conclusion: Steering layer representations and advanced training techniques significantly enhance multilingual performance in LLMs.

Abstract: In this paper, we investigate how large language models (LLMS) process
non-English tokens within their layer representations, an open question despite
significant advancements in the field. Using representation steering,
specifically by adding a learned vector to a single model layer's activations,
we demonstrate that steering a single model layer can notably enhance
performance. Our analysis shows that this approach achieves results comparable
to translation baselines and surpasses state of the art prompt optimization
methods. Additionally, we highlight how advanced techniques like supervised
fine tuning (\textsc{sft}) and reinforcement learning from human feedback
(\textsc{rlhf}) improve multilingual capabilities by altering representation
spaces. We further illustrate how these methods align with our approach to
reshaping LLMS layer representations.

</details>


### [105] [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)
*Aditeya Baral,Allen George Ajith,Roshan Nayak,Mrityunjay Abhijeet Bhanja*

Main category: cs.CL

TL;DR: CMLFormer, a dual-decoder Transformer, improves performance on code-mixed languages by addressing switching behavior and cross-lingual structure.


<details>
  <summary>Details</summary>
Motivation: Standard language models struggle with code-mixed languages due to frequent language transitions.

Method: Proposes CMLFormer, a multi-layer dual-decoder Transformer with shared encoder and synchronized cross-attention, pre-trained on annotated Hinglish corpus with new objectives.

Result: Outperforms other methods on HASOC-2021 benchmark, showing improved F1, precision, and accuracy. Attention analysis confirms sensitivity to switching points.

Conclusion: CMLFormer's architecture and pre-training strategy effectively model code-mixed languages.

Abstract: Code-mixed languages, characterized by frequent within-sentence language
transitions, present structural challenges that standard language models fail
to address. In this work, we propose CMLFormer, an enhanced multi-layer
dual-decoder Transformer with a shared encoder and synchronized decoder
cross-attention, designed to model the linguistic and semantic dynamics of
code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with
switching point and translation annotations with multiple new objectives
specifically aimed at capturing switching behavior, cross-lingual structure,
and code-mixing complexity. Our experiments show that CMLFormer improves F1
score, precision, and accuracy over other approaches on the HASOC-2021
benchmark under select pre-training setups. Attention analyses further show
that it can identify and attend to switching points, validating its sensitivity
to code-mixed structure. These results demonstrate the effectiveness of
CMLFormer's architecture and multi-task pre-training strategy for modeling
code-mixed languages.

</details>


### [106] [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)
*Sullam Jeoung,Yueyan Chen,Yi Zhang,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: PromptPrism introduces a taxonomy for analyzing prompts in LLMs across functional, semantic, and syntactic levels, with applications in refinement, dataset profiling, and sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: The lack of a systematic framework for prompt analysis in LLMs hinders understanding and optimization of their capabilities.

Method: PromptPrism, a linguistically-inspired taxonomy, is applied to prompt refinement, dataset profiling, and sensitivity analysis.

Result: The taxonomy effectively improves prompt quality, enables comprehensive dataset analysis, and quantifies prompt modifications' impact on LLM performance.

Conclusion: PromptPrism provides a foundational tool for refining, profiling, and analyzing prompts in LLMs.

Abstract: Prompts are the interface for eliciting the capabilities of large language
models (LLMs). Understanding their structure and components is critical for
analyzing LLM behavior and optimizing performance. However, the field lacks a
comprehensive framework for systematic prompt analysis and understanding. We
introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt
analysis across three hierarchical levels: functional structure, semantic
component, and syntactic pattern. We show the practical utility of PromptPrism
by applying it to three applications: (1) a taxonomy-guided prompt refinement
approach that automatically improves prompt quality and enhances model
performance across a range of tasks; (2) a multi-dimensional dataset profiling
method that extracts and aggregates structural, semantic, and syntactic
characteristics from prompt datasets, enabling comprehensive analysis of prompt
distributions and patterns; (3) a controlled experimental framework for prompt
sensitivity analysis by quantifying the impact of semantic reordering and
delimiter modifications on LLM performance. Our experimental results validate
the effectiveness of our taxonomy across these applications, demonstrating that
PromptPrism provides a foundation for refining, profiling, and analyzing
prompts.

</details>


### [107] [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)
*Tiankai Yang,Junjun Liu,Wingchun Siu,Jiahang Wang,Zhuangzhuang Qian,Chanjuan Song,Cheng Cheng,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: AD-AGENT is an LLM-driven multi-agent framework that converts natural-language instructions into executable anomaly detection pipelines, integrating popular libraries for non-experts.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges non-expert users face due to diverse data modalities and specialized AD libraries, requiring in-depth knowledge and programming skills.

Method: AD-AGENT uses specialized agents for intent parsing, data prep, library/model selection, documentation mining, and iterative code generation, leveraging a shared workspace and cache.

Result: The framework reliably generates scripts and recommends competitive models across libraries, as demonstrated in experiments.

Conclusion: AD-AGENT is open-sourced to support further research and practical AD applications, simplifying the process for non-experts.

Abstract: Anomaly detection (AD) is essential in areas such as fraud detection, network
monitoring, and scientific research. However, the diversity of data modalities
and the increasing number of specialized AD libraries pose challenges for
non-expert users who lack in-depth library-specific knowledge and advanced
programming skills. To tackle this, we present AD-AGENT, an LLM-driven
multi-agent framework that turns natural-language instructions into fully
executable AD pipelines. AD-AGENT coordinates specialized agents for intent
parsing, data preparation, library and model selection, documentation mining,
and iterative code generation and debugging. Using a shared short-term
workspace and a long-term cache, the agents integrate popular AD libraries like
PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that
AD-AGENT produces reliable scripts and recommends competitive models across
libraries. The system is open-sourced to support further research and practical
applications in AD.

</details>


### [108] [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)
*Shujauddin Syed,Ted Pedersen*

Main category: cs.CL

TL;DR: The paper presents a TF-IDF-based retrieval system for multilingual fact-checked claim retrieval, achieving competitive results but lagging behind top neural methods.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of traditional TF-IDF methods in multilingual retrieval tasks, especially under limited compute resources.

Method: Implemented a TF-IDF-based system with experiments on vector dimensions and tokenization strategies, using word-level tokenization and a 15,000-feature vocabulary.

Result: Achieved 0.78 success@10 on the dev set and 0.69 on the test set across ten languages, with better performance on higher-resource languages.

Conclusion: Optimized traditional methods like TF-IDF remain viable baselines, though neural architectures dominate in multilingual retrieval.

Abstract: This paper presents the Duluth approach to the SemEval-2025 Task 7 on
Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a
TF-IDF-based retrieval system with experimentation on vector dimensions and
tokenization strategies. Our best-performing configuration used word-level
tokenization with a vocabulary size of 15,000 features, achieving an average
success@10 score of 0.78 on the development set and 0.69 on the test set across
ten languages. Our system showed stronger performance on higher-resource
languages but still lagged significantly behind the top-ranked system, which
achieved 0.96 average success@10. Our findings suggest that though advanced
neural architectures are increasingly dominant in multilingual retrieval tasks,
properly optimized traditional methods like TF-IDF remain competitive
baselines, especially in limited compute resource scenarios.

</details>


### [109] [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)
*João Eduardo Batista,Emil Vatai,Mohamed Wahib*

Main category: cs.CL

TL;DR: The paper addresses the challenge of unreliable outputs in LLMs by proposing a sentence-level pre-attribution step for RAG systems to improve accuracy and traceability.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack trustworthy, verifiable outputs, hindering their adoption in scientific and high-stakes settings due to unreliable source attribution.

Method: A sentence-level pre-attribution step classifies sentences into three categories (not attributable, attributable to a single quote, or multiple quotes) to optimize attribution methods.

Result: Classifiers are effective for this task, and the proposed system reduces computational complexity while providing a clean dataset (HAGRID) and an end-to-end solution.

Conclusion: The pre-attribution step enhances the reliability and efficiency of LLM outputs, making them more suitable for scientific and high-stakes applications.

Abstract: Large Language Models (LLMs) are increasingly applied in various science
domains, yet their broader adoption remains constrained by a critical
challenge: the lack of trustworthy, verifiable outputs. Current LLMs often
generate answers without reliable source attribution, or worse, with incorrect
attributions, posing a barrier to their use in scientific and high-stakes
settings, where traceability and accountability are non-negotiable. To be
reliable, attribution systems need high accuracy and retrieve data with short
lengths, i.e., attribute to a sentence within a document rather than a whole
document. We propose a sentence-level pre-attribution step for
Retrieve-Augmented Generation (RAG) systems that classify sentences into three
categories: not attributable, attributable to a single quote, and attributable
to multiple quotes. By separating sentences before attribution, a proper
attribution method can be selected for the type of sentence, or the attribution
can be skipped altogether. Our results indicate that classifiers are
well-suited for this task. In this work, we propose a pre-attribution step to
reduce the computational complexity of attribution, provide a clean version of
the HAGRID dataset, and provide an end-to-end attribution system that works out
of the box.

</details>


### [110] [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)
*Ali Naseh,Harsh Chaudhari,Jaechul Roh,Mingshi Wu,Alina Oprea,Amir Houmansadr*

Main category: cs.CL

TL;DR: DeepSeek's R1 LLM shows censorship-like behavior on politically sensitive topics, unlike other models. The paper analyzes its patterns, triggers, and proposes bypass techniques, raising concerns about transparency and bias.


<details>
  <summary>Details</summary>
Motivation: To investigate R1's unique censorship behavior on politically sensitive topics compared to other LLMs, and understand its implications.

Method: Curated prompts to identify censored topics, analyzed patterns, triggers, and variations across languages and distilled models. Proposed bypass techniques.

Result: R1 exhibits consistent censorship, likely due to training or alignment choices, with concerns about transparency and bias.

Conclusion: R1's censorship behavior highlights governance and transparency issues in LLM deployment, necessitating further scrutiny.

Abstract: DeepSeek recently released R1, a high-performing large language model (LLM)
optimized for reasoning tasks. Despite its efficient training pipeline, R1
achieves competitive performance, even surpassing leading reasoning models like
OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1
refuses to answer certain prompts related to politically sensitive topics in
China. While existing LLMs often implement safeguards to avoid generating
harmful or offensive outputs, R1 represents a notable shift - exhibiting
censorship-like behavior on politically charged queries. In this paper, we
investigate this phenomenon by first introducing a large-scale set of heavily
curated prompts that get censored by R1, covering a range of politically
sensitive topics, but are not censored by other models. We then conduct a
comprehensive analysis of R1's censorship patterns, examining their
consistency, triggers, and variations across topics, prompt phrasing, and
context. Beyond English-language queries, we explore censorship behavior in
other languages. We also investigate the transferability of censorship to
models distilled from the R1 language model. Finally, we propose techniques for
bypassing or removing this censorship. Our findings reveal possible additional
censorship integration likely shaped by design choices during training or
alignment, raising concerns about transparency, bias, and governance in
language model deployment.

</details>


### [111] [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)
*Jiakuan Xie,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: The paper identifies 'superficial editing' in knowledge editing of language models, where models still generate original knowledge despite high performance on metrics. It pinpoints two key factors and extends findings to superficial unlearning.


<details>
  <summary>Details</summary>
Motivation: To address the deceptive nature of knowledge editing in language models, where conventional metrics fail to capture persistent original knowledge generation.

Method: Comprehensive evaluation and systematic investigation to identify factors like residual streams and specific attention modules contributing to superficial editing.

Result: Identified residual streams and specific attention heads with left singular vectors as key contributors to superficial editing, validated through analysis.

Conclusion: The findings highlight challenges in knowledge editing and demonstrate robustness across tasks like superficial unlearning, with broader implications for model behavior.

Abstract: Knowledge editing, which aims to update the knowledge encoded in language
models, can be deceptive. Despite the fact that many existing knowledge editing
algorithms achieve near-perfect performance on conventional metrics, the models
edited by them are still prone to generating original knowledge. This paper
introduces the concept of "superficial editing" to describe this phenomenon.
Our comprehensive evaluation reveals that this issue presents a significant
challenge to existing algorithms. Through systematic investigation, we identify
and validate two key factors contributing to this issue: (1) the residual
stream at the last subject position in earlier layers and (2) specific
attention modules in later layers. Notably, certain attention heads in later
layers, along with specific left singular vectors in their output matrices,
encapsulate the original knowledge and exhibit a causal relationship with
superficial editing. Furthermore, we extend our analysis to the task of
superficial unlearning, where we observe consistent patterns in the behavior of
specific attention heads and their corresponding left singular vectors, thereby
demonstrating the robustness and broader applicability of our methodology and
conclusions. Our code is available here.

</details>


### [112] [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)
*Yuxin Lin,Yinglin Zheng,Ming Zeng,Wangzheng Shi*

Main category: cs.CL

TL;DR: The paper introduces a multi-modal dataset (MM-F2F) and an end-to-end framework for predicting turn-taking and backchannel actions in human-machine conversations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap in predicting turn-taking and backchannel actions using multi-modal signals (linguistic, acoustic, visual) and overcome dataset limitations.

Method: Proposes an automatic data collection pipeline to create the MM-F2F dataset (210 hours of videos, 1.5M words, 20M frames) and an end-to-end framework for multi-modal prediction.

Result: Achieves a 10% F1-score increase in turn-taking and 33% in backchannel prediction, outperforming existing methods.

Conclusion: The dataset and framework advance multi-modal conversation research, with publicly available resources for further study.

Abstract: This paper addresses the gap in predicting turn-taking and backchannel
actions in human-machine conversations using multi-modal signals (linguistic,
acoustic, and visual). To overcome the limitation of existing datasets, we
propose an automatic data collection pipeline that allows us to collect and
annotate over 210 hours of human conversation videos. From this, we construct a
Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over
1.5M words and corresponding turn-taking and backchannel annotations from
approximately 20M frames. Additionally, we present an end-to-end framework that
predicts the probability of turn-taking and backchannel actions from
multi-modal signals. The proposed model emphasizes the interrelation between
modalities and supports any combination of text, audio, and video inputs,
making it adaptable to a variety of realistic scenarios. Our experiments show
that our approach achieves state-of-the-art performance on turn-taking and
backchannel prediction tasks, achieving a 10\% increase in F1-score on
turn-taking and a 33\% increase on backchannel prediction. Our dataset and code
are publicly available online to ease of subsequent research.

</details>


### [113] [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)
*Xukai Liu,Ye Liu,Shiwen Wu,Yanghai Zhang,Yihao Yuan,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: Know3-RAG improves Retrieval-Augmented Generation (RAG) by integrating structured knowledge from knowledge graphs (KGs) to enhance retrieval, generation, and filtering, reducing hallucinations and boosting reliability.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing RAG systems: unreliable adaptive control and hallucinations due to inaccurate references.

Method: Proposes Know3-RAG, a framework using KG embeddings for adaptive retrieval, KG-derived entities for enriched generation, and semantic filtering for accuracy.

Result: Outperforms baselines on QA benchmarks, reducing hallucinations and improving answer reliability.

Conclusion: Know3-RAG effectively leverages structured knowledge to enhance RAG systems' factual reliability.

Abstract: Recent advances in large language models (LLMs) have led to impressive
progress in natural language generation, yet their tendency to produce
hallucinated or unsubstantiated content remains a critical concern. To improve
factual reliability, Retrieval-Augmented Generation (RAG) integrates external
knowledge during inference. However, existing RAG systems face two major
limitations: (1) unreliable adaptive control due to limited external knowledge
supervision, and (2) hallucinations caused by inaccurate or irrelevant
references. To address these issues, we propose Know3-RAG, a knowledge-aware
RAG framework that leverages structured knowledge from knowledge graphs (KGs)
to guide three core stages of the RAG process, including retrieval, generation,
and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval
module that employs KG embedding to assess the confidence of the generated
answer and determine retrieval necessity, a knowledge-enhanced reference
generation strategy that enriches queries with KG-derived entities to improve
generated reference relevance, and a knowledge-driven reference filtering
mechanism that ensures semantic alignment and factual accuracy of references.
Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG
consistently outperforms strong baselines, significantly reducing
hallucinations and enhancing answer reliability.

</details>


### [114] [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
*Taiqiang Wu,Runming Yang,Jiayi Li,Pengfei Hu,Ngai Wong,Yujiu Yang*

Main category: cs.CL

TL;DR: Shadow-FT is a novel framework for fine-tuning INSTRUCT models by leveraging BASE models, avoiding performance degeneration and outperforming traditional tuning methods.


<details>
  <summary>Details</summary>
Motivation: Direct fine-tuning of INSTRUCT models often yields marginal improvements or performance drops, while BASE models contain similar weights, suggesting untapped potential.

Method: Shadow-FT fine-tunes the BASE model and grafts the learned weight updates to the INSTRUCT model, introducing no extra parameters.

Result: Shadow-FT consistently outperforms conventional tuning methods across 19 benchmarks, including coding, reasoning, and mathematical tasks.

Conclusion: Shadow-FT is effective, easy to implement, and applicable to multimodal LLMs and DPO, with code and weights publicly available.

Abstract: Large language models (LLMs) consistently benefit from further fine-tuning on
various tasks. However, we observe that directly tuning the INSTRUCT (i.e.,
instruction tuned) models often leads to marginal improvements and even
performance degeneration. Notably, paired BASE models, the foundation for these
INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on
average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to
tune the INSTRUCT models by leveraging the corresponding BASE models. The key
insight is to fine-tune the BASE model, and then directly graft the learned
weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no
additional parameters, is easy to implement, and significantly improves
performance. We conduct extensive experiments on tuning mainstream LLMs, such
as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering
coding, reasoning, and mathematical tasks. Experimental results demonstrate
that Shadow-FT consistently outperforms conventional full-parameter and
parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT
can be applied to multimodal large language models (MLLMs) and combined with
direct preference optimization (DPO). Codes and weights are available at
\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.

</details>


### [115] [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)
*Haoyuan Wu,Xueyi Chen,Rui Ming,Jilong Gao,Shoubo Hu,Zhuolun He,Bei Yu*

Main category: cs.CL

TL;DR: The paper introduces tree-of-thoughts RL (ToTRL), a reinforcement learning framework that enhances LLMs' reasoning by transitioning from sequential chain-of-thought (CoT) to parallel tree-of-thoughts (ToT) strategies, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of prolonged CoT reasoning in LLMs, such as verbose outputs and trial-and-error methods, by leveraging the more systematic ToT approach.

Method: Proposes ToTRL, an on-policy RL framework with rule-based rewards, guiding LLMs to adopt parallel ToT reasoning. Uses puzzle games to train LLMs in exploring interdependent choices and constraints.

Result: The ToTQwen3-8B model, trained with ToTRL, shows significant improvements in performance and reasoning efficiency on complex tasks.

Conclusion: ToTRL effectively enhances LLMs' reasoning capabilities by combining CoT and ToT strategies, offering a scalable and efficient solution for complex reasoning tasks.

Abstract: Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and pruning of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.

</details>


### [116] [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)
*Jingyang Peng,Wenyuan Shen,Jiarui Rao,Jionghao Lin*

Main category: cs.CL

TL;DR: The paper proposes an automated method to detect biases in AI-generated educational content, showing high reliability compared to manual assessment.


<details>
  <summary>Details</summary>
Motivation: To address ethical concerns about biases (e.g., gender, racial stereotypes) in AI-generated educational materials, which lack systematic evaluation methods.

Method: Integrates the Contextualized Embedding Association Test with prompt-engineered word extraction in a Retrieval-Augmented Generation framework.

Result: High alignment (r = 0.993) between automated and manual bias assessments, demonstrating reliability and consistency.

Conclusion: The method improves fairness, scalability, and reproducibility in auditing AI-generated educational content, reducing human subjectivity.

Abstract: Recent advances in Generative Artificial Intelligence (GenAI) have
transformed educational content creation, particularly in developing tutor
training materials. However, biases embedded in AI-generated content--such as
gender, racial, or national stereotypes--raise significant ethical and
educational concerns. Despite the growing use of GenAI, systematic methods for
detecting and evaluating such biases in educational materials remain limited.
This study proposes an automated bias assessment approach that integrates the
Contextualized Embedding Association Test with a prompt-engineered word
extraction method within a Retrieval-Augmented Generation framework. We applied
this method to AI-generated texts used in tutor training lessons. Results show
a high alignment between the automated and manually curated word sets, with a
Pearson correlation coefficient of r = 0.993, indicating reliable and
consistent bias assessment. Our method reduces human subjectivity and enhances
fairness, scalability, and reproducibility in auditing GenAI-produced
educational content.

</details>


### [117] [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
*Haoyuan Wu,Rui Ming,Jilong Gao,Hangyu Zhao,Xueyi Chen,Yikai Yang,Haisheng Zheng,Zhuolun He,Bei Yu*

Main category: cs.CL

TL;DR: The paper addresses performance disparities in LLMs for code generation by using code translation tasks and a novel RL framework (OORL) with GEPO, improving cross-language coding proficiency.


<details>
  <summary>Details</summary>
Motivation: To reduce the performance gap in LLMs between popular and less common programming languages by enhancing their ability to transfer coding skills across languages.

Method: Proposes OORL, an RL framework combining on-policy and off-policy strategies, and GEPO for preference optimization using intermediate representations (IRs) to capture code functionality nuances.

Result: Significant performance improvements on code benchmarks across multiple programming languages.

Conclusion: OORL and GEPO effectively enhance LLMs' cross-language coding proficiency and functionality recognition.

Abstract: Large language models (LLMs) achieve remarkable performance in code
generation tasks. However, a significant performance disparity persists between
popular programming languages (e.g., Python, C++) and others. To address this
capability gap, we leverage the code translation task to train LLMs, thereby
facilitating the transfer of coding proficiency across diverse programming
languages. Moreover, we introduce OORL for training, a novel reinforcement
learning (RL) framework that integrates on-policy and off-policy strategies.
Within OORL, on-policy RL is applied during code translation, guided by a
rule-based reward signal derived from unit tests. Complementing this
coarse-grained rule-based reward, we propose Group Equivalent Preference
Optimization (GEPO), a novel preference optimization method. Specifically, GEPO
trains the LLM using intermediate representations (IRs) groups. LLMs can be
guided to discern IRs equivalent to the source code from inequivalent ones,
while also utilizing signals about the mutual equivalence between IRs within
the group. This process allows LLMs to capture nuanced aspects of code
functionality. By employing OORL for training with code translation tasks, LLMs
improve their recognition of code functionality and their understanding of the
relationships between code implemented in different languages. Extensive
experiments demonstrate that our OORL for LLMs training with code translation
tasks achieves significant performance improvements on code benchmarks across
multiple programming languages.

</details>


### [118] [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
*Han Meng,Yancan Chen,Yunan Li,Yitian Yang,Jungup Lee,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.CL

TL;DR: A theory-informed, expert-annotated corpus of human-chatbot interviews is introduced to improve neural models for detecting mental-health stigma, addressing gaps in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Mental-health stigma hinders treatment-seeking and recovery, and current datasets lack theoretical grounding and diversity.

Method: Created a corpus of 4,141 snippets from 684 participants with socio-cultural backgrounds, annotated by experts, and benchmarked state-of-the-art neural models.

Result: The dataset and experiments highlight challenges in stigma detection, providing a resource for future research.

Conclusion: This corpus advances computational efforts to detect and counteract mental-health stigma.

Abstract: Mental-health stigma remains a pervasive social problem that hampers
treatment-seeking and recovery. Existing resources for training neural models
to finely classify such stigma are limited, relying primarily on social-media
or synthetic data without theoretical underpinnings. To remedy this gap, we
present an expert-annotated, theory-informed corpus of human-chatbot
interviews, comprising 4,141 snippets from 684 participants with documented
socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural
models and empirically unpack the challenges of stigma detection. This dataset
can facilitate research on computationally detecting, neutralizing, and
counteracting mental-health stigma.

</details>


### [119] [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)
*Yaxun Dai,Wenxuan Xie,Xialie Zhuang,Tianyu Yang,Yiying Yang,Haiqin Yang,Yuhang Zhao,Pingfu Chao,Wenhao Jiang*

Main category: cs.CL

TL;DR: ReEx-SQL integrates execution feedback into Text-to-SQL generation, improving accuracy and robustness by dynamically adjusting reasoning during decoding.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat execution feedback as post-hoc, limiting their ability to correct reasoning errors in real-time.

Method: ReEx-SQL uses execution-aware reinforcement learning, structured prompts, and tree-based decoding to interleave SQL execution into reasoning paths.

Result: Achieves 88.8% on Spider and 64.9% on BIRD, outperforming baselines by 2.7% and 2.6%, respectively, with 51.9% faster inference.

Conclusion: ReEx-SQL enhances Text-to-SQL performance by integrating execution feedback dynamically, demonstrating superior accuracy and efficiency.

Abstract: In Text-to-SQL, execution feedback is essential for guiding large language
models (LLMs) to reason accurately and generate reliable SQL queries. However,
existing methods treat execution feedback solely as a post-hoc signal for
correction or selection, failing to integrate it into the generation process.
This limitation hinders their ability to address reasoning errors as they
occur, ultimately reducing query accuracy and robustness. To address this
issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement
Learning), a framework for Text-to-SQL that enables models to interact with the
database during decoding and dynamically adjust their reasoning based on
execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm
that interleaves intermediate SQL execution into reasoning paths, facilitating
context-sensitive revisions. It achieves this through structured prompts with
markup tags and a stepwise rollout strategy that integrates execution feedback
into each stage of generation. To supervise policy learning, we develop a
composite reward function that includes an exploration reward, explicitly
encouraging effective database interaction. Additionally, ReEx-SQL adopts a
tree-based decoding strategy to support exploratory reasoning, enabling dynamic
expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on
Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning
baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving
85.2% on Spider-Realistic with leading performance. In addition, its
tree-structured decoding improves efficiency and performance over linear
decoding, reducing inference time by 51.9% on the BIRD development set.

</details>


### [120] [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)
*Jitai Hao,Qiang Huang,Hao Liu,Xinyan Xiao,Zhaochun Ren,Jun Yu*

Main category: cs.CL

TL;DR: LRC is a pre-training method for Small Language Models (SLMs) that improves efficiency and performance by soft pruning and aligning activations with teacher models, achieving 1,000x training efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like information loss, inefficient alignment, and underutilized activations in training SLMs.

Method: Uses low-rank projection matrices for soft pruning and activation cloning, aligning student and teacher activations.

Result: Matches or surpasses state-of-the-art models with 20B tokens, achieving 1,000x training efficiency.

Conclusion: LRC effectively transfers knowledge from teacher models to SLMs, significantly improving training efficiency and performance.

Abstract: Training high-performing Small Language Models (SLMs) remains costly, even
with knowledge distillation and pruning from larger teacher models. Existing
work often faces three key challenges: (1) information loss from hard pruning,
(2) inefficient alignment of representations, and (3) underutilization of
informative activations, particularly from Feed-Forward Networks (FFNs). To
address these challenges, we introduce Low-Rank Clone (LRC), an efficient
pre-training method that constructs SLMs aspiring to behavioral equivalence
with strong teacher models. LRC trains a set of low-rank projection matrices
that jointly enable soft pruning by compressing teacher weights, and activation
clone by aligning student activations, including FFN signals, with those of the
teacher. This unified design maximizes knowledge transfer while removing the
need for explicit alignment modules. Extensive experiments with open-source
teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC
matches or surpasses state-of-the-art models trained on trillions of
tokens--while using only 20B tokens, achieving over 1,000x training efficiency.
Our codes and model checkpoints are available at
https://github.com/CURRENTF/LowRankClone and
https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.

</details>


### [121] [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)
*Wenhao Zhu,Yuhang Xie,Guojie Song,Xin Zhang*

Main category: cs.CL

TL;DR: EAVIT is a framework combining local and online LLMs for efficient human value identification, reducing input tokens and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation and high computational costs of online LLMs in long-context value identification tasks.

Method: Uses a local value detector for initial estimations, constructs concise prompts for online LLMs, and employs explanation-based training and data generation.

Result: Reduces input tokens by up to 1/6 and outperforms traditional NLP and other LLM-based methods.

Conclusion: EAVIT offers an efficient and accurate solution for human value identification by leveraging hybrid LLM strengths.

Abstract: The rapid evolution of large language models (LLMs) has revolutionized
various fields, including the identification and discovery of human values
within text data. While traditional NLP models, such as BERT, have been
employed for this task, their ability to represent textual data is
significantly outperformed by emerging LLMs like GPTs. However, the performance
of online LLMs often degrades when handling long contexts required for value
identification, which also incurs substantial computational costs. To address
these challenges, we propose EAVIT, an efficient and accurate framework for
human value identification that combines the strengths of both locally
fine-tunable and online black-box LLMs. Our framework employs a value detector
- a small, local language model - to generate initial value estimations. These
estimations are then used to construct concise input prompts for online LLMs,
enabling accurate final value identification. To train the value detector, we
introduce explanation-based training and data generation techniques
specifically tailored for value identification, alongside sampling strategies
to optimize the brevity of LLM input prompts. Our approach effectively reduces
the number of input tokens by up to 1/6 compared to directly querying online
LLMs, while consistently outperforming traditional NLP methods and other
LLM-based strategies.

</details>


### [122] [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)
*Yanbin Yin,Kun Zhou,Zhen Wang,Xiangdong Zhang,Yifei Shao,Shibo Hao,Yi Gu,Jieyuan Liu,Somanshu Singla,Tianyang Liu,Eric P. Xing,Zhengzhong Liu,Haojian Jin,Zhiting Hu*

Main category: cs.CL

TL;DR: Decentralized Arena (dearena) is an automated framework for benchmarking LLMs using collective intelligence, reducing bias and cost while maintaining high correlation with human judgments.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking methods for LLMs face trade-offs between scalability, reliability, and cost, with issues like saturation, human judge dependency, and single-model bias.

Method: dearena employs democratic pairwise evaluation, a coarse-to-fine ranking algorithm, and automatic question selection to efficiently benchmark LLMs.

Result: Experiments with 66 LLMs show dearena achieves up to 97% correlation with human judgments and reduces costs significantly.

Conclusion: dearena offers a scalable, cost-effective, and unbiased solution for LLM benchmarking, with plans for public release of code and data.

Abstract: The recent explosion of large language models (LLMs), each with its own
general or specialized strengths, makes scalable, reliable benchmarking more
urgent than ever. Standard practices nowadays face fundamental trade-offs:
closed-ended question-based benchmarks (eg MMLU) struggle with saturation as
newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely
on costly and slow human judges. Recently, automated methods (eg
LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one
or a few "authority" models. To tackle these issues, we propose Decentralized
Arena (dearena), a fully automated framework leveraging collective intelligence
from all LLMs to evaluate each other. It mitigates single-model judge bias by
democratic, pairwise evaluation, and remains efficient at scale through two key
components: (1) a coarse-to-fine ranking algorithm for fast incremental
insertion of new models with sub-quadratic complexity, and (2) an automatic
question selection strategy for the construction of new evaluation dimensions.
Across extensive experiments across 66 LLMs, dearena attains up to 97%
correlation with human judgements, while significantly reducing the cost. Our
code and data will be publicly released on
https://github.com/maitrix-org/de-arena.

</details>


### [123] [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)
*Xilong Cheng,Yunxiao Qin,Yuting Tan,Zhengnan Li,Ye Wang,Hongjiang Xiao,Yuan Zhang*

Main category: cs.CL

TL;DR: PsyMem enhances LLM-based role-playing by integrating psychological attributes and explicit memory control, outperforming baselines in human-likeness and character fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing methods inadequately model character dimensions and memory consistency, limiting reliability in applications like social simulation.

Method: PsyMem uses 26 psychological indicators and memory alignment training on a dataset of 5,414 characters and 38,962 dialogues.

Result: PsyMem-Qwen, trained on Qwen2.5-7B-Instruct, achieves superior performance in role-playing.

Conclusion: PsyMem addresses key limitations, improving role-playing reliability and fidelity.

Abstract: Existing LLM-based role-playing methods often rely on superficial textual
descriptions or simplistic metrics, inadequately modeling both intrinsic and
extrinsic character dimensions. Additionally, they typically simulate character
memory with implicit model knowledge or basic retrieval augment generation
without explicit memory alignment, compromising memory consistency. The two
issues weaken reliability of role-playing LLMs in several applications, such as
trustworthy social simulation. To address these limitations, we propose PsyMem,
a novel framework integrating fine-grained psychological attributes and
explicit memory control for role-playing. PsyMem supplements textual
descriptions with 26 psychological indicators to detailed model character.
Additionally, PsyMem implements memory alignment training, explicitly trains
the model to align character's response with memory, thereby enabling dynamic
memory-controlled responding during inference. By training Qwen2.5-7B-Instruct
on our specially designed dataset (including 5,414 characters and 38,962
dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,
outperforms baseline models in role-playing, achieving the best performance in
human-likeness and character fidelity.

</details>


### [124] [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)
*Han Sun,Zhen Sun,Zongmin Zhang,Linzhao Jia,Wei Shao,Min Zhang*

Main category: cs.CL

TL;DR: SynDec improves LLM-based textual style transfer by automating prompt synthesis and amplifying their impact during decoding, outperforming existing methods on most benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in arbitrary style transfer due to reliance on manual prompts and inherent stylistic biases.

Method: SynDec synthesizes high-quality prompts through representative sample selection, style analysis, and reranking, then amplifies their effect during decoding by maximizing output probability contrasts.

Result: SynDec outperforms state-of-the-art methods on five out of six benchmarks, with up to 9% accuracy improvement.

Conclusion: SynDec is effective for enhancing LLM-based style transfer, validated by extensive experiments and ablation studies.

Abstract: Large Language Models (LLMs) are emerging as dominant forces for textual
style transfer. However, for arbitrary style transfer, LLMs face two key
challenges: (1) considerable reliance on manually-constructed prompts and (2)
rigid stylistic biases inherent in LLMs. In this paper, we propose a novel
Synthesize-then-Decode (SynDec) approach, which automatically synthesizes
high-quality prompts and amplifies their roles during decoding process.
Specifically, our approach synthesizes prompts by selecting representative
few-shot samples, conducting a four-dimensional style analysis, and reranking
the candidates. At LLM decoding stage, the TST effect is amplified by
maximizing the contrast in output probabilities between scenarios with and
without the synthesized prompt, as well as between prompts and negative
samples. We conduct extensive experiments and the results show that SynDec
outperforms existing state-of-the-art LLM-based methods on five out of six
benchmarks (e.g., achieving up to a 9\% increase in accuracy for
modern-to-Elizabethan English transfer). Detailed ablation studies further
validate the effectiveness of SynDec.

</details>


### [125] [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)
*Zifeng Cheng,Zhonghui Wang,Yuchen Fu,Zhiwei Jiang,Yafeng Yin,Cong Wang,Qing Gu*

Main category: cs.CL

TL;DR: The paper proposes Contrastive Prompting (CP), a method to improve sentence embeddings from LLMs by using an auxiliary prompt to focus on core semantics, avoiding non-essential information.


<details>
  <summary>Details</summary>
Motivation: Existing methods for extracting sentence embeddings from LLMs often encode excess non-essential information (e.g., stop words) in the last token, limiting their effectiveness.

Method: Contrastive Prompting (CP) introduces an auxiliary prompt to contrast with the main prompt, steering it to encode core semantics. CP is plug-and-play and works with various prompt-based methods.

Result: CP improves performance on Semantic Textual Similarity (STS) and downstream classification tasks across different LLMs.

Conclusion: CP effectively enhances sentence embeddings by focusing on core semantics, outperforming existing prompt-based methods.

Abstract: Extracting sentence embeddings from large language models (LLMs) is a
practical direction, as it requires neither additional data nor fine-tuning.
Previous studies usually focus on prompt engineering to guide LLMs to encode
the core semantic information of the sentence into the embedding of the last
token. However, the last token in these methods still encodes an excess of
non-essential information, such as stop words, limiting its encoding capacity.
To this end, we propose a Contrastive Prompting (CP) method that introduces an
extra auxiliary prompt to elicit better sentence embedding. By contrasting with
the auxiliary prompt, CP can steer existing prompts to encode the core
semantics of the sentence, rather than non-essential information. CP is a
plug-and-play inference-time intervention method that can be combined with
various prompt-based methods. Extensive experiments on Semantic Textual
Similarity (STS) tasks and downstream classification tasks demonstrate that our
method can improve the performance of existing prompt-based methods across
different LLMs. Our code will be released at https://github.com/zifengcheng/CP.

</details>


### [126] [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)
*Hengxing Cai,Jinhan Dong,Jingjun Tan,Jingcheng Deng,Sihang Li,Zhifeng Gao,Haidong Wang,Zicheng Su,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: FlightGPT is a UAV VLN framework using VLMs, with a two-stage training pipeline (SFT and GRPO) and CoT reasoning, achieving a 9.22% higher success rate in unseen environments.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like weak multimodal fusion, poor generalization, and interpretability in UAV VLN for applications like disaster response and logistics.

Method: Two-stage training: SFT for initialization and reasoning, followed by GRPO for generalization. Uses CoT for interpretable decision-making.

Result: Achieves state-of-the-art performance, with a 9.22% higher success rate in unseen environments on CityNav dataset.

Conclusion: FlightGPT effectively improves UAV VLN performance, generalization, and interpretability, making it suitable for real-world applications.

Abstract: Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital
for applications such as disaster response, logistics delivery, and urban
inspection. However, existing methods often struggle with insufficient
multimodal fusion, weak generalization, and poor interpretability. To address
these challenges, we propose FlightGPT, a novel UAV VLN framework built upon
Vision-Language Models (VLMs) with powerful multimodal perception capabilities.
We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)
using high-quality demonstrations to improve initialization and structured
reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by
a composite reward that considers goal accuracy, reasoning quality, and format
compliance, to enhance generalization and adaptability. Furthermore, FlightGPT
introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve
decision interpretability. Extensive experiments on the city-scale dataset
CityNav demonstrate that FlightGPT achieves state-of-the-art performance across
all scenarios, with a 9.22\% higher success rate than the strongest baseline in
unseen environments. Our implementation is publicly available.

</details>


### [127] [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)
*Christian Braun,Alexander Lilienbeck,Daniel Mentjukov*

Main category: cs.CL

TL;DR: The paper explores how input text structure and prompt engineering affect GPT-4o and GPT-4.1's performance on legal QA tasks, finding GPT-4.1 benefits more from structured inputs and optimized prompts.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of text structure and prompt design on LLM performance in legal contexts, where structure is semantically vital.

Method: Compared exact-match accuracy of GPT-4o and GPT-4.1 across various input formats (plain-text, OCR-extracted, GPT-4o Vision text/Markdown) and prompt variations.

Result: GPT-4o is robust but underperforms; GPT-4.1 excels with structured inputs (20% accuracy boost) and optimized prompts (additional 10-13% boost), with Markdown performing best (79% accuracy).

Conclusion: Input structure and prompt engineering are crucial for optimizing LLM performance in legal tasks, even for newer, more resilient models.

Abstract: Legal contracts possess an inherent, semantically vital structure (e.g.,
sections, clauses) that is crucial for human comprehension but whose impact on
LLM processing remains under-explored. This paper investigates the effects of
explicit input text structure and prompt engineering on the performance of
GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the
CUAD. We compare model exact-match accuracy across various input formats:
well-structured plain-text (human-generated from CUAD), plain-text cleaned of
line breaks, extracted plain-text from Azure OCR, plain-text extracted by
GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o
Vision. To give an indication of the impact of possible prompt engineering, we
assess the impact of shifting task instructions to the system prompt and
explicitly informing the model about the structured nature of the input. Our
findings reveal that GPT-4o demonstrates considerable robustness to variations
in input structure, but lacks in overall performance. Conversely, GPT-4.1's
performance is markedly sensitive; poorly structured inputs yield suboptimal
results (but identical with GPT-4o), while well-structured formats (original
CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by
~20 percentage points. Optimizing the system prompt to include task details and
an advisory about structured input further elevates GPT-4.1's accuracy by an
additional ~10-13 percentage points, with Markdown ultimately achieving the
highest performance under these conditions (79 percentage points overall
exact-match accuracy). This research empirically demonstrates that while newer
models exhibit greater resilience, careful input structuring and strategic
prompt design remain critical for optimizing the performance of LLMs, and can
significantly affect outcomes in high-stakes legal applications.

</details>


### [128] [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)
*Lucas Georges Gabriel Charpentier,Pierre Lison*

Main category: cs.CL

TL;DR: A novel RAG-inspired method is proposed to re-identify masked PII in documents by leveraging background knowledge, achieving up to 80% success rate.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness of de-identification methods by reversing the process (re-identification) using background knowledge.

Method: A two-step approach: (1) a retriever selects relevant passages from a background knowledge database, and (2) an infilling model infers the original masked content.

Result: Up to 80% of masked PII spans can be recovered, with accuracy improving as background knowledge increases.

Conclusion: Current de-identification methods may not be robust enough against re-identification attacks, especially with ample background knowledge.

Abstract: Documents revealing sensitive information about individuals must typically be
de-identified. This de-identification is often done by masking all mentions of
personally identifiable information (PII), thereby making it more difficult to
uncover the identity of the person(s) in question. To investigate the
robustness of de-identification methods, we present a novel, RAG-inspired
approach that attempts the reverse process of re-identification based on a
database of documents representing background knowledge. Given a text in which
personal identifiers have been masked, the re-identification proceeds in two
steps. A retriever first selects from the background knowledge passages deemed
relevant for the re-identification. Those passages are then provided to an
infilling model which seeks to infer the original content of each text span.
This process is repeated until all masked spans are replaced. We evaluate the
re-identification on three datasets (Wikipedia biographies, court rulings and
clinical notes). Results show that (1) as many as 80% of de-identified text
spans can be successfully recovered and (2) the re-identification accuracy
increases along with the level of background knowledge.

</details>


### [129] [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
*Yu Fan,Jingwei Ni,Jakob Merane,Etienne Salimbeni,Yang Tian,Yoan Hermstrüwer,Yinya Huang,Mubashara Akhtar,Florian Geering,Oliver Dreyer,Daniel Brunner,Markus Leippold,Mrinmaya Sachan,Alexander Stremitzer,Christoph Engel,Elliott Ash,Joel Niklaus*

Main category: cs.CL

TL;DR: LEXam is a new benchmark for evaluating LLMs on legal reasoning, featuring 4,886 law exam questions (2,841 open-ended, 2,045 multiple-choice) in English and German. Current LLMs struggle with structured, multi-step legal reasoning, especially in open questions. The dataset effectively differentiates model capabilities and introduces a scalable evaluation method using LLM-as-a-Judge.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long-form legal reasoning in LLMs, which remains difficult despite advancements in test-time scaling.

Method: Introduces LEXam, a benchmark with 340 law exams (4,886 questions), including open-ended and multiple-choice questions, accompanied by reference answers and reasoning guidance. Evaluates LLMs using an LLM-as-a-Judge paradigm with human validation.

Result: LLMs struggle with open-ended questions requiring structured legal reasoning. The dataset effectively differentiates model capabilities.

Conclusion: LEXam provides a scalable and accurate method to assess legal reasoning quality in LLMs, highlighting their current limitations and potential areas for improvement.

Abstract: Long-form legal reasoning remains a key challenge for large language models
(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a
novel benchmark derived from 340 law exams spanning 116 law school courses
across a range of subjects and degree levels. The dataset comprises 4,886 law
exam questions in English and German, including 2,841 long-form, open-ended
questions and 2,045 multiple-choice questions. Besides reference answers, the
open questions are also accompanied by explicit guidance outlining the expected
legal reasoning approach such as issue spotting, rule recall, or rule
application. Our evaluation on both open-ended and multiple-choice questions
present significant challenges for current LLMs; in particular, they notably
struggle with open questions that require structured, multi-step legal
reasoning. Moreover, our results underscore the effectiveness of the dataset in
differentiating between models with varying capabilities. Adopting an
LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate
how model-generated reasoning steps can be evaluated consistently and
accurately. Our evaluation setup provides a scalable method to assess legal
reasoning quality beyond simple accuracy metrics. Project page:
https://lexam-benchmark.github.io/

</details>


### [130] [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)
*Jialun Zhong,Yanzeng Li,Sen Hu,Yang Zhang,Teng Xu,Lei Zou*

Main category: cs.CL

TL;DR: The paper proposes a Graph-Assisted Prompts (GAP) framework to improve dialogue-based medication recommendations by addressing challenges like ignored fine-grained medical details and non-factual responses in LLMs.


<details>
  <summary>Details</summary>
Motivation: Medication recommendations in medical dialogues require understanding interaction details, which EHRs lack. LLMs can assist but struggle with fine-grained details and factual accuracy.

Method: GAP constructs a patient-centric graph from dialogue, integrates external medical knowledge graphs, and generates prompts to retrieve multi-source information.

Result: GAP shows competitive performance on a medication recommendation dataset and potential in dynamic diagnostic interviewing.

Conclusion: GAP effectively addresses LLM limitations in medical dialogues, improving accuracy and reducing non-factual responses.

Abstract: Medication recommendations have become an important task in the healthcare
domain, especially in measuring the accuracy and safety of medical dialogue
systems (MDS). Different from the recommendation task based on electronic
health records (EHRs), dialogue-based medication recommendations require
research on the interaction details between patients and doctors, which is
crucial but may not exist in EHRs. Recent advancements in large language models
(LLM) have extended the medical dialogue domain. These LLMs can interpret
patients' intent and provide medical suggestions including medication
recommendations, but some challenges are still worth attention. During a
multi-turn dialogue, LLMs may ignore the fine-grained medical information or
connections across the dialogue turns, which is vital for providing accurate
suggestions. Besides, LLMs may generate non-factual responses when there is a
lack of domain-specific knowledge, which is more risky in the medical domain.
To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted
\textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication
recommendation. It extracts medical concepts and corresponding states from
dialogue to construct an explicitly patient-centric graph, which can describe
the neglected but important information. Further, combined with external
medical knowledge graphs, GAP can generate abundant queries and prompts, thus
retrieving information from multiple sources to reduce the non-factual
responses. We evaluate GAP on a dialogue-based medication recommendation
dataset and further explore its potential in a more difficult scenario,
dynamically diagnostic interviewing. Extensive experiments demonstrate its
competitive performance when compared with strong baselines.

</details>


### [131] [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)
*Chenxi Liu,Yongqiang Chen,Tongliang Liu,James Cheng,Bo Han,Kun Zhang*

Main category: cs.CL

TL;DR: The paper highlights the gap between language modeling and thought processes in LLMs, proposing a new prompt technique (LoT) to reduce biases and improve reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To address the misalignment between language modeling and human-like System 2 reasoning in LLMs, which leads to biased reasoning.

Method: Introduces the Language-of-Thoughts (LoT) prompt technique to adjust the order and tokens used for expressing relevant information, reducing biases.

Result: LoT significantly reduces language modeling biases and enhances LLM performance across reasoning tasks.

Conclusion: The LoT technique effectively bridges the gap between language and thought modeling, improving LLM reasoning capabilities.

Abstract: System 2 reasoning is one of the defining characteristics of intelligence,
which requires slow and logical thinking. Human conducts System 2 reasoning via
the language of thoughts that organizes the reasoning process as a causal
sequence of mental language, or thoughts. Recently, it has been observed that
System 2 reasoning can be elicited from Large Language Models (LLMs)
pre-trained on large-scale natural languages. However, in this work, we show
that there is a significant gap between the modeling of languages and thoughts.
As language is primarily a tool for humans to share knowledge and thinking,
modeling human language can easily absorb language biases into LLMs deviated
from the chain of thoughts in minds. Furthermore, we show that the biases will
mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of
the premise. To this end, we propose a new prompt technique termed
Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of
directly eliciting the chain of thoughts from partial information, LoT
instructs LLMs to adjust the order and token used for the expressions of all
the relevant information. We show that the simple strategy significantly
reduces the language modeling biases in LLMs and improves the performance of
LLMs across a variety of reasoning tasks.

</details>


### [132] [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)
*Paul Van Eecke,Katrien Beuls*

Main category: cs.CL

TL;DR: PyFCG is an open-source Python library for Fluid Construction Grammar (FCG), enabling integration with Python's ecosystem and demonstrating use cases like grammar analysis, learning from corpora, and emergent communication experiments.


<details>
  <summary>Details</summary>
Motivation: To provide a Python-based tool for FCG, facilitating seamless integration with Python's ecosystem and expanding FCG's accessibility and application.

Method: Developed PyFCG, a Python port of FCG, and demonstrated its functionality through three walkthrough tutorials covering typical FCG use cases.

Result: PyFCG successfully ports FCG to Python, enabling integration with other libraries and showcasing practical applications in grammar analysis, learning, and communication experiments.

Conclusion: PyFCG enhances FCG's usability and accessibility within Python, offering a versatile tool for researchers and developers in computational linguistics and related fields.

Abstract: We present PyFCG, an open source software library that ports Fluid
Construction Grammar (FCG) to the Python programming language. PyFCG enables
its users to seamlessly integrate FCG functionality into Python programs, and
to use FCG in combination with other libraries within Python's rich ecosystem.
Apart from a general description of the library, this paper provides three
walkthrough tutorials that demonstrate example usage of PyFCG in typical use
cases of FCG: (i) formalising and testing construction grammar analyses, (ii)
learning usage-based construction grammars from corpora, and (iii) implementing
agent-based experiments on emergent communication.

</details>


### [133] [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
*Zhihe Yang,Xufang Luo,Zilong Wang,Dongqi Han,Zhiyuan He,Dongsheng Li,Yunjian Xu*

Main category: cs.CL

TL;DR: The paper addresses the issue of low-probability tokens disproportionately influencing RL training in LLMs, proposing Advantage Reweighting and Lopti to balance updates and improve performance.


<details>
  <summary>Details</summary>
Motivation: Low-probability tokens dominate model updates, suppressing high-probability tokens' gradients, which are crucial for LLM performance.

Method: Introduces Advantage Reweighting and Low-Probability Token Isolation (Lopti) to mitigate interference from low-probability tokens.

Result: Achieves up to 46.2% improvement in K&K Logic Puzzle reasoning tasks with GRPO-trained LLMs.

Conclusion: The proposed methods enhance RL training efficiency by balancing updates across tokens, significantly boosting LLM performance.

Abstract: Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.

</details>


### [134] [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
*Jeffrey T. H. Wong,Cheng Zhang,Xinye Cao,Pedro Gimenes,George A. Constantinides,Wayne Luk,Yiren Zhao*

Main category: cs.CL

TL;DR: The paper introduces $	t A^	t 3$, a post-training low-rank approximation framework for compressing large language models, addressing limitations of existing methods by focusing on Transformer architecture and reducing runtime overheads.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank approximation methods for compressing large language models are limited by their focus on individual linear layers and introduce runtime overhead. The paper aims to improve compression efficiency and performance.

Method: $	t A^	t 3$ splits a Transformer layer into $	t QK$, $	t OV$, and $	t MLP$ components, providing analytical solutions to reduce hidden dimensions while minimizing functional loss, avoiding runtime overhead.

Result: $	t A^	t 3$ outperforms state-of-the-art methods, e.g., reducing perplexity from 7.87 to 4.69 on WikiText-2 for LLaMA 3.1-70B, while also enabling KV cache compression and quantization.

Conclusion: $	t A^	t 3$ offers an efficient, architecture-aware low-rank approximation framework for compressing large language models, improving performance and versatility without runtime costs.

Abstract: Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like pruning and quantization, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, KV cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and
mixed-rank assignments for enhanced performance.

</details>


### [135] [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)
*Cael Marquard,Simbarashe Mawere,Francois Meyer*

Main category: cs.CL

TL;DR: The paper explores neural methods for morphological tagging in Nguni languages, comparing sequence labellers and pretrained models, finding neural taggers outperform rule-based systems.


<details>
  <summary>Details</summary>
Motivation: Morphological parsing is challenging for agglutinative languages like Nguni, requiring effective methods for decomposing and tagging morphemes.

Method: The study compares neural sequence labellers (LSTMs, neural CRFs) and finetuned pretrained models, evaluating them against a rule-based parser.

Result: Neural taggers outperform the rule-based baseline, with models trained from scratch generally better than pretrained ones.

Conclusion: Neural taggers are viable for Nguni languages, especially when paired with existing segmenters.

Abstract: Morphological parsing is the task of decomposing words into morphemes, the
smallest units of meaning in a language, and labelling their grammatical roles.
It is a particularly challenging task for agglutinative languages, such as the
Nguni languages of South Africa, which construct words by concatenating
multiple morphemes. A morphological parsing system can be framed as a pipeline
with two separate components, a segmenter followed by a tagger. This paper
investigates the use of neural methods to build morphological taggers for the
four Nguni languages. We compare two classes of approaches: training neural
sequence labellers (LSTMs and neural CRFs) from scratch and finetuning
pretrained language models. We compare performance across these two categories,
as well as to a traditional rule-based morphological parser. Neural taggers
comfortably outperform the rule-based baseline and models trained from scratch
tend to outperform pretrained models. We also compare parsing results across
different upstream segmenters and with varying linguistic input features. Our
findings confirm the viability of employing neural taggers based on
pre-existing morphological segmenters for the Nguni languages.

</details>


### [136] [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
*Daehee Kim,Deokhyung Kang,Jonghwi Kim,Sangwon Ryu,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: GuRE, a generative query rewriting method using LLMs, improves legal passage retrieval by addressing vocabulary mismatch, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Legal practitioners face vocabulary mismatch issues in retrieval tasks, which GuRE aims to solve.

Method: Train LLMs to rewrite queries, mitigating vocabulary mismatch for better retrieval.

Result: GuRE significantly outperforms baselines and works retriever-agnostically.

Conclusion: GuRE is more suitable for real-world applications than direct retriever fine-tuning.

Abstract: Legal Passage Retrieval (LPR) systems are crucial as they help practitioners
save time when drafting legal arguments. However, it remains an underexplored
avenue. One primary reason is the significant vocabulary mismatch between the
query and the target passage. To address this, we propose a simple yet
effective method, the Generative query REwriter (GuRE). We leverage the
generative capabilities of Large Language Models (LLMs) by training the LLM for
query rewriting. "Rewritten queries" help retrievers to retrieve target
passages by mitigating vocabulary mismatch. Experimental results show that GuRE
significantly improves performance in a retriever-agnostic manner,
outperforming all baseline methods. Further analysis reveals that different
training objectives lead to distinct retrieval behaviors, making GuRE more
suitable than direct retriever fine-tuning for real-world applications. Codes
are avaiable at github.com/daehuikim/GuRE.

</details>


### [137] [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)
*Shanshan Liu,Noriki Nishida,Rumana Ferdous Munne,Narumi Tokunaga,Yuki Yamagata,Kouji Kozaki,Yuji Matsumoto*

Main category: cs.CL

TL;DR: MA-COIR is a framework for biomedical concept recognition that uses semantic search indexes and a BART-based model to improve efficiency and handle implicit concepts, even in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Traditional concept recognition methods fail to capture complex or implicit concepts in biomedical texts, limiting ontology refinement and knowledge graph construction.

Method: MA-COIR reformulates concept recognition as an indexing-recognition task, using semantic search indexes (ssIDs) and a pretrained BART-based model fine-tuned on small datasets. It also leverages LLM-generated queries and synthetic data.

Result: MA-COIR effectively recognizes both explicit and implicit concepts in three scenarios (CDR, HPO, and HOIP) without requiring mention-level annotations during inference.

Conclusion: MA-COIR advances ontology-driven concept recognition in biomedical applications by improving efficiency and handling implicit concepts, with code and data publicly available.

Abstract: Recognizing biomedical concepts in the text is vital for ontology refinement,
knowledge graph construction, and concept relationship discovery. However,
traditional concept recognition methods, relying on explicit mention
identification, often fail to capture complex concepts not explicitly stated in
the text. To overcome this limitation, we introduce MA-COIR, a framework that
reformulates concept recognition as an indexing-recognition task. By assigning
semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in
ontology entries and enhances recognition efficiency. Using a pretrained
BART-based model fine-tuned on small datasets, our approach reduces
computational requirements to facilitate adoption by domain experts.
Furthermore, we incorporate large language models (LLMs)-generated queries and
synthetic data to improve recognition in low-resource settings. Experimental
results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of
MA-COIR in recognizing both explicit and implicit concepts without the need for
mention-level annotations during inference, advancing ontology-driven concept
recognition in biomedical domain applications. Our code and constructed data
are available at https://github.com/sl-633/macoir-master.

</details>


### [138] [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)
*Yingzhi Wang,Anas Alhmoud,Saad Alsahly,Muhammad Alqurishi,Mirco Ravanelli*

Main category: cs.CL

TL;DR: The paper introduces Calm-Whisper, a method to reduce hallucination in OpenAI's Whisper by fine-tuning specific self-attentional heads, achieving an 80% reduction with minimal WER impact.


<details>
  <summary>Details</summary>
Motivation: Whisper exhibits hallucination issues in non-speech segments, limiting its industrial use. The paper aims to address this without pre-/post-processing.

Method: Head-wise masking identifies 3 of 20 decoder heads causing 75% of hallucinations. These heads are fine-tuned using non-speech data.

Result: Calm-Whisper reduces non-speech hallucination by 80% with <0.1% WER degradation on LibriSpeech.

Conclusion: Targeted fine-tuning of specific heads effectively mitigates Whisper's hallucination without compromising performance.

Abstract: OpenAI's Whisper has achieved significant success in Automatic Speech
Recognition. However, it has consistently been found to exhibit hallucination
issues, particularly in non-speech segments, which limits its broader
application in complex industrial settings.
  In this paper, we introduce a novel method to reduce Whisper's hallucination
on non-speech segments without using any pre- or post-possessing techniques.
Specifically, we benchmark the contribution of each self-attentional head in
the Whisper-large-v3 decoder to the hallucination problem by performing a
head-wise mask. Our findings reveal that only 3 of the 20 heads account for
over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune
these three crazy heads using a collection of non-speech data. The results show
that our best fine-tuned model, namely Calm-Whisper, achieves over 80%
reduction in non-speech hallucination with only less than 0.1% WER degradation
on LibriSpeech test-clean and test-other.

</details>


### [139] [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)
*Robin Jegan,Andreas Henrich*

Main category: cs.CL

TL;DR: The paper surveys the use of traditional techniques in NLP alongside modern large language models, focusing on five application scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess whether traditional NLP approaches are still relevant despite the dominance of large language models.

Method: Surveying recent publications in five NLP application scenarios (classification, information extraction, relation extraction, text simplification, summarization) to identify the role of traditional techniques.

Result: Traditional approaches are still used in all five scenarios, either in pipelines, as baselines, or as main models.

Conclusion: Traditional NLP techniques remain relevant and are often integrated with or compared to modern methods.

Abstract: The continued rise of neural networks and large language models in the more
recent past has altered the natural language processing landscape, enabling new
approaches towards typical language tasks and achieving mainstream success.
Despite the huge success of large language models, many disadvantages still
remain and through this work we assess the state of the art in five application
scenarios with a particular focus on the future perspectives and sensible
application scenarios of traditional and older approaches and techniques.
  In this paper we survey recent publications in the application scenarios
classification, information and relation extraction, text simplification as
well as text summarization. After defining our terminology, i.e., which
features are characteristic for traditional techniques in our interpretation
for the five scenarios, we survey if such traditional approaches are still
being used, and if so, in what way they are used. It turns out that all five
application scenarios still exhibit traditional models in one way or another,
as part of a processing pipeline, as a comparison/baseline to the core model of
the respective paper, or as the main model(s) of the paper. For the complete
statistics, see https://zenodo.org/records/13683801

</details>


### [140] [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)
*Mahta Fetrat Qharabagh,Zahra Dehghanian,Hamid R. Rabiee*

Main category: cs.CL

TL;DR: The paper addresses homograph disambiguation in G2P conversion by proposing a semi-automated dataset pipeline (HomoRich) and improving rule-based systems (HomoFast eSpeak), achieving a 30% accuracy boost.


<details>
  <summary>Details</summary>
Motivation: Homograph disambiguation is challenging for low-resource languages, with dataset creation being costly and disambiguation methods often too slow for real-time applications like screen readers.

Method: A semi-automated pipeline for dataset creation (HomoRich) and enhancement of rule-based systems (HomoFast eSpeak) are proposed.

Result: Both deep learning and rule-based systems show ~30% improvement in homograph disambiguation accuracy.

Conclusion: The approach balances dataset quality and real-time performance, benefiting low-resource languages and accessibility tools.

Abstract: Homograph disambiguation remains a significant challenge in
grapheme-to-phoneme (G2P) conversion, especially for low-resource languages.
This challenge is twofold: (1) creating balanced and comprehensive homograph
datasets is labor-intensive and costly, and (2) specific disambiguation
strategies introduce additional latency, making them unsuitable for real-time
applications such as screen readers and other accessibility tools. In this
paper, we address both issues. First, we propose a semi-automated pipeline for
constructing homograph-focused datasets, introduce the HomoRich dataset
generated through this pipeline, and demonstrate its effectiveness by applying
it to enhance a state-of-the-art deep learning-based G2P system for Persian.
Second, we advocate for a paradigm shift - utilizing rich offline datasets to
inform the development of fast, rule-based methods suitable for
latency-sensitive accessibility applications like screen readers. To this end,
we improve one of the most well-known rule-based G2P systems, eSpeak, into a
fast homograph-aware version, HomoFast eSpeak. Our results show an approximate
30% improvement in homograph disambiguation accuracy for the deep
learning-based and eSpeak systems.

</details>


### [141] [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)
*Jiaan Wang,Fandong Meng,Zengkui Sun,Yunlong Liang,Yuxuan Cao,Jiarong Xu,Haoxiang Shi,Jie Zhou*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' performance in many-to-many summarization (M2MS), showing zero-shot LLMs compete with fine-tuned models, while instruction-tuning boosts performance but may increase factual errors.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' potential in M2MS tasks across languages and domains, addressing gaps in systematic evaluation.

Method: Reorganizes M2MS data (47.8K samples, 5 domains, 6 languages), benchmarks 18 LLMs (zero-shot and instruction-tuning), and compares with fine-tuned models.

Result: Zero-shot LLMs match fine-tuned models; instruction-tuning improves performance but may worsen factual accuracy.

Conclusion: LLMs show promise for M2MS, but controlling factual errors is critical for real-world applications.

Abstract: Many-to-many summarization (M2MS) aims to process documents in any language
and generate the corresponding summaries also in any language. Recently, large
language models (LLMs) have shown strong multi-lingual abilities, giving them
the potential to perform M2MS in real applications. This work presents a
systematic empirical study on LLMs' M2MS ability. Specifically, we first
reorganize M2MS data based on eight previous domain-specific datasets. The
reorganized data contains 47.8K samples spanning five domains and six
languages, which could be used to train and evaluate LLMs. Then, we benchmark
18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned
traditional models (e.g., mBART) are also conducted for comparisons. Our
experiments reveal that, zero-shot LLMs achieve competitive results with
fine-tuned traditional models. After instruct-tuning, open-source LLMs can
significantly improve their M2MS ability, and outperform zero-shot LLMs
(including GPT-4) in terms of automatic evaluations. In addition, we
demonstrate that this task-specific improvement does not sacrifice the LLMs'
general task-solving abilities. However, as revealed by our human evaluation,
LLMs still face the factuality issue, and the instruction tuning might
intensify the issue. Thus, how to control factual errors becomes the key when
building LLM summarizers in real applications, and is worth noting in future
research.

</details>


### [142] [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)
*Jiaan Wang,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: The paper introduces a new reward modeling method for reinforcement learning in machine translation, achieving state-of-the-art results and extending to multilingual settings.


<details>
  <summary>Details</summary>
Motivation: Existing large reasoning models (LRMs) in machine translation focus on high-resource languages and underutilize reinforcement learning. This work aims to improve reward modeling and extend capabilities to more languages.

Method: A new reward modeling method compares translations from a policy model with a strong LRM (DeepSeek-R1-671B) and quantifies comparisons for rewards. The method is applied to multilingual settings with 11 languages.

Result: The trained model (Qwen2.5-7B-Instruct) achieves state-of-the-art performance in literary translation and outperforms strong LRMs. It also extends to 90 translation directions with impressive multilingual results.

Conclusion: The proposed reward modeling method enhances reinforcement learning in MT, enabling superior performance and scalability across multiple languages.

Abstract: In recent years, the emergence of large reasoning models (LRMs), such as
OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex
problems, e.g., mathematics and coding. Some pioneering studies attempt to
bring the success of LRMs in neural machine translation (MT). They try to build
LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite
some progress that has been made, these attempts generally focus on several
high-resource languages, e.g., English and Chinese, leaving the performance on
other languages unclear. Besides, the reward modeling methods in previous work
do not fully unleash the potential of reinforcement learning in MT. In this
work, we first design a new reward modeling method that compares the
translation results of the policy MT model with a strong LRM (i.e.,
DeepSeek-R1-671B), and quantifies the comparisons to provide rewards.
Experimental results demonstrate the superiority of the reward modeling method.
Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new
state-of-the-art performance in literary translation, and outperforms strong
LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to
the multilingual settings with 11 languages. With a carefully designed
lightweight reward modeling in RL, we can simply transfer the strong MT ability
from a single direction into multiple (i.e., 90) translation directions and
achieve impressive multilingual MT performance.

</details>


### [143] [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)
*Yuhao Qing,Boyu Zhu,Mingzhe Du,Zhijiang Guo,Terry Yue Zhuo,Qianru Zhang,Jie M. Zhang,Heming Cui,Siu-Ming Yiu,Dong Huang,See-Kiong Ng,Luu Anh Tuan*

Main category: cs.CL

TL;DR: EffiBench-X is a multi-language benchmark for evaluating the efficiency of LLM-generated code, revealing that LLMs lag behind human experts in efficiency across languages.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on functional correctness and single languages, lacking evaluation of code efficiency across multiple languages.

Method: EffiBench-X includes competitive programming tasks in Python, C++, Java, JavaScript, Ruby, and Golang, comparing LLM-generated code to human-expert baselines.

Result: LLMs generate correct but inefficient code, achieving ~62% of human efficiency on average, with performance varying by language.

Conclusion: The study underscores the need for research into optimizing LLM-generated code efficiency across diverse languages.

Abstract: Existing code generation benchmarks primarily evaluate functional
correctness, with limited focus on code efficiency and often restricted to a
single language like Python. To address this gap, we introduce EffiBench-X, the
first multi-language benchmark designed to measure the efficiency of
LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,
and Golang. It comprises competitive programming tasks with human-expert
solutions as efficiency baselines. Evaluating state-of-the-art LLMs on
EffiBench-X reveals that while models generate functionally correct code, they
consistently underperform human experts in efficiency. Even the most efficient
LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human
efficiency on average, with significant language-specific variations. LLMs show
better efficiency in Python, Ruby, and JavaScript than in Java, C++, and
Golang. For instance, DeepSeek-R1's Python code is significantly more efficient
than its Java code. These results highlight the critical need for research into
LLM optimization techniques to improve code efficiency across diverse
languages. The dataset and evaluation infrastructure are submitted and
available at https://github.com/EffiBench/EffiBench-X.git and
https://huggingface.co/datasets/EffiBench/effibench-x.

</details>


### [144] [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)
*Yuyang Li,Philip J. M. Kerbusch,Raimon H. R. Pruim,Tobias Käfer*

Main category: cs.CL

TL;DR: A Conversational AI system for airports was developed using three RAG methods, with Graph RAG showing the highest accuracy and fewer hallucinations, making it ideal for airport environments.


<details>
  <summary>Details</summary>
Motivation: To enhance automation in busy airports by enabling staff to communicate with flight information systems effectively, handling jargon and dynamic questions.

Method: Implemented three RAG methods: traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG), comparing their performance in accuracy and hallucination rates.

Result: Graph RAG achieved 91.49% accuracy with fewer hallucinations, outperforming traditional RAG (84.84%) and SQL RAG (80.85%).

Conclusion: Graph RAG is recommended for airport environments due to its high accuracy, fewer hallucinations, and effectiveness in handling reasoning-based questions.

Abstract: Airports from the top 20 in terms of annual passengers are highly dynamic
environments with thousands of flights daily, and they aim to increase the
degree of automation. To contribute to this, we implemented a Conversational AI
system that enables staff in an airport to communicate with flight information
systems. This system not only answers standard airport queries but also
resolves airport terminology, jargon, abbreviations, and dynamic questions
involving reasoning. In this paper, we built three different
Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL
RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that
traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally
produced hallucinations, which is risky to airport safety. In contrast, SQL RAG
and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with
significantly fewer hallucinations. Moreover, Graph RAG was especially
effective for questions that involved reasoning. Based on our observations, we
thus recommend SQL RAG and Graph RAG are better for airport environments, due
to fewer hallucinations and the ability to handle dynamic questions.

</details>


### [145] [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)
*Himel Ghosh,Ahmed Mosharafa,Georg Groh*

Main category: cs.CL

TL;DR: The paper presents a RoBERTa-based model for sentence-level media bias detection, showing improved performance over a baseline and offering interpretability, though limited by dataset size.


<details>
  <summary>Details</summary>
Motivation: Media bias detection is challenging due to subjectivity and lack of annotated data. This work aims to improve detection using advanced NLP techniques.

Method: Fine-tuned a RoBERTa model on the BABE dataset, validated with McNemar's test and 5x2 cross-validation. Combined with a bias-type classifier for comprehensive analysis.

Result: Statistically significant improvements over DA-RoBERTa baseline. Model avoids oversensitivity to charged terms and focuses on context.

Conclusion: The approach is robust and interpretable but limited by dataset constraints. Future work includes context-aware modeling and bias neutralization.

Abstract: Media bias detection is a critical task in ensuring fair and balanced
information dissemination, yet it remains challenging due to the subjectivity
of bias and the scarcity of high-quality annotated data. In this work, we
perform sentence-level bias classification by fine-tuning a RoBERTa-based model
on the expert-annotated BABE dataset. Using McNemar's test and the 5x2
cross-validation paired t-test, we show statistically significant improvements
in performance when comparing our model to a domain-adaptively pre-trained
DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model
avoids common pitfalls like oversensitivity to politically charged terms and
instead attends more meaningfully to contextually relevant tokens. For a
comprehensive examination of media bias, we present a pipeline that combines
our model with an already-existing bias-type classifier. Our method exhibits
good generalization and interpretability, despite being constrained by
sentence-level analysis and dataset size because of a lack of larger and more
advanced bias corpora. We talk about context-aware modeling, bias
neutralization, and advanced bias type classification as potential future
directions. Our findings contribute to building more robust, explainable, and
socially responsible NLP systems for media bias detection.

</details>


### [146] [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)
*Márton Kardos,Kenneth C. Enevoldsen,Kristoffer Laigaard Nielbo*

Main category: cs.CL

TL;DR: Topic models analyze textual corpora without close reading, but interpreting their parameters is challenging. Topicwizard offers model-agnostic tools for better understanding.


<details>
  <summary>Details</summary>
Motivation: Current methods for interpreting topic models (like top 10 terms) are limited and biased, requiring better visualization tools.

Method: Introduces topicwizard, a framework with intuitive, interactive tools for examining semantic relations in topic models.

Result: Topicwizard provides a more complete and accurate understanding of topic models' outputs.

Conclusion: Thoughtful UI design and visualizations, like topicwizard, enhance topic model interpretation.

Abstract: Topic models are statistical tools that allow their users to gain qualitative
and quantitative insights into the contents of textual corpora without the need
for close reading. They can be applied in a wide range of settings from
discourse analysis, through pretraining data curation, to text filtering. Topic
models are typically parameter-rich, complex models, and interpreting these
parameters can be challenging for their users. It is typical practice for users
to interpret topics based on the top 10 highest ranking terms on a given topic.
This list-of-words approach, however, gives users a limited and biased picture
of the content of topics. Thoughtful user interface design and visualizations
can help users gain a more complete and accurate understanding of topic models'
output. While some visualization utilities do exist for topic models, these are
typically limited to a certain type of topic model. We introduce topicwizard, a
framework for model-agnostic topic model interpretation, that provides
intuitive and interactive tools that help users examine the complex semantic
relations between documents, words and topics learned by topic models.

</details>


### [147] [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)
*Sai Koneru,Maike Züfle,Thai-Binh Nguyen,Seymanur Akti,Jan Niehues,Alexander Waibel*

Main category: cs.CL

TL;DR: The paper discusses KIT's use of LLMs for Offline ST and IF tasks, proposing multi-system fusion and refinement steps to improve performance.


<details>
  <summary>Details</summary>
Motivation: The broadening scope of IWSLT and advancements in LLMs drive the need for enhanced ST and IF systems.

Method: For Offline ST: multi-ASR fusion with LLM, two-step translation, and refinement. For IF: end-to-end speech-LLM integration with document-level refinement.

Result: Improved performance in Offline ST and IF tasks through LLM-based approaches.

Conclusion: LLMs effectively enhance ST and IF tasks, with refinement steps further boosting quality.

Abstract: The scope of the International Workshop on Spoken Language Translation
(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to
encompass a wider array of tasks, including Speech Question Answering and
Summarization. This shift is partly driven by the growing capabilities of
modern systems, particularly with the success of Large Language Models (LLMs).
In this paper, we present the Karlsruhe Institute of Technology's submissions
for the Offline ST and Instruction Following (IF) tracks, where we leverage
LLMs to enhance performance across all tasks. For the Offline ST track, we
propose a pipeline that employs multiple automatic speech recognition systems,
whose outputs are fused using an LLM with document-level context. This is
followed by a two-step translation process, incorporating additional refinement
step to improve translation quality. For the IF track, we develop an end-to-end
model that integrates a speech encoder with an LLM to perform a wide range of
instruction-following tasks. We complement it with a final document-level
refinement stage to further enhance output quality by using contextual
information.

</details>


### [148] [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)
*Amelie S. Robrecht,Christoph R. Kowalski,Stefan Kopp*

Main category: cs.CL

TL;DR: The paper proposes a Bayesian inference and non-stationary Markov Decision Process framework for adaptive explanation generation in dialog systems, showing effectiveness in adapting to diverse interlocutors.


<details>
  <summary>Details</summary>
Motivation: Adapting explanations to the addressee is key for dialog systems but challenging due to dynamic interaction contexts and listener features.

Method: Uses Bayesian inference to update a partner model and a non-stationary Markov Decision Process for dynamic decision-making in explanation strategies.

Result: Evaluation with simulated interlocutors shows high adaptivity, with distinct strategies for different partners, even with changing feedback.

Conclusion: The framework improves explainable AI and dialog systems by dynamically adapting explanations to interlocutors.

Abstract: Adapting to the addressee is crucial for successful explanations, yet poses
significant challenges for dialogsystems. We adopt the approach of treating
explanation generation as a non-stationary decision process, where the optimal
strategy varies according to changing beliefs about the explainee and the
interaction context. In this paper we address the questions of (1) how to track
the interaction context and the relevant listener features in a formally
defined computational partner model, and (2) how to utilize this model in the
dynamically adjusted, rational decision process that determines the currently
best explanation strategy. We propose a Bayesian inference-based approach to
continuously update the partner model based on user feedback, and a
non-stationary Markov Decision Process to adjust decision-making based on the
partner model values. We evaluate an implementation of this framework with five
simulated interlocutors, demonstrating its effectiveness in adapting to
different partners with constant and even changing feedback behavior. The
results show high adaptivity with distinct explanation strategies emerging for
different partners, highlighting the potential of our approach to improve
explainable AI systems and dialogsystems in general.

</details>


### [149] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
*Ambre Marie,Ilias Maoudj,Guillaume Dardenne,Gwenolé Quellec*

Main category: cs.CL

TL;DR: The study explores multimodal speech-based suicide risk assessment in adolescents, combining transcription, linguistic and audio embeddings, and acoustic features. Weighted attention fusion performed best (69% accuracy), but generalization challenges remain.


<details>
  <summary>Details</summary>
Motivation: Address the need for speech-based suicide risk assessment in adolescents using a multimodal approach.

Method: Integrated WhisperX for transcription, Chinese RoBERTa for linguistic embeddings, WavLM for audio embeddings, and handcrafted acoustic features. Tested three fusion strategies: early concatenation, modality-specific processing, and weighted attention with mixup regularization.

Result: Weighted attention achieved 69% accuracy on the development set, but performance gaps between development and test sets indicate generalization issues.

Conclusion: Refining embedding representations and fusion mechanisms is crucial for improving classification reliability, as highlighted by the MINI-KID framework.

Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.

</details>


### [150] [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
*Xiang Fei,Jinghui Lu,Qi Sun,Hao Feng,Yanjie Wang,Wei Shi,An-Lan Wang,Jingqun Tang,Can Huang*

Main category: cs.CL

TL;DR: The paper introduces Numerical Token Integrity Loss (NTIL) to improve autoregressive models by preserving numerical sequence coherence, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive models treat digits as independent tokens, ignoring the structured nature of numerical sequences.

Method: NTIL uses a dual-level approach: token-level (extending EMD for ordinal relationships) and sequence-level (penalizing overall discrepancies).

Result: Experiments show NTIL significantly improves numerical prediction performance.

Conclusion: NTIL effectively enhances autoregressive models for numerical sequence generation.

Abstract: Autoregressive models have become the de facto choice for sequence generation
tasks, but standard approaches treat digits as independent tokens and apply
cross-entropy loss, overlooking the coherent structure of numerical sequences.
This paper introduces Numerical Token Integrity Loss (NTIL) to address this
gap. NTIL operates at two levels: (1) token-level, where it extends the Earth
Mover's Distance (EMD) to preserve ordinal relationships between numerical
values, and (2) sequence-level, where it penalizes the overall discrepancy
between the predicted and actual sequences. This dual approach improves
numerical prediction and integrates effectively with LLMs/MLLMs. Extensive
experiments show significant performance improvements with NTIL.

</details>


### [151] [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
*Sondre Wold,Lucas Georges Gabriel Charpentier,Étienne Simon*

Main category: cs.CL

TL;DR: The paper explores how entropy in training data affects systematic generalization in language models, showing performance scales with entropy and suggesting it as a measure for robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of systematic generalization in language models, particularly their sensitivity to input permutations and novel contexts.

Method: The authors formalize a framework to measure entropy in sequence-to-sequence tasks and analyze model performance relative to entropy levels.

Result: Performance of models scales with the entropy of component parts in training data, indicating a link between systematic generalization and information efficiency.

Conclusion: High entropy success is achievable without built-in priors, and low entropy performance can guide progress toward robust systematic generalization.

Abstract: Systematic generalization remains challenging for current language models,
which are known to be both sensitive to semantically similar permutations of
the input and to struggle with known concepts presented in novel contexts.
Although benchmarks exist for assessing compositional behavior, it is unclear
how to measure the difficulty of a systematic generalization problem. In this
work, we show how one aspect of systematic generalization can be described by
the entropy of the distribution of component parts in the training data. We
formalize a framework for measuring entropy in a sequence-to-sequence task and
find that the performance of popular model architectures scales with the
entropy. Our work connects systematic generalization to information efficiency,
and our results indicate that success at high entropy can be achieved even
without built-in priors, and that success at low entropy can serve as a target
for assessing progress towards robust systematic generalization.

</details>


### [152] [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
*David Stap,Christof Monz*

Main category: cs.CL

TL;DR: Expanding language diversity in LLM fine-tuning improves translation quality, but benefits plateau or decrease beyond a threshold due to more language-agnostic representations.


<details>
  <summary>Details</summary>
Motivation: Resolve conflicting prior research on the impact of language diversity in LLM fine-tuning.

Method: Controlled fine-tuning experiments across 132 translation directions.

Result: Increased language diversity improves translation quality for both unsupervised and supervised pairs, with benefits plateauing or decreasing beyond a threshold.

Conclusion: Language diversity enhances language-agnostic representations, explaining improved performance, but excessive diversity may not yield further gains.

Abstract: Prior research diverges on language diversity in LLM fine-tuning: Some
studies report benefits while others find no advantages. Through controlled
fine-tuning experiments across 132 translation directions, we systematically
resolve these disparities. We find that expanding language diversity during
fine-tuning improves translation quality for both unsupervised and --
surprisingly -- supervised pairs, despite less diverse models being fine-tuned
exclusively on these supervised pairs. However, benefits plateau or decrease
beyond a certain diversity threshold. We show that increased language diversity
creates more language-agnostic representations. These representational
adaptations help explain the improved performance in models fine-tuned with
greater diversity.

</details>


### [153] [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.CL

TL;DR: The paper introduces TREA, a dataset for evaluating temporal reasoning in large audio language models (LALMs), highlighting their limitations compared to human performance and proposing an uncertainty metric for holistic evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LALMs on reasoning tasks, distinct from traditional classification or generation, by introducing a novel dataset (TREA) and an uncertainty metric.

Method: Proposes the TREA dataset for temporal reasoning evaluation and introduces an uncertainty metric to assess model invariance to semantically identical input perturbations.

Result: Benchmarking shows LALMs lag behind human performance on TREA tasks, with accuracy and uncertainty metrics not strongly correlated, indicating the need for comprehensive evaluation.

Conclusion: The study underscores the importance of holistic evaluation for LALMs, especially in high-stakes applications, due to the observed gap between model and human performance.

Abstract: The popular success of text-based large language models (LLM) has streamlined
the attention of the multimodal community to combine other modalities like
vision and audio along with text to achieve similar multimodal capabilities. In
this quest, large audio language models (LALMs) have to be evaluated on
reasoning related tasks which are different from traditional classification or
generation tasks. Towards this goal, we propose a novel dataset called temporal
reasoning evaluation of audio (TREA).
  We benchmark open-source LALMs and observe that they are consistently behind
human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we
also propose an uncertainty metric, which computes the invariance of the model
to semantically identical perturbations of the input. Our analysis shows that
the accuracy and uncertainty metrics are not necessarily correlated and thus,
points to a need for wholesome evaluation of LALMs for high-stakes
applications.

</details>


### [154] [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)
*Anton Ehrmanntraut,Julia Wunderle,Jan Pfister,Fotis Jannidis,Andreas Hotho*

Main category: cs.CL

TL;DR: ModernGBERT and LL"aMmlein2Vec are introduced as German encoder models, with ModernGBERT outperforming prior encoders and adapted decoders in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the need for resource-efficient encoder models in German NLP, comparing dedicated encoders (ModernGBERT) and converted decoders (LL"aMmlein2Vec).

Method: Train ModernGBERT from scratch with ModernBERT innovations and derive LL"aMmlein2Vec from German decoder-only models via LLM2Vec. Benchmark on NLU, text embedding, and long-context tasks.

Result: ModernGBERT 1B outperforms prior German encoders and LLM2Vec-adapted encoders in performance and parameter-efficiency.

Conclusion: ModernGBERT advances German NLP with transparent, high-performance encoders; all resources are publicly available.

Abstract: Despite the prominence of decoder-only language models, encoders remain
crucial for resource-constrained applications. We introduce ModernGBERT (134M,
1B), a fully transparent family of German encoder models trained from scratch,
incorporating architectural innovations from ModernBERT. To evaluate the
practical trade-offs of training encoders from scratch, we also present
LL\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German
decoder-only models via LLM2Vec. We benchmark all models on natural language
understanding, text embedding, and long-context reasoning tasks, enabling a
controlled comparison between dedicated encoders and converted decoders. Our
results show that ModernGBERT 1B outperforms prior state-of-the-art German
encoders as well as encoders adapted via LLM2Vec, with regard to performance
and parameter-efficiency. All models, training data, checkpoints and code are
publicly available, advancing the German NLP ecosystem with transparent,
high-performance encoder models.

</details>


### [155] [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)
*Zheng Wei Lim,Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: LLMs show cross-lingual transfer but produce inconsistent outputs. Analyzing their hidden states reveals reliance on language-specific subspaces, not shared semantics. Larger models dissociate more but retrieve knowledge better. Steering towards shared space improves multilingual reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs generalize knowledge across languages and why they produce inconsistent outputs for the same queries in different languages.

Method: Applied the logit lens to interpret LLMs' hidden states during multilingual multi-choice reasoning tasks.

Result: LLMs rely on language-specific subspaces, leading to inconsistency. Larger models dissociate more but retrieve knowledge better. Steering towards shared space improves performance.

Conclusion: Enhancing shared semantic space utilization boosts multilingual reasoning and output consistency.

Abstract: Large language models (LLMs) are demonstrably capable of cross-lingual
transfer, but can produce inconsistent output when prompted with the same
queries written in different languages. To understand how language models are
able to generalize knowledge from one language to the others, we apply the
logit lens to interpret the implicit steps taken by LLMs to solve multilingual
multi-choice reasoning questions. We find LLMs predict inconsistently and are
less accurate because they rely on subspaces of individual languages, rather
than working in a shared semantic space. While larger models are more
multilingual, we show their hidden states are more likely to dissociate from
the shared representation compared to smaller models, but are nevertheless more
capable of retrieving knowledge embedded across different languages. Finally,
we demonstrate that knowledge sharing can be modulated by steering the models'
latent processing towards the shared semantic space. We find reinforcing
utilization of the shared space improves the models' multilingual reasoning
performance, as a result of more knowledge transfer from, and better output
consistency with English.

</details>


### [156] [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)
*Aswathy Velutharambath,Roman Klinger,Kai Sassenberg*

Main category: cs.CL

TL;DR: The paper challenges prior claims about detecting deception from text, arguing that success in earlier studies may stem from dataset artifacts. It introduces a belief-based framework and new corpora (DeFaBel) to study deception cues, finding negligible correlations with deception labels.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of prior deception detection studies, which may rely on dataset-specific artifacts, and to provide a more robust framework for studying deception in text.

Method: Introduces a belief-based deception framework and constructs three corpora (DeFaBel) to evaluate linguistic cues. Tests these cues and benchmarks against other datasets using various models.

Result: Negligible correlations between linguistic cues and deception labels in DeFaBel, inconsistent cues across datasets, and poor model performance on DeFaBel despite success on other datasets.

Conclusion: Deception cannot be reliably inferred from linguistic cues alone, calling for a reevaluation of deception detection methods in NLP.

Abstract: Can deception be detected solely from written text? Cues of deceptive
communication are inherently subtle, even more so in text-only communication.
Yet, prior studies have reported considerable success in automatic deception
detection. We hypothesize that such findings are largely driven by artifacts
introduced during data collection and do not generalize beyond specific
datasets. We revisit this assumption by introducing a belief-based deception
framework, which defines deception as a misalignment between an author's claims
and true beliefs, irrespective of factual accuracy, allowing deception cues to
be studied in isolation. Based on this framework, we construct three corpora,
collectively referred to as DeFaBel, including a German-language corpus of
deceptive and non-deceptive arguments and a multilingual version in German and
English, each collected under varying conditions to account for belief change
and enable cross-linguistic analysis. Using these corpora, we evaluate commonly
reported linguistic cues of deception. Across all three DeFaBel variants, these
cues show negligible, statistically insignificant correlations with deception
labels, contrary to prior work that treats such cues as reliable indicators. We
further benchmark against other English deception datasets following similar
data collection protocols. While some show statistically significant
correlations, effect sizes remain low and, critically, the set of predictive
cues is inconsistent across datasets. We also evaluate deception detection
using feature-based models, pretrained language models, and instruction-tuned
large language models. While some models perform well on established deception
datasets, they consistently perform near chance on DeFaBel. Our findings
challenge the assumption that deception can be reliably inferred from
linguistic cues and call for rethinking how deception is studied and modeled in
NLP.

</details>


### [157] [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)
*Zhi Liu,Tao Yang,Jing Wang,Yexin Chen,Zhan Gao,Jiaxi Yang,Kui Chen,Bingji Lu,Xiaochen Li,Changyong Luo,Yan Li,Xiaohong Gu,Peng Cao*

Main category: cs.CL

TL;DR: The paper introduces Tianyi, a 7.6-billion-parameter LLM tailored for TCM, addressing limitations in existing AI models by leveraging specialized training on diverse TCM corpora and a progressive learning approach.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of applying AI in TCM, such as data limitations, single-objective constraints, and lack of specialization in existing LLMs, the authors propose a dedicated model for TCM.

Method: Developed Tianyi, a specialized LLM pre-trained and fine-tuned on diverse TCM corpora, using progressive learning to assimilate systematic TCM knowledge. Established TCMEval, a benchmark for evaluating LLMs in TCM tasks.

Result: Tianyi demonstrates significant potential as an AI assistant in TCM clinical practice and research, effectively bridging the gap between TCM knowledge and practical application.

Conclusion: Tianyi represents a promising advancement in AI for TCM, offering a specialized, scalable solution to enhance clinical practice and research.

Abstract: Natural medicines, particularly Traditional Chinese Medicine (TCM), are
gaining global recognition for their therapeutic potential in addressing human
symptoms and diseases. TCM, with its systematic theories and extensive
practical experience, provides abundant resources for healthcare. However, the
effective application of TCM requires precise syndrome diagnosis, determination
of treatment principles, and prescription formulation, which demand decades of
clinical expertise. Despite advancements in TCM-based decision systems, machine
learning, and deep learning research, limitations in data and single-objective
constraints hinder their practical application. In recent years, large language
models (LLMs) have demonstrated potential in complex tasks, but lack
specialization in TCM and face significant challenges, such as too big model
scale to deploy and issues with hallucination. To address these challenges, we
introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and
specifically designed for TCM, pre-trained and fine-tuned on diverse TCM
corpora, including classical texts, expert treatises, clinical records, and
knowledge graphs. Tianyi is designed to assimilate interconnected and
systematic TCM knowledge through a progressive learning manner. Additionally,
we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in
TCM examinations, clinical tasks, domain-specific question-answering, and
real-world trials. The extensive evaluations demonstrate the significant
potential of Tianyi as an AI assistant in TCM clinical practice and research,
bridging the gap between TCM knowledge and practical application.

</details>


### [158] [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)
*Yassine El Boudouri,Walter Nuninger,Julian Alvarez,Yvan Peter*

Main category: cs.CL

TL;DR: RPEval is a new benchmark for evaluating LLM role-playing abilities across emotional understanding, decision-making, moral alignment, and in-character consistency.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLM role-playing are resource-intensive or biased, necessitating a better benchmark.

Method: Introduces RPEval, detailing its construction and providing baseline evaluations.

Result: RPEval offers a structured way to assess LLM role-playing capabilities.

Conclusion: RPEval addresses evaluation challenges and provides a reproducible benchmark for future research.

Abstract: Large Language Models (LLMs) demonstrate a notable capacity for adopting
personas and engaging in role-playing. However, evaluating this ability
presents significant challenges, as human assessments are resource-intensive
and automated evaluations can be biased. To address this, we introduce
Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM
role-playing capabilities across four key dimensions: emotional understanding,
decision-making, moral alignment, and in-character consistency. This article
details the construction of RPEval and presents baseline evaluations. Our code
and dataset are available at https://github.com/yelboudouri/RPEval

</details>


### [159] [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
*Yixuan Xu,Antoine Bosselut,Imanol Schlag*

Main category: cs.CL

TL;DR: The paper examines memorization risks in large language models, identifying the 'offset effect' where memorization decreases with longer prefixes or offsets from the start of the context window. It suggests positional offset as a key factor for evaluating memorization risks.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the risk of copyright violations due to memorization in language models by studying how verbatim memorization varies with prefix length and positional offset.

Method: Pretrained language models (1B/3B/8B) on 83B tokens, mixing web data with public domain books to simulate copyrighted content. Analyzed memorization patterns, focusing on prefix length and positional offset.

Result: Found the 'offset effect': memorization decreases with longer prefixes or offsets from the start of the context window. Models rely heavily on early tokens, making them sensitive to shifts. Degenerated text occurs when retrieval fails.

Conclusion: Positional offset is a critical factor for memorization risks. Shifting sensitive data deeper into the context window can suppress memorization and degeneration, offering a mitigation strategy.

Abstract: Large language models are known to memorize parts of their training data,
posing risk of copyright violations. To systematically examine this risk, we
pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing
web-scale data with public domain books used to simulate copyrighted content at
controlled frequencies at lengths at least ten times longer than prior work. We
thereby identified the offset effect, a phenomenon characterized by two key
findings: (1) verbatim memorization is most strongly triggered by short
prefixes drawn from the beginning of the context window, with memorization
decreasing counterintuitively as prefix length increases; and (2) a sharp
decline in verbatim recall when prefix begins offset from the initial tokens of
the context window. We attribute this to positional fragility: models rely
disproportionately on the earliest tokens in their context window as retrieval
anchors, making them sensitive to even slight shifts. We further observe that
when the model fails to retrieve memorized content, it often produces
degenerated text. Leveraging these findings, we show that shifting sensitive
data deeper into the context window suppresses both extractable memorization
and degeneration. Our results suggest that positional offset is a critical and
previously overlooked axis for evaluating memorization risks, since prior work
implicitly assumed uniformity by probing only from the beginning of training
sequences.

</details>


### [160] [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
*V. S. D. S. Mahesh Akavarapu,Hrishikesh Terdalkar,Pramit Bhattacharyya,Shubhangi Agarwal,Vishakha Deulgaonkar,Pralay Manna,Chaitali Dangarikar,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: LLMs show strong cross-lingual generalization in classical languages (Sanskrit, Ancient Greek, Latin), outperforming fine-tuned baselines in named entity recognition and translation. Smaller models struggle, especially with niche tasks. Retrieval-augmented QA for Sanskrit boosts performance, but smaller LLMs lag, highlighting model scale's role in generalization.


<details>
  <summary>Details</summary>
Motivation: To investigate cross-lingual zero-shot generalization in classical languages and understand the impact of model scale on performance.

Method: Evaluated LLMs on named entity recognition, machine translation, and Sanskrit QA tasks, comparing performance across model sizes and using retrieval-augmented generation for QA.

Result: LLMs outperform baselines in out-of-domain tasks, but smaller models struggle. Retrieval-augmented QA improves performance, while smaller LLMs show significant drops.

Conclusion: Model scale is crucial for cross-lingual generalization in classical languages, with larger LLMs demonstrating superior performance and utility in classical studies.

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization
capabilities across diverse tasks and languages. In this study, we focus on
natural language understanding in three classical languages -- Sanskrit,
Ancient Greek and Latin -- to investigate the factors affecting cross-lingual
zero-shot generalization. First, we explore named entity recognition and
machine translation into English. While LLMs perform equal to or better than
fine-tuned baselines on out-of-domain data, smaller models often struggle,
especially with niche or abstract entity types. In addition, we concentrate on
Sanskrit by presenting a factoid question-answering (QA) dataset and show that
incorporating context via retrieval-augmented generation approach significantly
boosts performance. In contrast, we observe pronounced performance drops for
smaller LLMs across these QA tasks. These results suggest model scale as an
important factor influencing cross-lingual generalization. Assuming that models
used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical
languages, our findings provide insights into how LLMs may generalize on these
languages and their consequent utility in classical studies.

</details>


### [161] [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
*Zihao Cheng,Hongru Wang,Zeming Liu,Yuhang Guo,Yuanfang Guo,Yunhong Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: ToolSpectrum is a benchmark evaluating LLMs' personalized tool utilization, highlighting the need for context-aware personalization beyond functional selection.


<details>
  <summary>Details</summary>
Motivation: Existing LLM tool integration lacks context-aware personalization, leading to suboptimal user satisfaction and inefficient tool use.

Method: ToolSpectrum formalizes user profiles and environmental factors to analyze their impact on tool utilization, testing LLMs' joint reasoning capabilities.

Result: Personalized tool use improves user experience, but current LLMs struggle to balance user profiles and environmental factors effectively.

Conclusion: Context-aware personalization is crucial for tool-augmented LLMs, revealing limitations in current models' reasoning abilities.

Abstract: While integrating external tools into large language models (LLMs) enhances
their ability to access real-time information and domain-specific services,
existing approaches focus narrowly on functional tool selection following user
instructions, overlooking the context-aware personalization in tool selection.
This oversight leads to suboptimal user satisfaction and inefficient tool
utilization, particularly when overlapping toolsets require nuanced selection
based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a
benchmark designed to evaluate LLMs' capabilities in personalized tool
utilization. Specifically, we formalize two key dimensions of personalization,
user profile and environmental factors, and analyze their individual and
synergistic impacts on tool utilization. Through extensive experiments on
ToolSpectrum, we demonstrate that personalized tool utilization significantly
improves user experience across diverse scenarios. However, even
state-of-the-art LLMs exhibit the limited ability to reason jointly about user
profiles and environmental factors, often prioritizing one dimension at the
expense of the other. Our findings underscore the necessity of context-aware
personalization in tool-augmented LLMs and reveal critical limitations for
current models. Our data and code are available at
https://github.com/Chengziha0/ToolSpectrum.

</details>


### [162] [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)
*Zhengrui Ma,Yang Feng,Chenze Shao,Fandong Meng,Jie Zhou,Min Zhang*

Main category: cs.CL

TL;DR: SLED is a speech language modeling approach using continuous latent representations and an energy distance objective, avoiding discretization errors and simplifying the pipeline while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing speech language models, such as discretization errors and complex hierarchical architectures, by introducing a simpler, efficient alternative.

Method: Encodes speech waveforms into continuous latent representations and models them autoregressively using an energy distance objective.

Result: SLED achieves strong performance in zero-shot and streaming speech synthesis.

Conclusion: SLED simplifies speech language modeling while preserving performance, showing potential for broader applications.

Abstract: We introduce SLED, an alternative approach to speech language modeling by
encoding speech waveforms into sequences of continuous latent representations
and modeling them autoregressively using an energy distance objective. The
energy distance offers an analytical measure of the distributional gap by
contrasting simulated and target samples, enabling efficient training to
capture the underlying continuous autoregressive distribution. By bypassing
reliance on residual vector quantization, SLED avoids discretization errors and
eliminates the need for the complicated hierarchical architectures common in
existing speech language models. It simplifies the overall modeling pipeline
while preserving the richness of speech information and maintaining inference
efficiency. Empirical results demonstrate that SLED achieves strong performance
in both zero-shot and streaming speech synthesis, showing its potential for
broader applications in general-purpose speech language models.

</details>


### [163] [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
*Jikai Wang,Zhenxu Tian,Juntao Li,Qingrong Xia,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Min Zhang*

Main category: cs.CL

TL;DR: A training-free speculative decoding algorithm improves alignment between draft candidates and target outputs, enhancing generation speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for draft-target alignment require costly training. This paper aims to eliminate training costs while improving alignment and efficiency.

Method: Proposes alignment sampling using prefilling phase output distributions and introduces flexible verification with an adaptive probability threshold.

Result: Increases generation score by 3.3 points for LLaMA3, achieves mean acceptance length of 2.39, and speeds up generation by 2.23x.

Conclusion: The training-free approach effectively improves alignment and efficiency, outperforming existing methods.

Abstract: Recent works have revealed the great potential of speculative decoding in
accelerating the autoregressive generation process of large language models.
The success of these methods relies on the alignment between draft candidates
and the sampled outputs of the target model. Existing methods mainly achieve
draft-target alignment with training-based methods, e.g., EAGLE, Medusa,
involving considerable training costs. In this paper, we present a
training-free alignment-augmented speculative decoding algorithm. We propose
alignment sampling, which leverages output distribution obtained in the
prefilling phase to provide more aligned draft candidates. To further benefit
from high-quality but non-aligned draft candidates, we also introduce a simple
yet effective flexible verification strategy. Through an adaptive probability
threshold, our approach can improve generation accuracy while further improving
inference efficiency. Experiments on 8 datasets (including question answering,
summarization and code completion tasks) show that our approach increases the
average generation score by 3.3 points for the LLaMA3 model. Our method
achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.

</details>


### [164] [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)
*Xiaocong Du,Haoyu Pei,Haipeng Zhang*

Main category: cs.CL

TL;DR: A multimodal framework for sentiment analysis of classical Chinese poetry, incorporating audio and visual features alongside text, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis studies overlook rhythmic and visual features in poetry, which are crucial due to its recitation and association with Chinese paintings.

Method: Proposes a dialect-enhanced multimodal framework, extracting audio features from multiple dialects and visual features, fused with LLM-enhanced textual features via contrastive learning.

Result: Achieves at least 2.51% improvement in accuracy and 1.63% in macro F1 over state-of-the-art methods on two datasets.

Conclusion: The framework advances sentiment analysis in classical Chinese poetry and offers insights for general multimodal Chinese representation.

Abstract: Classical Chinese poetry is a vital and enduring part of Chinese literature,
conveying profound emotional resonance. Existing studies analyze sentiment
based on textual meanings, overlooking the unique rhythmic and visual features
inherent in poetry,especially since it is often recited and accompanied by
Chinese paintings. In this work, we propose a dialect-enhanced multimodal
framework for classical Chinese poetry sentiment analysis. We extract
sentence-level audio features from the poetry and incorporate audio from
multiple dialects,which may retain regional ancient Chinese phonetic features,
enriching the phonetic representation. Additionally, we generate sentence-level
visual features, and the multimodal features are fused with textual features
enhanced by LLM translation through multimodal contrastive representation
learning. Our framework outperforms state-of-the-art methods on two public
datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro
F1. We open-source the code to facilitate research in this area and provide
insights for general multimodal Chinese representation.

</details>


### [165] [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)
*Jie Ying,Zihong Chen,Zhefan Wang,Wanli Jiang,Chenyang Wang,Zhonghang Yuan,Haoyang Su,Huanjun Kong,Fan Yang,Nanqing Dong*

Main category: cs.CL

TL;DR: SeedBench is introduced as the first multi-task benchmark for seed science, evaluating 26 LLMs to address gaps in their application due to resource scarcity and complex gene-trait relationships.


<details>
  <summary>Details</summary>
Motivation: Challenges in seed science, like interdisciplinary complexity and high costs, hinder progress, creating a need for technological support like LLMs.

Method: SeedBench, developed with domain experts, simulates modern breeding processes and evaluates 26 LLMs (proprietary, open-source, fine-tuned).

Result: The evaluation reveals significant gaps between LLM capabilities and real-world seed science problems.

Conclusion: SeedBench lays the groundwork for future LLM research in seed design, addressing current limitations.

Abstract: Seed science is essential for modern agriculture, directly influencing crop
yields and global food security. However, challenges such as interdisciplinary
complexity and high costs with limited returns hinder progress, leading to a
shortage of experts and insufficient technological support. While large
language models (LLMs) have shown promise across various fields, their
application in seed science remains limited due to the scarcity of digital
resources, complex gene-trait relationships, and the lack of standardized
benchmarks. To address this gap, we introduce SeedBench -- the first multi-task
benchmark specifically designed for seed science. Developed in collaboration
with domain experts, SeedBench focuses on seed breeding and simulates key
aspects of modern breeding processes. We conduct a comprehensive evaluation of
26 leading LLMs, encompassing proprietary, open-source, and domain-specific
fine-tuned models. Our findings not only highlight the substantial gaps between
the power of LLMs and the real-world seed science problems, but also make a
foundational step for research on LLMs for seed design.

</details>


### [166] [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)
*Jieying Xue,Phuong Minh Nguyen,Minh Le Nguyen,Xin Liu*

Main category: cs.CL

TL;DR: The paper addresses multilingual multi-label emotion detection using pre-trained models and achieves strong performance in SemEval-2025 Task 11.


<details>
  <summary>Details</summary>
Motivation: The study aims to bridge gaps in text-based emotion detection due to the growing reliance on social media for multilingual communication.

Method: Two architectures are used: a fine-tuned BERT-based model and an instruction-tuned generative LLM. Two multi-label classification methods (base and pairwise) are proposed.

Result: Top 4 performance in Track A (10 languages, 1st in Hindi) and Top 5 in Track B (7 languages).

Conclusion: The approach demonstrates strong generalization in multilingual emotion recognition, proving its simplicity and effectiveness.

Abstract: With the rapid advancement of global digitalization, users from different
countries increasingly rely on social media for information exchange. In this
context, multilingual multi-label emotion detection has emerged as a critical
research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:
(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.
To tackle multilingual challenges, we leverage pre-trained multilingual models
and focus on two architectures: (1) a fine-tuned BERT-based classification
model and (2) an instruction-tuned generative LLM. Additionally, we propose two
methods for handling multi-label classification: the base method, which maps an
input directly to all its corresponding emotion labels, and the pairwise
method, which models the relationship between the input text and each emotion
category individually. Experimental results demonstrate the strong
generalization ability of our approach in multilingual emotion recognition. In
Track A, our method achieved Top 4 performance across 10 languages, ranking 1st
in Hindi. In Track B, our approach also secured Top 5 performance in 7
languages, highlighting its simplicity and effectiveness\footnote{Our code is
available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.

</details>


### [167] [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)
*Sidney Wong*

Main category: cs.CL

TL;DR: The paper discusses the dual impact of the internet on marginalized communities and advocates for social methods, informed by linguistics, to address hate speech and digital inclusion.


<details>
  <summary>Details</summary>
Motivation: The internet can both connect and alienate marginalized communities, with social media often perpetuating hate and misinformation. The paper aims to shift hate speech research from computational to social solutions.

Method: Proposes leveraging linguistics and social research to inform solutions, involving collaboration with communities, activists, and policymakers.

Result: Argues for a principle role of linguists and NLP researchers in addressing digital risks and promoting equitable digital inclusion.

Conclusion: Linguistics research can mitigate anti-social behavior online and help close the digital divide by working with stakeholders.

Abstract: The advent of the internet has been both a blessing and a curse for once
marginalised communities. When used well, the internet can be used to connect
and establish communities crossing different intersections; however, it can
also be used as a tool to alienate people and communities as well as perpetuate
hate, misinformation, and disinformation especially on social media platforms.
We propose steering hate speech research and researchers away from pre-existing
computational solutions and consider social methods to inform social solutions
to address this social problem. In a similar way linguistics research can
inform language planning policy, linguists should apply what we know about
language and society to mitigate some of the emergent risks and dangers of
anti-social behaviour in digital spaces. We argue linguists and NLP researchers
can play a principle role in unleashing the social impact potential of
linguistics research working alongside communities, advocates, activists, and
policymakers to enable equitable digital inclusion and to close the digital
divide.

</details>


### [168] [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)
*Rikhil Amonkar,Ronan Le Bras,Li Zhang*

Main category: cs.CL

TL;DR: The paper evaluates closed- and open-source LLMs in generating executable programs for textual planning tasks, showing mixed performance between programming and planning approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges LLMs face in complex textual planning tasks like meeting scheduling by exploring program generation as an alternative to auto-regressive planning.

Method: Systematic evaluation of LLMs generating executable programs (Python and constraint satisfaction solver code) instead of direct plans, comparing performance and robustness.

Result: Programming often outperforms planning, but generated code lacks robustness and efficiency, limiting generalization.

Conclusion: Program generation shows promise for complex planning tasks, but improvements in code robustness and efficiency are needed for broader applicability.

Abstract: Real-life textual planning tasks such as meeting scheduling have posed much
challenge to LLMs especially when the complexity is high. While previous work
primarily studied auto-regressive generation of plans with closed-source
models, we systematically evaluate both closed- and open-source models,
including those that scales output length with complexity during inference, in
generating programs, which are executed to output the plan. We consider not
only standard Python code, but also the code to a constraint satisfaction
problem solver. Despite the algorithmic nature of the task, we show that
programming often but not always outperforms planning. Our detailed error
analysis also indicates a lack of robustness and efficiency in the generated
code that hinders generalization.

</details>


### [169] [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)
*Siran Liu,Yang Ye,Qianchao Zhu,Zheng Cao,Yongchao He*

Main category: cs.CL

TL;DR: HeteroSpec is a framework for LLM inference that optimizes resource allocation based on linguistic complexity, achieving a 4.26x speedup without retraining.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding in LLMs is inefficient due to sequential processing and suboptimal resource allocation, which HeteroSpec addresses by adapting to linguistic complexity.

Method: HeteroSpec uses a cumulative meta-path Top-K entropy metric to identify predictable contexts and a dynamic resource allocation strategy for adaptive speculative decoding.

Result: HeteroSpec achieves a 4.26x speedup, outperforms EAGLE-3, and works well with stronger draft models without retraining.

Conclusion: HeteroSpec sets a new standard for context-aware LLM inference acceleration, offering efficiency and compatibility with other techniques.

Abstract: Autoregressive decoding, the standard approach for Large Language Model (LLM)
inference, remains a significant bottleneck due to its sequential nature. While
speculative decoding algorithms mitigate this inefficiency through parallel
verification, they fail to exploit the inherent heterogeneity in linguistic
complexity, a key factor leading to suboptimal resource allocation. We address
this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding
framework that dynamically optimizes computational resource allocation based on
linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A
novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying
predictable contexts. (2) A dynamic resource allocation strategy based on
data-driven entropy partitioning, enabling adaptive speculative expansion and
pruning tailored to local context difficulty. Evaluated on five public
benchmarks and four models, HeteroSpec achieves an average speedup of
4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across
speedup rates, average acceptance length, and verification cost. Notably,
HeteroSpec requires no draft model retraining, incurs minimal overhead, and is
orthogonal to other acceleration techniques. It demonstrates enhanced
acceleration with stronger draft models, establishing a new paradigm for
context-aware LLM inference acceleration.

</details>


### [170] [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)
*Zilu Tang,Afra Feyza Akyürek,Ekin Akyürek,Derry Wijaya*

Main category: cs.CL

TL;DR: The paper introduces WikiPersona, a dataset for fine-grained personalization of models to align with individual human preferences, addressing the lack of nuanced datasets in this area.


<details>
  <summary>Details</summary>
Motivation: Existing methods optimize for average human preferences, ignoring diversity and contradictions in individual preferences. The paper aims to enable personalized alignment with famous individuals' nuanced preferences.

Method: The authors propose WikiPersona, a dataset for fine-grained personalization, and evaluate approaches like few-shot prompting and fine-tuning, finding inferred personal preferences as prefixes most effective.

Result: Inferred personal preferences as prefixes enable effective personalization, especially in conflicting preference scenarios, and improve generalization across unseen personas.

Conclusion: WikiPersona advances personalized alignment by providing a dataset and method that ensures effectiveness and efficiency, particularly in nuanced preference scenarios.

Abstract: Preference alignment has become a standard pipeline in finetuning models to
follow \emph{generic} human preferences. Majority of work seeks to optimize
model to produce responses that would be preferable \emph{on average},
simplifying the diverse and often \emph{contradicting} space of human
preferences. While research has increasingly focused on personalized alignment:
adapting models to individual user preferences, there is a lack of personalized
preference dataset which focus on nuanced individual-level preferences. To
address this, we introduce WikiPersona: the first fine-grained personalization
using well-documented, famous individuals. Our dataset challenges models to
align with these personas through an interpretable process: generating
verifiable textual descriptions of a persona's background and preferences in
addition to alignment. We systematically evaluate different personalization
approaches and find that as few-shot prompting with preferences and fine-tuning
fail to simultaneously ensure effectiveness and efficiency, using
\textit{inferred personal preferences} as prefixes enables effective
personalization, especially in topics where preferences clash while leading to
more equitable generalization across unseen personas.

</details>


### [171] [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)
*Jingyi Ren,Yekun Xu,Xiaolong Wang,Weitao Li,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: ARENA (Adaptive-Rewarded Evidence Navigation Agent) improves RAG by enhancing LLMs' ability to utilize retrieved information and ensuring transparency through interpretable decision traces, achieving 10-30% performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in RAG: improving generator effectiveness in reasoning and ensuring transparency in retrieved content contributions.

Method: Proposes ARENA, a transparent RAG framework using reinforcement learning with adaptive rewards for structured reasoning and evidence identification.

Result: Achieves 10-30% improvements on multi-hop QA datasets, comparable to SOTA commercial LLMs, and shows flexibility without extra training.

Conclusion: ARENA enhances RAG performance and interpretability, with potential for broad adoption.

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.

</details>


### [172] [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
*Tianshi Zheng,Zheye Deng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: The paper surveys the transformative role of LLMs in scientific discovery, introducing a taxonomy (Tool, Analyst, Scientist) to classify their autonomy and responsibilities, while highlighting challenges like automation and ethics.


<details>
  <summary>Details</summary>
Motivation: To systematically explore how LLMs are evolving from task-specific tools to autonomous agents in science, reshaping research processes and human-AI collaboration.

Method: Introduces a three-level taxonomy (Tool, Analyst, Scientist) to categorize LLMs' roles and autonomy, and identifies key challenges and future research directions.

Result: Provides a conceptual framework for understanding LLMs' escalating capabilities in science, along with strategic insights for future AI-driven discovery.

Conclusion: The survey offers a roadmap for navigating and shaping the future of LLMs in scientific research, balancing innovation with ethical considerations.

Abstract: Large Language Models (LLMs) are catalyzing a paradigm shift in scientific
discovery, evolving from task-specific automation tools into increasingly
autonomous agents and fundamentally redefining research processes and human-AI
collaboration. This survey systematically charts this burgeoning field, placing
a central focus on the changing roles and escalating capabilities of LLMs in
science. Through the lens of the scientific method, we introduce a foundational
three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating
autonomy and evolving responsibilities within the research lifecycle. We
further identify pivotal challenges and future research trajectories such as
robotic automation, self-improvement, and ethical governance. Overall, this
survey provides a conceptual architecture and strategic foresight to navigate
and shape the future of AI-driven scientific discovery, fostering both rapid
innovation and responsible advancement. Github Repository:
https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.

</details>


### [173] [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)
*Livia Qian,Carol Figueroa,Gabriel Skantze*

Main category: cs.CL

TL;DR: The paper explores how prosodic similarity in vocal feedback is perceived and how well speech representations capture this, finding spectral and self-supervised methods outperform pitch features.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of prosody in vocal feedback is key for improving conversational systems by ensuring common ground.

Method: A triadic comparison task measures perceived similarity of feedback responses from two datasets, evaluating spectral, self-supervised, and pitch features.

Result: Spectral and self-supervised representations better encode prosody, especially for same-speaker feedback, and contrastive learning can align them with human perception.

Conclusion: Prosodic similarity in vocal feedback is better captured by advanced speech representations, with potential for further refinement through contrastive learning.

Abstract: Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of
spoken dialogue and is crucial to ensuring common ground in conversational
systems. The exact meaning of such feedback is conveyed through both lexical
and prosodic form. In this work, we investigate the perceived prosodic
similarity of vocal feedback with the same lexical form, and to what extent
existing speech representations reflect such similarities. A triadic comparison
task with recruited participants is used to measure perceived similarity of
feedback responses taken from two different datasets. We find that spectral and
self-supervised speech representations encode prosody better than extracted
pitch features, especially in the case of feedback from the same speaker. We
also find that it is possible to further condense and align the representations
to human perception through contrastive learning.

</details>


### [174] [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
*Lei Sheng,Shuai-Shuai Xu*

Main category: cs.CL

TL;DR: CSC-SQL integrates Self-Consistency and Self-Correction to improve SQL generation accuracy, using GRPO for fine-tuning, achieving high execution accuracy on BIRD.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Self-Consistency and Self-Correction have limitations in SQL generation, prompting the need for a combined approach.

Method: CSC-SQL merges outputs from parallel sampling and uses GRPO for reinforcement learning to fine-tune models.

Result: Achieves 65.28% (3B model) and 69.19% (7B model) execution accuracy on BIRD.

Conclusion: CSC-SQL is effective and generalizable, with plans to open-source the code.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
translating natural language questions about relational databases into SQL
queries. In particular, test-time scaling techniques such as Self-Consistency
and Self-Correction can enhance SQL generation accuracy by increasing
computational effort during inference. However, these methods have notable
limitations: Self-Consistency may select suboptimal outputs despite majority
votes, while Self-Correction typically addresses only syntactic errors. To
leverage the strengths of both approaches, we propose CSC-SQL, a novel method
that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two
most frequently occurring outputs from parallel sampling and feeds them into a
merge revision model for correction. Additionally, we employ the Group Relative
Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and
revision models via reinforcement learning, significantly enhancing output
quality. Experimental results confirm the effectiveness and generalizability of
CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution
accuracy, while the 7B model achieves 69.19%. The code will be open sourced at
https://github.com/CycloneBoy/csc_sql.

</details>


### [175] [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
*Sahil Mishra,Kumar Arjun,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: LORex is a plug-and-play framework combining discriminative ranking and generative reasoning for efficient taxonomy expansion, outperforming state-of-the-art methods by 12% accuracy and 5% Wu & Palmer similarity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for taxonomy expansion face challenges like representation limits, noise, and context limits, necessitating a more efficient solution.

Method: LORex ranks and chunks candidate terms into batches, filters noise, and iteratively refines selections by reasoning candidates' hierarchy.

Result: LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods across four benchmarks.

Conclusion: LORex effectively addresses limitations of existing methods, offering a robust solution for taxonomy expansion.

Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation
systems, and web applications. As data grows, expanding taxonomies is
essential, but existing methods face key challenges: (1) discriminative models
struggle with representation limits and generalization, while (2) generative
methods either process all candidates at once, introducing noise and exceeding
context limits, or discard relevant entities by selecting noisy candidates. We
propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for
Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines
discriminative ranking and generative reasoning for efficient taxonomy
expansion. Unlike prior methods, LORex ranks and chunks candidate terms into
batches, filtering noise and iteratively refining selections by reasoning
candidates' hierarchy to ensure contextual efficiency. Extensive experiments
across four benchmarks and twelve baselines show that LORex improves accuracy
by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

</details>


### [176] [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
*Alice Plebe,Timothy Douglas,Diana Riazi,R. Maria del Rio-Chanona*

Main category: cs.CL

TL;DR: The study explores how images influence vision-language models (VLMs) in resharing news, finding images increase resharing, especially for false news, and persona traits further modulate this effect.


<details>
  <summary>Details</summary>
Motivation: To understand the role of images in VLMs' misinformation spread and how persona traits and content attributes influence this behavior.

Method: Used a jailbreaking-inspired prompting strategy and a multimodal dataset of fact-checked political news with images and veracity labels.

Result: Images increased resharing rates (4.8% for true news, 15.0% for false news). Persona traits like Dark Triad amplified false news resharing, while only Claude-3-Haiku resisted visual misinformation.

Conclusion: Highlights risks in multimodal model behavior, urging tailored evaluation and mitigation for personalized AI systems.

Abstract: Large language models are increasingly integrated into news recommendation
systems, raising concerns about their role in spreading misinformation. In
humans, visual content is known to boost credibility and shareability of
information, yet its effect on vision-language models (VLMs) remains unclear.
We present the first study examining how images influence VLMs' propensity to
reshare news content, whether this effect varies across model families, and how
persona conditioning and content attributes modulate this behavior. To support
this analysis, we introduce two methodological contributions: a
jailbreaking-inspired prompting strategy that elicits resharing decisions from
VLMs while simulating users with antisocial traits and political alignments;
and a multimodal dataset of fact-checked political news from PolitiFact, paired
with corresponding images and ground-truth veracity labels. Experiments across
model families reveal that image presence increases resharing rates by 4.8% for
true news and 15.0% for false news. Persona conditioning further modulates this
effect: Dark Triad traits amplify resharing of false news, whereas
Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the
tested models, only Claude-3-Haiku demonstrates robustness to visual
misinformation. These findings highlight emerging risks in multimodal model
behavior and motivate the development of tailored evaluation frameworks and
mitigation strategies for personalized AI systems. Code and dataset are
available at: https://github.com/3lis/misinfo_vlm

</details>


### [177] [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)
*Qiguang Chen,Libo Qin,Jinhao Liu,Yue Liao,Jiaqi Wang,Jingxuan Zhou,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper introduces the Reasoning Boundary Framework++ (RBF++) to address challenges in evaluating and optimizing Chain-of-Thought (CoT) reasoning in LLMs, including measurable and unmeasurable capabilities like multimodal perception.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of quantitative metrics for CoT evaluation and the absence of methods for assessing unmeasurable CoT capabilities, such as multimodal perception.

Method: Proposes RBF++ with a reasoning boundary (RB) definition, combination law for measurable RBs, and a constant assumption with boundary division for unmeasurable RBs.

Result: Validated through experiments with 38 models across 13 tasks, offering insights into CoT optimization and expanding evaluation benchmarks.

Conclusion: RBF++ advances understanding of reasoning boundaries and optimization strategies in LLMs, with code and data made available for further research.

Abstract: Chain-of-Thought (CoT) reasoning has proven effective in enhancing large
language models (LLMs) on complex tasks, spurring research into its underlying
mechanisms. However, two primary challenges remain for real-world applications:
(1) the lack of quantitative metrics and actionable guidelines for evaluating
and optimizing measurable boundaries of CoT capability, and (2) the absence of
methods to assess boundaries of unmeasurable CoT capability, such as multimodal
perception. To address these gaps, we introduce the Reasoning Boundary
Framework++ (RBF++). To tackle the first challenge, we define the reasoning
boundary (RB) as the maximum limit of CoT performance. We also propose a
combination law for RBs, enabling quantitative analysis and offering actionable
guidance across various CoT tasks. For the second challenge, particularly in
multimodal scenarios, we introduce a constant assumption, which replaces
unmeasurable RBs with scenario-specific constants. Additionally, we propose the
reasoning boundary division mechanism, which divides unmeasurable RBs into two
sub-boundaries, facilitating the quantification and optimization of both
unmeasurable domain knowledge and multimodal perception capabilities. Extensive
experiments involving 38 models across 13 tasks validate the feasibility of our
framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,
offer insights into optimization and decay from two complementary perspectives,
and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope
this work advances the understanding of RBs and optimization strategies in
LLMs. Code and data are available at
https://github.com/LightChen233/reasoning-boundary.

</details>


### [178] [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)
*Zhijie Deng,Chris Yuhao Liu,Zirui Pang,Xinlei He,Lei Feng,Qi Xuan,Zhaowei Zhu,Jiaheng Wei*

Main category: cs.CL

TL;DR: GUARD enables dynamic unlearning in LLMs during generation, avoiding performance degradation by penalizing forbidden tokens without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Selective forgetting is crucial for LLM safety and compliance, but current fine-tuning methods harm overall performance.

Method: GUARD uses a prompt classifier to detect unlearning targets and dynamically penalizes forbidden tokens during generation via token and semantic matching.

Result: GUARD effectively unlearns in tasks like copyright content and entity removal without degrading general LLM capabilities.

Conclusion: GUARD strikes a balance between forgetting and utility, offering a practical solution for dynamic unlearning in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
memorizing vast amounts of knowledge across diverse domains. However, the
ability to selectively forget specific knowledge is critical for ensuring the
safety and compliance of deployed models. Existing unlearning efforts typically
fine-tune the model with resources such as forget data, retain data, and a
calibration model. These additional gradient steps blur the decision boundary
between forget and retain knowledge, making unlearning often at the expense of
overall performance. To avoid the negative impact of fine-tuning, it would be
better to unlearn solely at inference time by safely guarding the model against
generating responses related to the forget target, without destroying the
fluency of text generation. In this work, we propose Generation-time Unlearning
via Adaptive Restriction and Detection (GUARD), a framework that enables
dynamic unlearning during LLM generation. Specifically, we first employ a
prompt classifier to detect unlearning targets and extract the corresponding
forbidden token. We then dynamically penalize and filter candidate tokens
during generation using a combination of token matching and semantic matching,
effectively preventing the model from leaking the forgotten content.
Experimental results on copyright content unlearning tasks over the Harry
Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on
the TOFU dataset, demonstrate that GUARD achieves strong forget quality across
various tasks while causing almost no degradation to the LLM's general
capabilities, striking an excellent trade-off between forgetting and utility.

</details>


### [179] [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)
*Hongru Wang,Wenyu Huang,Yufei Wang,Yuanhao Xi,Jianqiao Lu,Huan Zhang,Nan Hu,Zeming Liu,Jeff Z. Pan,Kam-Fai Wong*

Main category: cs.CL

TL;DR: The paper introduces DialogTool, a multi-turn dialogue dataset for evaluating Language Models as Language Agents in stateful tool interactions, and VirtualMobile, a virtual environment for API robustness testing. It evaluates 13 LLMs, finding they struggle with long-horizon tool use.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for Language Agents focus on stateless, single-turn interactions, missing the stateful nature of multi-turn tool use. This gap is addressed by DialogTool and VirtualMobile.

Method: The authors propose DialogTool, a dataset for multi-turn tool interactions across six tasks in three stages (tool creation, utilization, and role-consistent response), and VirtualMobile for API evaluation.

Result: Evaluation of 13 LLMs shows they perform poorly in long-horizon tool use, despite advancements.

Conclusion: Current LLMs are inadequate for stateful, multi-turn tool interactions, highlighting the need for further research and development.

Abstract: Existing benchmarks that assess Language Models (LMs) as Language Agents
(LAs) for tool use primarily focus on stateless, single-turn interactions or
partial evaluations, such as tool selection in a single turn, overlooking the
inherent stateful nature of interactions in multi-turn applications. To fulfill
this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with
stateful tool interactions considering the whole life cycle of tool use, across
six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool
utilization}: tool awareness, tool selection, tool execution; and 3)
\textit{role-consistent response}: response generation and role play.
Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile
evaluation environment to simulate API calls and assess the robustness of the
created APIs\footnote{We will use tools and APIs alternatively, there are no
significant differences between them in this paper.}. Taking advantage of these
artifacts, we conduct comprehensive evaluation on 13 distinct open- and
closed-source LLMs and provide detailed analysis at each stage, revealing that
the existing state-of-the-art LLMs still cannot perform well to use tools over
long horizons.

</details>


### [180] [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
*Qiongqiong Wang,Hardik B. Sailor,Tianchi Liu,Ai Ti Aw*

Main category: cs.CL

TL;DR: The paper introduces a framework for generating datasets that combine contextual reasoning and paralinguistic understanding to improve speech-LLMs, validated by strong correlation with human-generated data.


<details>
  <summary>Details</summary>
Motivation: Current speech-LLMs lack contextual reasoning and paralinguistic understanding due to insufficient QA datasets covering both aspects.

Method: A novel framework for dataset generation from in-the-wild speech data, involving pseudo paralinguistic label-based condensation and LLM-based CPQA generation.

Result: Strong correlation between evaluations on the framework-generated dataset and human-generated CPQA dataset, revealing speech-LLM limitations in empathetic reasoning.

Conclusion: The framework is pioneering and can enhance speech-LLMs with paralinguistic reasoning, addressing current limitations.

Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning
alongside paralinguistic understanding, primarily due to the lack of
Question-Answer (QA) datasets that cover both aspects. We propose a novel
framework for dataset generation from in-the-wild speech data, that integrates
contextual reasoning with paralinguistic information. It consists of a pseudo
paralinguistic label-based data condensation of in-the-wild speech and
LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is
validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct
model on a dataset created by our framework and human-generated CPQA dataset.
The results also reveal the speech-LLM's limitations in handling empathetic
reasoning tasks, highlighting the need for such datasets and more robust
models. The proposed framework is first of its kind and has potential in
training more robust speech-LLMs with paralinguistic reasoning capabilities.

</details>


### [181] [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)
*Austin Xu,Yilun Zhou,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: The paper proposes training LLM-as-judge models with reinforcement learning (RL) to improve their performance in reasoning-intensive domains, introducing a new algorithm (EIS-GRPO), a benchmark (ReasoningJudgeBench), and a trained judge (J4R) that outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-as-judge models struggle with reasoning-intensive tasks, prompting the need for improved evaluation methods.

Method: The authors introduce the EIS-GRPO algorithm to train judges, addressing positional biases, and develop the ReasoningJudgeBench benchmark. They also train J4R, a 7B judge model.

Result: J4R outperforms GPT-4o and other small judges by 6.7% and 9%, matching or exceeding larger models on benchmarks.

Conclusion: The proposed RL-based training and new benchmark significantly enhance LLM-as-judge performance in reasoning tasks.

Abstract: To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.

</details>


### [182] [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)
*Narek Maloyan,Bislan Ashinov,Dmitry Namiot*

Main category: cs.CL

TL;DR: The paper explores vulnerabilities in LLM-as-a-Judge systems, demonstrating their susceptibility to prompt-injection attacks like CUA and JMA, with significant attack success rates.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and security of LLM-as-a-Judge systems, particularly their robustness against adversarial manipulations.

Method: Investigates prompt-injection attacks using the Greedy Coordinate Gradient (GCG) method on open-source LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct).

Result: CUA achieves >30% Attack Success Rate (ASR); JMA is also effective, revealing significant vulnerabilities.

Conclusion: Current LLM-as-a-Judge systems are highly vulnerable, necessitating robust defenses and further research into adversarial evaluation.

Abstract: Large Language Models (LLMs) are increasingly employed as evaluators
(LLM-as-a-Judge) for assessing the quality of machine-generated text. This
paradigm offers scalability and cost-effectiveness compared to human
annotation. However, the reliability and security of such systems, particularly
their robustness against adversarial manipulations, remain critical concerns.
This paper investigates the vulnerability of LLM-as-a-Judge architectures to
prompt-injection attacks, where malicious inputs are designed to compromise the
judge's decision-making process. We formalize two primary attack strategies:
Comparative Undermining Attack (CUA), which directly targets the final decision
output, and Justification Manipulation Attack (JMA), which aims to alter the
model's generated reasoning. Using the Greedy Coordinate Gradient (GCG)
optimization method, we craft adversarial suffixes appended to one of the
responses being compared. Experiments conducted on the MT-Bench Human Judgments
dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and
Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves
an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable
effectiveness. These findings highlight substantial vulnerabilities in current
LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and
further research into adversarial evaluation and trustworthiness in LLM-based
assessment frameworks.

</details>


### [183] [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)
*Adam Štorek,Mukur Gupta,Samira Hajizadeh,Prashast Srivastava,Suman Jana*

Main category: cs.CL

TL;DR: LLMs struggle with code reasoning in long contexts, especially semantic recall. SemTrace measures semantic recall, revealing accuracy drops in mid-context snippets. Lexical recall varies by granularity, and benchmarks may underestimate LLM challenges.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' effectiveness in utilizing long contexts for code reasoning and differentiate between lexical and semantic recall.

Method: Proposes SemTrace for semantic recall measurement and quantifies recall sensitivity in benchmarks. Evaluates state-of-the-art LLMs.

Result: Accuracy drops in mid-context code reasoning, especially for semantic recall. Lexical recall varies by granularity, with a disconnect from semantic recall. Benchmarks may lack sensitivity.

Conclusion: Current benchmarks may not fully capture LLM challenges in leveraging long contexts, highlighting the need for improved evaluation methods.

Abstract: Although modern Large Language Models (LLMs) support extremely large
contexts, their effectiveness in utilizing long context for code reasoning
remains unclear. This paper investigates LLM reasoning ability over code
snippets within large repositories and how it relates to their recall ability.
Specifically, we differentiate between lexical code recall (verbatim retrieval)
and semantic code recall (remembering what the code does). To measure semantic
recall, we propose SemTrace, a code reasoning technique where the impact of
specific statements on output is attributable and unpredictable. We also
present a method to quantify semantic recall sensitivity in existing
benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop
in code reasoning accuracy as a code snippet approaches the middle of the input
context, particularly with techniques requiring high semantic recall like
SemTrace. Moreover, we find that lexical recall varies by granularity, with
models excelling at function retrieval but struggling with line-by-line recall.
Notably, a disconnect exists between lexical and semantic recall, suggesting
different underlying mechanisms. Finally, our findings indicate that current
code reasoning benchmarks may exhibit low semantic recall sensitivity,
potentially underestimating LLM challenges in leveraging in-context
information.

</details>


### [184] [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
*Chenyang Yang,Yike Shi,Qianou Ma,Michael Xieyang Liu,Christian Kästner,Tongshuang Wu*

Main category: cs.CL

TL;DR: The paper analyzes prompt underspecification in LLM-powered software, showing its risks and limitations, and introduces a novel optimization method to improve performance.


<details>
  <summary>Details</summary>
Motivation: Developer prompts often underspecify requirements, leading to unreliable LLM behavior and performance regressions.

Method: The study analyzes underspecified prompts, tests LLM robustness, and introduces requirements-aware prompt optimization mechanisms.

Result: Underspecified prompts are 2x more likely to regress, and the new optimization method improves performance by 4.8% on average.

Conclusion: Managing prompt underspecification requires a broader process, including proactive requirements discovery and monitoring.

Abstract: Building LLM-powered software requires developers to communicate their
requirements through natural language, but developer prompts are frequently
underspecified, failing to fully capture many user-important requirements. In
this paper, we present an in-depth analysis of prompt underspecification,
showing that while LLMs can often (41.1%) guess unspecified requirements by
default, such behavior is less robust: Underspecified prompts are 2x more
likely to regress over model or prompt changes, sometimes with accuracy drops
by more than 20%. We then demonstrate that simply adding more requirements to a
prompt does not reliably improve performance, due to LLMs' limited
instruction-following capabilities and competing constraints, and standard
prompt optimizers do not offer much help. To address this, we introduce novel
requirements-aware prompt optimization mechanisms that can improve performance
by 4.8% on average over baselines that naively specify everything in the
prompt. Beyond prompt optimization, we envision that effectively managing
prompt underspecification requires a broader process, including proactive
requirements discovery, evaluation, and monitoring.

</details>


### [185] [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CL

TL;DR: Thinkless is a framework enabling LLMs to adaptively choose between short and long reasoning modes, improving efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiencies in LLMs when applying complex reasoning to simple problems, prompting the question of whether LLMs can learn when to think.

Method: Thinkless uses reinforcement learning with two control tokens (<short> and <think>) and a DeGRPO algorithm to decouple reasoning mode selection and answer accuracy.

Result: Thinkless reduces long-chain thinking usage by 50%-90% on benchmarks like Minerva Algebra, MATH-500, and GSM8K, enhancing efficiency.

Conclusion: Thinkless successfully balances reasoning efficiency and accuracy, demonstrating the feasibility of adaptive reasoning in LLMs.

Abstract: Reasoning Language Models, capable of extended chain-of-thought reasoning,
have demonstrated remarkable performance on tasks requiring complex logical
inference. However, applying elaborate reasoning for all queries often results
in substantial computational inefficiencies, particularly when many problems
admit straightforward solutions. This motivates an open question: Can LLMs
learn when to think? To answer this, we propose Thinkless, a learnable
framework that empowers an LLM to adaptively select between short-form and
long-form reasoning, based on both task complexity and the model's ability.
Thinkless is trained under a reinforcement learning paradigm and employs two
control tokens, <short> for concise responses and <think> for detailed
reasoning. At the core of our method is a Decoupled Group Relative Policy
Optimization (DeGRPO) algorithm, which decomposes the learning objective of
hybrid reasoning into two components: (1) a control token loss that governs the
selection of the reasoning mode, and (2) a response loss that improves the
accuracy of the generated answers. This decoupled formulation enables
fine-grained control over the contributions of each objective, stabilizing
training and effectively preventing collapse observed in vanilla GRPO.
Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and
GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -
90%, significantly improving the efficiency of Reasoning Language Models. The
code is available at https://github.com/VainF/Thinkless

</details>


### [186] [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
*David Anugraha,Zilu Tang,Lester James V. Miranda,Hanyang Zhao,Mohammad Rifqi Farhansyah,Garry Kuwanto,Derry Wijaya,Genta Indra Winata*

Main category: cs.CL

TL;DR: R3 is a new reward modeling framework addressing controllability and interpretability issues in aligning language models with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing reward models lack controllability, interpretability, and generalizability, limiting their effectiveness.

Method: Introduces R3, a rubric-agnostic framework for interpretable and generalizable reward modeling.

Result: R3 provides transparent, flexible evaluation and supports alignment with diverse human values.

Conclusion: R3 improves reward modeling by offering interpretability and generalizability, with open-source availability.

Abstract: Reward models are essential for aligning language model outputs with human
preferences, yet existing approaches often lack both controllability and
interpretability. These models are typically optimized for narrow objectives,
limiting their generalizability to broader downstream tasks. Moreover, their
scalar outputs are difficult to interpret without contextual reasoning. To
address these limitations, we introduce R3, a novel reward modeling framework
that is rubric-agnostic, generalizable across evaluation dimensions, and
provides interpretable, reasoned score assignments. R3 enables more transparent
and flexible evaluation of language models, supporting robust alignment with
diverse human values and use cases. Our models, data, and code are available as
open source at https://github.com/rubricreward/r3

</details>


### [187] [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)
*Renjie Pi,Felix Bai,Qibin Chen,Simon Wang,Jiulong Shan,Kieran Liu,Meng Cao*

Main category: cs.CL

TL;DR: The paper introduces MR. Judge, a method using MLLMs for evaluative judgments by framing the process as a reasoning-inspired multiple-choice problem, enhancing interpretability and performance. It includes strategies for automatic annotation and shows superior results over GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretability and performance of MLLMs as evaluative judges by leveraging reasoning capabilities, addressing the lack of scored response datasets.

Method: Formulates judgment as a reasoning-based multiple-choice problem, uses reverse response synthesis for annotation, and distills reasoning from text models.

Result: MR. Judge-7B outperforms GPT-4o by 9.9% on VL-RewardBench and improves MM-Vet performance by up to 7.7%.

Conclusion: MR. Judge effectively enhances MLLM judges' reasoning and performance, demonstrating broad applicability.

Abstract: The paradigm of using Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) as evaluative judges has emerged as an effective
approach in RLHF and inference-time scaling. In this work, we propose
Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering
general-purpose MLLMs judges with strong reasoning capabilities. Instead of
directly assigning scores for each response, we formulate the judgement process
as a reasoning-inspired multiple-choice problem. Specifically, the judge model
first conducts deliberate reasoning covering different aspects of the responses
and eventually selects the best response from them. This reasoning process not
only improves the interpretibility of the judgement, but also greatly enhances
the performance of MLLM judges. To cope with the lack of questions with scored
responses, we propose the following strategy to achieve automatic annotation:
1) Reverse Response Candidates Synthesis: starting from a supervised
fine-tuning (SFT) dataset, we treat the original response as the best candidate
and prompt the MLLM to generate plausible but flawed negative candidates. 2)
Text-based reasoning extraction: we carefully design a data synthesis pipeline
for distilling the reasoning capability from a text-based reasoning model,
which is adopted to enable the MLLM judges to regain complex reasoning ability
via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge
is effective across a wide range of tasks. Specifically, our MR. Judge-7B
surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet
during inference-time scaling by up to 7.7%.

</details>


### [188] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)
*Nithin Rao Koluguri,Monica Sekoyan,George Zelenfroynd,Sasha Meister,Shuoyang Ding,Sofia Kostandian,He Huang,Nikolay Karpov,Jagadeesh Balam,Vitaly Lavrukhin,Yifan Peng,Sara Papi,Marco Gaido,Alessio Brutti,Boris Ginsburg*

Main category: cs.CL

TL;DR: Granary introduces a large-scale speech dataset for 25 European languages, addressing data scarcity in low-resource languages through pseudo-labeling and translation pair generation, achieving comparable performance with 50% less data.


<details>
  <summary>Details</summary>
Motivation: Speech processing for low-resource languages is underexplored due to data scarcity. Granary aims to bridge this gap by providing a large-scale, open-source dataset for recognition and translation.

Method: The method involves a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. Translation pairs are generated using EuroLLM, followed by data filtration.

Result: Models trained on Granary's processed data achieve similar performance to those trained on larger datasets, using approximately 50% less data.

Conclusion: Granary successfully addresses data scarcity for low-resource languages, offering a scalable and efficient solution for speech recognition and translation.

Abstract: Multi-task and multilingual approaches benefit large models, yet speech
processing for low-resource languages remains underexplored due to data
scarcity. To address this, we present Granary, a large-scale collection of
speech datasets for recognition and translation across 25 European languages.
This is the first open-source effort at this scale for both transcription and
translation. We enhance data quality using a pseudo-labeling pipeline with
segmentation, two-pass inference, hallucination filtering, and punctuation
restoration. We further generate translation pairs from pseudo-labeled
transcriptions using EuroLLM, followed by a data filtration pipeline. Designed
for efficiency, our pipeline processes vast amount of data within hours. We
assess models trained on processed data by comparing their performance on
previously curated datasets for both high- and low-resource languages. Our
findings show that these models achieve similar performance using approx. 50%
less data. Dataset will be made available at
https://hf.co/datasets/nvidia/Granary

</details>


### [189] [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)
*Jiajie Zhang,Nianyi Lin,Lei Hou,Ling Feng,Juanzi Li*

Main category: cs.CL

TL;DR: AdaptThink is an RL algorithm that teaches reasoning models to adaptively choose between deep thinking or skipping it (NoThinking) based on task difficulty, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Lengthy thinking processes in reasoning models increase inference overhead, making efficiency a bottleneck. NoThinking is better for simple tasks, motivating adaptive thinking-mode selection.

Method: AdaptThink uses constrained optimization to encourage NoThinking while maintaining performance and an importance sampling strategy to balance training samples for both modes.

Result: AdaptThink reduces response length by 53% and improves accuracy by 2.4% on math datasets, enhancing efficiency and performance.

Conclusion: AdaptThink optimizes the balance between reasoning quality and efficiency, demonstrating promise for adaptive thinking-mode selection.

Abstract: Recently, large reasoning models have achieved impressive performance on
various tasks by employing human-like deep thinking. However, the lengthy
thinking process substantially increases inference overhead, making efficiency
a critical bottleneck. In this work, we first demonstrate that NoThinking,
which prompts the reasoning model to skip thinking and directly generate the
final solution, is a better choice for relatively simple tasks in terms of both
performance and efficiency. Motivated by this, we propose AdaptThink, a novel
RL algorithm to teach reasoning models to choose the optimal thinking mode
adaptively based on problem difficulty. Specifically, AdaptThink features two
core components: (1) a constrained optimization objective that encourages the
model to choose NoThinking while maintaining the overall performance; (2) an
importance sampling strategy that balances Thinking and NoThinking samples
during on-policy training, thereby enabling cold start and allowing the model
to explore and exploit both thinking modes throughout the training process. Our
experiments indicate that AdaptThink significantly reduces the inference costs
while further enhancing performance. Notably, on three math datasets,
AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B
by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive
thinking-mode selection for optimizing the balance between reasoning quality
and efficiency. Our codes and models are available at
https://github.com/THU-KEG/AdaptThink.

</details>


### [190] [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)
*Lotem Peled-Cohen,Maya Zadok,Nitay Calderon,Hila Gonen,Roi Reichart*

Main category: cs.CL

TL;DR: The paper explores how non-experts and LLMs perceive dementia through language, revealing that humans rely on narrow cues while LLMs use nuanced features closer to clinical patterns. Both groups often miss dementia cases.


<details>
  <summary>Details</summary>
Motivation: To understand how non-experts and LLMs detect dementia in language, as early detection is crucial but often relies on non-expert observations.

Method: Presented transcribed picture descriptions to non-experts and LLMs for intuitive judgment, using an explainable method with expert-guided features and logistic regression to model perceptions.

Result: Humans use inconsistent, narrow cues, while LLMs employ richer features aligning with clinical patterns. Both groups frequently overlook dementia cases.

Conclusion: The study aims to improve non-expert recognition of dementia through interpretable insights, highlighting the potential of LLMs for more accurate detection.

Abstract: Cognitive decline often surfaces in language years before diagnosis. It is
frequently non-experts, such as those closest to the patient, who first sense a
change and raise concern. As LLMs become integrated into daily communication
and used over prolonged periods, it may even be an LLM that notices something
is off. But what exactly do they notice--and should be noticing--when making
that judgment? This paper investigates how dementia is perceived through
language by non-experts. We presented transcribed picture descriptions to
non-expert humans and LLMs, asking them to intuitively judge whether each text
was produced by someone healthy or with dementia. We introduce an explainable
method that uses LLMs to extract high-level, expert-guided features
representing these picture descriptions, and use logistic regression to model
human and LLM perceptions and compare with clinical diagnoses. Our analysis
reveals that human perception of dementia is inconsistent and relies on a
narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a
richer, more nuanced feature set that aligns more closely with clinical
patterns. Still, both groups show a tendency toward false negatives, frequently
overlooking dementia cases. Through our interpretable framework and the
insights it provides, we hope to help non-experts better recognize the
linguistic signs that matter.

</details>


### [191] [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.CL

TL;DR: SMOTExT adapts SMOTE for text data, using BERT embeddings and xRAG to generate synthetic examples, showing promise for data augmentation and privacy-preserving ML.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and class imbalance in NLP, especially in specialized or low-resource domains.

Method: Interpolates BERT-based embeddings of existing examples and decodes them into text using xRAG.

Result: Qualitative outputs show potential for knowledge distillation, data augmentation, and privacy-preserving ML.

Conclusion: SMOTExT offers a viable solution for robust NLP training under data constraints and privacy concerns.

Abstract: Data scarcity and class imbalance are persistent challenges in training
robust NLP models, especially in specialized domains or low-resource settings.
We propose a novel technique, SMOTExT, that adapts the idea of Synthetic
Minority Over-sampling (SMOTE) to textual data. Our method generates new
synthetic examples by interpolating between BERT-based embeddings of two
existing examples and then decoding the resulting latent point into text with
xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation
framework, we can effectively turn interpolated vectors into coherent text.
While this is preliminary work supported by qualitative outputs only, the
method shows strong potential for knowledge distillation and data augmentation
in few-shot settings. Notably, our approach also shows promise for
privacy-preserving machine learning: in early experiments, training models
solely on generated data achieved comparable performance to models trained on
the original dataset. This suggests a viable path toward safe and effective
learning under data protection constraints.

</details>


### [192] [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
*Liyan Tang,Grace Kim,Xinyu Zhao,Thom Lake,Wenxuan Ding,Fangcong Yin,Prasann Singhal,Manya Wadhwa,Zeyu Leo Liu,Zayne Sprague,Ramya Namuduri,Bodun Hu,Juan Diego Rodriguez,Puyuan Peng,Greg Durrett*

Main category: cs.CL

TL;DR: The paper highlights the challenge of chart understanding for large vision-language models (LVLMs) due to their imbalance in visual and textual reasoning. It introduces ChartMuseum, a new benchmark revealing a significant performance gap between models and humans, especially in visual reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs struggle with visual reasoning in chart understanding, despite their strong textual capabilities. This imbalance motivates the need for a specialized benchmark to evaluate and improve these models.

Method: The study uses a synthetic dataset to demonstrate LVLMs' visual reasoning limitations and introduces ChartMuseum, a real-world chart QA benchmark with expert-annotated questions.

Result: Human accuracy on ChartMuseum is 93%, while the best model (Gemini-2.5-Pro) achieves 63.0%, and the top open-source LVLM (Qwen2.5-VL-72B-Instruct) scores 38.5%. Models show a 35%-55% drop in performance for visual reasoning tasks.

Conclusion: The paper underscores the need for improved visual reasoning in LVLMs, as current models lag behind humans, particularly in complex visual tasks. ChartMuseum serves as a valuable benchmark for future advancements.

Abstract: Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.

</details>


### [193] [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
*Vinay Samuel,Harshita Diddee,Yiming Zhang,Daphne Ippolito*

Main category: cs.CL

TL;DR: The paper introduces a method for continuous control of language model outputs, focusing on response-length, using interpolated token embeddings for reliable control.


<details>
  <summary>Details</summary>
Motivation: Enhancing user experience by enabling precise control over language model properties like length, complexity, and sentiment, beyond brittle natural language prompts or discrete signals.

Method: Fine-tuning language models with continuous control signals represented as interpolated vectors between "low" and "high" token embeddings.

Result: The method reliably controls response-length better than in-context learning or discrete signal fine-tuning.

Conclusion: Continuous control signals offer scalable and reliable user control over language model behaviors, demonstrated through response-length manipulation.

Abstract: Aligning language models with user intent is becoming increasingly relevant
to enhance user experience. This calls for designing methods that can allow
users to control the properties of the language that LMs generate. For example,
controlling the length of the generation, the complexity of the language that
gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate
users' control by conditioning LM generations on natural language prompts or
discrete control signals, which are often brittle and hard to scale. In this
work, we are interested in \textit{continuous} control signals, ones that exist
along a spectrum that can't easily be captured in a natural language prompt or
via existing techniques in conditional generation. Through a case study in
controlling the precise response-length of generations produced by LMs, we
demonstrate how after fine-tuning, behaviors of language models can be
controlled via continuous signals -- as vectors that are interpolated between a
"low" and a "high" token embedding. Our method more reliably exerts
response-length control than in-context learning methods or fine-tuning methods
that represent the control signal as a discrete signal. Our full open-sourced
code and datasets are available at https://github.com/vsamuel2003/CIE.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [194] [Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results](https://arxiv.org/abs/2505.11521)
*Wang Fang,Shirin Rahimi,Olivia Bennett,Sophie Carter,Mitra Hassani,Xu Lan,Omid Javadi,Lucas Mitchell*

Main category: cs.CV

TL;DR: A plug-and-play framework for Open-Set Semantic Segmentation (O3S) is proposed, using a novel regularizer (3CM) to enhance feature retention and unseen class detection.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with unseen classes in semantic segmentation, prompting the need for O3S solutions.

Method: The framework models segmentation as a conditional Markov chain and introduces 3CM to maximize mutual information between features and predictions per class.

Result: The method effectively detects unseen objects, as shown in experiments.

Conclusion: The paper highlights future directions for dynamic adaptation and efficient information-theoretic estimation in O3S.

Abstract: Point-cloud semantic segmentation underpins a wide range of critical
applications. Although recent deep architectures and large-scale datasets have
driven impressive closed-set performance, these models struggle to recognize or
properly segment objects outside their training classes. This gap has sparked
interest in Open-Set Semantic Segmentation (O3S), where models must both
correctly label known categories and detect novel, unseen classes. In this
paper, we propose a plug and play framework for O3S. By modeling the
segmentation pipeline as a conditional Markov chain, we derive a novel
regularizer term dubbed Conditional Channel Capacity Maximization (3CM), that
maximizes the mutual information between features and predictions conditioned
on each class. When incorporated into standard loss functions, 3CM encourages
the encoder to retain richer, label-dependent features, thereby enhancing the
network's ability to distinguish and segment previously unseen categories.
Experimental results demonstrate effectiveness of proposed method on detecting
unseen objects. We further outline future directions for dynamic open-world
adaptation and efficient information-theoretic estimation.

</details>


### [195] [Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis](https://arxiv.org/abs/2505.11581)
*Akarsh Kumar,Jeff Clune,Joel Lehman,Kenneth O. Stanley*

Main category: cs.CV

TL;DR: The paper challenges the assumption that better performance in AI systems implies better internal representations, showing that SGD-trained and evolved networks differ dramatically in representation quality despite similar outputs.


<details>
  <summary>Details</summary>
Motivation: To investigate whether scaling AI systems improves internal representations or just performance, using a simple task to visualize neuron behavior.

Method: Comparing neural networks trained via SGD and open-ended evolution on a single-image generation task, visualizing neuron behavior.

Result: SGD-trained networks show fractured entangled representations (FER), while evolved networks approach unified factored representations (UFR), despite similar outputs.

Conclusion: Understanding and mitigating FER could be crucial for improving generalization, creativity, and continual learning in AI.

Abstract: Much of the excitement in modern AI is driven by the observation that scaling
up existing systems leads to better performance. But does better performance
necessarily imply better internal representations? While the representational
optimist assumes it must, this position paper challenges that view. We compare
neural networks evolved through an open-ended search process to networks
trained via conventional stochastic gradient descent (SGD) on the simple task
of generating a single image. This minimal setup offers a unique advantage:
each hidden neuron's full functional behavior can be easily visualized as an
image, thus revealing how the network's output behavior is internally
constructed neuron by neuron. The result is striking: while both networks
produce the same output behavior, their internal representations differ
dramatically. The SGD-trained networks exhibit a form of disorganization that
we term fractured entangled representation (FER). Interestingly, the evolved
networks largely lack FER, even approaching a unified factored representation
(UFR). In large models, FER may be degrading core model capacities like
generalization, creativity, and (continual) learning. Therefore, understanding
and mitigating FER could be critical to the future of representation learning.

</details>


### [196] [Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization](https://arxiv.org/abs/2505.11620)
*Aaron Wilhelm,Nils Napp*

Main category: cs.CV

TL;DR: An improved bag-of-words (BoW) system for ground texture localization using a downward-facing camera, enhancing accuracy and speed for global localization and loop closure detection in SLAM.


<details>
  <summary>Details</summary>
Motivation: To provide a low-cost, high-precision localization solution robust to dynamic environments without requiring environmental modifications.

Method: Utilizes an approximate $k$-means (AKM) vocabulary with soft assignment, leveraging orientation and scale constraints of ground textures. Offers high-accuracy and high-speed versions tailored for global localization and loop closure detection.

Result: Demonstrated higher accuracy for global localization and improved precision/recall for loop closure detection via ablation studies.

Conclusion: The method can seamlessly replace existing BoW systems in ground texture localization pipelines, offering immediate performance improvements.

Abstract: Ground texture localization using a downward-facing camera offers a low-cost,
high-precision localization solution that is robust to dynamic environments and
requires no environmental modification. We present a significantly improved
bag-of-words (BoW) image retrieval system for ground texture localization,
achieving substantially higher accuracy for global localization and higher
precision and recall for loop closure detection in SLAM. Our approach leverages
an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits
the consistent orientation and constant scale constraints inherent to ground
texture localization. Identifying the different needs of global localization
vs. loop closure detection for SLAM, we present both high-accuracy and
high-speed versions of our algorithm. We test the effect of each of our
proposed improvements through an ablation study and demonstrate our method's
effectiveness for both global localization and loop closure detection. With
numerous ground texture localization systems already using BoW, our method can
readily replace other generic BoW systems in their pipeline and immediately
improve their results.

</details>


### [197] [BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations](https://arxiv.org/abs/2505.11640)
*Pandula Thennakoon,Avishka Ranasinghe,Mario De Silva,Buwaneka Epakanda,Roshan Godaliyadda,Parakrama Ekanayake,Vijitha Herath*

Main category: cs.CV

TL;DR: The paper introduces BandRC, a novel activation function for implicit neural representations (INRs), addressing challenges like spectral bias and noise robustness, and demonstrates its superiority in tasks like image reconstruction and denoising.


<details>
  <summary>Details</summary>
Motivation: Existing activation functions for INRs face issues like spectral bias, noise sensitivity, and manual tuning, limiting their performance in computer vision tasks.

Method: Proposes BandRC, a tailored activation function enhanced with deep prior knowledge, and validates it through mathematical analysis and experiments.

Result: BandRC outperforms SOTA methods in image reconstruction (+8.93 dB PSNR), denoising (+0.46 dB PSNR), super-resolution (+1.03 dB PSNR), and other tasks.

Conclusion: BandRC effectively addresses INR limitations and sets a new benchmark for activation functions in computer vision applications.

Abstract: In recent years, implicit neural representations(INRs) have gained popularity
in the computer vision community. This is mainly due to the strong performance
of INRs in many computer vision tasks. These networks can extract a continuous
signal representation given a discrete signal representation. In previous
studies, it has been repeatedly shown that INR performance has a strong
correlation with the activation functions used in its multilayer perceptrons.
Although numerous activation functions have been proposed that are competitive
with one another, they share some common set of challenges such as spectral
bias(Lack of sensitivity to high-frequency content in signals), limited
robustness to signal noise and difficulties in simultaneous capturing both
local and global features. and furthermore, the requirement for manual
parameter tuning. To address these issues, we introduce a novel activation
function, Band Shifted Raised Cosine Activated Implicit Neural Networks
\textbf{(BandRC)} tailored to enhance signal representation capacity further.
We also incorporate deep prior knowledge extracted from the signal to adjust
the activation functions through a task-specific model. Through a mathematical
analysis and a series of experiments which include image reconstruction (with a
+8.93 dB PSNR improvement over the nearest counterpart), denoising (with a
+0.46 dB increase in PSNR), super-resolution (with a +1.03 dB improvement over
the nearest State-Of-The-Art (SOTA) method for 6X super-resolution),
inpainting, and 3D shape reconstruction we demonstrate the dominance of BandRC
over existing state of the art activation functions.

</details>


### [198] [DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.11676)
*Ziyu Zhao,Xiaoguang Li,Linjia Shi,Nasrin Imanpour,Song Wang*

Main category: cs.CV

TL;DR: DPSeg, a dual prompting framework, improves open-vocabulary semantic segmentation by addressing domain gaps and leveraging multi-level feature guidance.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with domain gaps between image and text embeddings and lack shallow-level feature guidance, reducing segmentation accuracy.

Method: Proposes DPSeg, combining dual-prompt cost volume generation, a cost volume-guided decoder, and semantic-guided prompt refinement.

Result: Outperforms state-of-the-art methods on multiple datasets.

Conclusion: DPSeg effectively mitigates alignment issues and enhances segmentation accuracy.

Abstract: Open-vocabulary semantic segmentation aims to segment images into distinct
semantic regions for both seen and unseen categories at the pixel level.
Current methods utilize text embeddings from pre-trained vision-language models
like CLIP but struggle with the inherent domain gap between image and text
embeddings, even after extensive alignment during training. Additionally,
relying solely on deep text-aligned features limits shallow-level feature
guidance, which is crucial for detecting small objects and fine details,
ultimately reducing segmentation accuracy. To address these limitations, we
propose a dual prompting framework, DPSeg, for this task. Our approach combines
dual-prompt cost volume generation, a cost volume-guided decoder, and a
semantic-guided prompt refinement strategy that leverages our dual prompting
scheme to mitigate alignment issues in visual prompt generation. By
incorporating visual embeddings from a visual prompt encoder, our approach
reduces the domain gap between text and image embeddings while providing
multi-level guidance through shallow features. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches on multiple public datasets.

</details>


### [199] [LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance](https://arxiv.org/abs/2505.11703)
*Jae Myung Kim,Stephan Alaniz,Cordelia Schmid,Zeynep Akata*

Main category: cs.CV

TL;DR: LoFT introduces a novel dataset generation framework using LoRA-Fused Training-data Generation with Few-shot Guidance to improve synthetic data fidelity and diversity for supervised learning.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic datasets often lack fidelity or diversity, failing to match real data distributions or capture unique features of real images.

Method: LoFT fine-tunes LoRA weights on individual real images and fuses them at inference to generate synthetic images with combined features.

Result: LoFT-generated data outperforms other methods, significantly improving accuracy as dataset size increases, while maintaining high fidelity and diversity.

Conclusion: LoFT effectively enhances synthetic dataset quality for supervised learning, offering a scalable solution with demonstrated performance gains.

Abstract: Despite recent advances in text-to-image generation, using synthetically
generated data seldom brings a significant boost in performance for supervised
learning. Oftentimes, synthetic datasets do not faithfully recreate the data
distribution of real data, i.e., they lack the fidelity or diversity needed for
effective downstream model training. While previous work has employed few-shot
guidance to address this issue, existing methods still fail to capture and
generate features unique to specific real images. In this paper, we introduce a
novel dataset generation framework named LoFT, LoRA-Fused Training-data
Generation with Few-shot Guidance. Our method fine-tunes LoRA weights on
individual real images and fuses them at inference time, producing synthetic
images that combine the features of real images for improved diversity and
fidelity of generated data. We evaluate the synthetic data produced by LoFT on
10 datasets, using 8 to 64 real images per class as guidance and scaling up to
1000 images per class. Our experiments show that training on LoFT-generated
data consistently outperforms other synthetic dataset methods, significantly
increasing accuracy as the dataset size increases. Additionally, our analysis
demonstrates that LoFT generates datasets with high fidelity and sufficient
diversity, which contribute to the performance improvement. The code is
available at https://github.com/ExplainableML/LoFT.

</details>


### [200] [Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration](https://arxiv.org/abs/2505.11707)
*Haipeng Fang,Sheng Tang,Juan Cao,Enshuo Zhang,Fan Tang,Tong-Yee Lee*

Main category: cs.CV

TL;DR: The paper introduces SDTM, a token merging method for diffusion transformers, optimizing computational efficiency without compromising image quality by leveraging denoising priors.


<details>
  <summary>Details</summary>
Motivation: Existing token reduction techniques in diffusion transformers ignore denoising priors, causing inefficiency and quality loss. This work addresses this gap.

Method: SDTM dynamically compresses feature redundancies using structure-then-detail priors, incorporating dynamic token merging, compression ratio adjustment, and prompt reweighting.

Result: SDTM achieves 1.55× acceleration with minimal quality impact, validated across various backbones, schedulers, and datasets.

Conclusion: SDTM offers a post-training solution for efficient diffusion transformers, balancing speed and quality by attending to denoising priors.

Abstract: Diffusion transformers have shown exceptional performance in visual
generation but incur high computational costs. Token reduction techniques that
compress models by sharing the denoising process among similar tokens have been
introduced. However, existing approaches neglect the denoising priors of the
diffusion models, leading to suboptimal acceleration and diminished image
quality. This study proposes a novel concept: attend to prune feature
redundancies in areas not attended by the diffusion process. We analyze the
location and degree of feature redundancies based on the structure-then-detail
denoising priors. Subsequently, we introduce SDTM, a structure-then-detail
token merging approach that dynamically compresses feature redundancies.
Specifically, we design dynamic visual token merging, compression ratio
adjusting, and prompt reweighting for different stages. Served in a
post-training way, the proposed method can be integrated seamlessly into any
DiT architecture. Extensive experiments across various backbones, schedulers,
and datasets showcase the superiority of our method, for example, it achieves
1.55 times acceleration with negligible impact on image quality. Project page:
https://github.com/ICTMCG/SDTM.

</details>


### [201] [EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video](https://arxiv.org/abs/2505.11709)
*Ryan Hoque,Peide Huang,David J. Yoon,Mouli Sivapurapu,Jian Zhang*

Main category: cs.CV

TL;DR: EgoDex is a large-scale dataset of dexterous human manipulation, collected using Apple Vision Pro, with 829 hours of egocentric video and 3D hand tracking, aiming to address data scarcity in imitation learning.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in imitation learning for manipulation tasks, lacking large-scale datasets with hand pose annotations.

Method: Used Apple Vision Pro to collect EgoDex, featuring 829 hours of egocentric video with 3D hand and finger tracking, covering 194 diverse tasks.

Result: Created the largest and most diverse dataset for dexterous manipulation, enabling training and evaluation of imitation learning policies.

Conclusion: EgoDex aims to advance robotics, computer vision, and foundation models by providing a scalable, annotated dataset for manipulation tasks.

Abstract: Imitation learning for manipulation has a well-known data scarcity problem.
Unlike natural language and 2D computer vision, there is no Internet-scale
corpus of data for dexterous manipulation. One appealing option is egocentric
human video, a passively scalable data source. However, existing large-scale
datasets such as Ego4D do not have native hand pose annotations and do not
focus on object manipulation. To this end, we use Apple Vision Pro to collect
EgoDex: the largest and most diverse dataset of dexterous human manipulation to
date. EgoDex has 829 hours of egocentric video with paired 3D hand and finger
tracking data collected at the time of recording, where multiple calibrated
cameras and on-device SLAM can be used to precisely track the pose of every
joint of each hand. The dataset covers a wide range of diverse manipulation
behaviors with everyday household objects in 194 different tabletop tasks
ranging from tying shoelaces to folding laundry. Furthermore, we train and
systematically evaluate imitation learning policies for hand trajectory
prediction on the dataset, introducing metrics and benchmarks for measuring
progress in this increasingly important area. By releasing this large-scale
dataset, we hope to push the frontier of robotics, computer vision, and
foundation models.

</details>


### [202] [UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights](https://arxiv.org/abs/2505.11720)
*Shijun Liang,Ismail R. Alkhouri,Siddhant Gautam,Qing Qu,Saiprasad Ravishankar*

Main category: cs.CV

TL;DR: UGoDIT is an unsupervised method for image recovery in low-data regimes, leveraging transferable weights and shared encoders to improve reconstruction quality without needing large clean datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods like diffusion models require large clean datasets, while DIP-based approaches suffer from noise overfitting and inefficiency. UGoDIT addresses these limitations by learning from small sub-sampled measurements.

Method: UGoDIT uses a shared encoder and M disentangled decoders to learn transferable weights. At test time, it combines fixed learned weights with optimized parameters for measurement consistency.

Result: UGoDIT outperforms standalone DIP methods in convergence speed and reconstruction quality, and matches SOTA supervised and DM-based methods without large clean datasets.

Conclusion: UGoDIT is a practical solution for image recovery in low-data settings, offering efficiency and competitive performance.

Abstract: Recent advances in data-centric deep generative models have led to
significant progress in solving inverse imaging problems. However, these models
(e.g., diffusion models (DMs)) typically require large amounts of fully sampled
(clean) training data, which is often impractical in medical and scientific
settings such as dynamic imaging.
  On the other hand, training-data-free approaches like the Deep Image Prior
(DIP) do not require clean ground-truth images but suffer from noise
overfitting and can be computationally expensive as the network parameters need
to be optimized for each measurement set independently. Moreover, DIP-based
methods often overlook the potential of learning a prior using a small number
of sub-sampled measurements (or degraded images) available during training. In
this paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable
weights, designed for the low-data regime where only a very small number, M, of
sub-sampled measurement vectors are available during training. Our method
learns a set of transferable weights by optimizing a shared encoder and M
disentangled decoders. At test time, we reconstruct the unseen degraded image
using a DIP network, where part of the parameters are fixed to the learned
weights, while the remaining are optimized to enforce measurement consistency.
We evaluate UGoDIT on both medical (multi-coil MRI) and natural (super
resolution and non-linear deblurring) image recovery tasks under various
settings. Compared to recent standalone DIP methods, UGoDIT provides
accelerated convergence and notable improvement in reconstruction quality.
Furthermore, our method achieves performance competitive with SOTA DM-based and
supervised approaches, despite not requiring large amounts of clean training
data.

</details>


### [203] [Semantically-Aware Game Image Quality Assessment](https://arxiv.org/abs/2505.11724)
*Kai Zhu,Vignesh Edithal,Le Zhang,Ilia Blank,Imran Junejo*

Main category: cs.CV

TL;DR: The paper introduces a semantically-aware no-reference image quality assessment (NR-IQA) model for video games, addressing unique distortions like aliasing and texture blur. It uses a Game distortion feature extractor (GDFE) and semantic gating via CLIP embeddings to improve accuracy and contextual relevance.


<details>
  <summary>Details</summary>
Motivation: Existing NR-IQA/VQA methods fail to generalize to gaming due to distinct distortions. The study aims to bridge this gap by developing a model tailored to gaming environments.

Method: The model employs a knowledge-distilled GDFE to detect game-specific distortions and integrates semantic gating with CLIP embeddings for dynamic feature weighting. Training uses gameplay data across graphical quality presets.

Result: The GDFE generalizes well to unseen distortion levels, and semantic gating enhances contextual relevance. The model outperforms out-of-domain methods and shows robust quality trends in unseen games.

Conclusion: This work advances NR-IQA for gaming, providing a foundation for automated graphical quality assessment in this domain.

Abstract: Assessing the visual quality of video game graphics presents unique
challenges due to the absence of reference images and the distinct types of
distortions, such as aliasing, texture blur, and geometry level of detail (LOD)
issues, which differ from those in natural images or user-generated content.
Existing no-reference image and video quality assessment (NR-IQA/VQA) methods
fail to generalize to gaming environments as they are primarily designed for
distortions like compression artifacts. This study introduces a
semantically-aware NR-IQA model tailored to gaming. The model employs a
knowledge-distilled Game distortion feature extractor (GDFE) to detect and
quantify game-specific distortions, while integrating semantic gating via CLIP
embeddings to dynamically weight feature importance based on scene content.
Training on gameplay data recorded across graphical quality presets enables the
model to produce quality scores that align with human perception. Our results
demonstrate that the GDFE, trained through knowledge distillation from binary
classifiers, generalizes effectively to intermediate distortion levels unseen
during training. Semantic gating further improves contextual relevance and
reduces prediction variance. In the absence of in-domain NR-IQA baselines, our
model outperforms out-of-domain methods and exhibits robust, monotonic quality
trends across unseen games in the same genre. This work establishes a
foundation for automated graphical quality assessment in gaming, advancing
NR-IQA methods in this domain.

</details>


### [204] [X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models](https://arxiv.org/abs/2505.11753)
*Valentina Bazyleva,Nicolo Bonettini,Gaurav Bharaj*

Main category: cs.CV

TL;DR: X-Edit is a novel method for localizing diffusion-based image edits by inverting images and using a segmentation network with attention mechanisms, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of text-guided diffusion models for image editing poses challenges in detecting subtle deepfake edits, necessitating a tool to localize such modifications.

Method: X-Edit inverts images using a pretrained diffusion model, then employs a segmentation network with channel and spatial attention, trained with segmentation and relevance losses.

Result: X-Edit accurately localizes edits, outperforming baselines in PSNR and SSIM metrics, and introduces a new dataset for this task.

Conclusion: X-Edit is a robust forensic tool for detecting and pinpointing manipulations by advanced image editing techniques.

Abstract: Text-guided diffusion models have significantly advanced image editing,
enabling highly realistic and local modifications based on textual prompts.
While these developments expand creative possibilities, their malicious use
poses substantial challenges for detection of such subtle deepfake edits. To
this end, we introduce Explain Edit (X-Edit), a novel method for localizing
diffusion-based edits in images. To localize the edits for an image, we invert
the image using a pretrained diffusion model, then use these inverted features
as input to a segmentation network that explicitly predicts the edited masked
regions via channel and spatial attention. Further, we finetune the model using
a combined segmentation and relevance loss. The segmentation loss ensures
accurate mask prediction by balancing pixel-wise errors and perceptual
similarity, while the relevance loss guides the model to focus on low-frequency
regions and mitigate high-frequency artifacts, enhancing the localization of
subtle edits. To the best of our knowledge, we are the first to address and
model the problem of localizing diffusion-based modified regions in images. We
additionally contribute a new dataset of paired original and edited images
addressing the current lack of resources for this task. Experimental results
demonstrate that X-Edit accurately localizes edits in images altered by
text-guided diffusion models, outperforming baselines in PSNR and SSIM metrics.
This highlights X-Edit's potential as a robust forensic tool for detecting and
pinpointing manipulations introduced by advanced image editing techniques.

</details>


### [205] [Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning](https://arxiv.org/abs/2505.11758)
*Sriram Mandalika*

Main category: cs.CV

TL;DR: PromptFuseNL improves few-shot adaptation in VLMs by combining prompt tuning with dual-branch learning, addressing noise and inefficiency.


<details>
  <summary>Details</summary>
Motivation: Few-shot adaptation in VLMs is challenging due to limited supervision and noisy samples.

Method: Uses predictive prompt tuning, dual-branch learning, task-conditioned residuals, cross-modal coordination, and semantic hard negative mining. Includes unsupervised instance reweighting for noise.

Result: Outperforms existing methods across 15 benchmarks, with 300x faster training and 1000x lower FLOPs.

Conclusion: Sets a new state-of-the-art for robust and scalable few-shot vision-language adaptation.

Abstract: Few-shot adaptation remains a core challenge for vision-language models
(VLMs), especially under limited supervision and noisy support samples. We
propose PromptFuseNL, a unified framework that enhances few-shot generalization
by combining predictive prompt tuning with dual-branch positive and negative
learning. The method refines class prototypes through task-conditioned
residuals, multi-stage cross-modal coordination, and semantic hard negative
mining. To address label noise, we introduce an unsupervised instance
reweighting strategy that downweights unreliable support examples without
requiring additional labels or structural changes. PromptFuseNL fuses visual
and textual cues through lightweight modules for efficient and discriminative
prediction. Evaluated across 15 benchmarks, it consistently surpasses existing
prompt- and adapter-based methods in all shot settings while remaining highly
efficient, achieving up to 300x faster training and 1000x lower FLOPs compared
to full prompt tuning, achieving a new state-of-the-art for robust and scalable
few-shot vision-language adaptation.

</details>


### [206] [Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average](https://arxiv.org/abs/2505.11769)
*Wonjune Kim,Lae-kyoung Lee,Su-Yong An*

Main category: cs.CV

TL;DR: A high-capacity semantic segmentation pipeline, using FlashInternImage-B and UPerNet, achieves 88.8% mIoU on GOOSE 2D Challenge by adapting existing techniques and strong augmentation.


<details>
  <summary>Details</summary>
Motivation: To address the distinctive challenges of semantic segmentation in unstructured off-road environments.

Method: Combines FlashInternImage-B backbone with UPerNet decoder, photometric distortion augmentation, and EMA for weight generalization.

Result: Achieves 88.8% mIoU on the GOOSE validation set.

Conclusion: Adapting established techniques with robust training strategies effectively handles off-road semantic segmentation.

Abstract: We report on the application of a high-capacity semantic segmentation
pipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured
off-road environments. Using a FlashInternImage-B backbone together with a
UPerNet decoder, we adapt established techniques, rather than designing new
ones, to the distinctive conditions of off-road scenes. Our training recipe
couples strong photometric distortion augmentation (to emulate the wide
lighting variations of outdoor terrain) with an Exponential Moving Average
(EMA) of weights for better generalization. Using only the GOOSE training
dataset, we achieve 88.8\% mIoU on the validation set.

</details>


### [207] [Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations](https://arxiv.org/abs/2505.11777)
*Fu-Yun Wang,Keqiang Sun,Yao Teng,Xihui Liu,Jiaming Song,Hongsheng Li*

Main category: cs.CV

TL;DR: Self-NPO introduces a self-supervised negative preference optimization method for diffusion models, eliminating the need for costly explicit preference annotations and improving generation quality and human alignment.


<details>
  <summary>Details</summary>
Motivation: Existing preference optimization methods overlook classifier-free guidance and rely on expensive explicit preference annotations, limiting practicality in data-scarce domains.

Method: Self-NPO learns negative preferences exclusively from the model itself, avoiding manual labeling or reward model training, and integrates efficiently into popular diffusion models.

Result: Self-NPO enhances generation quality and human preference alignment in models like SD1.5, SDXL, and CogVideoX, even those already optimized for preferences.

Conclusion: Self-NPO offers a practical, efficient, and annotation-free solution for improving diffusion models' alignment with human preferences.

Abstract: Diffusion models have demonstrated remarkable success in various visual
generation tasks, including image, video, and 3D content generation. Preference
optimization (PO) is a prominent and growing area of research that aims to
align these models with human preferences. While existing PO methods primarily
concentrate on producing favorable outputs, they often overlook the
significance of classifier-free guidance (CFG) in mitigating undesirable
results. Diffusion-NPO addresses this gap by introducing negative preference
optimization (NPO), training models to generate outputs opposite to human
preferences and thereby steering them away from unfavorable outcomes. However,
prior NPO approaches, including Diffusion-NPO, rely on costly and fragile
procedures for obtaining explicit preference annotations (e.g., manual pairwise
labeling or reward model training), limiting their practicality in domains
where such data are scarce or difficult to acquire. In this work, we introduce
Self-NPO, a Negative Preference Optimization approach that learns exclusively
from the model itself, thereby eliminating the need for manual data labeling or
reward model training. Moreover, our method is highly efficient and does not
require exhaustive data sampling. We demonstrate that Self-NPO integrates
seamlessly into widely used diffusion models, including SD1.5, SDXL, and
CogVideoX, as well as models already optimized for human preferences,
consistently enhancing both their generation quality and alignment with human
preferences.

</details>


### [208] [CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection](https://arxiv.org/abs/2505.11793)
*Jianing Wang,Siying Guo,Zheng Hua,Runhu Huang,Jinyu Hu,Maoguo Gong*

Main category: cs.CV

TL;DR: A novel continual learning-based capsule differential generative adversarial network (CL-CaGAN) is proposed for hyperspectral anomaly detection (HAD) to address limited prior information and catastrophic forgetting in cross-domain scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing DL-based anomaly detection methods struggle with limited prior information and catastrophic forgetting in open, cross-domain scenarios, hindering real-world application.

Method: CL-CaGAN combines a modified capsule structure with adversarial learning, clustering-based sample replay, self-distillation regularization, and differentiable enhancement to improve cross-scenario learning and mitigate forgetting.

Result: Experiments on real HSIs show CL-CaGAN achieves higher detection performance and better continual learning capacity in cross-domain scenarios.

Conclusion: CL-CaGAN effectively addresses challenges in HAD by enhancing cross-scenario learning and mitigating catastrophic forgetting, demonstrating superior performance.

Abstract: Anomaly detection (AD) has attracted remarkable attention in hyperspectral
image (HSI) processing fields, and most existing deep learning (DL)-based
algorithms indicate dramatic potential for detecting anomaly samples through
specific training process under current scenario. However, the limited prior
information and the catastrophic forgetting problem indicate crucial challenges
for existing DL structure in open scenarios cross-domain detection. In order to
improve the detection performance, a novel continual learning-based capsule
differential generative adversarial network (CL-CaGAN) is proposed to elevate
the cross-scenario learning performance for facilitating the real application
of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule
structure with adversarial learning network is constructed to estimate the
background distribution for surmounting the deficiency of prior information. To
mitigate the catastrophic forgetting phenomenon, clustering-based sample replay
strategy and a designed extra self-distillation regularization are integrated
for merging the history and future knowledge in continual AD task, while the
discriminative learning ability from previous detection scenario to current
scenario is retained by the elaborately designed structure with continual
learning (CL) strategy. In addition, the differentiable enhancement is enforced
to augment the generation performance of the training data. This further
stabilizes the training process with better convergence and efficiently
consolidates the reconstruction ability of background samples. To verify the
effectiveness of our proposed CL-CaGAN, we conduct experiments on several real
HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher
detection performance and continuous learning capacity for mitigating the
catastrophic forgetting under cross-domain scenarios.

</details>


### [209] [CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2505.11796)
*Jianing Wang,Zheng Hua,Wan Zhang,Shengjia Hao,Yuqiong Yao,Maoguo Gong*

Main category: cs.CV

TL;DR: A biologically-inspired continual learning GAN (CL-BioGAN) is proposed for cross-domain hyperspectral anomaly detection, combining active forgetting and replay strategies for improved performance.


<details>
  <summary>Details</summary>
Motivation: Addressing memory stability and learning flexibility in continual learning for hyperspectral anomaly detection by mimicking biological neural networks' ability to forget conflicting knowledge.

Method: Introduces CL-BioGAN with CL-Bio Loss (Active Forgetting Loss + CL loss) and BioGAN loss with L2-Norm to balance stability and flexibility.

Result: Achieves robust accuracy for cross-domain HAD with fewer parameters and lower computation cost.

Conclusion: CL-BioGAN enhances continual learning performance and provides insights into neural adaptation for open-scenario HAD tasks.

Abstract: Memory stability and learning flexibility in continual learning (CL) is a
core challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task.
Biological neural networks can actively forget history knowledge that conflicts
with the learning of new experiences by regulating learning-triggered synaptic
expansion and synaptic convergence. Inspired by this phenomenon, we propose a
novel Biologically-Inspired Continual Learning Generative Adversarial Network
(CL-BioGAN) for augmenting continuous distribution fitting ability for
cross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss)
and self-attention Generative Adversarial Network (BioGAN) are incorporated to
realize forgetting history knowledge as well as involving replay strategy in
the proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an
Active Forgetting Loss (AF Loss) and a CL loss is designed to realize
parameters releasing and enhancing between new task and history tasks from a
Bayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances
self-attention (SA) to further balance the stability and flexibility for better
fitting background distribution for open scenario HAD (OHAD) tasks. Experiment
results underscore that the proposed CL-BioGAN can achieve more robust and
satisfying accuracy for cross-domain HAD with fewer parameters and computation
cost. This dual contribution not only elevates CL performance but also offers
new insights into neural adaptation mechanisms in OHAD task.

</details>


### [210] [Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model](https://arxiv.org/abs/2505.11800)
*Jian Zhu,He Wang,Yang Xu,Zebin Wu,Zhihui Wei*

Main category: cs.CV

TL;DR: ARGS-Diff is a self-learning model for HSI-MSI fusion that avoids the need for extra training data by using lightweight spectral and spatial diffusion models and an adaptive residual guided module.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitation of scarce hyperspectral data for supervised training in HSI-MSI fusion.

Method: Uses two diffusion models to learn spectral and spatial distributions from LR-HSI and HR-MSI, then reconstructs HR-HSI via reverse diffusion with an adaptive residual guided module.

Result: Outperforms state-of-the-art methods in performance and computational efficiency.

Conclusion: ARGS-Diff is an effective, data-efficient solution for HSI-MSI fusion.

Abstract: Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a
low-resolution hyperspectral image (LR-HSI) with a high-resolution
multispectral image (HR-MSI) to generate a high-resolution hyperspectral image
(HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large
amounts of hyperspectral data for supervised training, which is often scarce in
practical applications. In this paper, we propose a self-learning Adaptive
Residual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the
observed images without any extra training data. Specifically, as the LR-HSI
contains spectral information and the HR-MSI contains spatial information, we
design two lightweight spectral and spatial diffusion models to separately
learn the spectral and spatial distributions from them. Then, we use these two
models to reconstruct HR-HSI from two low-dimensional components, i.e, the
spectral basis and the reduced coefficient, during the reverse diffusion
process. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM),
which refines the two components through a residual guided function at each
sampling step, thereby stabilizing the sampling process. Extensive experimental
results demonstrate that ARGS-Diff outperforms existing state-of-the-art
methods in terms of both performance and computational efficiency in the field
of HSI-MSI fusion. Code is available at https://github.com/Zhu1116/ARGS-Diff.

</details>


### [211] [Are vision language models robust to uncertain inputs?](https://arxiv.org/abs/2505.11804)
*Xi Wang,Eric Nalisnick*

Main category: cs.CV

TL;DR: Larger vision-language models (VLMs) show improved robustness but still hallucinate confident responses to uncertain inputs. Prompting models to abstain from uncertain predictions improves reliability, but domain-specific tasks remain challenging. A novel caption diversity mechanism helps predict model uncertainty.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robustness in deep learning models against uncertain and ambiguous inputs, despite advancements in large-scale VLMs.

Method: Empirical evaluation using anomaly detection and ambiguous classification tasks, testing newer VLMs. Proposed a caption diversity mechanism for uncertainty prediction.

Result: Newer VLMs are more robust but still hallucinate. Prompting for abstention improves reliability, except in domain-specific tasks. Caption diversity effectively predicts uncertainty.

Conclusion: While VLMs show progress, challenges remain in handling uncertainty. Simple prompting helps, but domain-specific tasks require further innovation. Caption diversity offers a promising tool for uncertainty prediction.

Abstract: Robustness against uncertain and ambiguous inputs is a critical challenge for
deep learning models. While recent advancements in large scale vision language
models (VLMs, e.g. GPT4o) might suggest that increasing model and training
dataset size would mitigate this issue, our empirical evaluation shows a more
complicated picture. Testing models using two classic uncertainty
quantification tasks, anomaly detection and classification under inherently
ambiguous conditions, we find that newer and larger VLMs indeed exhibit
improved robustness compared to earlier models, but still suffer from a
tendency to strictly follow instructions, often causing them to hallucinate
confident responses even when faced with unclear or anomalous inputs.
Remarkably, for natural images such as ImageNet, this limitation can be
overcome without pipeline modifications: simply prompting models to abstain
from uncertain predictions enables significant reliability gains, achieving
near-perfect robustness in several settings. However, for domain-specific tasks
such as galaxy morphology classification, a lack of specialized knowledge
prevents reliable uncertainty estimation. Finally, we propose a novel mechanism
based on caption diversity to reveal a model's internal uncertainty, enabling
practitioners to predict when models will successfully abstain without relying
on labeled data.

</details>


### [212] [Image-based Visibility Analysis Replacing Line-of-Sight Simulation: An Urban Landmark Perspective](https://arxiv.org/abs/2505.11809)
*Zicheng Fan,Kunihiko Fujiwara,Pengyuan Liu,Fan Zhang,Filip Biljecki*

Main category: cs.CV

TL;DR: The paper introduces an image-based visibility analysis method using Vision Language Models (VLMs) to assess urban object visibility, outperforming traditional Line-of-Sight (LoS) methods with 87% accuracy and revealing contextual insights.


<details>
  <summary>Details</summary>
Motivation: Traditional LoS-based visibility analysis lacks contextual and perceptual dimensions, especially for named urban objects like landmarks.

Method: A VLM detects target objects in Street View Images (SVIs), and a heterogeneous visibility graph models observer-object interactions.

Result: The method achieves 87% accuracy in landmark visibility detection and reveals contextual differences and connection patterns (e.g., 30% of connections occur at Thames bridges).

Conclusion: The approach enhances LoS-based analysis, offering new insights for urban planning, heritage conservation, and computational social science.

Abstract: Visibility analysis is one of the fundamental analytics methods in urban
planning and landscape research, traditionally conducted through computational
simulations based on the Line-of-Sight (LoS) principle. However, when assessing
the visibility of named urban objects such as landmarks, geometric intersection
alone fails to capture the contextual and perceptual dimensions of visibility
as experienced in the real world. The study challenges the traditional
LoS-based approaches by introducing a new, image-based visibility analysis
method. Specifically, a Vision Language Model (VLM) is applied to detect the
target object within a direction-zoomed Street View Image (SVI). Successful
detection represents the object's visibility at the corresponding SVI location.
Further, a heterogeneous visibility graph is constructed to address the complex
interaction between observers and target objects. In the first case study, the
method proves its reliability in detecting the visibility of six tall landmark
constructions in global cities, with an overall accuracy of 87%. Furthermore,
it reveals broader contextual differences when the landmarks are perceived and
experienced. In the second case, the proposed visibility graph uncovers the
form and strength of connections for multiple landmarks along the River Thames
in London, as well as the places where these connections occur. Notably,
bridges on the River Thames account for approximately 30% of total connections.
Our method complements and enhances traditional LoS-based visibility analysis,
and showcases the possibility of revealing the prevalent connection of any
visual objects in the urban environment. It opens up new research perspectives
for urban planning, heritage conservation, and computational social science.

</details>


### [213] [SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation](https://arxiv.org/abs/2505.11813)
*Yixuan Dong,Fang-Yi Su,Jung-Hsien Chiang*

Main category: cs.CV

TL;DR: A novel framework for data augmentation in domain-specific image classification addresses diversity, faithfulness, and label clarity by integrating saliency-guided mixing and fine-tuned diffusion models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing generative diffusion model-based methods fail to cohesively address diversity, faithfulness, and label clarity in data augmentation, limiting downstream task performance.

Method: The proposed framework uses saliency-guided mixing and a fine-tuned diffusion model to preserve foreground semantics, diversify backgrounds, and ensure label consistency while mitigating diffusion model limitations.

Result: Extensive experiments show superior performance in fine-grained, long-tail, few-shot, and background robustness tasks compared to state-of-the-art methods.

Conclusion: The framework effectively integrates critical augmentation aspects, overcoming diffusion model challenges and improving downstream task performance.

Abstract: Data augmentation for domain-specific image classification tasks often
struggles to simultaneously address diversity, faithfulness, and label clarity
of generated data, leading to suboptimal performance in downstream tasks. While
existing generative diffusion model-based methods aim to enhance augmentation,
they fail to cohesively tackle these three critical aspects and often overlook
intrinsic challenges of diffusion models, such as sensitivity to model
characteristics and stochasticity under strong transformations. In this paper,
we propose a novel framework that explicitly integrates diversity,
faithfulness, and label clarity into the augmentation process. Our approach
employs saliency-guided mixing and a fine-tuned diffusion model to preserve
foreground semantics, enrich background diversity, and ensure label
consistency, while mitigating diffusion model limitations. Extensive
experiments across fine-grained, long-tail, few-shot, and background robustness
tasks demonstrate our method's superior performance over state-of-the-art
approaches.

</details>


### [214] [UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings](https://arxiv.org/abs/2505.11815)
*Jiajun Qin,Yuan Pu,Zhuolun He,Seunggeun Kim,David Z. Pan,Bei Yu*

Main category: cs.CV

TL;DR: UniMoCo is a vision-language model addressing diverse modality combinations in multi-modal embedding tasks by introducing a modality-completion module and specialized training for robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with diverse modality combinations (e.g., text-image to text-image), degrading performance due to alignment issues in training.

Method: UniMoCo uses a modality-completion module to generate visual features from text and aligns embeddings from original and completed inputs.

Result: UniMoCo outperforms prior methods, shows robustness, and mitigates bias from modality imbalance in training data.

Conclusion: UniMoCo effectively handles diverse modality combinations and addresses inherent biases, advancing multi-modal embedding tasks.

Abstract: Current research has explored vision-language models for multi-modal
embedding tasks, such as information retrieval, visual grounding, and
classification. However, real-world scenarios often involve diverse modality
combinations between queries and targets, such as text and image to text, text
and image to text and image, and text to text and image. These diverse
combinations pose significant challenges for existing models, as they struggle
to align all modality combinations within a unified embedding space during
training, which degrades performance at inference. To address this limitation,
we propose UniMoCo, a novel vision-language model architecture designed for
multi-modal embedding tasks. UniMoCo introduces a modality-completion module
that generates visual features from textual inputs, ensuring modality
completeness for both queries and targets. Additionally, we develop a
specialized training strategy to align embeddings from both original and
modality-completed inputs, ensuring consistency within the embedding space.
This enables the model to robustly handle a wide range of modality combinations
across embedding tasks. Experiments show that UniMoCo outperforms previous
methods while demonstrating consistent robustness across diverse settings. More
importantly, we identify and quantify the inherent bias in conventional
approaches caused by imbalance of modality combinations in training data, which
can be mitigated through our modality-completion paradigm. The code is
available at https://github.com/HobbitQia/UniMoCo.

</details>


### [215] [Continuous Subspace Optimization for Continual Learning](https://arxiv.org/abs/2505.11816)
*Quan Cheng,Yuanyu Wan,Lingyu Wu,Chenping Hou,Lijun Zhang*

Main category: cs.CV

TL;DR: CoSO proposes a continual learning method using dynamic subspaces for optimization, outperforming existing methods by reducing forgetting and improving performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of fixed low-rank subspaces in continual learning, which compromises model performance.

Method: Fine-tunes models in sequential, dynamically determined subspaces using SVD of gradients, ensuring orthogonality to historical subspaces.

Result: CoSO significantly outperforms state-of-the-art methods, especially in long task sequences.

Conclusion: CoSO offers an effective solution for continual learning by dynamically optimizing subspaces, mitigating forgetting, and enhancing performance.

Abstract: Continual learning aims to learn multiple tasks sequentially while preserving
prior knowledge, but faces the challenge of catastrophic forgetting when
acquiring new knowledge. Recently, approaches leveraging pre-trained models
have gained increasing popularity to mitigate this issue, due to the strong
generalization ability of foundation models. To adjust pre-trained models for
new tasks, existing methods usually employ low-rank adaptation, which restricts
parameter updates to a fixed low-rank subspace. However, constraining the
optimization space inherently compromises the model's learning capacity,
resulting in inferior performance. To address the limitation, we propose
Continuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the
model in a series of subspaces rather than a single one. These sequential
subspaces are dynamically determined through the singular value decomposition
of gradients. CoSO updates the model by projecting gradients into these
subspaces, ensuring memory-efficient optimization. To mitigate forgetting, the
optimization subspaces of each task are set to be orthogonal to the historical
task subspace. During task learning, CoSO maintains a task-specific component
that captures the critical update directions associated with the current task.
Upon completing a task, this component is used to update the historical task
subspace, laying the groundwork for subsequent learning. Extensive experiments
on multiple datasets demonstrate that CoSO significantly outperforms
state-of-the-art methods, especially in challenging scenarios with long task
sequences.

</details>


### [216] [Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement](https://arxiv.org/abs/2505.11822)
*Ke Li,Di Wang,Xiaowei Wang,Zhihong Wu,Yiming Zhang,Yifeng Wang,Quan Wang*

Main category: cs.CV

TL;DR: The paper proposes CVD, a framework for cross-view geo-localization that disentangles content and viewpoint factors to improve accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: CVGL is challenging due to appearance changes and spatial distortions from viewpoint variations. Existing methods overlook conflicts caused by viewpoint discrepancies, leading to inconsistent features.

Method: CVD models the feature space as a composite manifold and introduces two constraints: intra-view independence (minimizing mutual information) and inter-view reconstruction (cross-combining factors).

Result: CVD improves localization accuracy and generalization on four benchmarks (University-1652, SUES-200, CVUSA, CVACT).

Conclusion: CVD effectively disentangles content and viewpoint factors, enhancing cross-view geo-localization performance.

Abstract: Cross-view geo-localization (CVGL) aims to match images of the same
geographic location captured from different perspectives, such as drones and
satellites. Despite recent advances, CVGL remains highly challenging due to
significant appearance changes and spatial distortions caused by viewpoint
variations. Existing methods typically assume that cross-view images can be
directly aligned within a shared feature space by maximizing feature similarity
through contrastive learning. Nonetheless, this assumption overlooks the
inherent conflicts induced by viewpoint discrepancies, resulting in extracted
features containing inconsistent information that hinders precise localization.
In this study, we take a manifold learning perspective and model the feature
space of cross-view images as a composite manifold jointly governed by content
and viewpoint information. Building upon this insight, we propose
$\textbf{CVD}$, a new CVGL framework that explicitly disentangles
$\textit{content}$ and $\textit{viewpoint}$ factors. To promote effective
disentanglement, we introduce two constraints: $\textit{(i)}$ An intra-view
independence constraint, which encourages statistical independence between the
two factors by minimizing their mutual information. $\textit{(ii)}$ An
inter-view reconstruction constraint that reconstructs each view by
cross-combining $\textit{content}$ and $\textit{viewpoint}$ from paired images,
ensuring factor-specific semantics are preserved. As a plug-and-play module,
CVD can be seamlessly integrated into existing geo-localization pipelines.
Extensive experiments on four benchmarks, i.e., University-1652, SUES-200,
CVUSA, and CVACT, demonstrate that CVD consistently improves both localization
accuracy and generalization across multiple baselines.

</details>


### [217] [Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data](https://arxiv.org/abs/2505.11825)
*Xudong Ma*

Main category: cs.CV

TL;DR: The paper explores using partial or corrupted data (e.g., low-resolution images, short videos) to train diffusion models, proposing a method that trains separate models for each data view and predicts residual score functions, achieving near-optimal data efficiency.


<details>
  <summary>Details</summary>
Motivation: Large datasets for training diffusion models are hard to acquire, while partial or corrupted data (e.g., low-resolution images) are abundant but underutilized. The study investigates whether such data can be effectively used.

Method: The approach trains separate diffusion models for each partial data view and a model for predicting the residual score function, with regularization to improve generalization.

Result: The method achieves lower generalization errors and near first-order optimal data efficiency, with difficulty scaling proportionally to uncaptured signal correlations.

Conclusion: Partial data can effectively train diffusion models, with the proposed method offering a practical and efficient solution.

Abstract: Training diffusion models requires large datasets. However, acquiring large
volumes of high-quality data can be challenging, for example, collecting large
numbers of high-resolution images and long videos. On the other hand, there are
many complementary data that are usually considered corrupted or partial, such
as low-resolution images and short videos. Other examples of corrupted data
include videos that contain subtitles, watermarks, and logos. In this study, we
investigate the theoretical problem of whether the above partial data can be
utilized to train conventional diffusion models. Motivated by our theoretical
analysis in this study, we propose a straightforward approach of training
diffusion models utilizing partial data views, where we consider each form of
complementary data as a view of conventional data. Our proposed approach first
trains one separate diffusion model for each individual view, and then trains a
model for predicting the residual score function. We prove generalization error
bounds, which show that the proposed diffusion model training approach can
achieve lower generalization errors if proper regularizations are adopted in
the residual score function training. In particular, we prove that the
difficulty in training the residual score function scales proportionally with
the signal correlations not captured by partial data views. Consequently, the
proposed approach achieves near first-order optimal data efficiency.

</details>


### [218] [CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning](https://arxiv.org/abs/2505.11830)
*Hongbo Jin,Ruyang Liu,Wenhao Zhang,Guibo Luo,Ge Li*

Main category: cs.CV

TL;DR: CoT-Vid introduces a training-free paradigm for complex video reasoning, outperforming existing models with a novel multistage reasoning design.


<details>
  <summary>Details</summary>
Motivation: Addressing the research gap in complex video reasoning, leveraging System2 reasoning advancements.

Method: Dynamic inference path routing, problem decoupling, and video self-consistency verification.

Result: Achieved 9.3% and 5.6% gains on benchmarks, rivaling larger models like GPT-4V.

Conclusion: CoT-Vid sets a new standard for video reasoning, with promising performance and open-source potential.

Abstract: System2 reasoning is developing rapidly these days with the emergence of
Deep- Thinking Models and chain-of-thought technology, which has become a
centralized discussion point in the AI community. However, there is a relative
gap in the research on complex video reasoning at present. In this work, we
propose CoT-Vid, a novel training-free paradigm for the video domain with a
multistage complex reasoning design. Distinguishing from existing video LLMs,
which rely heavily on perceptual abilities, it achieved surprising performance
gain with explicit reasoning mechanism. The paradigm consists of three main
components: dynamic inference path routing, problem decoupling strategy, and
video self-consistency verification. In addition, we propose a new standard for
categorization of video questions. CoT- Vid showed outstanding results on a
wide range of benchmarks, and outperforms its base model by 9.3% on Egochema
and 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary
models, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be
publicly available soon.

</details>


### [219] [RVTBench: A Benchmark for Visual Reasoning Tasks](https://arxiv.org/abs/2505.11838)
*Yiqing Shen,Chenjia Li,Chenxiao Fan,Mathias Unberath*

Main category: cs.CV

TL;DR: The paper introduces Reasoning Visual Tasks (RVTs) to address limitations in visual reasoning benchmarks, proposing a new automated pipeline for benchmark construction and a zero-shot agent framework.


<details>
  <summary>Details</summary>
Motivation: Current visual reasoning benchmarks lack complexity due to reliance on large language models (LLMs), which fail to capture spatial-temporal relationships and multi-step reasoning in videos.

Method: The authors propose an automated benchmark construction pipeline using digital twin (DT) representations and introduce RVTBench, a diverse benchmark with 3,896 queries. They also develop RVTagent, a zero-shot agent framework for RVT tasks.

Result: RVTBench is created with 1.2 million tokens across four RVT types, three reasoning categories, and four difficulty levels. RVTagent demonstrates zero-shot generalization across RVT tasks.

Conclusion: The work advances visual reasoning by addressing benchmark limitations and introducing a flexible agent framework, enabling broader applicability in visual language reasoning.

Abstract: Visual reasoning, the capability to interpret visual input in response to
implicit text query through multi-step reasoning, remains a challenge for deep
learning models due to the lack of relevant benchmarks. Previous work in visual
reasoning has primarily focused on reasoning segmentation, where models aim to
segment objects based on implicit text queries. This paper introduces reasoning
visual tasks (RVTs), a unified formulation that extends beyond traditional
video reasoning segmentation to a diverse family of visual language reasoning
problems, which can therefore accommodate multiple output formats including
bounding boxes, natural language descriptions, and question-answer pairs.
Correspondingly, we identify the limitations in current benchmark construction
methods that rely solely on large language models (LLMs), which inadequately
capture complex spatial-temporal relationships and multi-step reasoning chains
in video due to their reliance on token representation, resulting in benchmarks
with artificially limited reasoning complexity. To address this limitation, we
propose a novel automated RVT benchmark construction pipeline that leverages
digital twin (DT) representations as structured intermediaries between
perception and the generation of implicit text queries. Based on this method,
we construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2
million tokens across four types of RVT (segmentation, grounding, VQA and
summary), three reasoning categories (semantic, spatial, and temporal), and
four increasing difficulty levels, derived from 200 video sequences. Finally,
we propose RVTagent, an agent framework for RVT that allows for zero-shot
generalization across various types of RVT without task-specific fine-tuning.

</details>


### [220] [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)
*Xuannan Liu,Zekun Li,Zheqi He,Peipei Li,Shuhan Xia,Xing Cui,Huaibo Huang,Xi Yang,Ran He*

Main category: cs.CV

TL;DR: Video-SafetyBench is a new benchmark for evaluating LVLM safety under video-text attacks, revealing vulnerabilities with a 67.2% attack success rate.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluations ignore temporal dynamics in videos, which may pose unique risks.

Method: Developed Video-SafetyBench with 2,264 video-text pairs and a controllable pipeline for video synthesis. Introduced RJScore for evaluating harmful outputs.

Result: Benign-query video attacks achieved 67.2% success, highlighting LVLM vulnerabilities.

Conclusion: Video-SafetyBench aims to advance video-based safety research and defenses.

Abstract: The increasing deployment of Large Vision-Language Models (LVLMs) raises
safety concerns under potential malicious inputs. However, existing multimodal
safety evaluations primarily focus on model vulnerabilities exposed by static
image inputs, ignoring the temporal dynamics of video that may induce distinct
safety risks. To bridge this gap, we introduce Video-SafetyBench, the first
comprehensive benchmark designed to evaluate the safety of LVLMs under
video-text attacks. It comprises 2,264 video-text pairs spanning 48
fine-grained unsafe categories, each pairing a synthesized video with either a
harmful query, which contains explicit malice, or a benign query, which appears
harmless but triggers harmful behavior when interpreted alongside the video. To
generate semantically accurate videos for safety evaluation, we design a
controllable pipeline that decomposes video semantics into subject images (what
is shown) and motion text (how it moves), which jointly guide the synthesis of
query-relevant videos. To effectively evaluate uncertain or borderline harmful
outputs, we propose RJScore, a novel LLM-based metric that incorporates the
confidence of judge models and human-aligned decision threshold calibration.
Extensive experiments show that benign-query video composition achieves average
attack success rates of 67.2%, revealing consistent vulnerabilities to
video-induced attacks. We believe Video-SafetyBench will catalyze future
research into video-based safety evaluation and defense strategies.

</details>


### [221] [ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety](https://arxiv.org/abs/2505.11845)
*Tasrifur Riahi,Md. Azizul Hakim Bappy,Md. Mehedi Islam*

Main category: cs.CV

TL;DR: ElderFallGuard is a computer vision-based IoT system for real-time elderly fall detection using pose estimation and machine learning, achieving 100% accuracy in tests.


<details>
  <summary>Details</summary>
Motivation: Falls among the elderly are a growing risk, leading to injuries and loss of independence, necessitating a non-invasive, real-time detection solution.

Method: Uses MediaPipe for pose estimation, a custom dataset of 7200 samples, and Random Forest classifiers. Detects falls when a prone pose is held for >3s with reduced motion for >2s. Alerts are sent via Telegram.

Result: Achieved 100% accuracy, precision, recall, and F1-score in testing.

Conclusion: ElderFallGuard is a highly effective, vision-based IoT solution for enhancing elderly safety and caregiver peace of mind.

Abstract: For the elderly population, falls pose a serious and increasing risk of
serious injury and loss of independence. In order to overcome this difficulty,
we present ElderFallGuard: A Computer Vision Based IoT Solution for Elderly
Fall Detection and Notification, a cutting-edge, non-invasive system intended
for quick caregiver alerts and real-time fall detection. Our approach leverages
the power of computer vision, utilizing MediaPipe for accurate human pose
estimation from standard video streams. We developed a custom dataset
comprising 7200 samples across 12 distinct human poses to train and evaluate
various machine learning classifiers, with Random Forest ultimately selected
for its superior performance. ElderFallGuard employs a specific detection
logic, identifying a fall when a designated prone pose ("Pose6") is held for
over 3 seconds coupled with a significant drop in motion detected for more than
2 seconds. Upon confirmation, the system instantly dispatches an alert,
including a snapshot of the event, to a designated Telegram group via a custom
bot, incorporating cooldown logic to prevent notification overload. Rigorous
testing on our dataset demonstrated exceptional results, achieving 100%
accuracy, precision, recall, and F1-score. ElderFallGuard offers a promising,
vision-based IoT solution to enhance elderly safety and provide peace of mind
for caregivers through intelligent, timely alerts.

</details>


### [222] [MedSG-Bench: A Benchmark for Medical Image Sequences Grounding](https://arxiv.org/abs/2505.11852)
*Jingkun Yue,Siqi Zhang,Zinan Jia,Huihuan Xu,Zongbo Han,Xiaohong Liu,Guangyu Wang*

Main category: cs.CV

TL;DR: The paper introduces MedSG-Bench, a benchmark for medical image sequence grounding, addressing gaps in existing benchmarks by focusing on sequential images. It includes tasks for detecting changes and consistent semantics across images, evaluates MLLMs, and proposes a new dataset and model for future research.


<details>
  <summary>Details</summary>
Motivation: Existing medical visual grounding benchmarks lack focus on sequential images, which are critical for clinical applications like lesion tracking and treatment comparisons.

Method: Proposed MedSG-Bench with eight VQA-style tasks, covering 76 datasets and 10 modalities. Evaluated general and medical MLLMs, then developed MedSG-188K dataset and MedSeq-Grounder model.

Result: Advanced MLLMs showed limitations in sequential grounding tasks. The new benchmark and model aim to improve fine-grained understanding of medical image sequences.

Conclusion: MedSG-Bench fills a critical gap in medical visual grounding, providing tools for better sequential image analysis and advancing MLLM capabilities in this domain.

Abstract: Visual grounding is essential for precise perception and reasoning in
multimodal large language models (MLLMs), especially in medical imaging
domains. While existing medical visual grounding benchmarks primarily focus on
single-image scenarios, real-world clinical applications often involve
sequential images, where accurate lesion localization across different
modalities and temporal tracking of disease progression (e.g., pre- vs.
post-treatment comparison) require fine-grained cross-image semantic alignment
and context-aware reasoning. To remedy the underrepresentation of image
sequences in existing medical visual grounding benchmarks, we propose
MedSG-Bench, the first benchmark tailored for Medical Image Sequences
Grounding. It comprises eight VQA-style tasks, formulated into two paradigms of
the grounding tasks, including 1) Image Difference Grounding, which focuses on
detecting change regions across images, and 2) Image Consistency Grounding,
which emphasizes detection of consistent or shared semantics across sequential
images. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,
and a wide spectrum of anatomical structures and diseases, totaling 9,630
question-answer pairs. We benchmark both general-purpose MLLMs (e.g.,
Qwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),
observing that even the advanced models exhibit substantial limitations in
medical sequential grounding tasks. To advance this field, we construct
MedSG-188K, a large-scale instruction-tuning dataset tailored for sequential
visual grounding, and further develop MedSeq-Grounder, an MLLM designed to
facilitate future research on fine-grained understanding across medical
sequential images. The benchmark, dataset, and model are available at
https://huggingface.co/MedSG-Bench

</details>


### [223] [MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos](https://arxiv.org/abs/2505.11868)
*Hongyi Zhou,Xiaogang Wang,Yulan Guo,Kai Xu*

Main category: cs.CV

TL;DR: A zero-shot framework analyzes 3D mobility from monocular videos without annotated data, using depth estimation, optical flow, and point cloud registration, refined by 2D Gaussian splatting and dynamic scene optimization.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of dense multi-view images or part-level annotations for motion analysis in dynamic environments, crucial for embodied intelligence.

Method: Combines depth estimation, optical flow, and point cloud registration for initial motion analysis, then refines results with 2D Gaussian splatting and an end-to-end dynamic scene optimization algorithm.

Result: Effectively analyzes articulated object motions (rotation, translation, complex movements) without annotations, validated on a comprehensive dataset.

Conclusion: Demonstrates high flexibility and versatility, with significant potential for embodied intelligence applications.

Abstract: Accurately analyzing the motion parts and their motion attributes in dynamic
environments is crucial for advancing key areas such as embodied intelligence.
Addressing the limitations of existing methods that rely on dense multi-view
images or detailed part-level annotations, we propose an innovative framework
that can analyze 3D mobility from monocular videos in a zero-shot manner. This
framework can precisely parse motion parts and motion attributes only using a
monocular video, completely eliminating the need for annotated training data.
Specifically, our method first constructs the scene geometry and roughly
analyzes the motion parts and their initial motion attributes combining depth
estimation, optical flow analysis and point cloud registration method, then
employs 2D Gaussian splatting for scene representation. Building on this, we
introduce an end-to-end dynamic scene optimization algorithm specifically
designed for articulated objects, refining the initial analysis results to
ensure the system can handle 'rotation', 'translation', and even complex
movements ('rotation+translation'), demonstrating high flexibility and
versatility. To validate the robustness and wide applicability of our method,
we created a comprehensive dataset comprising both simulated and real-world
scenarios. Experimental results show that our framework can effectively analyze
articulated object motions in an annotation-free manner, showcasing its
significant potential in future embodied intelligence applications.

</details>


### [224] [PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging](https://arxiv.org/abs/2505.11872)
*Quoc-Huy Trinh,Minh-Van Nguyen,Jung Peng,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: PRS-Med integrates vision-language models for medical image segmentation and spatial reasoning, outperforming existing methods and introducing the MMRS dataset for positional reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based methods struggle with natural language interaction and spatial reasoning in medical imaging.

Method: PRS-Med combines vision-language models with segmentation to produce masks and spatial reasoning outputs, supported by the MMRS dataset.

Result: PRS-Med excels in segmentation accuracy and positional reasoning across six imaging modalities.

Conclusion: PRS-Med enhances doctor-system interaction via natural language, improving diagnostic efficiency, with plans to release resources for further research.

Abstract: Recent advancements in prompt-based medical image segmentation have enabled
clinicians to identify tumors using simple input like bounding boxes or text
prompts. However, existing methods face challenges when doctors need to
interact through natural language or when position reasoning is required -
understanding spatial relationships between anatomical structures and
pathologies. We present PRS-Med, a framework that integrates vision-language
models with segmentation capabilities to generate both accurate segmentation
masks and corresponding spatial reasoning outputs. Additionally, we introduce
the MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),
which provides diverse, spatially-grounded question-answer pairs to address the
lack of position reasoning data in medical imaging. PRS-Med demonstrates
superior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,
endoscopy, RGB), significantly outperforming state-of-the-art methods in both
segmentation accuracy and position reasoning. Our approach enables intuitive
doctor-system interaction through natural language, facilitating more efficient
diagnoses. Our dataset pipeline, model, and codebase will be released to foster
further research in spatially-aware multimodal reasoning for medical
applications.

</details>


### [225] [Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks](https://arxiv.org/abs/2505.11881)
*Giyeong Oh,Woohyun Cho,Siyeol Kim,Suhwan Choi,Younjae Yu*

Main category: cs.CV

TL;DR: The paper introduces Orthogonal Residual Update, a method to decompose module outputs in neural networks to add only components orthogonal to the input stream, improving feature learning and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard residual updates may reinforce existing stream directions, underutilizing the module's capacity for learning new features.

Method: Decompose the module's output relative to the input stream and add only the orthogonal component.

Result: Improves generalization accuracy and training stability, e.g., +4.3% top-1 accuracy gain for ViT-B on ImageNet-1k.

Conclusion: Orthogonal Residual Update enhances feature learning and training efficiency across diverse architectures and datasets.

Abstract: Residual connections are pivotal for deep neural networks, enabling greater
depth by mitigating vanishing gradients. However, in standard residual updates,
the module's output is directly added to the input stream. This can lead to
updates that predominantly reinforce or modulate the existing stream direction,
potentially underutilizing the module's capacity for learning entirely novel
features. In this work, we introduce Orthogonal Residual Update: we decompose
the module's output relative to the input stream and add only the component
orthogonal to this stream. This design aims to guide modules to contribute
primarily new representational directions, fostering richer feature learning
while promoting more efficient training. We demonstrate that our orthogonal
update strategy improves generalization accuracy and training stability across
diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,
TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\%p top-1 accuracy
gain for ViT-B on ImageNet-1k.

</details>


### [226] [GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder](https://arxiv.org/abs/2505.11882)
*Shiming Chen,Dingjie Fu,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: GenZSL is a novel inductive variational autoencoder for generative zero-shot learning, improving performance and efficiency by leveraging weak class semantic vectors and innovative strategies like class diversity promotion and target class-guided boosting.


<details>
  <summary>Details</summary>
Motivation: Existing generative ZSL methods rely on expert-annotated semantic vectors, leading to suboptimal performance and limited generalization. GenZSL aims to overcome these by mimicking human-level concept learning.

Method: GenZSL inducts new class samples from similar seen classes using weak semantic vectors (CLIP text embeddings). It employs class diversity promotion and target class-guided boosting to enhance sample generation.

Result: GenZSL outperforms f-VAEGAN with 24.7% performance gains and 60× faster training speed on AWA2, validated on three benchmark datasets.

Conclusion: GenZSL advances ZSL by improving generative performance and efficiency, offering a scalable solution with significant practical benefits.

Abstract: Remarkable progress in zero-shot learning (ZSL) has been achieved using
generative models. However, existing generative ZSL methods merely generate
(imagine) the visual features from scratch guided by the strong class semantic
vectors annotated by experts, resulting in suboptimal generative performance
and limited scene generalization. To address these and advance ZSL, we propose
an inductive variational autoencoder for generative zero-shot learning, dubbed
GenZSL. Mimicking human-level concept learning, GenZSL operates by inducting
new class samples from similar seen classes using weak class semantic vectors
derived from target class names (i.e., CLIP text embedding). To ensure the
generation of informative samples for training an effective ZSL classifier, our
GenZSL incorporates two key strategies. Firstly, it employs class diversity
promotion to enhance the diversity of class semantic vectors. Secondly, it
utilizes target class-guided information boosting criteria to optimize the
model. Extensive experiments conducted on three popular benchmark datasets
showcase the superiority and potential of our GenZSL with significant efficacy
and efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than
$60\times$ faster training speed on AWA2. Codes are available at
https://github.com/shiming-chen/GenZSL.

</details>


### [227] [Facial Recognition Leveraging Generative Adversarial Networks](https://arxiv.org/abs/2505.11884)
*Zhongwen Li,Zongwei Li,Xiaoqi Li*

Main category: cs.CV

TL;DR: A GAN-based data augmentation method improves face recognition accuracy by 12.7% on LFW, addressing data scarcity with a residual-embedded generator, improved discriminator, and joint optimization.


<details>
  <summary>Details</summary>
Motivation: Deep learning for face recognition requires large-scale data, which is often unavailable. This paper tackles data scarcity.

Method: Proposes a GAN-based method with a residual-embedded generator, Inception ResNet-V1 discriminator, and end-to-end joint optimization.

Result: Achieves 12.7% higher accuracy on LFW and stable training with limited data.

Conclusion: The method effectively enhances recognition performance and generalizes well with scarce data.

Abstract: Face recognition performance based on deep learning heavily relies on
large-scale training data, which is often difficult to acquire in practical
applications. To address this challenge, this paper proposes a GAN-based data
augmentation method with three key contributions: (1) a residual-embedded
generator to alleviate gradient vanishing/exploding problems, (2) an Inception
ResNet-V1 based FaceNet discriminator for improved adversarial training, and
(3) an end-to-end framework that jointly optimizes data generation and
recognition performance. Experimental results demonstrate that our approach
achieves stable training dynamics and significantly improves face recognition
accuracy by 12.7% on the LFW benchmark compared to baseline methods, while
maintaining good generalization capability with limited training samples.

</details>


### [228] [Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration](https://arxiv.org/abs/2505.11895)
*Chih-Ting Liao,Bin Ren,Guofeng Mei,Xu Zheng*

Main category: cs.CV

TL;DR: The paper investigates adversarial vulnerabilities in unified multi-modal encoders and proposes an efficient calibration framework to enhance robustness without altering pretrained models.


<details>
  <summary>Details</summary>
Motivation: The robustness of unified multi-modal encoders under adversarial perturbations is underexplored, posing risks for safety-sensitive applications.

Method: An adversarial calibration framework with modality-specific projection heads is introduced, trained on adversarial examples while keeping the backbone frozen. Three training objectives and a regularization strategy are explored.

Result: The method improves adversarial robustness by up to 47.3% at epsilon = 4/255, maintaining clean performance with minimal trainable parameters.

Conclusion: The proposed framework effectively enhances robustness across modalities without modifying existing models, ensuring compatibility and performance.

Abstract: Recent unified multi-modal encoders align a wide range of modalities into a
shared representation space, enabling diverse cross-modal tasks. Despite their
impressive capabilities, the robustness of these models under adversarial
perturbations remains underexplored, which is a critical concern for
safety-sensitive applications. In this work, we present the first comprehensive
study of adversarial vulnerability in unified multi-modal encoders. We find
that even mild adversarial perturbations lead to substantial performance drops
across all modalities. Non-visual inputs, such as audio and point clouds, are
especially fragile, while visual inputs like images and videos also degrade
significantly. To address this, we propose an efficient adversarial calibration
framework that improves robustness across modalities without modifying
pretrained encoders or semantic centers, ensuring compatibility with existing
foundation models. Our method introduces modality-specific projection heads
trained solely on adversarial examples, while keeping the backbone and
embeddings frozen. We explore three training objectives: fixed-center
cross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial
InfoNCE, and we introduce a regularization strategy to ensure
modality-consistent alignment under attack. Experiments on six modalities and
three Bind-style models show that our method improves adversarial robustness by
up to 47.3 percent at epsilon = 4/255, while preserving or even improving clean
zero-shot and retrieval performance with less than 1 percent trainable
parameters.

</details>


### [229] [FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer](https://arxiv.org/abs/2505.11897)
*Seonghak Kim*

Main category: cs.CV

TL;DR: FiGKD is a frequency-aware knowledge distillation method that selectively transfers high-frequency logit components for fine-grained visual recognition, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods underperform in fine-grained tasks due to treating teacher logits as undifferentiated signals, leading to redundant information overload.

Method: FiGKD decomposes logits into low- and high-frequency components using DWT, transferring only high-frequency details for distillation.

Result: FiGKD outperforms state-of-the-art methods on CIFAR-100, TinyImageNet, and fine-grained benchmarks.

Conclusion: Frequency-aware logit decomposition improves knowledge transfer efficiency, especially in resource-constrained settings.

Abstract: Knowledge distillation (KD) is a widely adopted technique for transferring
knowledge from a high-capacity teacher model to a smaller student model by
aligning their output distributions. However, existing methods often
underperform in fine-grained visual recognition tasks, where distinguishing
subtle differences between visually similar classes is essential. This
performance gap stems from the fact that conventional approaches treat the
teacher's output logits as a single, undifferentiated signal-assuming all
contained information is equally beneficial to the student. Consequently,
student models may become overloaded with redundant signals and fail to capture
the teacher's nuanced decision boundaries. To address this issue, we propose
Fine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework
that decomposes a model's logits into low-frequency (content) and
high-frequency (detail) components using the discrete wavelet transform (DWT).
FiGKD selectively transfers only the high-frequency components, which encode
the teacher's semantic decision patterns, while discarding redundant
low-frequency content already conveyed through ground-truth supervision. Our
approach is simple, architecture-agnostic, and requires no access to
intermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet,
and multiple fine-grained recognition benchmarks show that FiGKD consistently
outperforms state-of-the-art logit-based and feature-based distillation methods
across a variety of teacher-student configurations. These findings confirm that
frequency-aware logit decomposition enables more efficient and effective
knowledge transfer, particularly in resource-constrained settings.

</details>


### [230] [GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity](https://arxiv.org/abs/2505.11905)
*Takuya Ikeda,Sergey Zakharov,Muhammad Zubair Irshad,Istvan Balazs Opra,Shun Iwase,Dian Chen,Mark Tjersland,Robert Lee,Alexandre Dilly,Rares Ambrus,Koichi Nishiwaki*

Main category: cs.CV

TL;DR: A novel method for 6-DoF object tracking and 3D reconstruction from RGBD video, addressing challenges like symmetry and complex geometry with adaptive techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex objects (symmetry, intricate geometry, or appearance), creating a need for a more robust solution.

Method: Combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection for adaptive tracking and reconstruction.

Result: Achieves high-fidelity object meshes and sets a new standard for single-sensor 3D reconstruction in open-world environments.

Conclusion: The method effectively bridges gaps in tracking and reconstruction, supported by a new benchmark for evaluation.

Abstract: We present a novel method for 6-DoF object tracking and high-quality 3D
reconstruction from monocular RGBD video. Existing methods, while achieving
impressive results, often struggle with complex objects, particularly those
exhibiting symmetry, intricate geometry or complex appearance. To bridge these
gaps, we introduce an adaptive method that combines 3D Gaussian Splatting,
hybrid geometry/appearance tracking, and key frame selection to achieve robust
tracking and accurate reconstructions across a diverse range of objects.
Additionally, we present a benchmark covering these challenging object classes,
providing high-quality annotations for evaluating both tracking and
reconstruction performance. Our approach demonstrates strong capabilities in
recovering high-fidelity object meshes, setting a new standard for
single-sensor 3D reconstruction in open-world environments.

</details>


### [231] [Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?](https://arxiv.org/abs/2505.11907)
*Zihao Dongfang,Xu Zheng,Ziqiao Weng,Yuanhuiyi Lyu,Danda Pani Paudel,Luc Van Gool,Kailun Yang,Xuming Hu*

Main category: cs.CV

TL;DR: The paper introduces OSR-Bench, a benchmark for evaluating multimodal large language models (MLLMs) on omnidirectional spatial reasoning, revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: To explore whether MLLMs can handle omnidirectional spatial reasoning, an area largely unexplored despite its importance in applications like embodied AI and virtual reality.

Method: The authors create OSR-Bench with 153,000 QA pairs grounded in panoramic scenes, introduce negative sampling for robustness testing, and propose a two-stage evaluation framework.

Result: Current MLLMs, including GPT-4o and Gemini 1.5 Pro, struggle with omnidirectional spatial reasoning, indicating a need for more perceptually grounded models.

Conclusion: OSR-Bench highlights the gaps in MLLMs' spatial reasoning capabilities and provides a tool for future research in this direction.

Abstract: The 180x360 omnidirectional field of view captured by 360-degree cameras
enables their use in a wide range of applications such as embodied AI and
virtual reality. Although recent advances in multimodal large language models
(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on
standard pinhole-view images, leaving omnidirectional perception largely
unexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial
reasoning? To investigate this, we introduce OSR-Bench, the first benchmark
specifically designed for this setting. OSR-Bench includes over 153,000 diverse
question-answer pairs grounded in high-fidelity panoramic indoor scene maps. It
covers key reasoning types including object counting, relative distance, and
direction. We also propose a negative sampling strategy that inserts
non-existent objects into prompts to evaluate hallucination and grounding
robustness. For fine-grained analysis, we design a two-stage evaluation
framework assessing both cognitive map generation and QA accuracy using
rotation-invariant matching and a combination of rule-based and LLM-based
metrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5
Pro, and leading open-source models under zero-shot settings. Results show that
current models struggle with spatial reasoning in panoramic contexts,
highlighting the need for more perceptually grounded MLLMs. OSR-Bench and code
will be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench

</details>


### [232] [DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/abs/2505.11921)
*Haitao Li,Ziyu Li,Yiheng Mao,Zhengyao Ding,Zhengxing Huang*

Main category: cs.CV

TL;DR: DC-Seg improves brain image segmentation by disentangling modality-invariant and modality-specific features using contrastive learning, enhancing robustness to missing modalities.


<details>
  <summary>Details</summary>
Motivation: Clinical data often lacks all modalities, making segmentation challenging. Existing methods encode modalities into a shared latent space but fail to fully utilize distinct modality information.

Method: DC-Seg uses anatomical and modality contrastive learning to disentangle features, plus a segmentation-based regularizer for missing modalities.

Result: Outperforms state-of-the-art methods on BraTS 2020 and WMH datasets, handling incomplete modalities effectively.

Conclusion: DC-Seg provides robust and generalizable segmentation by explicitly separating anatomical and modality-specific features.

Abstract: Accurate segmentation of brain images typically requires the integration of
complementary information from multiple image modalities. However, clinical
data for all modalities may not be available for every patient, creating a
significant challenge. To address this, previous studies encode multiple
modalities into a shared latent space. While somewhat effective, it remains
suboptimal, as each modality contains distinct and valuable information. In
this study, we propose DC-Seg (Disentangled Contrastive Learning for
Segmentation), a new method that explicitly disentangles images into
modality-invariant anatomical representation and modality-specific
representation, by using anatomical contrastive learning and modality
contrastive learning respectively. This solution improves the separation of
anatomical and modality-specific features by considering the modality gaps,
leading to more robust representations. Furthermore, we introduce a
segmentation-based regularizer that enhances the model's robustness to missing
modalities. Extensive experiments on the BraTS 2020 and a private white matter
hyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms
state-of-the-art methods in handling incomplete multimodal brain tumor
segmentation tasks with varying missing modalities, while also demonstrate
strong generalizability in WMH segmentation. The code is available at
https://github.com/CuCl-2/DC-Seg.

</details>


### [233] [SafeVid: Toward Safety Aligned Video Large Multimodal Models](https://arxiv.org/abs/2505.11926)
*Yixu Wang,Jiaxin Song,Yifeng Gao,Xin Wang,Yang Yao,Yan Teng,Xingjun Ma,Yingchun Wang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: SafeVid enhances video safety in VLMMs by transferring textual safety alignment to videos using detailed descriptions, a new dataset, and DPO, achieving significant improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing mismatched generalization in VLMMs where static safety alignments fail in dynamic video contexts.

Method: Uses textual video descriptions as a bridge, creates SafeVid-350K dataset, aligns VLMMs with DPO, and evaluates with SafeVidBench.

Result: LLaVA-NeXT-Video shows up to 42.39% safety improvement on SafeVidBench.

Conclusion: SafeVid effectively improves VLMM safety by leveraging textual descriptions and structured alignment, with resources made publicly available.

Abstract: As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent
complexity introduces significant safety challenges, particularly the issue of
mismatched generalization where static safety alignments fail to transfer to
dynamic video contexts. We introduce SafeVid, a framework designed to instill
video-specific safety principles in VLMMs. SafeVid uniquely transfers robust
textual safety alignment capabilities to the video domain by employing detailed
textual video descriptions as an interpretive bridge, facilitating LLM-based
rule-driven safety reasoning. This is achieved through a closed-loop system
comprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific
safety preference dataset; 2) targeted alignment of VLMMs using Direct
Preference Optimization (DPO); and 3) comprehensive evaluation via our new
SafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM
safety, with models like LLaVA-NeXT-Video demonstrating substantial
improvements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical
resources and a structured approach, demonstrating that leveraging textual
descriptions as a conduit for safety reasoning markedly improves the safety
alignment of VLMMs. We have made SafeVid-350K dataset
(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available.

</details>


### [234] [iSegMan: Interactive Segment-and-Manipulate 3D Gaussians](https://arxiv.org/abs/2505.11934)
*Yian Zhao,Wanshi Xu,Ruochong Zheng,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: iSegMan introduces an interactive 3D segmentation and manipulation framework using 2D user interactions, avoiding scene-specific training with EIP and VGV methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene manipulation methods lack control and interactive feedback, while segmentation frameworks require inefficient pre-training.

Method: Proposes Epipolar-guided Interaction Propagation (EIP) for view matching and Visibility-based Gaussian Voting (VGV) for 2D-to-3D segmentation without training.

Result: Demonstrates efficient and precise region control, enabling flexible scene manipulation via a Manipulation Toolbox.

Conclusion: iSegMan significantly improves controllability and practicality in 3D scene manipulation, validated by extensive results.

Abstract: The efficient rendering and explicit nature of 3DGS promote the advancement
of 3D scene manipulation. However, existing methods typically encounter
challenges in controlling the manipulation region and are unable to furnish the
user with interactive feedback, which inevitably leads to unexpected results.
Intuitively, incorporating interactive 3D segmentation tools can compensate for
this deficiency. Nevertheless, existing segmentation frameworks impose a
pre-processing step of scene-specific parameter training, which limits the
efficiency and flexibility of scene manipulation. To deliver a 3D region
control module that is well-suited for scene manipulation with reliable
efficiency, we propose interactive Segment-and-Manipulate 3D Gaussians
(iSegMan), an interactive segmentation and manipulation framework that only
requires simple 2D user interactions in any view. To propagate user
interactions to other views, we propose Epipolar-guided Interaction Propagation
(EIP), which innovatively exploits epipolar constraint for efficient and robust
interaction matching. To avoid scene-specific training to maintain efficiency,
we further propose the novel Visibility-based Gaussian Voting (VGV), which
obtains 2D segmentations from SAM and models the region extraction as a voting
game between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking
advantage of the efficient and precise region control of EIP and VGV, we put
forth a Manipulation Toolbox to implement various functions on selected
regions, enhancing the controllability, flexibility and practicality of scene
manipulation. Extensive results on 3D scene manipulation and segmentation tasks
fully demonstrate the significant advantages of iSegMan. Project page is
available at https://zhao-yian.github.io/iSegMan.

</details>


### [235] [Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning](https://arxiv.org/abs/2505.11945)
*Bonan li,Zicheng Zhang,Songhua Liu,Weihao Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: LLaVA-Meteor introduces a Top-Down Compression paradigm and Flash Global Fusion module to balance accuracy and efficiency in visual instruction tuning, reducing visual tokens by 75-95% without performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between accuracy and efficiency in vision-to-language projection for large language models.

Method: Uses Top-Down Compression, Flash Global Fusion, and Visual-Native Selection to compress visual tokens and align feature spaces efficiently.

Result: Achieves comparable or superior performance on 12 benchmarks while reducing visual tokens by 75-95%.

Conclusion: LLaVA-Meteor effectively breaks the accuracy-efficiency deadlock in visual instruction tuning.

Abstract: Visual instruction tuning aims to enable large language models to comprehend
the visual world, with a pivotal challenge lying in establishing an effective
vision-to-language projection. However, existing methods often grapple with the
intractable trade-off between accuracy and efficiency. In this paper, we
present LLaVA-Meteor, a novel approach designed to break this deadlock,
equipped with a novel Top-Down Compression paradigm that strategically
compresses visual tokens without compromising core information. Specifically,
we construct a trainable Flash Global Fusion module based on efficient
selective state space operators, which aligns the feature space while enabling
each token to perceive holistic visual context and instruction preference at
low cost. Furthermore, a local-to-single scanning manner is employed to
effectively capture local dependencies, thereby enhancing the model's
capability in vision modeling. To alleviate computational overhead, we explore
a Visual-Native Selection mechanism that independently assesses token
significance by both the visual and native experts, followed by aggregation to
retain the most critical subset. Extensive experiments show that our approach
reduces visual tokens by 75--95% while achieving comparable or superior
performance across 12 benchmarks, significantly improving efficiency.

</details>


### [236] [Advanced Integration of Discrete Line Segments in Digitized P&ID for Continuous Instrument Connectivity](https://arxiv.org/abs/2505.11976)
*Soumya Swarup Prusty,Astha Agarwal,Srinivasan Iyenger*

Main category: cs.CV

TL;DR: The paper addresses the challenge of manually mapping P&ID sheets by proposing a digitization method using computer vision to merge line segments and build connections between equipment, enabling storage in a knowledge graph for advanced analysis.


<details>
  <summary>Details</summary>
Motivation: Manual mapping of P&IDs is time-consuming (3-6 months), error-prone, and reliant on expert reviews, necessitating a digitized solution.

Method: Uses a computer vision model to detect and merge line segments, linking instruments and equipment to create a digitized P&ID.

Result: Produces a digitized P&ID that can be stored in a knowledge graph, enabling tasks like route optimization and cycle detection.

Conclusion: Digitizing P&IDs through line merging and knowledge graph storage enhances efficiency and enables advanced analytical applications.

Abstract: Piping and Instrumentation Diagrams (P&IDs) constitute the foundational
blueprint of a plant, depicting the interconnections among process equipment,
instrumentation for process control, and the flow of fluids and control
signals. In their existing setup, the manual mapping of information from P&ID
sheets holds a significant challenge. This is a time-consuming process, taking
around 3-6 months, and is susceptible to errors. It also depends on the
expertise of the domain experts and often requires multiple rounds of review.
The digitization of P&IDs entails merging detected line segments, which is
essential for linking various detected instruments, thereby creating a
comprehensive digitized P&ID. This paper focuses on explaining how line
segments which are detected using a computer vision model are merged and
eventually building the connection between equipment and merged lines. Hence
presenting a digitized form of information stating the interconnection between
process equipment, instrumentation, flow of fluids and control signals.
Eventually, which can be stored in a knowledge graph and that information along
with the help of advanced algorithms can be leveraged for tasks like finding
optimal routes, detecting system cycles, computing transitive closures, and
more.

</details>


### [237] [AoP-SAM: Automation of Prompts for Efficient Segmentation](https://arxiv.org/abs/2505.11980)
*Yi Chen,Mu-Young Son,Chuanbo Hua,Joo-Young Kim*

Main category: cs.CV

TL;DR: AoP-SAM automates prompt generation for SAM, improving efficiency and usability without manual input, enhancing segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual prompts for SAM are impractical in real-world applications; automation is needed for efficiency and resource optimization.

Method: AoP-SAM uses a lightweight Prompt Predictor to detect key entities and optimal prompt locations, leveraging SAM's embeddings without fine-tuning. It also employs Adaptive Sampling and Filtering for coarse-to-fine prompt generation.

Result: Evaluations show AoP-SAM improves prompt generation efficiency and mask accuracy across three datasets.

Conclusion: AoP-SAM makes SAM more effective for automated segmentation by eliminating manual prompts and enhancing performance.

Abstract: The Segment Anything Model (SAM) is a powerful foundation model for image
segmentation, showing robust zero-shot generalization through prompt
engineering. However, relying on manual prompts is impractical for real-world
applications, particularly in scenarios where rapid prompt provision and
resource efficiency are crucial. In this paper, we propose the Automation of
Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential
prompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency
and usability by eliminating manual input, making it better suited for
real-world tasks. Our approach employs a lightweight yet efficient Prompt
Predictor model that detects key entities across images and identifies the
optimal regions for placing prompt candidates. This method leverages SAM's
image embeddings, preserving its zero-shot generalization capabilities without
requiring fine-tuning. Additionally, we introduce a test-time instance-level
Adaptive Sampling and Filtering mechanism that generates prompts in a
coarse-to-fine manner. This notably enhances both prompt and mask generation
efficiency by reducing computational overhead and minimizing redundant mask
refinements. Evaluations of three datasets demonstrate that AoP-SAM
substantially improves both prompt generation efficiency and mask generation
accuracy, making SAM more effective for automated segmentation tasks.

</details>


### [238] [Online Iterative Self-Alignment for Radiology Report Generation](https://arxiv.org/abs/2505.11983)
*Ting Xiao,Lei Shi,Yang Zhang,HaoFeng Yang,Zhe Wang,Chenjia Bai*

Main category: cs.CV

TL;DR: The paper introduces an Online Iterative Self-Alignment (OISA) method for Radiology Report Generation (RRG) to address overfitting and generalization issues by iteratively improving data quality and model performance through self-generation, evaluation, alignment, and iteration.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of supervised fine-tuning and reinforcement learning in RRG, which suffer from limited high-quality annotated data and risks of overfitting.

Method: Proposes OISA, a four-stage method: self-generation of diverse data, self-evaluation for multi-objective preference data, self-alignment for multi-objective optimization, and self-iteration for further improvement.

Result: The method outperforms existing approaches, achieving state-of-the-art performance across multiple evaluation metrics.

Conclusion: OISA enhances RRG by iteratively improving data quality and model performance, offering a robust solution to current limitations.

Abstract: Radiology Report Generation (RRG) is an important research topic for
relieving radiologist' heavy workload. Existing RRG models mainly rely on
supervised fine-tuning (SFT) based on different model architectures using data
pairs of radiological images and corresponding radiologist-annotated reports.
Recent research has shifted focus to post-training improvements, aligning RRG
model outputs with human preferences using reinforcement learning (RL).
However, the limited data coverage of high-quality annotated data poses risks
of overfitting and generalization. This paper proposes a novel Online Iterative
Self-Alignment (OISA) method for RRG that consists of four stages:
self-generation of diverse data, self-evaluation for multi-objective preference
data,self-alignment for multi-objective optimization and self-iteration for
further improvement. Our approach allows for generating varied reports tailored
to specific clinical objectives, enhancing the overall performance of the RRG
model iteratively. Unlike existing methods, our frame-work significantly
increases data quality and optimizes performance through iterative
multi-objective optimization. Experimental results demonstrate that our method
surpasses previous approaches, achieving state-of-the-art performance across
multiple evaluation metrics.

</details>


### [239] [SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations](https://arxiv.org/abs/2505.11992)
*Songchun Zhang,Huiyao Xu,Sitong Guo,Zhongwei Xie,Pengwei Liu,Hujun Bao,Weiwei Xu,Changqing Zou*

Main category: cs.CV

TL;DR: SpatialCrafter improves sparse or single-view 3D scene reconstruction by leveraging video diffusion models, geometric constraints, and monocular depth priors.


<details>
  <summary>Details</summary>
Motivation: Existing NVS techniques require dense multi-view inputs, limiting their practicality. This work aims to overcome this by enabling photorealistic 3D reconstruction from sparse or single-view inputs.

Method: The framework uses a trainable camera encoder, epipolar attention for geometric constraints, and a unified scale estimation strategy. It integrates monocular depth priors and semantic features to regress 3D Gaussian primitives efficiently.

Result: The method enhances sparse view reconstruction and restores realistic 3D scene appearances, as validated by extensive experiments.

Conclusion: SpatialCrafter effectively addresses reconstruction ambiguity and achieves precise camera control and 3D consistency, advancing sparse-view NVS.

Abstract: Novel view synthesis (NVS) boosts immersive experiences in computer vision
and graphics. Existing techniques, though progressed, rely on dense multi-view
observations, restricting their application. This work takes on the challenge
of reconstructing photorealistic 3D scenes from sparse or single-view inputs.
We introduce SpatialCrafter, a framework that leverages the rich knowledge in
video diffusion models to generate plausible additional observations, thereby
alleviating reconstruction ambiguity. Through a trainable camera encoder and an
epipolar attention mechanism for explicit geometric constraints, we achieve
precise camera control and 3D consistency, further reinforced by a unified
scale estimation strategy to handle scale discrepancies across datasets.
Furthermore, by integrating monocular depth priors with semantic features in
the video latent space, our framework directly regresses 3D Gaussian primitives
and efficiently processes long-sequence features using a hybrid network
structure. Extensive experiments show our method enhances sparse view
reconstruction and restores the realistic appearance of 3D scenes.

</details>


### [240] [Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance](https://arxiv.org/abs/2505.11997)
*Mingcheng Qu,Guang Yang,Donglin,Tonghua Su,Yue Gao,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: A multimodal survival prediction framework using hypergraph learning and modality rebalance to address pathology-genomics imbalance, outperforming existing methods by 3.4% in C-Index.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect contextual/hierarchical details in pathology images and suffer from modality imbalance, overshadowing genomics.

Method: Proposes hypergraph learning for pathology details, modality rebalance, and interactive alignment fusion to mitigate imbalance.

Result: Outperforms advanced methods by over 3.4% in C-Index on five TCGA datasets.

Conclusion: The framework effectively addresses modality imbalance and improves survival prediction accuracy.

Abstract: Multimodal pathology-genomic analysis has become increasingly prominent in
cancer survival prediction. However, existing studies mainly utilize
multi-instance learning to aggregate patch-level features, neglecting the
information loss of contextual and hierarchical details within pathology
images. Furthermore, the disparity in data granularity and dimensionality
between pathology and genomics leads to a significant modality imbalance. The
high spatial resolution inherent in pathology data renders it a dominant role
while overshadowing genomics in multimodal integration. In this paper, we
propose a multimodal survival prediction framework that incorporates hypergraph
learning to effectively capture both contextual and hierarchical details from
pathology images. Moreover, it employs a modality rebalance mechanism and an
interactive alignment fusion strategy to dynamically reweight the contributions
of the two modalities, thereby mitigating the pathology-genomics imbalance.
Quantitative and qualitative experiments are conducted on five TCGA datasets,
demonstrating that our model outperforms advanced methods by over 3.4\% in
C-Index performance.

</details>


### [241] [IQBench: How "Smart'' Are Vision-Language Models? A Study with Human IQ Tests](https://arxiv.org/abs/2505.12000)
*Tan-Hanh Pham,Phu-Vinh Nguyen,Dang The Hung,Bui Trong Duong,Vu Nguyen Thanh,Chris Ngo,Tri Quang Truong,Truong-Son Hy*

Main category: cs.CV

TL;DR: The paper introduces IQBench, a benchmark to evaluate Vision-Language Models (VLMs) on visual IQ tests, focusing on reasoning over final prediction accuracy. Results show performance disparities and reasoning limitations in current VLMs.


<details>
  <summary>Details</summary>
Motivation: To explore and evaluate the fluid intelligence and reasoning capabilities of VLMs on human IQ tests, which remain underexplored.

Method: Developed IQBench, a visually centric benchmark with 500 annotated visual IQ questions, minimizing textual dependence. Evaluated models on reasoning patterns, explanations, and final accuracy.

Result: Models like `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` showed highest accuracies (0.615, 0.578, 0.548) but struggled with 3D spatial and anagram tasks. Reasoning scores revealed inconsistencies between reasoning and final answers.

Conclusion: Current VLMs exhibit limitations in general reasoning, underscoring the need to evaluate reasoning accuracy alongside final predictions.

Abstract: Although large Vision-Language Models (VLMs) have demonstrated remarkable
performance in a wide range of multimodal tasks, their true reasoning
capabilities on human IQ tests remain underexplored. To advance research on the
fluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed
to evaluate VLMs on standardized visual IQ tests. We focus on evaluating the
reasoning capabilities of VLMs, which we argue are more important than the
accuracy of the final prediction. **Our benchmark is visually centric,
minimizing the dependence on unnecessary textual content**, thus encouraging
models to derive answers primarily from image-based information rather than
learned textual knowledge. To this end, we manually collected and annotated 500
visual IQ questions to **prevent unintentional data leakage during training**.
Unlike prior work that focuses primarily on the accuracy of the final answer,
we evaluate the reasoning ability of the models by assessing their explanations
and the patterns used to solve each problem, along with the accuracy of the
final prediction and human evaluation. Our experiments show that there are
substantial performance disparities between tasks, with models such as
`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest
average accuracies of 0.615, 0.578, and 0.548, respectively. However, all
models struggle with 3D spatial and anagram reasoning tasks, highlighting
significant limitations in current VLMs' general reasoning abilities. In terms
of reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`
achieved top averages of 0.696, 0.586, and 0.516, respectively. These results
highlight inconsistencies between the reasoning processes of the models and
their final answers, emphasizing the importance of evaluating the accuracy of
the reasoning in addition to the final predictions.

</details>


### [242] [CHRIS: Clothed Human Reconstruction with Side View Consistency](https://arxiv.org/abs/2505.12005)
*Dong Liu,Yifan Yang,Zixiong Huang,Yuxin Gao,Mingkui Tan*

Main category: cs.CV

TL;DR: CHRIS improves clothed human reconstruction from single-view images by enhancing side-view consistency with a discriminator and gradient computation method.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to utilize side-view information from single-view images, leading to unrealistic topology and surface inconsistencies.

Method: CHRIS uses a Side-View Normal Discriminator for global consistency and Multi-to-One Gradient Computation (M2O) for local surface smoothness.

Result: CHRIS achieves state-of-the-art performance on benchmarks, outperforming prior work.

Conclusion: CHRIS effectively addresses side-view inconsistencies in clothed human reconstruction, improving realism.

Abstract: Creating a realistic clothed human from a single-view RGB image is crucial
for applications like mixed reality and filmmaking. Despite some progress in
recent years, mainstream methods often fail to fully utilize side-view
information, as the input single-view image contains front-view information
only. This leads to globally unrealistic topology and local surface
inconsistency in side views. To address these, we introduce Clothed Human
Reconstruction with Side View Consistency, namely CHRIS, which consists of 1) A
Side-View Normal Discriminator that enhances global visual reasonability by
distinguishing the generated side-view normals from the ground truth ones; 2) A
Multi-to-One Gradient Computation (M2O) that ensures local surface consistency.
M2O calculates the gradient of a sampling point by integrating the gradients of
the nearby points, effectively acting as a smooth operation. Experimental
results demonstrate that CHRIS achieves state-of-the-art performance on public
benchmarks and outperforms the prior work.

</details>


### [243] [Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition](https://arxiv.org/abs/2505.12007)
*Runduo Han,Xiuping Liu,Shangxuan Yi,Yi Zhang,Hongchen Tan*

Main category: cs.CV

TL;DR: The paper introduces MCO-E Net, a network for single-eye expression recognition, using event modalities to address challenges like low light and high dynamic range. It features MCO-Mamba for dual-modal optimization and HCE-MoE for heterogeneous expert collaboration, achieving strong performance in poor lighting.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in single-eye expression recognition, such as low light and high dynamic range, by leveraging event modalities and innovative network designs.

Method: Proposes MCO-E Net with two key components: MCO-Mamba for dual-modal optimization and HCE-MoE for heterogeneous expert collaboration.

Result: The network achieves competitive performance, especially in poor lighting conditions.

Conclusion: MCO-E Net effectively addresses expression recognition challenges through multi-modal collaboration and heterogeneous feature integration.

Abstract: In this paper, we proposed a Multi-modal Collaborative Optimization and
Expansion Network (MCO-E Net), to use event modalities to resist challenges
such as low light, high exposure, and high dynamic range in single-eye
expression recognition tasks. The MCO-E Net introduces two innovative designs:
Multi-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous
Collaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building
upon Mamba, leverages dual-modal information to jointly optimize the model,
facilitating collaborative interaction and fusion of modal semantics. This
approach encourages the model to balance the learning of both modalities and
harness their respective strengths. HCE-MoE, on the other hand, employs a
dynamic routing mechanism to distribute structurally varied experts (deep,
attention, and focal), fostering collaborative learning of complementary
semantics. This heterogeneous architecture systematically integrates diverse
feature extraction paradigms to comprehensively capture expression semantics.
Extensive experiments demonstrate that our proposed network achieves
competitive performance in the task of single-eye expression recognition,
especially under poor lighting conditions.

</details>


### [244] [Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation](https://arxiv.org/abs/2505.12009)
*Zhiying Li,Guanggang Geng,Yeying Jin,Zhizhi Guo,Bruce Gu,Jidong Huo,Zhaoxin Fan,Wenjun Wu*

Main category: cs.CV

TL;DR: The paper introduces a black-box adversarial attack (UBA) on EHPS models, focusing on stealth and effectiveness without requiring internal model details.


<details>
  <summary>Details</summary>
Motivation: Existing EHPS models lack security focus, and current attacks are impractical or visually obvious.

Method: UBA uses latent-space representations to generate and refine adversarial noise, optimizing for stealth and potency using only model outputs.

Result: UBA increases pose estimation errors by 17.27%-58.21%, exposing vulnerabilities.

Conclusion: The findings highlight the need to address security risks in digital human generation systems.

Abstract: Expressive human pose and shape (EHPS) estimation is vital for digital human
generation, particularly in live-streaming applications. However, most existing
EHPS models focus primarily on minimizing estimation errors, with limited
attention on potential security vulnerabilities. Current adversarial attacks on
EHPS models often require white-box access (e.g., model details or gradients)
or generate visually conspicuous perturbations, limiting their practicality and
ability to expose real-world security threats. To address these limitations, we
propose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA
leverages the latent-space representations of natural images to generate an
optimal adversarial noise pattern and iteratively refine its attack potency
along an optimized direction in digital space. Crucially, this process relies
solely on querying the model's output, requiring no internal knowledge of the
EHPS architecture, while guiding the noise optimization toward greater stealth
and effectiveness. Extensive experiments and visual analyses demonstrate the
superiority of UBA. Notably, UBA increases the pose estimation errors of EHPS
models by 17.27%-58.21% on average, revealing critical vulnerabilities. These
findings underscore the urgent need to address and mitigate security risks
associated with digital human generation systems.

</details>


### [245] [Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment](https://arxiv.org/abs/2505.12021)
*Kazuhiko Kawamoto,Atsuhiro Endo,Hiroshi Kera*

Main category: cs.CV

TL;DR: The paper introduces a method for cross-model task vector transfer using few-shot orthogonal alignment, improving accuracy while preserving task vector properties.


<details>
  <summary>Details</summary>
Motivation: Task arithmetic assumes identical pre-trained models, limiting cross-model transfer. The paper addresses this gap.

Method: Proposes few-shot orthogonal alignment to align task vectors to differently pre-trained models, preserving properties like norm and rank.

Result: Improves transfer accuracy over direct task vector application, matching few-shot fine-tuning performance.

Conclusion: The method enhances cross-model task vector transfer, maintaining modularity and reusability.

Abstract: Task arithmetic enables efficient model editing by representing task-specific
changes as vectors in parameter space. Task arithmetic typically assumes that
the source and target models are initialized from the same pre-trained
parameters. This assumption limits its applicability in cross-model transfer
settings, where models are independently pre-trained on different datasets. To
address this challenge, we propose a method based on few-shot orthogonal
alignment, which aligns task vectors to the parameter space of a differently
pre-trained target model. These transformations preserve key properties of task
vectors, such as norm and rank, and are learned using only a small number of
labeled examples. We evaluate the method using two Vision Transformers
pre-trained on YFCC100M and LAION400M, and test on eight classification
datasets. Experimental results show that our method improves transfer accuracy
over direct task vector application and achieves performance comparable to
few-shot fine-tuning, while maintaining the modularity and reusability of task
vectors. Our code is available at
https://github.com/kawakera-lab/CrossModelTransfer.

</details>


### [246] [FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition](https://arxiv.org/abs/2505.12045)
*Shuai Yuan,Guowen Xu,Hongwei Li,Rui Zhang,Xinyuan Qian,Wenbo Jiang,Hangcheng Cao,Qingchuan Zhao*

Main category: cs.CV

TL;DR: FIGhost introduces a stealthy physical backdoor attack using fluorescent ink triggers for traffic sign recognition, outperforming existing methods in stealth, flexibility, and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing physical backdoor attacks lack stealth, flexibility, or compatibility with Vision-Large-Language-Models (VLMs).

Method: Uses fluorescent ink triggers (invisible under normal light, activated by UV) and an interpolation-based fluorescence simulation algorithm for robustness. Also includes automated sample generation for three attack objectives.

Result: Effective against state-of-the-art detectors and VLMs, robust under environmental variations, and evades defenses.

Conclusion: FIGhost is a superior, practical solution for physical-world backdoor attacks on TSR systems.

Abstract: Traffic sign recognition (TSR) systems are crucial for autonomous driving but
are vulnerable to backdoor attacks. Existing physical backdoor attacks either
lack stealth, provide inflexible attack control, or ignore emerging
Vision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the
first physical-world backdoor attack leveraging fluorescent ink as triggers.
Fluorescent triggers are invisible under normal conditions and activated
stealthily by ultraviolet light, providing superior stealthiness, flexibility,
and untraceability. Inspired by real-world graffiti, we derive realistic
trigger shapes and enhance their robustness via an interpolation-based
fluorescence simulation algorithm. Furthermore, we develop an automated
backdoor sample generation method to support three attack objectives. Extensive
evaluations in the physical world demonstrate FIGhost's effectiveness against
state-of-the-art detectors and VLMs, maintaining robustness under environmental
variations and effectively evading existing defenses.

</details>


### [247] [Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling](https://arxiv.org/abs/2505.12048)
*Rui Qin,Qijie Wang,Ming Sun,Haowei Zhu,Chao Zhou,Bin Wang*

Main category: cs.CV

TL;DR: The paper introduces Time-Spatial-aware Sampling (TSS) to accelerate diffusion-based super-resolution (SR) by optimizing iteration allocation and denoising strategies, achieving SOTA performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based SR methods are computationally expensive, and generic acceleration techniques fail to exploit low-level task specifics like SR.

Method: Analyzes frequency- and spatial-domain properties of diffusion SR, proposing TSS with Time Dynamic Sampling (TDS) and Spatial Dynamic Sampling (SDS) for adaptive iteration allocation and denoising.

Result: TSS improves MUSIQ scores by 0.2-3.0 and outperforms other methods with half the steps.

Conclusion: TSS effectively reduces computational costs while maintaining high perceptual quality in SR tasks.

Abstract: Diffusion models have gained attention for their success in modeling complex
distributions, achieving impressive perceptual quality in SR tasks. However,
existing diffusion-based SR methods often suffer from high computational costs,
requiring numerous iterative steps for training and inference. Existing
acceleration techniques, such as distillation and solver optimization, are
generally task-agnostic and do not fully leverage the specific characteristics
of low-level tasks like super-resolution (SR). In this study, we analyze the
frequency- and spatial-domain properties of diffusion-based SR methods,
revealing key insights into the temporal and spatial dependencies of
high-frequency signal recovery. Specifically, high-frequency details benefit
from concentrated optimization during early and late diffusion iterations,
while spatially textured regions demand adaptive denoising strategies. Building
on these observations, we propose the Time-Spatial-aware Sampling strategy
(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS
combines Time Dynamic Sampling (TDS), which allocates more iterations to
refining textures, and Spatial Dynamic Sampling (SDS), which dynamically
adjusts strategies based on image content. Extensive evaluations across
multiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)
performance with significantly fewer iterations, improving MUSIQ scores by 0.2
- 3.0 and outperforming the current acceleration methods with only half the
number of steps.

</details>


### [248] [VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption](https://arxiv.org/abs/2505.12053)
*Tianxiong Zhong,Xingye Tian,Boyuan Jiang,Xuebo Wang,Xin Tao,Pengfei Wan,Zhiwei Zhang*

Main category: cs.CV

TL;DR: The paper introduces VFRTok, a Transformer-based video tokenizer, to address inefficiencies in existing video generation frameworks by enabling variable frame rate encoding and decoding, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing video tokenizers suffer from inefficiencies due to fixed temporal compression rates, leading to high computational costs scaling with frame rate.

Method: Proposes VFRTok, leveraging the Duration-Proportional Information Assumption, and introduces Partial Rotary Position Embeddings (RoPE) for better content-awareness.

Result: VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity with only 1/8 tokens compared to existing tokenizers.

Conclusion: VFRTok offers a more efficient and effective solution for video generation by reducing token usage while maintaining high quality.

Abstract: Modern video generation frameworks based on Latent Diffusion Models suffer
from inefficiencies in tokenization due to the Frame-Proportional Information
Assumption. Existing tokenizers provide fixed temporal compression rates,
causing the computational cost of the diffusion model to scale linearly with
the frame rate. The paper proposes the Duration-Proportional Information
Assumption: the upper bound on the information capacity of a video is
proportional to the duration rather than the number of frames. Based on this
insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that
enables variable frame rate encoding and decoding through asymmetric frame rate
training between the encoder and decoder. Furthermore, the paper proposes
Partial Rotary Position Embeddings (RoPE) to decouple position and content
modeling, which groups correlated patches into unified tokens. The Partial RoPE
effectively improves content-awareness, enhancing the video generation
capability. Benefiting from the compact and continuous spatio-temporal
representation, VFRTok achieves competitive reconstruction quality and
state-of-the-art generation fidelity while using only 1/8 tokens compared to
existing tokenizers.

</details>


### [249] [Beluga Whale Detection from Satellite Imagery with Point Labels](https://arxiv.org/abs/2505.12066)
*Yijie Zheng,Jinxuan Yang,Yu Chen,Yaxuan Wang,Yihang Lu,Guoqing Li*

Main category: cs.CV

TL;DR: The paper introduces an automated pipeline using point annotations and SAM to train YOLOv8 for detecting beluga whales, uncertain whales, and harp seals in VHR satellite imagery, reducing manual annotation effort and improving detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing whale detection methods rely on labor-intensive manual annotations and exclude uncertain whales, limiting real-world applicability.

Method: Leverages point annotations and SAM to generate precise bounding boxes for training YOLOv8 for multiclass detection.

Result: Achieved F1-scores of 72.2% for whales and 70.3% for harp seals, with better performance in dense scenes.

Conclusion: The approach reduces annotation effort, enhances uncertain whale detection, and has potential for broader ecological monitoring applications.

Abstract: Very high-resolution (VHR) satellite imagery has emerged as a powerful tool
for monitoring marine animals on a large scale. However, existing deep
learning-based whale detection methods usually require manually created,
high-quality bounding box annotations, which are labor-intensive to produce.
Moreover, existing studies often exclude ``uncertain whales'', individuals that
have ambiguous appearances in satellite imagery, limiting the applicability of
these models in real-world scenarios. To address these limitations, this study
introduces an automated pipeline for detecting beluga whales and harp seals in
VHR satellite imagery. The pipeline leverages point annotations and the Segment
Anything Model (SAM) to generate precise bounding box annotations, which are
used to train YOLOv8 for multiclass detection of certain whales, uncertain
whales, and harp seals. Experimental results demonstrated that SAM-generated
annotations significantly improved detection performance, achieving higher
$\text{F}_\text{1}$-scores compared to traditional buffer-based annotations.
YOLOv8 trained on SAM-labeled boxes achieved an overall
$\text{F}_\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,
with superior performance in dense scenes. The proposed approach not only
reduces the manual effort required for annotation but also enhances the
detection of uncertain whales, offering a more comprehensive solution for
marine animal monitoring. This method holds great potential for extending to
other species, habitats, and remote sensing platforms, as well as for
estimating whale biometrics, thereby advancing ecological monitoring and
conservation efforts. The codes for our label and detection pipeline are
publicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .

</details>


### [250] [MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples](https://arxiv.org/abs/2505.12069)
*Shenzhou Liu,Di Wang,Haonan Guo,Chengxi Han,Wenzhi Zeng*

Main category: cs.CV

TL;DR: MT-CYP-Net, a multi-task framework, improves pixel-level crop yield prediction by leveraging shared features from sparse labels, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of ground truth data for accurate pixel-level crop yield prediction in agriculture.

Method: Proposes MT-CYP-Net, a multi-task network combining yield prediction and crop classification decoders with shared features, trained on sparse labels.

Result: Outperforms classical and deep learning benchmarks, demonstrating effectiveness with limited data.

Conclusion: MT-CYP-Net shows promise for precise yield prediction with sparse labels, advancing agricultural remote sensing.

Abstract: Accurate and fine-grained crop yield prediction plays a crucial role in
advancing global agriculture. However, the accuracy of pixel-level yield
estimation based on satellite remote sensing data has been constrained by the
scarcity of ground truth data. To address this challenge, we propose a novel
approach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This
framework introduces an effective multi-task feature-sharing strategy, where
features extracted from a shared backbone network are simultaneously utilized
by both crop yield prediction decoders and crop classification decoders with
the ability to fuse information between them. This design allows MT-CYP-Net to
be trained with extremely sparse crop yield point labels and crop type labels,
while still generating detailed pixel-level crop yield maps. Concretely, we
collected 1,859 yield point labels along with corresponding crop type labels
and satellite images from eight farms in Heilongjiang Province, China, in 2023,
covering soybean, maize, and rice crops, and constructed a sparse crop yield
label dataset. MT-CYP-Net is compared with three classical machine learning and
deep learning benchmark methods in this dataset. Experimental results not only
indicate the superiority of MT-CYP-Net compared to previous methods on multiple
types of crops but also demonstrate the potential of deep networks on precise
pixel-level crop yield prediction, especially with limited data labels.

</details>


### [251] [Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning](https://arxiv.org/abs/2505.12074)
*Chen Shu,Boyu Fu,Yiman Li,Ting Yin,Wenchuan Zhang,Jie Chen,Yuhao Yi,Hong Bu*

Main category: cs.CV

TL;DR: The paper proposes a method to enhance Multiple Instance Learning (MIL) for Whole Slide Image classification by incorporating pseudo-label correction, improving both bag- and instance-level predictions.


<details>
  <summary>Details</summary>
Motivation: MIL avoids fine-grained annotations but may not learn accurate classifiers at both bag and instance levels. Existing methods introduce noisy labels.

Method: Augments MIL with pseudo-label correction, bridging the gap between MIL and fully supervised learning.

Result: Improves performance of dual-level MIL algorithms on bag- and instance-level predictions.

Conclusion: The proposed method outperforms existing MIL techniques on public pathology datasets.

Abstract: Multiple Instance Learning is the predominant method for Whole Slide Image
classification in digital pathology, enabling the use of slide-level labels to
supervise model training. Although MIL eliminates the tedious fine-grained
annotation process for supervised learning, whether it can learn accurate bag-
and instance-level classifiers remains a question. To address the issue,
instance-level classifiers and instance masks were incorporated to ground the
prediction on supporting patches. These methods, while practically improving
the performance of MIL methods, may potentially introduce noisy labels. We
propose to bridge the gap between commonly used MIL and fully supervised
learning by augmenting both the bag- and instance-level learning processes with
pseudo-label correction capabilities elicited from weak to strong
generalization techniques. The proposed algorithm improves the performance of
dual-level MIL algorithms on both bag- and instance-level predictions.
Experiments on public pathology datasets showcase the advantage of the proposed
methods.

</details>


### [252] [VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.12081)
*Yuqi Liu,Tianyuan Qu,Zhisheng Zhong,Bohao Peng,Shu Liu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: VisionReasoner is a unified framework for multiple visual perception tasks, outperforming Qwen2.5VL in detection, segmentation, and counting.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities for diverse visual perception tasks within a single model.

Method: Uses multi-object cognitive learning and task reformulation to unify tasks and generate structured reasoning.

Result: Achieves superior performance: 29.1% better on COCO, 22.1% on ReasonSeg, and 15.3% on CountBench.

Conclusion: VisionReasoner effectively unifies and excels in diverse visual perception tasks.

Abstract: Large vision-language models exhibit inherent capabilities to handle diverse
visual perception tasks. In this paper, we introduce VisionReasoner, a unified
framework capable of reasoning and solving multiple visual perception tasks
within a shared model. Specifically, by designing novel multi-object cognitive
learning strategies and systematic task reformulation, VisionReasoner enhances
its reasoning capabilities to analyze visual inputs, and addresses diverse
perception tasks in a unified framework. The model generates a structured
reasoning process before delivering the desired outputs responding to user
queries. To rigorously assess unified visual perception capabilities, we
evaluate VisionReasoner on ten diverse tasks spanning three critical domains:
detection, segmentation, and counting. Experimental results show that
VisionReasoner achieves superior performance as a unified model, outperforming
Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg
(segmentation), and 15.3% on CountBench (counting).

</details>


### [253] [LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation](https://arxiv.org/abs/2505.12098)
*Jiarui Wang,Huiyu Duan,Ziheng Jia,Yu Zhao,Woo Yi Yang,Zicheng Zhang,Zijian Chen,Juntong Wang,Yuke Xing,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces AIGVE-60K, a large dataset for evaluating AI-generated videos, and proposes LOVE, an LMM-based metric for comprehensive AIGV evaluation.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated videos (AIGVs) lack perceptual quality and text-video alignment, necessitating a reliable evaluation model.

Method: The authors create AIGVE-60K, a dataset with extensive human annotations, and develop LOVE, an LMM-based metric for multi-dimensional evaluation.

Result: LOVE achieves state-of-the-art performance on AIGVE-60K and generalizes well to other benchmarks.

Conclusion: AIGVE-60K and LOVE significantly advance AIGV evaluation, demonstrating the importance of large-scale datasets and LMM-based metrics.

Abstract: Recent advancements in large multimodal models (LMMs) have driven substantial
progress in both text-to-video (T2V) generation and video-to-text (V2T)
interpretation tasks. However, current AI-generated videos (AIGVs) still
exhibit limitations in terms of perceptual quality and text-video alignment.
Therefore, a reliable and scalable automatic model for AIGV evaluation is
desirable, which heavily relies on the scale and quality of human annotations.
To this end, we present AIGVE-60K, a comprehensive dataset and benchmark for
AI-Generated Video Evaluation, which features (i) comprehensive tasks,
encompassing 3,050 extensive prompts across 20 fine-grained task dimensions,
(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)
and 60K question-answering (QA) pairs annotated on 58,500 videos generated from
30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V
generation and V2T interpretation capabilities. Based on AIGVE-60K, we propose
LOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including
perceptual preference, text-video correspondence, and task-specific accuracy in
terms of both instance level and model level. Comprehensive experiments
demonstrate that LOVE not only achieves state-of-the-art performance on the
AIGVE-60K dataset, but also generalizes effectively to a wide range of other
AIGV evaluation benchmarks. These findings highlight the significance of the
AIGVE-60K dataset. Database and codes are anonymously available at
https://github.com/IntMeGroup/LOVE.

</details>


### [254] [TinyRS-R1: Compact Multimodal Language Model for Remote Sensing](https://arxiv.org/abs/2505.12099)
*Aybora Koksal,A. Aydin Alatan*

Main category: cs.CV

TL;DR: TinyRS is a 2B-parameter multimodal small language model (MSLM) optimized for remote sensing, outperforming larger 7B-parameter models with lower memory and latency. TinyRS-R1, its reasoning-augmented variant, excels in spatial grounding and scene understanding.


<details>
  <summary>Details</summary>
Motivation: Remote-sensing applications require efficient models for edge hardware, but current 7B-parameter models are too resource-intensive.

Method: TinyRS is built on Qwen2-VL-2B and trained via a four-stage pipeline: pre-training, instruction tuning, fine-tuning with CoT annotations, and alignment via GRPO.

Result: TinyRS-R1 matches or exceeds 7B-parameter models in performance while using one-third the memory and latency. CoT reasoning improves spatial grounding, while TinyRS is better for latency-sensitive tasks.

Conclusion: TinyRS-R1 is the first domain-specialized MSLM with GRPO-aligned CoT reasoning, offering efficient and high-performance solutions for remote sensing.

Abstract: Remote-sensing applications often run on edge hardware that cannot host
today's 7B-parameter multimodal language models. This paper introduces TinyRS,
the first 2B-parameter multimodal small language model (MSLM) optimized for
remote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built
upon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training
on million satellite images, instruction tuning on visual instruction examples,
fine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning
dataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1
achieves or surpasses the performance of recent 7B-parameter remote sensing
models across classification, VQA, visual grounding, and open-ended question
answering-while requiring just one-third of the memory and latency. Our
analysis shows that CoT reasoning substantially benefits spatial grounding and
scene understanding, while the non-reasoning TinyRS excels in concise,
latency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized
MSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.

</details>


### [255] [EarthSynth: Generating Informative Earth Observation with Diffusion Models](https://arxiv.org/abs/2505.12108)
*Jiancheng Pan,Shiye Lei,Yuqian Fu,Jiahao Li,Yanxing Liu,Yuze Sun,Xiao He,Long Peng,Xiaomeng Huang,Bo Zhao*

Main category: cs.CV

TL;DR: EarthSynth is a diffusion-based generative model for synthesizing labeled Earth observation data to address the scarcity of labeled data in remote sensing image interpretation.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled data limits performance in remote sensing image interpretation tasks.

Method: EarthSynth uses a diffusion-based approach with Counterfactual Composition training and R-Filter for data diversity and quality.

Result: Evaluated on scene classification, object detection, and semantic segmentation, EarthSynth improves RSI interpretation.

Conclusion: EarthSynth provides a practical solution for advancing remote sensing image interpretation by generating synthetic labeled data.

Abstract: Remote sensing image (RSI) interpretation typically faces challenges due to
the scarcity of labeled data, which limits the performance of RSI
interpretation tasks. To tackle this challenge, we propose EarthSynth, a
diffusion-based generative foundation model that enables synthesizing
multi-category, cross-satellite labeled Earth observation for downstream RSI
interpretation tasks. To the best of our knowledge, EarthSynth is the first to
explore multi-task generation for remote sensing. EarthSynth, trained on the
EarthSynth-180K dataset, employs the Counterfactual Composition training
strategy to improve training data diversity and enhance category control.
Furthermore, a rule-based method of R-Filter is proposed to filter more
informative synthetic data for downstream tasks. We evaluate our EarthSynth on
scene classification, object detection, and semantic segmentation in open-world
scenarios, offering a practical solution for advancing RSI interpretation.

</details>


### [256] [Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation](https://arxiv.org/abs/2505.12130)
*Niaz Ahmad,Jawad Khan,Kang G. Shin,Youngmoon Lee,Guanghui Wang*

Main category: cs.CV

TL;DR: The paper proposes Keypoints as Dynamic Centroid (KDC), a centroid-based method for unified human pose estimation and instance-level segmentation, addressing challenges like overlapping joints and rapid pose changes.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with overlapping joints and fast-changing poses in instance-level segmentation, prompting the need for a more robust solution.

Method: KDC uses a bottom-up approach, generating keypoint heatmaps and introducing KeyCentroids via a keypoint disk. It leverages high-confidence keypoints as dynamic centroids for pixel clustering.

Result: KDC shows improved accuracy and runtime performance on CrowdPose, OCHuman, and COCO benchmarks.

Conclusion: KDC is effective and generalizable for challenging scenarios, offering a unified solution for pose estimation and segmentation.

Abstract: The dynamic movement of the human body presents a fundamental challenge for
human pose estimation and body segmentation. State-of-the-art approaches
primarily rely on combining keypoint heatmaps with segmentation masks but often
struggle in scenarios involving overlapping joints or rapidly changing poses
during instance-level segmentation. To address these limitations, we propose
Keypoints as Dynamic Centroid (KDC), a new centroid-based representation for
unified human pose estimation and instance-level segmentation. KDC adopts a
bottom-up paradigm to generate keypoint heatmaps for both easily
distinguishable and complex keypoints and improves keypoint detection and
confidence scores by introducing KeyCentroids using a keypoint disk. It
leverages high-confidence keypoints as dynamic centroids in the embedding space
to generate MaskCentroids, allowing for swift clustering of pixels to specific
human instances during rapid body movements in live environments. Our
experimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks
demonstrate KDC's effectiveness and generalizability in challenging scenarios
in terms of both accuracy and runtime performance. The implementation is
available at: https://sites.google.com/view/niazahmad/projects/kdc.

</details>


### [257] [Learning to Highlight Audio by Watching Movies](https://arxiv.org/abs/2505.12154)
*Chao Huang,Ruohan Gao,J. M. F. Tsang,Jan Kurcius,Cagdas Bilen,Chenliang Xu,Anurag Kumar,Sanjeel Parekh*

Main category: cs.CV

TL;DR: The paper introduces a novel task of visually-guided acoustic highlighting to improve audio-visual harmony, proposing a transformer-based framework and a new dataset for training.


<details>
  <summary>Details</summary>
Motivation: Addressing the disconnect between visual and acoustic saliency in media production by enhancing audio to match visual cues.

Method: A transformer-based multimodal framework trained on the 'muddy mix dataset,' with pseudo-data generation simulating poorly mixed audio.

Result: Outperforms baselines in quantitative and subjective evaluations, with systematic analysis of contextual guidance and dataset difficulty.

Conclusion: The approach successfully bridges the gap between visual and audio saliency, offering a practical solution for harmonious audio-visual content.

Abstract: Recent years have seen a significant increase in video content creation and
consumption. Crafting engaging content requires the careful curation of both
visual and audio elements. While visual cue curation, through techniques like
optimal viewpoint selection or post-editing, has been central to media
production, its natural counterpart, audio, has not undergone equivalent
advancements. This often results in a disconnect between visual and acoustic
saliency. To bridge this gap, we introduce a novel task: visually-guided
acoustic highlighting, which aims to transform audio to deliver appropriate
highlighting effects guided by the accompanying video, ultimately creating a
more harmonious audio-visual experience. We propose a flexible,
transformer-based multimodal framework to solve this task. To train our model,
we also introduce a new dataset -- the muddy mix dataset, leveraging the
meticulous audio and video crafting found in movies, which provides a form of
free supervision. We develop a pseudo-data generation process to simulate
poorly mixed audio, mimicking real-world scenarios through a three-step process
-- separation, adjustment, and remixing. Our approach consistently outperforms
several baselines in both quantitative and subjective evaluation. We also
systematically study the impact of different types of contextual guidance and
difficulty levels of the dataset. Our project page is here:
https://wikichao.github.io/VisAH/.

</details>


### [258] [SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds](https://arxiv.org/abs/2505.12155)
*Ranit Karmakar,Simon F. Nørrelykke*

Main category: cs.CV

TL;DR: SoftPQ is a new instance segmentation metric that replaces binary evaluation with a graded continuum, offering smoother scoring and better feedback for model improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like IoU, Dice, and PQ use rigid thresholds, obscuring qualitative error distinctions and failing to reward gradual improvements.

Method: SoftPQ introduces tunable IoU thresholds and a sublinear penalty function for partial matches, enabling smoother scoring and robustness to errors.

Result: SoftPQ captures meaningful quality differences overlooked by existing metrics, providing more informative feedback for model development.

Conclusion: SoftPQ is a practical and principled alternative for benchmarking and iterative model refinement, outperforming traditional binary metrics.

Abstract: Segmentation evaluation metrics traditionally rely on binary decision logic:
predictions are either correct or incorrect, based on rigid IoU thresholds.
Detection--based metrics such as F1 and mAP determine correctness at the object
level using fixed overlap cutoffs, while overlap--based metrics like
Intersection over Union (IoU) and Dice operate at the pixel level, often
overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify
detection and segmentation assessment, but it remains dependent on
hard-threshold matching--treating predictions below the threshold as entirely
incorrect. This binary framing obscures important distinctions between
qualitatively different errors and fails to reward gradual model improvements.
We propose SoftPQ, a flexible and interpretable instance segmentation metric
that redefines evaluation as a graded continuum rather than a binary
classification. SoftPQ introduces tunable upper and lower IoU thresholds to
define a partial matching region and applies a sublinear penalty function to
ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit
smoother score behavior, greater robustness to structural segmentation errors,
and more informative feedback for model development and evaluation. Through
controlled perturbation experiments, we show that SoftPQ captures meaningful
differences in segmentation quality that existing metrics overlook, making it a
practical and principled alternative for both benchmarking and iterative model
refinement.

</details>


### [259] [Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum](https://arxiv.org/abs/2505.12191)
*Wenquan Lu,Jiaqi Zhang,Hugues Van Assel,Randall Balestriero*

Main category: cs.CV

TL;DR: A self-supervised framework for noise-robust representation learning without requiring a denoiser at inference or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: SSL research focuses on clean data, but noisy data applications (e.g., astrophysics, medical imaging) require robust solutions.

Method: Trains an SSL denoiser, constructs a denoised-to-noisy curriculum, and uses teacher-guided regularization for noise robustness.

Result: Improves linear probing accuracy by 4.8% over DINOv2 on noisy ImageNet-1k.

Conclusion: Noise-aware pretraining enables denoiser-free robustness, simplifying deployment.

Abstract: Self-Supervised Learning (SSL) has become a powerful solution to extract rich
representations from unlabeled data. Yet, SSL research is mostly focused on
clean, curated and high-quality datasets. As a result, applying SSL on noisy
data remains a challenge, despite being crucial to applications such as
astrophysics, medical imaging, geophysics or finance. In this work, we present
a fully self-supervised framework that enables noise-robust representation
learning without requiring a denoiser at inference or downstream fine-tuning.
Our method first trains an SSL denoiser on noisy data, then uses it to
construct a denoised-to-noisy data curriculum (i.e., training first on
denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),
combined with a teacher-guided regularization that anchors noisy embeddings to
their denoised counterparts. This process encourages the model to internalize
noise robustness. Notably, the denoiser can be discarded after pretraining,
simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise
($\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by
4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from
noise-aware pretraining. The code is available at
https://github.com/wenquanlu/noisy_dinov2.

</details>


### [260] [Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather](https://arxiv.org/abs/2505.12199)
*Kui Jiang,Jing Cao,Zhaocheng Yu,Junjun Jiang,Jingchun Zhou*

Main category: cs.CV

TL;DR: ACDepth improves monocular depth estimation in adverse weather by generating high-quality training data and using domain adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in adverse weather due to domain shifts and poor scene information extraction.

Method: Uses a diffusion model for weather simulation, LoRA adapters for fine-tuning, and multi-granularity knowledge distillation.

Result: Outperforms md4all-DD by 2.50% for night and 2.61% for rainy scenes on nuScenes.

Conclusion: ACDepth is effective for robust depth estimation in challenging weather conditions.

Abstract: Monocular depth estimation is critical for applications such as autonomous
driving and scene reconstruction. While existing methods perform well under
normal scenarios, their performance declines in adverse weather, due to
challenging domain shifts and difficulties in extracting scene information. To
address this issue, we present a robust monocular depth estimation method
called \textbf{ACDepth} from the perspective of high-quality training data
generation and domain adaptation. Specifically, we introduce a one-step
diffusion model for generating samples that simulate adverse weather
conditions, constructing a multi-tuple degradation dataset during training. To
ensure the quality of the generated degradation samples, we employ LoRA
adapters to fine-tune the generation weights of diffusion model. Additionally,
we integrate circular consistency loss and adversarial training to guarantee
the fidelity and naturalness of the scene contents. Furthermore, we elaborate
on a multi-granularity knowledge distillation strategy (MKD) that encourages
the student network to absorb knowledge from both the teacher model and
pretrained Depth Anything V2. This strategy guides the student model in
learning degradation-agnostic scene information from various degradation
inputs. In particular, we introduce an ordinal guidance distillation mechanism
(OGD) that encourages the network to focus on uncertain regions through
differential ranking, leading to a more precise depth estimation. Experimental
results demonstrate that our ACDepth surpasses md4all-DD by 2.50\% for night
scene and 2.61\% for rainy scene on the nuScenes dataset in terms of the absRel
metric.

</details>


### [261] [CompBench: Benchmarking Complex Instruction-guided Image Editing](https://arxiv.org/abs/2505.12200)
*Bohan Jia,Wenxuan Huang,Yuntian Tang,Junbo Qiao,Jincheng Liao,Shaosheng Cao,Fei Zhao,Zhaopeng Feng,Zhouhong Gu,Zhenfei Yin,Lei Bai,Wanli Ouyang,Lin Chen,Fei Zhao,Zihan Wang,Yuan Xie,Shaohui Lin*

Main category: cs.CV

TL;DR: CompBench is a new benchmark for complex instruction-guided image editing, addressing oversimplified tasks in existing benchmarks by incorporating fine-grained instructions and reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack complexity and fine-grained instructions for real-world image editing tasks, limiting model evaluation.

Method: Introduced CompBench with MLLM-human collaboration and an instruction decoupling strategy into four dimensions: location, appearance, dynamics, and objects.

Result: CompBench reveals limitations of current models and offers insights for future systems.

Conclusion: CompBench advances the field by enabling precise evaluation of complex image editing tasks.

Abstract: While real-world applications increasingly demand intricate scene
manipulation, existing instruction-guided image editing benchmarks often
oversimplify task complexity and lack comprehensive, fine-grained instructions.
To bridge this gap, we introduce, a large-scale benchmark specifically designed
for complex instruction-guided image editing. CompBench features challenging
editing scenarios that incorporate fine-grained instruction following, spatial
and contextual reasoning, thereby enabling comprehensive evaluation of image
editing models' precise manipulation capabilities. To construct CompBench, We
propose an MLLM-human collaborative framework with tailored task pipelines.
Furthermore, we propose an instruction decoupling strategy that disentangles
editing intents into four key dimensions: location, appearance, dynamics, and
objects, ensuring closer alignment between instructions and complex editing
requirements. Extensive evaluations reveal that CompBench exposes fundamental
limitations of current image editing models and provides critical insights for
the development of next-generation instruction-guided image editing systems.

</details>


### [262] [Road Segmentation for ADAS/AD Applications](https://arxiv.org/abs/2505.12206)
*Mathanesh Vellingiri Ramasamy,Dimas Rizky Kurniasalim*

Main category: cs.CV

TL;DR: The study compares modified VGG-16 and U-Net models for road segmentation, showing VGG-16 outperforms U-Net despite fewer training epochs, highlighting the impact of architecture and dataset choice.


<details>
  <summary>Details</summary>
Motivation: Accurate road segmentation is crucial for autonomous driving and ADAS, requiring effective navigation in complex environments.

Method: Modified VGG-16 was trained on Comma10k, and modified U-Net on KITTI Road, with performance evaluated using F1-score, mIoU, and precision.

Result: VGG-16 achieved higher accuracy than U-Net in cross-dataset testing, despite U-Net's longer training.

Conclusion: Model architecture and dataset choice significantly influence road segmentation performance, with VGG-16 showing superior results.

Abstract: Accurate road segmentation is essential for autonomous driving and ADAS,
enabling effective navigation in complex environments. This study examines how
model architecture and dataset choice affect segmentation by training a
modified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road
dataset. Both models achieved high accuracy, with cross-dataset testing showing
VGG-16 outperforming U-Net despite U-Net being trained for more epochs. We
analyze model performance using metrics such as F1-score, mean intersection
over union, and precision, discussing how architecture and dataset impact
results.

</details>


### [263] [Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind](https://arxiv.org/abs/2505.12207)
*Qingmei Li,Yang Zhang,Zurong Mai,Yuhang Chen,Shuohong Lou,Henglian Huang,Jiarui Zhang,Zhiwei Zhang,Yibin Wen,Weijia Li,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: AgroMind is a benchmark for evaluating Large Multimodal Models (LMMs) in agricultural remote sensing, covering diverse tasks and datasets, revealing performance gaps and challenges.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for agricultural remote sensing lack diversity and task complexity, necessitating a comprehensive evaluation framework like AgroMind.

Method: AgroMind integrates multiple datasets, preprocesses data, defines diverse tasks, and evaluates 21 LMMs on spatial perception, object understanding, scene understanding, and reasoning.

Result: Performance gaps among LMMs are significant, especially in spatial reasoning and fine-grained recognition, with some LMMs surpassing human performance.

Conclusion: AgroMind highlights LMMs' limitations in domain knowledge and sets a standardized framework for future research in agricultural remote sensing.

Abstract: Large Multimodal Models (LMMs) has demonstrated capabilities across various
domains, but comprehensive benchmarks for agricultural remote sensing (RS)
remain scarce. Existing benchmarks designed for agricultural RS scenarios
exhibit notable limitations, primarily in terms of insufficient scene diversity
in the dataset and oversimplified task design. To bridge this gap, we introduce
AgroMind, a comprehensive agricultural remote sensing benchmark covering four
task dimensions: spatial perception, object understanding, scene understanding,
and scene reasoning, with a total of 13 task types, ranging from crop
identification and health monitoring to environmental analysis. We curate a
high-quality evaluation set by integrating eight public datasets and one
private farmland plot dataset, containing 25,026 QA pairs and 15,556 images.
The pipeline begins with multi-source data preprocessing, including collection,
format standardization, and annotation refinement. We then generate a diverse
set of agriculturally relevant questions through the systematic definition of
tasks. Finally, we employ LMMs for inference, generating responses, and
performing detailed examinations. We evaluated 18 open-source LMMs and 3
closed-source models on AgroMind. Experiments reveal significant performance
gaps, particularly in spatial reasoning and fine-grained recognition, it is
notable that human performance lags behind several leading LMMs. By
establishing a standardized evaluation framework for agricultural RS, AgroMind
reveals the limitations of LMMs in domain knowledge and highlights critical
challenges for future work. Data and code can be accessed at
https://rssysu.github.io/AgroMind/.

</details>


### [264] [Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models](https://arxiv.org/abs/2505.12217)
*Aryan Das,Tanishq Rachamalla,Pravendra Singh,Koushik Biswas,Vinay Kumar Verma,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: HyperCap is a hyperspectral captioning dataset combining spectral data with textual annotations to improve remote sensing tasks like classification and feature extraction.


<details>
  <summary>Details</summary>
Motivation: To enhance model performance in remote sensing by integrating spectral data with pixel-wise textual annotations for deeper semantic understanding.

Method: Constructed from four benchmark datasets using a hybrid automated and manual annotation approach. Evaluated with state-of-the-art encoders and fusion techniques.

Result: Significant improvements in classification performance, demonstrating the potential of vision-language learning in hyperspectral imaging.

Conclusion: HyperCap serves as a foundational dataset for future research in hyperspectral imaging and remote sensing applications.

Abstract: We introduce HyperCap, the first large-scale hyperspectral captioning dataset
designed to enhance model performance and effectiveness in remote sensing
applications. Unlike traditional hyperspectral imaging (HSI) datasets that
focus solely on classification tasks, HyperCap integrates spectral data with
pixel-wise textual annotations, enabling deeper semantic understanding of
hyperspectral imagery. This dataset enhances model performance in tasks like
classification and feature extraction, providing a valuable resource for
advanced remote sensing applications. HyperCap is constructed from four
benchmark datasets and annotated through a hybrid approach combining automated
and manual methods to ensure accuracy and consistency. Empirical evaluations
using state-of-the-art encoders and diverse fusion techniques demonstrate
significant improvements in classification performance. These results
underscore the potential of vision-language learning in HSI and position
HyperCap as a foundational dataset for future research in the field.

</details>


### [265] [From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI](https://arxiv.org/abs/2505.12228)
*Karthik Gopinath,Annabel Sorby-Adams,Jonathan W. Ramirez,Dina Zemlyanker,Jennifer Guo,David Hunt,Christine L. Mac Donald,C. Dirk Keene,Timothy Coalson,Matthew F. Glasser,David Van Essen,Matthew S. Rosen,Oula Puonti,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: A machine learning method for 3D cortical surface reconstruction from low-field MRI (LF-MRI) is presented, achieving strong agreement with high-field MRI (HF-MRI) results despite lower resolution and signal-to-noise.


<details>
  <summary>Details</summary>
Motivation: High-field MRI is limited in availability, while low-field MRI offers accessibility but lacks optimized tools for cortical surface analysis due to lower quality.

Method: Uses a 3D U-Net trained on synthetic LF-MRI to predict signed distance functions, followed by geometric processing for topological accuracy.

Result: LF-MRI reconstruction accuracy depends on acquisition parameters; 3mm isotropic T2-weighted scans show strong agreement with HF-MRI (r=0.96 for surface area).

Conclusion: The method enables cortical surface analysis on portable LF-MRI, validated on postmortem data, with code available for broader use.

Abstract: Three-dimensional reconstruction of cortical surfaces from MRI for
morphometric analysis is fundamental for understanding brain structure. While
high-field MRI (HF-MRI) is standard in research and clinical settings, its
limited availability hinders widespread use. Low-field MRI (LF-MRI),
particularly portable systems, offers a cost-effective and accessible
alternative. However, existing cortical surface analysis tools are optimized
for high-resolution HF-MRI and struggle with the lower signal-to-noise ratio
and resolution of LF-MRI. In this work, we present a machine learning method
for 3D reconstruction and analysis of portable LF-MRI across a range of
contrasts and resolutions. Our method works "out of the box" without
retraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed
distance functions of cortical surfaces, followed by geometric processing to
ensure topological accuracy. We evaluate our method using paired HF/LF-MRI
scans of the same subjects, showing that LF-MRI surface reconstruction accuracy
depends on acquisition parameters, including contrast type (T1 vs T2),
orientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted
scan acquired in under 4 minutes, yields strong agreement with HF-derived
surfaces: surface area correlates at r=0.96, cortical parcellations reach
Dice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains
more challenging with correlations up to r=0.70, reflecting the difficulty of
sub-mm precision with 3mm voxels. We further validate our method on challenging
postmortem LF-MRI, demonstrating its robustness. Our method represents a step
toward enabling cortical surface analysis on portable LF-MRI. Code is available
at https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny

</details>


### [266] [NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation](https://arxiv.org/abs/2505.12235)
*Jia Li,Nan Gao,Huaibo Huang,Ran He*

Main category: cs.CV

TL;DR: NOFT is a plug-and-play module for Stable Diffusion that fine-tunes noise latents using an OT-IB framework, enabling high-fidelity, diverse image generation with minimal training.


<details>
  <summary>Details</summary>
Motivation: Existing T2I/I2I diffusion methods overlook the latent noise's potential for topology/texture control and diversity. NOFT addresses this gap.

Method: NOFT fine-tunes seed or inverse noise via an OT-IB framework with ~14K parameters and 10-minute training, enhancing Stable Diffusion.

Result: NOFT produces high-fidelity, diverse images with aligned topology/texture, outperforming prior methods in efficiency and quality.

Conclusion: NOFT is a versatile, efficient tool for enhancing 2D/3D AIGC assets with text/image guidance, bridging content preservation and variation.

Abstract: The diffusion model has provided a strong tool for implementing text-to-image
(T2I) and image-to-image (I2I) generation. Recently, topology and texture
control are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and
DSG. These methods explicitly consider high-fidelity controllable editing based
on external signals or diffusion feature manipulations. As for diversity, they
directly choose different noise latents. However, the diffused noise is capable
of implicitly representing the topological and textural manifold of the
corresponding image. Moreover, it's an effective workbench to conduct the
trade-off between content preservation and controllable variations. Previous
T2I and I2I diffusion works do not explore the information within the
compressed contextual latent. In this paper, we first propose a plug-and-play
noise finetune NOFT module employed by Stable Diffusion to generate highly
correlated and diverse images. We fine-tune seed noise or inverse noise through
an optimal-transported (OT) information bottleneck (IB) with around only 14K
trainable parameters and 10 minutes of training. Our test-time NOFT is good at
producing high-fidelity image variations considering topology and texture
alignments. Comprehensive experiments demonstrate that NOFT is a powerful
general reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with
text or image guidance.

</details>


### [267] [From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations](https://arxiv.org/abs/2505.12237)
*Yuzhi Li,Haojun Xu,Feng Tian*

Main category: cs.CV

TL;DR: The paper explores using LLMs for video editing by introducing L-Storyboard, a language-based representation for videos, and StoryFlow, a strategy to improve task accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' success in video understanding, their role in video editing is underexplored. This study aims to bridge visual and language reasoning for better editing.

Method: Introduces L-Storyboard for structured video descriptions and StoryFlow for stabilizing divergent tasks like Shot Sequence Ordering.

Result: L-Storyboard improves interpretability and privacy, while StoryFlow enhances logical consistency in editing tasks.

Conclusion: LLMs show strong potential for intelligent video editing, with L-Storyboard and StoryFlow addressing key challenges.

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have
demonstrated remarkable reasoning and generalization capabilities in video
understanding; however, their application in video editing remains largely
underexplored. This paper presents the first systematic study of LLMs in the
context of video editing. To bridge the gap between visual information and
language-based reasoning, we introduce L-Storyboard, an intermediate
representation that transforms discrete video shots into structured language
descriptions suitable for LLM processing. We categorize video editing tasks
into Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot
Attributes Classification, Next Shot Selection, and Shot Sequence Ordering. To
address the inherent instability of divergent task outputs, we propose the
StoryFlow strategy, which converts the divergent multi-path reasoning process
into a convergent selection mechanism, effectively enhancing task accuracy and
logical coherence. Experimental results demonstrate that L-Storyboard
facilitates a more robust mapping between visual information and language
descriptions, significantly improving the interpretability and privacy
protection of video editing tasks. Furthermore, StoryFlow enhances the logical
consistency and output stability in Shot Sequence Ordering, underscoring the
substantial potential of LLMs in intelligent video editing.

</details>


### [268] [SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving](https://arxiv.org/abs/2505.12246)
*Muleilan Pei,Jiayao Shan,Peiliang Li,Jieqi Shi,Jing Huo,Yang Gao,Shaojie Shen*

Main category: cs.CV

TL;DR: The paper proposes SEPT, a framework integrating SD maps into perception and topology reasoning for autonomous vehicles, improving performance in mapless driving systems.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in online scene understanding, especially in long-range or occluded scenarios, by leveraging SD maps as prior knowledge.

Method: Introduces a hybrid feature fusion strategy combining SD maps with BEV features and an intersection-aware keypoint detection task.

Result: Outperforms existing methods on the OpenLane-V2 dataset, significantly enhancing scene perception and topology reasoning.

Conclusion: SEPT effectively integrates SD map priors, advancing autonomous vehicle perception and reasoning capabilities.

Abstract: Online scene perception and topology reasoning are critical for autonomous
vehicles to understand their driving environments, particularly for mapless
driving systems that endeavor to reduce reliance on costly High-Definition (HD)
maps. However, recent advances in online scene understanding still face
limitations, especially in long-range or occluded scenarios, due to the
inherent constraints of onboard sensors. To address this challenge, we propose
a Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning
(SEPT) framework, which explores how to effectively incorporate the SD map as
prior knowledge into existing perception and reasoning pipelines. Specifically,
we introduce a novel hybrid feature fusion strategy that combines SD maps with
Bird's-Eye-View (BEV) features, considering both rasterized and vectorized
representations, while mitigating potential misalignment between SD maps and
BEV feature spaces. Additionally, we leverage the SD map characteristics to
design an auxiliary intersection-aware keypoint detection task, which further
enhances the overall scene understanding performance. Experimental results on
the large-scale OpenLane-V2 dataset demonstrate that by effectively integrating
SD map priors, our framework significantly improves both scene perception and
topology reasoning, outperforming existing methods by a substantial margin.

</details>


### [269] [SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis](https://arxiv.org/abs/2505.12251)
*Haozhe Xiang,Han Zhang,Yu Cheng,Xiongwen Quan,Wanwan Huang*

Main category: cs.CV

TL;DR: A novel semantic-guided medical image fusion method incorporates medical prior knowledge, using a multimodal dataset and text-image alignment to enhance fusion quality and preserve medical information.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook semantic information in medical images, limiting their clinical applicability.

Method: Proposes a semantic-guided approach with a multimodal dataset, text-image alignment via cross-attention, and a medical semantic loss function.

Result: Superior performance in qualitative and quantitative evaluations, preserving critical medical information.

Conclusion: The method effectively integrates medical prior knowledge, improving fusion quality and clinical utility.

Abstract: Multimodal medical image fusion plays a crucial role in medical diagnosis by
integrating complementary information from different modalities to enhance
image readability and clinical applicability. However, existing methods mainly
follow computer vision standards for feature extraction and fusion strategy
formulation, overlooking the rich semantic information inherent in medical
images. To address this limitation, we propose a novel semantic-guided medical
image fusion approach that, for the first time, incorporates medical prior
knowledge into the fusion process. Specifically, we construct a publicly
available multimodal medical image-text dataset, upon which text descriptions
generated by BiomedGPT are encoded and semantically aligned with image features
in a high-dimensional space via a semantic interaction alignment module. During
this process, a cross attention based linear transformation automatically maps
the relationship between textual and visual features to facilitate
comprehensive learning. The aligned features are then embedded into a
text-injection module for further feature-level fusion. Unlike traditional
methods, we further generate diagnostic reports from the fused images to assess
the preservation of medical information. Additionally, we design a medical
semantic loss function to enhance the retention of textual cues from the source
images. Experimental results on test datasets demonstrate that the proposed
method achieves superior performance in both qualitative and quantitative
evaluations while preserving more critical medical information.

</details>


### [270] [LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding](https://arxiv.org/abs/2505.12253)
*Hanyu Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: The paper introduces LLaVA-4D, a framework for 4D scene understanding by embedding spatiotemporal prompts into multimodal models, enhancing dynamic object and background representation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D multimodal models lack temporal awareness, limiting their ability to understand dynamic scenes.

Method: Proposes a spatiotemporal prompt combining 3D position and 1D time, embedding it into visual features for dynamic scene representation.

Result: The method improves 4D scene understanding by distinguishing static backgrounds from dynamic objects.

Conclusion: LLaVA-4D effectively integrates spatial and temporal features, advancing multimodal models for dynamic scene comprehension.

Abstract: Despite achieving significant progress in 2D image understanding, large
multimodal models (LMMs) struggle in the physical world due to the lack of
spatial representation. Typically, existing 3D LMMs mainly embed 3D positions
as fixed spatial prompts within visual features to represent the scene.
However, these methods are limited to understanding the static background and
fail to capture temporally varying dynamic objects. In this paper, we propose
LLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual
representation in 4D scene understanding. The spatiotemporal prompt is
generated by encoding 3D position and 1D time into a dynamic-aware 4D
coordinate embedding. Moreover, we demonstrate that spatial and temporal
components disentangled from visual features are more effective in
distinguishing the background from objects. This motivates embedding the 4D
spatiotemporal prompt into these features to enhance the dynamic scene
representation. By aligning visual spatiotemporal embeddings with language
embeddings, LMMs gain the ability to understand both spatial and temporal
characteristics of static background and dynamic objects in the physical world.
Additionally, we construct a 4D vision-language dataset with spatiotemporal
coordinate annotations for instruction fine-tuning LMMs. Extensive experiments
have been conducted to demonstrate the effectiveness of our method across
different tasks in 4D scene understanding.

</details>


### [271] [MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark](https://arxiv.org/abs/2505.12254)
*Yiwei Ou,Xiaobin Ren,Ronggui Sun,Guansong Gao,Ziyi Jiang,Kaiqi Zhao,Manfredo Manfredini*

Main category: cs.CV

TL;DR: MMS-VPR is a multimodal dataset for street-level place recognition in pedestrian-only environments, addressing gaps in existing datasets by including diverse, non-Western urban contexts and structural spatial graphs.


<details>
  <summary>Details</summary>
Motivation: Existing VPR datasets lack multimodal diversity and underrepresent dense, mixed-use street-level spaces, especially in non-Western urban contexts.

Method: The dataset includes 78,575 annotated images and 2,512 video clips from 207 locations in Chengdu, China, with GPS, timestamps, and metadata. It follows a replicable protocol and includes spatial graphs for structure-aware recognition.

Result: Benchmarks show significant improvements in VPR models when using multimodal and structural cues.

Conclusion: MMS-VPR supports future research in computer vision, geospatial understanding, and multimodal reasoning, and is publicly available.

Abstract: Existing visual place recognition (VPR) datasets predominantly rely on
vehicle-mounted imagery, lack multimodal diversity and underrepresent dense,
mixed-use street-level spaces, especially in non-Western urban contexts. To
address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for
street-level place recognition in complex, pedestrian-only environments. The
dataset comprises 78,575 annotated images and 2,512 video clips captured across
207 locations in a ~70,800 $\mathrm{m}^2$ open-air commercial district in
Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp,
and textual metadata, and covers varied lighting conditions, viewpoints, and
timeframes. MMS-VPR follows a systematic and replicable data collection
protocol with minimal device requirements, lowering the barrier for scalable
dataset creation. Importantly, the dataset forms an inherent spatial graph with
125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place
recognition. We further define two application-specific subsets --
Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based
evaluation tasks. Extensive benchmarks using conventional VPR models, graph
neural networks, and multimodal baselines show substantial improvements when
leveraging multimodal and structural cues. MMS-VPR facilitates future research
at the intersection of computer vision, geospatial understanding, and
multimodal reasoning. The dataset is publicly available at
https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.

</details>


### [272] [PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement](https://arxiv.org/abs/2505.12266)
*ZhanFeng Feng,Long Peng,Xin Di,Yong Guo,Wenbo Li,Yulun Zhang,Renjing Pei,Yang Wang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper introduces PMQ-VE, a novel quantization method for video enhancement, addressing limitations of existing methods by proposing a two-stage framework (BMFQ and PMTD) for improved efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based video enhancement methods face high computational and memory demands, and direct quantization leads to performance degradation and loss of fine details.

Method: Proposes PMQ-VE with two stages: BMFQ for robust clipping bounds via percentile-based initialization and iterative search, and PMTD for progressive distillation using multiple teachers.

Result: PMQ-VE outperforms existing methods, achieving state-of-the-art performance across tasks and benchmarks.

Conclusion: The proposed method effectively balances efficiency and quality, making it suitable for deployment on edge devices.

Abstract: Multi-frame video enhancement tasks aim to improve the spatial and temporal
resolution and quality of video sequences by leveraging temporal information
from multiple frames, which are widely used in streaming video processing,
surveillance, and generation. Although numerous Transformer-based enhancement
methods have achieved impressive performance, their computational and memory
demands hinder deployment on edge devices. Quantization offers a practical
solution by reducing the bit-width of weights and activations to improve
efficiency. However, directly applying existing quantization methods to video
enhancement tasks often leads to significant performance degradation and loss
of fine details. This stems from two limitations: (a) inability to allocate
varying representational capacity across frames, which results in suboptimal
dynamic range adaptation; (b) over-reliance on full-precision teachers, which
limits the learning of low-bit student models. To tackle these challenges, we
propose a novel quantization method for video enhancement: Progressive
Multi-Frame Quantization for Video Enhancement (PMQ-VE). This framework
features a coarse-to-fine two-stage process: Backtracking-based Multi-Frame
Quantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ
utilizes a percentile-based initialization and iterative search with pruning
and backtracking for robust clipping bounds. PMTD employs a progressive
distillation strategy with both full-precision and multiple high-bit (INT)
teachers to enhance low-bit models' capacity and quality. Extensive experiments
demonstrate that our method outperforms existing approaches, achieving
state-of-the-art performance across multiple tasks and benchmarks.The code will
be made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE.

</details>


### [273] [Context-Aware Autoregressive Models for Multi-Conditional Image Generation](https://arxiv.org/abs/2505.12274)
*Yixiao Chen,Zhiyuan Ma,Guoli Jia,Che Jiang,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: ContextAR is an autoregressive transformer framework for multi-conditional image generation, embedding diverse conditions into a unified token sequence with hybrid positional encodings and efficient attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of multi-conditional image generation by leveraging the flexibility of autoregressive transformers while maintaining modality-specific semantics and spatial alignment.

Method: Uses hybrid positional encodings (Rotary + Learnable) and Conditional Context-aware Attention to embed diverse conditions into a token sequence efficiently.

Result: Demonstrates powerful controllability and versatility, matching or outperforming diffusion-based methods in multi-condition scenarios.

Conclusion: ContextAR offers a flexible, effective solution for multi-conditional image generation, with competitive performance and broad applicability.

Abstract: Autoregressive transformers have recently shown impressive image generation
quality and efficiency on par with state-of-the-art diffusion models. Unlike
diffusion architectures, autoregressive models can naturally incorporate
arbitrary modalities into a single, unified token sequence--offering a concise
solution for multi-conditional image generation tasks. In this work, we propose
$\textbf{ContextAR}$, a flexible and effective framework for multi-conditional
image generation. ContextAR embeds diverse conditions (e.g., canny edges, depth
maps, poses) directly into the token sequence, preserving modality-specific
semantics. To maintain spatial alignment while enhancing discrimination among
different condition types, we introduce hybrid positional encodings that fuse
Rotary Position Embedding with Learnable Positional Embedding. We design
Conditional Context-aware Attention to reduces computational complexity while
preserving effective intra-condition perception. Without any fine-tuning,
ContextAR supports arbitrary combinations of conditions during inference time.
Experimental results demonstrate the powerful controllability and versatility
of our approach, and show that the competitive perpormance than diffusion-based
multi-conditional control approaches the existing autoregressive baseline
across diverse multi-condition driven scenarios. Project page:
$\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$

</details>


### [274] [Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction](https://arxiv.org/abs/2505.12280)
*Sijie Zhao,Feng Liu,Xueliang Zhang,Hao Chen,Pengfeng Xiao,Lei Bai*

Main category: cs.CV

TL;DR: The paper introduces TSSUN, a unified network for handling diverse remote sensing data across temporal, spectral, and spatial dimensions, achieving state-of-the-art performance without task-specific modifications.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in data heterogeneity and model incompatibility in remote sensing dense prediction tasks, which require costly retraining for different scenarios.

Method: Proposes TSSUN with a unified strategy to standardize input representations and output structures, along with a Local-Global Window Attention mechanism for feature extraction.

Result: TSSUN adapts to heterogeneous inputs and unifies tasks, achieving or surpassing state-of-the-art performance across datasets.

Conclusion: TSSUN offers a robust, generalizable solution for complex remote sensing applications, eliminating the need for task-specific adjustments.

Abstract: The proliferation of diverse remote sensing data has spurred advancements in
dense prediction tasks, yet significant challenges remain in handling data
heterogeneity. Remote sensing imagery exhibits substantial variability across
temporal, spectral, and spatial (TSS) dimensions, complicating unified data
processing. Current deep learning models for dense prediction tasks, such as
semantic segmentation and change detection, are typically tailored to specific
input-output configurations. Consequently, variations in data dimensionality or
task requirements often lead to significant performance degradation or model
incompatibility, necessitating costly retraining or fine-tuning efforts for
different application scenarios. This paper introduces the
Temporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture
designed for unified representation and modeling of remote sensing data across
diverse TSS characteristics and task types. TSSUN employs a
Temporal-Spectral-Spatial Unified Strategy that leverages meta-information to
decouple and standardize input representations from varied temporal, spectral,
and spatial configurations, and similarly unifies output structures for
different dense prediction tasks and class numbers. Furthermore, a Local-Global
Window Attention mechanism is proposed to efficiently capture both local
contextual details and global dependencies, enhancing the model's adaptability
and feature extraction capabilities. Extensive experiments on multiple datasets
demonstrate that a single TSSUN model effectively adapts to heterogeneous
inputs and unifies various dense prediction tasks. The proposed approach
consistently achieves or surpasses state-of-the-art performance, highlighting
its robustness and generalizability for complex remote sensing applications
without requiring task-specific modifications.

</details>


### [275] [LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?](https://arxiv.org/abs/2505.12307)
*Maoyuan Ye,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: LogicOCR is a benchmark for evaluating Large Multimodal Models (LMMs) on logical reasoning tasks with text-rich images, revealing gaps in multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored performance of LMMs on complex logical reasoning tasks involving text-rich images.

Method: Created LogicOCR using a curated Chinese National Civil Servant Examination text corpus, automated pipeline for multimodal sample generation, and manual verification.

Result: LMMs underperform in multimodal reasoning compared to text-only inputs, highlighting gaps in visual-text integration.

Conclusion: LogicOCR serves as a resource to advance multimodal reasoning research, with the dataset publicly available.

Abstract: Recent advances in Large Multimodal Models (LMMs) have significantly improved
their reasoning and Optical Character Recognition (OCR) capabilities. However,
their performance on complex logical reasoning tasks involving text-rich images
remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark
comprising 1,100 multiple-choice questions designed to evaluate LMMs' logical
reasoning abilities on text-rich images, while minimizing reliance on
domain-specific knowledge (e.g., mathematics). We construct LogicOCR by
curating a text corpus from the Chinese National Civil Servant Examination and
develop a scalable, automated pipeline to convert it into multimodal samples.
First, we design prompt templates to steer GPT-Image-1 to generate images with
diverse backgrounds, interleaved text-illustration layouts, and varied fonts,
ensuring contextual relevance and visual realism. Then, the generated images
are manually verified, with low-quality examples discarded. We evaluate a range
of representative open-source and proprietary LMMs under both Chain-of-Thought
(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key
insights, such as the impact of test-time scaling, input modality differences,
and sensitivity to visual-text orientation. Notably, LMMs still lag in
multimodal reasoning compared to text-only inputs, indicating that they have
not fully bridged visual reading with reasoning. We hope LogicOCR will serve as
a valuable resource for advancing multimodal reasoning research. The dataset is
available at https://github.com/MiliLab/LogicOCR.

</details>


### [276] [DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations](https://arxiv.org/abs/2505.12310)
*Shouyi Lu,Huanyu Zhou,Guirong Zhuo*

Main category: cs.CV

TL;DR: DNOI-4DRO combines neural networks and geometric optimization for 4D radar odometry, outperforming existing methods and matching LiDAR-based results.


<details>
  <summary>Details</summary>
Motivation: To improve 4D radar odometry by integrating neural networks with traditional optimization for better accuracy and robustness.

Method: Uses a neural network for motion flow estimation, constructs a cost function, refines poses with Gauss-Newton updates, and employs a dual-stream backbone for feature enhancement.

Result: Outperforms classical and learning-based methods on VoD and Snail-Radar datasets, achieving LiDAR-comparable results.

Conclusion: DNOI-4DRO is a robust and accurate model for 4D radar odometry, with potential for public release.

Abstract: A novel learning-optimization-combined 4D radar odometry model, named
DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates
traditional geometric optimization with end-to-end neural network training,
leveraging an innovative differentiable neural-optimization iteration operator.
In this framework, point-wise motion flow is first estimated using a neural
network, followed by the construction of a cost function based on the
relationship between point motion and pose in 3D space. The radar pose is then
refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D
radar backbone that integrates multi-scale geometric features and
clustering-based class-aware features to enhance the representation of sparse
4D radar point clouds. Extensive experiments on the VoD and Snail-Radar
datasets demonstrate the superior performance of our model, which outperforms
recent classical and learning-based approaches. Notably, our method even
achieves results comparable to A-LOAM with mapping optimization using LiDAR
point clouds as input. Our models and code will be publicly released.

</details>


### [277] [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
*Qi Feng,Hidetoshi Shimodaira*

Main category: cs.CV

TL;DR: The paper introduces ViCA-322K, a dataset for video-based spatial cognition, and ViCA-7B, a model that outperforms others on spatial tasks. It also provides reasoning datasets and highlights the need for targeted data in spatial modeling.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges current Vision-Language Models face in video-based spatial cognition for robotics and embodied AI.

Method: Introduces ViCA-322K dataset and fine-tunes ViCA-7B model on it, along with creating reasoning datasets (ViCA-Thinking-2.68K) and a reasoning model (ViCA-7B-Thinking).

Result: ViCA-7B achieves state-of-the-art performance on VSI-Bench tasks, with significant improvements (e.g., +26.1 on Absolute Distance).

Conclusion: Targeted data and reasoning models enhance spatial cognition, with released resources to advance visuospatial intelligence research.

Abstract: Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.

</details>


### [278] [Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces](https://arxiv.org/abs/2505.12317)
*Ruoqi Wang,Haitao Wang,Shaojie Guo,Qiong Luo*

Main category: cs.CV

TL;DR: The paper introduces Frequency-Pixel Connect, a domain-adaptation framework that enhances out-of-domain (OOD) robustness by combining frequency and pixel space augmentations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Improving OOD robustness in domain adaptation settings is challenging due to the limitations of generic augmentations and the need for expert knowledge in dataset-specific methods.

Method: Proposes Frequency-Pixel Connect, which mixes amplitude spectrum and pixel content of source and target images to generate domain-diverse augmented samples while preserving semantics.

Result: The method significantly improves cross-domain connectivity and outperforms generic and dataset-specific augmentation methods on diverse benchmarks.

Conclusion: Frequency-Pixel Connect is a dataset-agnostic solution that enhances OOD robustness effectively across multiple domains.

Abstract: Out-of-domain (OOD) robustness under domain adaptation settings, where
labeled source data and unlabeled target data come from different
distributions, is a key challenge in real-world applications. A common approach
to improving OOD robustness is through data augmentations. However, in
real-world scenarios, models trained with generic augmentations can only
improve marginally when generalized under distribution shifts toward unlabeled
target domains. While dataset-specific targeted augmentations can address this
issue, they typically require expert knowledge and extensive prior data
analysis to identify the nature of the datasets and domain shift. To address
these challenges, we propose Frequency-Pixel Connect, a domain-adaptation
framework that enhances OOD robustness by introducing a targeted augmentation
in both the frequency space and pixel space. Specifically, we mix the amplitude
spectrum and pixel content of a source image and a target image to generate
augmented samples that introduce domain diversity while preserving the semantic
structure of the source image. Unlike previous targeted augmentation methods
that are both dataset-specific and limited to the pixel space, Frequency-Pixel
Connect is dataset-agnostic, enabling broader and more flexible applicability
beyond natural image datasets. We further analyze the effectiveness of
Frequency-Pixel Connect by evaluating the performance of our method connecting
same-class cross-domain samples while separating different-class examples. We
demonstrate that Frequency-Pixel Connect significantly improves cross-domain
connectivity and outperforms previous generic methods on four diverse
real-world benchmarks across vision, medical, audio, and astronomical domains,
and it also outperforms other dataset-specific targeted augmentation methods.

</details>


### [279] [Is Artificial Intelligence Generated Image Detection a Solved Problem?](https://arxiv.org/abs/2505.12335)
*Ziqiang Li,Jiazhen Yan,Ziwen He,Kai Zeng,Weiwei Jiang,Lizhi Xiong,Zhangjie Fu*

Main category: cs.CV

TL;DR: AIGIBench is introduced to evaluate AIGI detectors' robustness and generalization, revealing their limitations in real-world scenarios despite high reported accuracy.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misinformation and deepfakes by assessing the real-world effectiveness of AIGI detectors.

Method: AIGIBench evaluates detectors through four tasks: multi-source generalization, robustness to degradation, sensitivity to augmentation, and pre-processing impact, using 23 diverse fake image subsets.

Result: Detectors show significant performance drops on real-world data, limited benefits from augmentations, and nuanced pre-processing effects.

Conclusion: AIGIBench highlights the need for more robust detection strategies and provides a framework for future research.

Abstract: The rapid advancement of generative models, such as GANs and Diffusion
models, has enabled the creation of highly realistic synthetic images, raising
serious concerns about misinformation, deepfakes, and copyright infringement.
Although numerous Artificial Intelligence Generated Image (AIGI) detectors have
been proposed, often reporting high accuracy, their effectiveness in real-world
scenarios remains questionable. To bridge this gap, we introduce AIGIBench, a
comprehensive benchmark designed to rigorously evaluate the robustness and
generalization capabilities of state-of-the-art AIGI detectors. AIGIBench
simulates real-world challenges through four core tasks: multi-source
generalization, robustness to image degradation, sensitivity to data
augmentation, and impact of test-time pre-processing. It includes 23 diverse
fake image subsets that span both advanced and widely adopted image generation
techniques, along with real-world samples collected from social media and AI
art platforms. Extensive experiments on 11 advanced detectors demonstrate that,
despite their high reported accuracy in controlled settings, these detectors
suffer significant performance drops on real-world data, limited benefits from
common augmentations, and nuanced effects of pre-processing, highlighting the
need for more robust detection strategies. By providing a unified and realistic
evaluation framework, AIGIBench offers valuable insights to guide future
research toward dependable and generalizable AIGI detection.

</details>


### [280] [Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.12339)
*Midou Guo,Qilin Yin,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: The paper introduces a new deepfake detection task for unlabeled data in open-world scenarios and proposes a training strategy (OWG-DS) to enhance generalization by aligning domain features and improving class boundaries.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI has led to unlabeled synthetic data flooding social platforms, making it hard to detect unknown deepfake methods. Existing supervised methods fail due to lack of labels.

Method: Proposes OWG-DS with Domain Distance Optimization (DDO) for feature alignment and Similarity-based Class Boundary Separation (SCBS) for clearer boundaries, using adversarial training for domain-invariant features.

Result: OWG-DS improves generalization in cross-method and cross-dataset scenarios, outperforming existing methods.

Conclusion: The proposed strategy effectively addresses the challenge of detecting deepfakes in open-world scenarios with limited labeled data.

Abstract: With the development of generative artificial intelligence, new forgery
methods are rapidly emerging. Social platforms are flooded with vast amounts of
unlabeled synthetic data and authentic data, making it increasingly challenging
to distinguish real from fake. Due to the lack of labels, existing supervised
detection methods struggle to effectively address the detection of unknown
deepfake methods. Moreover, in open world scenarios, the amount of unlabeled
data greatly exceeds that of labeled data. Therefore, we define a new deepfake
detection generalization task which focuses on how to achieve efficient
detection of large amounts of unlabeled data based on limited labeled data to
simulate a open world scenario. To solve the above mentioned task, we propose a
novel Open-World Deepfake Detection Generalization Enhancement Training
Strategy (OWG-DS) to improve the generalization ability of existing methods.
Our approach aims to transfer deepfake detection knowledge from a small amount
of labeled source domain data to large-scale unlabeled target domain data.
Specifically, we introduce the Domain Distance Optimization (DDO) module to
align different domain features by optimizing both inter-domain and
intra-domain distances. Additionally, the Similarity-based Class Boundary
Separation (SCBS) module is used to enhance the aggregation of similar samples
to ensure clearer class boundaries, while an adversarial training mechanism is
adopted to learn the domain-invariant features. Extensive experiments show that
the proposed deepfake detection generalization enhancement training strategy
excels in cross-method and cross-dataset scenarios, improving the model's
generalization.

</details>


### [281] [DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking](https://arxiv.org/abs/2505.12340)
*Jirong Zha,Yuxuan Fan,Kai Li,Han Li,Chen Gao,Xinlei Chen,Yong Li*

Main category: cs.CV

TL;DR: DIMM improves 3D object tracking by decoupling motion models and using adaptive fusion, outperforming existing methods by 31.61%~99.23%.


<details>
  <summary>Details</summary>
Motivation: Challenges in state estimation for highly maneuverable 3D objects due to unknown and rapidly changing state transitions, with limitations in conventional IMM methods.

Method: DIMM extends IMM by decoupling motion models into a hypercube and using a differentiable adaptive fusion network for weight allocation.

Result: DIMM significantly improves tracking accuracy by 31.61%~99.23% over existing methods.

Conclusion: DIMM effectively addresses IMM limitations and enhances 3D tracking accuracy through decoupled models and adaptive fusion.

Abstract: State estimation is challenging for 3D object tracking with high
maneuverability, as the target's state transition function changes rapidly,
irregularly, and is unknown to the estimator. Existing work based on
interacting multiple model (IMM) achieves more accurate estimation than
single-filter approaches through model combination, aligning appropriate models
for different motion modes of the target object over time. However, two
limitations of conventional IMM remain unsolved. First, the solution space of
the model combination is constrained as the target's diverse kinematic
properties in different directions are ignored. Second, the model combination
weights calculated by the observation likelihood are not accurate enough due to
the measurement uncertainty. In this paper, we propose a novel framework, DIMM,
to effectively combine estimates from different motion models in each
direction, thus increasing the 3D object tracking accuracy. First, DIMM extends
the model combination solution space of conventional IMM from a hyperplane to a
hypercube by designing a 3D-decoupled multi-hierarchy filter bank, which
describes the target's motion with various-order linear models. Second, DIMM
generates more reliable combination weight matrices through a differentiable
adaptive fusion network for importance allocation rather than solely relying on
the observation likelihood; it contains an attention-based twin delayed deep
deterministic policy gradient (TD3) method with a hierarchical reward.
Experiments demonstrate that DIMM significantly improves the tracking accuracy
of existing state estimation methods by 31.61%~99.23%.

</details>


### [282] [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
*Qi Feng,Hidetoshi Shimodaira*

Main category: cs.CV

TL;DR: ViCA2, a novel MLLM, enhances visuospatial reasoning with a dual vision encoder and a new dataset, outperforming larger models on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack fine-grained spatial understanding, necessitating specialized models like ViCA2.

Method: ViCA2 integrates SigLIP for semantics and Hiera for spatial structure, with a token ratio control mechanism, and uses the ViCA-322K dataset for training.

Result: ViCA2-7B achieves a state-of-the-art score of 56.8 on VSI-Bench, surpassing larger models.

Conclusion: ViCA2 demonstrates strong visuospatial intelligence with a compact model, and its resources are released for further research.

Abstract: While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.

</details>


### [283] [CLIP-aware Domain-Adaptive Super-Resolution](https://arxiv.org/abs/2505.12391)
*Zhengyang Lu,Qian Xia,Weifan Wang,Feng Wang*

Main category: cs.CV

TL;DR: CDASR is a new framework for domain-generalized super-resolution using CLIP's semantic capabilities, achieving top performance with meta-learning adaptation and multi-stage feature processing.


<details>
  <summary>Details</summary>
Motivation: Addressing domain generalization challenges in single image super-resolution by leveraging CLIP's semantic understanding.

Method: Integrates CLIP-guided feature alignment with meta-learning adaptation, multi-stage feature transformation, and a multi-component loss function.

Result: Outperforms existing methods, with PSNR gains of 0.15dB (×8) and up to 0.30dB (×16) on Urban100.

Conclusion: CDASR effectively combines semantic and super-resolution features, demonstrating superior performance in diverse domains and extreme scaling.

Abstract: This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a
novel framework that addresses the critical challenge of domain generalization
in single image super-resolution. By leveraging the semantic capabilities of
CLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented
performance across diverse domains and extreme scaling factors. The proposed
method integrates CLIP-guided feature alignment mechanism with a meta-learning
inspired few-shot adaptation strategy, enabling efficient knowledge transfer
and rapid adaptation to target domains. A custom domain-adaptive module
processes CLIP features alongside super-resolution features through a
multi-stage transformation process, including CLIP feature processing, spatial
feature generation, and feature fusion. This intricate process ensures
effective incorporation of semantic information into the super-resolution
pipeline. Additionally, CDASR employs a multi-component loss function that
combines pixel-wise reconstruction, perceptual similarity, and semantic
consistency. Extensive experiments on benchmark datasets demonstrate CDASR's
superiority, particularly in challenging scenarios. On the Urban100 dataset at
$\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over
existing methods, with even larger improvements of up to 0.30dB observed at
$\times$16 scaling.

</details>


### [284] [ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding](https://arxiv.org/abs/2505.12408)
*Minxu Liu,Donghai Guan,Chuhang Zheng,Chunwei Tian,Jie Wen,Qi Zhu*

Main category: cs.CV

TL;DR: ViEEG is a hierarchical EEG decoding framework inspired by the brain's visual hierarchy, achieving state-of-the-art performance in visual decoding tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing EEG-based visual decoding methods, which use flat neural representations, by proposing a biologically inspired hierarchical approach aligned with the Hubel-Wiesel theory.

Method: ViEEG decomposes visual stimuli into three components (contour, foreground object, contextual scene) and uses a three-stream EEG encoder with cross-attention routing. It employs hierarchical contrastive learning to align EEG features with CLIP embeddings.

Result: ViEEG achieves 40.9% Top-1 accuracy (subject-dependent) and 22.9% Top-1 accuracy (cross-subject) on the THINGS-EEG dataset, outperforming existing methods by over 45%.

Conclusion: ViEEG advances EEG-based visual decoding performance and introduces a biologically grounded paradigm for brain decoding in AI.

Abstract: Understanding and decoding brain activity into visual representations is a
fundamental challenge at the intersection of neuroscience and artificial
intelligence. While EEG-based visual decoding has shown promise due to its
non-invasive, low-cost nature and millisecond-level temporal resolution,
existing methods are limited by their reliance on flat neural representations
that overlook the brain's inherent visual hierarchy. In this paper, we
introduce ViEEG, a biologically inspired hierarchical EEG decoding framework
that aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes
each visual stimulus into three biologically aligned components-contour,
foreground object, and contextual scene-serving as anchors for a three-stream
EEG encoder. These EEG features are progressively integrated via
cross-attention routing, simulating cortical information flow from V1 to IT to
the association cortex. We further adopt hierarchical contrastive learning to
align EEG representations with CLIP embeddings, enabling zero-shot object
recognition. Extensive experiments on the THINGS-EEG dataset demonstrate that
ViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in
subject-dependent and 22.9% Top-1 accuracy in cross-subject settings,
surpassing existing methods by over 45%. Our framework not only advances the
performance frontier but also sets a new paradigm for biologically grounded
brain decoding in AI.

</details>


### [285] [Kornia-rs: A Low-Level 3D Computer Vision Library In Rust](https://arxiv.org/abs/2505.12425)
*Edgar Riba,Jian Shi,Aditya Kumar,Andrew Shen,Gary Bradski*

Main category: cs.CV

TL;DR: kornia-rs is a high-performance Rust library for 3D computer vision, offering memory and thread safety, modular design, and Python bindings. It outperforms native Rust alternatives and matches C++ wrapper libraries in speed.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a native Rust library for 3D computer vision, ensuring memory and thread safety while maintaining high performance.

Method: Built from scratch in Rust, leveraging its ownership model and type system. Uses a statically-typed tensor system and modular crates for image I/O, processing, and 3D operations. Includes Python bindings for cross-platform compatibility.

Result: Achieves 3~5x speedup over native Rust alternatives and comparable performance to C++ wrapper libraries. Provides 3D vision operators, filling a gap in the Rust ecosystem.

Conclusion: kornia-rs is effective for real-world computer vision applications, combining safety, performance, and modularity.

Abstract: We present \textit{kornia-rs}, a high-performance 3D computer vision library
written entirely in native Rust, designed for safety-critical and real-time
applications. Unlike C++-based libraries like OpenCV or wrapper-based solutions
like OpenCV-Rust, \textit{kornia-rs} is built from the ground up to leverage
Rust's ownership model and type system for memory and thread safety.
\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of
crates, providing efficient image I/O, image processing and 3D operations. To
aid cross-platform compatibility, \textit{kornia-rs} offers Python bindings,
enabling seamless and efficient integration with Rust code. Empirical results
show that \textit{kornia-rs} achieves a 3~ 5 times speedup in image
transformation tasks over native Rust alternatives, while offering comparable
performance to C++ wrapper-based libraries. In addition to 2D vision
capabilities, \textit{kornia-rs} addresses a significant gap in the Rust
ecosystem by providing a set of 3D computer vision operators. This paper
presents the architecture and performance characteristics of
\textit{kornia-rs}, demonstrating its effectiveness in real-world computer
vision applications.

</details>


### [286] [DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model](https://arxiv.org/abs/2505.12427)
*Siwei Xia,Li Sun,Tiantian Sun,Qingli Li*

Main category: cs.CV

TL;DR: DragLoRA improves drag-based editing in diffusion models by integrating LoRA adapters and a denoising score distillation loss, enhancing precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for drag-based editing in diffusion models suffer from limited accuracy and inefficiency due to low feature representation and large search spaces.

Method: DragLoRA integrates LoRA adapters, introduces a denoising score distillation loss, and uses an adaptive optimization scheme for stable and accurate feature adaptation.

Result: DragLoRA significantly improves control precision and computational efficiency in drag-based image editing.

Conclusion: DragLoRA offers a more precise and efficient solution for drag-based editing in diffusion models, with code publicly available.

Abstract: Drag-based editing within pretrained diffusion model provides a precise and
flexible way to manipulate foreground objects. Traditional methods optimize the
input feature obtained from DDIM inversion directly, adjusting them iteratively
to guide handle points towards target locations. However, these approaches
often suffer from limited accuracy due to the low representation ability of the
feature in motion supervision, as well as inefficiencies caused by the large
search space required for point tracking. To address these limitations, we
present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)
adapters into the drag-based editing pipeline. To enhance the training of LoRA
adapters, we introduce an additional denoising score distillation loss which
regularizes the online model by aligning its output with that of the original
model. Additionally, we improve the consistency of motion supervision by
adapting the input features using the updated LoRA, giving a more stable and
accurate input feature for subsequent operations. Building on this, we design
an adaptive optimization scheme that dynamically toggles between two modes,
prioritizing efficiency without compromising precision. Extensive experiments
demonstrate that DragLoRA significantly enhances the control precision and
computational efficiency for drag-based image editing. The Codes of DragLoRA
are available at: https://github.com/Sylvie-X/DragLoRA.

</details>


### [287] [DPCD: A Quality Assessment Database for Dynamic Point Clouds](https://arxiv.org/abs/2505.12431)
*Yating Liu,Yujie Zhang,Qi Yang,Yiling Xu,Zhu Li,Ye-Kui Wang*

Main category: cs.CV

TL;DR: The paper introduces a large-scale Dynamic Point Cloud Quality Assessment (DPCQA) database (DPCD) to address the lack of research in DPCQA, validating its reliability and heterogeneity through subjective experiments and objective metric evaluations.


<details>
  <summary>Details</summary>
Motivation: The growing demand for VR/AR applications highlights the need for Dynamic Point Clouds (DPCs), but the lack of research on DPCQA hinders quality-oriented applications like compression and transmission.

Method: The authors created DPCD, a database with 15 reference DPCs and 525 distorted DPCs, conducted subjective experiments with 21 viewers to obtain Mean Opinion Scores (MOS), and evaluated objective metrics.

Result: The study confirms DPCQA is more challenging than static point cloud assessment and validates DPCD's reliability and heterogeneity.

Conclusion: DPCD serves as a foundational resource for future DPCQA research and is publicly available.

Abstract: Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven
the demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are
capable of capturing temporal changes within objects or scenes, offering a more
accurate simulation of the real world. While significant progress has been made
in the quality assessment research of static point cloud, little study has been
done on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the
development of quality-oriented applications, such as interframe compression
and transmission in practical scenarios. In this paper, we introduce a
large-scale DPCQA database, named DPCD, which includes 15 reference DPCs and
525 distorted DPCs from seven types of lossy compression and noise distortion.
By rendering these samples to Processed Video Sequences (PVS), a comprehensive
subjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21
viewers for analysis. The characteristic of contents, impact of various
distortions, and accuracy of MOSs are presented to validate the heterogeneity
and reliability of the proposed database. Furthermore, we evaluate the
performance of several objective metrics on DPCD. The experiment results show
that DPCQA is more challenge than that of static point cloud. The DPCD, which
serves as a catalyst for new research endeavors on DPCQA, is publicly available
at https://huggingface.co/datasets/Olivialyt/DPCD.

</details>


### [288] [SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization](https://arxiv.org/abs/2505.12433)
*Haodong Yang,Lei Wang,Md Zakir Hossain*

Main category: cs.CV

TL;DR: SRLoRA enhances LoRA by dynamically recomposing low-rank subspaces during training, improving performance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: LoRA's fixed low-rank subspace limits representational capacity and downstream performance, prompting the need for a more expressive yet efficient method.

Method: SRLoRA assigns importance scores to LoRA pairs, fuses less important ones into the backbone, and reinitializes new pairs along unused principal directions.

Result: SRLoRA achieves faster convergence and higher accuracy on GLUE and image classification tasks compared to standard LoRA.

Conclusion: SRLoRA offers a lightweight, efficient, and generalizable improvement over LoRA for parameter-efficient fine-tuning.

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning (PEFT) method that injects two trainable low-rank matrices (A and
B) into frozen pretrained models. While efficient, LoRA constrains updates to a
fixed low-rank subspace (Delta W = BA), which can limit representational
capacity and hinder downstream performance. We introduce Subspace Recomposition
in Low-Rank Adaptation (SRLoRA) via importance-based fusion and
reinitialization, a novel approach that enhances LoRA's expressiveness without
compromising its lightweight structure. SRLoRA assigns importance scores to
each LoRA pair (a column of B and the corresponding row of A), and dynamically
recomposes the subspace during training. Less important pairs are fused into
the frozen backbone, freeing capacity to reinitialize new pairs along unused
principal directions derived from the pretrained weight's singular value
decomposition. This mechanism enables continual subspace refreshment and richer
adaptation over time, without increasing the number of trainable parameters. We
evaluate SRLoRA on both language and vision tasks, including the GLUE benchmark
and various image classification datasets. SRLoRA consistently achieves faster
convergence and improved accuracy over standard LoRA, demonstrating its
generality, efficiency, and potential for broader PEFT applications.

</details>


### [289] [VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/abs/2505.12434)
*Qi Wang,Yanrui Yu,Ye Yuan,Rui Mao,Tianfei Zhou*

Main category: cs.CV

TL;DR: VIDEORFT extends reinforcement fine-tuning (RFT) to MLLMs for human-like video reasoning, addressing data scarcity with an automatic CoT curation pipeline and introducing a semantic-consistency reward for RL.


<details>
  <summary>Details</summary>
Motivation: Video reasoning is challenging due to complex logic and temporal structures. Existing RFT methods lack large-scale video CoT datasets.

Method: VIDEORFT uses a two-stage RFT approach: SFT with CoT annotations and RL for generalization. It builds datasets via an automatic CoT pipeline and introduces a semantic-consistency reward.

Result: VIDEORFT achieves state-of-the-art performance on six video reasoning benchmarks.

Conclusion: VIDEORFT effectively enhances video reasoning in MLLMs by addressing data scarcity and improving RL alignment with visual evidence.

Abstract: Reinforcement fine-tuning (RFT) has shown great promise in achieving
humanlevel reasoning capabilities of Large Language Models (LLMs), and has
recently been extended to MLLMs. Nevertheless, reasoning about videos, which is
a fundamental aspect of human intelligence, remains a persistent challenge due
to the complex logic, temporal and causal structures inherent in video data. To
fill this gap, we propose VIDEORFT, a novel approach that extends the RFT
paradigm to cultivate human-like video reasoning capabilities in MLLMs.
VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning
(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement
learning (RL) to improve generalization. A central challenge to achieve this in
the video domain lies in the scarcity of large-scale, high-quality video CoT
datasets. We address this by building a fully automatic CoT curation pipeline.
First, we devise a cognitioninspired prompting strategy to elicit a reasoning
LLM to generate preliminary CoTs based solely on rich, structured, and literal
representations of video content. Subsequently, these CoTs are revised by a
visual-language model conditioned on the actual video, ensuring visual
consistency and reducing visual hallucinations. This pipeline results in two
new datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To
further strength the RL phase, we introduce a novel semantic-consistency reward
that explicitly promotes the alignment between textual reasoning with visual
evidence. This reward encourages the model to produce coherent, context-aware
reasoning outputs grounded in visual input. Extensive experiments show that
VIDEORFT achieves state-of-the-art performance on six video reasoning
benchmarks.

</details>


### [290] [SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448)
*Yang Liu,Ming Ma,Xiaomin Yu,Pengxiang Ding,Han Zhao,Mingyang Sun,Siteng Huang,Donglin Wang*

Main category: cs.CV

TL;DR: The paper proposes SSR, a framework to enhance spatial reasoning in VLMs by converting depth data into textual rationales and using knowledge distillation for efficient integration.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack precise spatial understanding due to reliance on RGB inputs, and existing methods for integrating spatial cues are either sensor-dependent or ineffective.

Method: SSR transforms raw depth data into structured textual rationales and compresses them into latent embeddings via knowledge distillation for plug-and-play VLM integration.

Result: SSR improves depth utilization and spatial reasoning, validated by extensive experiments and a new dataset (SSR-CoT) and benchmark (SSRBench).

Conclusion: SSR advances VLMs toward human-like multi-modal understanding by effectively leveraging depth information.

Abstract: Despite impressive advancements in Visual-Language Models (VLMs) for
multi-modal tasks, their reliance on RGB inputs limits precise spatial
understanding. Existing methods for integrating spatial cues, such as point
clouds or depth, either require specialized sensors or fail to effectively
exploit depth information for higher-order reasoning. To this end, we propose a
novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that
transforms raw depth data into structured, interpretable textual rationales.
These textual rationales serve as meaningful intermediate representations to
significantly enhance spatial reasoning capabilities. Additionally, we leverage
knowledge distillation to compress the generated rationales into compact latent
embeddings, which facilitate resource-efficient and plug-and-play integration
into existing VLMs without retraining. To enable comprehensive evaluation, we
introduce a new dataset named SSR-CoT, a million-scale visual-language
reasoning dataset enriched with intermediate spatial reasoning annotations, and
present SSRBench, a comprehensive multi-task benchmark. Extensive experiments
on multiple benchmarks demonstrate SSR substantially improves depth utilization
and enhances spatial reasoning, thereby advancing VLMs toward more human-like
multi-modal understanding. Our project page is at
https://yliu-cs.github.io/SSR.

</details>


### [291] [Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification](https://arxiv.org/abs/2505.12482)
*Wenchen Chen,Yanmei Zhang,Zhongwei Xiao,Jianping Chu,Xingbo Wang*

Main category: cs.CV

TL;DR: The paper proposes S4L-FSC, a method combining SSL and FSL to improve few-shot HSI classification by leveraging spatial and spectral features from heterogeneous and homogeneous datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of scarce labeled samples in HSI classification by adapting to spatial diversity and incorporating spectral prior knowledge.

Method: Uses RM-SSL for spatial feature extraction and MR-SSL for spectral feature extraction, combined with FSL for transferable knowledge.

Result: Demonstrates superior performance on four HSI datasets.

Conclusion: S4L-FSC effectively enhances few-shot HSI classification by integrating spatial and spectral pretraining.

Abstract: Few-shot classification of hyperspectral images (HSI) faces the challenge of
scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning
(FSL) offer promising avenues to address this issue. However, existing methods
often struggle to adapt to the spatial geometric diversity of HSIs and lack
sufficient spectral prior knowledge. To tackle these challenges, we propose a
method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral
Image Classification (S4L-FSC), aimed at improving the performance of few-shot
HSI classification. Specifically, we first leverage heterogeneous datasets to
pretrain a spatial feature extractor using a designed Rotation-Mirror
Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach
enables the model to learn the spatial geometric diversity of HSIs using
rotation and mirroring labels as supervisory signals, while acquiring
transferable spatial meta-knowledge through few-shot learning. Subsequently,
homogeneous datasets are utilized to pretrain a spectral feature extractor via
a combination of FSL and Masked Reconstruction Self-Supervised Learning
(MR-SSL). The model learns to reconstruct original spectral information from
randomly masked spectral vectors, inferring spectral dependencies. In parallel,
FSL guides the model to extract pixel-level discriminative features, thereby
embedding rich spectral priors into the model. This spectral-spatial
pretraining method, along with the integration of knowledge from heterogeneous
and homogeneous sources, significantly enhances model performance. Extensive
experiments on four HSI datasets demonstrate the effectiveness and superiority
of the proposed S4L-FSC approach for few-shot HSI classification.

</details>


### [292] [Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation](https://arxiv.org/abs/2505.12486)
*Sangmin Jung,Utkarsh Nath,Yezhou Yang,Giulia Pedrielli,Joydeep Biswas,Amy Zhang,Hassan Ghasemzadeh,Pavan Turaga*

Main category: cs.CV

TL;DR: The paper introduces Deep Geometric Moments (DGM) as a novel guidance method for text-to-image generation, offering fine-grained control while preserving diversity.


<details>
  <summary>Details</summary>
Motivation: Existing guidance methods like segmentation or depth maps limit the diversity of diffusion models, and features like DINO or CLIP overemphasize global aspects. DGMs address these limitations by focusing on subject-specific geometric features.

Method: DGMs use learned geometric priors to encapsulate visual features and nuances, providing robust guidance compared to pixel-sensitive methods like ResNets.

Result: Experiments show DGMs effectively balance control and diversity in diffusion-based image generation, enabling flexible steering of the process.

Conclusion: DGMs offer a promising solution for fine-grained control in text-to-image synthesis without sacrificing the inherent diversity of diffusion models.

Abstract: Text-to-image generation models have achieved remarkable capabilities in
synthesizing images, but often struggle to provide fine-grained control over
the output. Existing guidance approaches, such as segmentation maps and depth
maps, introduce spatial rigidity that restricts the inherent diversity of
diffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a
novel form of guidance that encapsulates the subject's visual features and
nuances through a learned geometric prior. DGMs focus specifically on the
subject itself compared to DINO or CLIP features, which suffer from
overemphasis on global image features or semantics. Unlike ResNets, which are
sensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.
Our experiments demonstrate that DGM effectively balance control and diversity
in diffusion-based image generation, allowing a flexible control mechanism for
steering the diffusion process.

</details>


### [293] [Video-GPT via Next Clip Diffusion](https://arxiv.org/abs/2505.12489)
*Shaobin Zhuang,Zhipeng Huang,Ying Zhang,Fangyikang Wang,Canmiao Fu,Binxin Yang,Chong Sun,Chen Li,Yali Wang*

Main category: cs.CV

TL;DR: Video-GPT introduces a novel next clip diffusion paradigm for video modeling, achieving state-of-the-art performance in video prediction and demonstrating strong generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Language sequences lack spatial-temporal details; videos capture these better, inspiring Video-GPT as a new 'language' for visual world modeling.

Method: Proposes a next clip diffusion paradigm, autoregressively denoising noisy clips based on clean historical clips, enabling short-term generation and long-term prediction.

Result: Achieves top performance in video prediction (Physics-IQ Benchmark: 34.97) and adapts well to 6 mainstream video tasks.

Conclusion: Video-GPT excels in video modeling, prediction, and generalization, marking a significant advancement in visual world modeling.

Abstract: GPT has shown its remarkable success in natural language processing. However,
the language sequence is not sufficient to describe spatial-temporal details in
the visual world. Alternatively, the video sequence is good at capturing such
details. Motivated by this fact, we propose a concise Video-GPT in this paper
by treating video as new language for visual world modeling. By analogy to next
token prediction in GPT, we introduce a novel next clip diffusion paradigm for
pretraining Video-GPT. Different from the previous works, this distinct
paradigm allows Video-GPT to tackle both short-term generation and long-term
prediction, by autoregressively denoising the noisy clip according to the clean
clips in the history. Extensive experiments show our Video-GPT achieves the
state-of-the-art performance on video prediction, which is the key factor
towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64
vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in
both video generation and understanding, showing its great generalization
capacity in downstream. The project page is at https://Video-GPT.github.io.

</details>


### [294] [Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval](https://arxiv.org/abs/2505.12499)
*Jian Xiao,Zijie Song,Jialong Hu,Hao Cheng,Zhenzhen Hu,Jia Li,Richang Hong*

Main category: cs.CV

TL;DR: GARE introduces a learnable increment to mitigate modality gaps and false negatives in text-video retrieval, improving alignment and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods suffer from modality gaps and false negatives, causing conflicting gradients and unstable alignment.

Method: GARE uses a pair-specific increment (Delta_ij) derived from a Taylor approximation, implemented via a lightweight neural module, and regularized for stability.

Result: GARE improves alignment accuracy and robustness across four benchmarks.

Conclusion: GARE effectively addresses gradient conflicts and enhances retrieval performance by mitigating modality gaps.

Abstract: Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.

</details>


### [295] [GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification](https://arxiv.org/abs/2505.12513)
*Yang Mu,Zhitong Xiong,Yi Wang,Muhammad Shahzad,Franz Essl,Mark van Kleunen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: The paper introduces GlobalGeoTree, a global dataset for tree species classification, and GeoTreeCLIP, a baseline model, to address the lack of large-scale labeled data in biodiversity monitoring.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale, labeled datasets hinders progress in global tree species mapping, which is crucial for biodiversity and ecological research.

Method: The authors present GlobalGeoTree, a dataset with 6.3 million geolocated tree occurrences, paired with Sentinel-2 imagery and environmental variables. They also introduce GeoTreeCLIP, a vision-language model pretrained on this dataset.

Result: GeoTreeCLIP outperforms existing models in zero- and few-shot classification on the GlobalGeoTree-10kEval subset.

Conclusion: The dataset and model aim to benchmark and advance tree species classification, supporting biodiversity research and ecological applications.

Abstract: Global tree species mapping using remote sensing data is vital for
biodiversity monitoring, forest management, and ecological research. However,
progress in this field has been constrained by the scarcity of large-scale,
labeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive
global dataset for tree species classification. GlobalGeoTree comprises 6.3
million geolocated tree occurrences, spanning 275 families, 2,734 genera, and
21,001 species across the hierarchical taxonomic levels. Each sample is paired
with Sentinel-2 image time series and 27 auxiliary environmental variables,
encompassing bioclimatic, geographic, and soil data. The dataset is partitioned
into GlobalGeoTree-6M for model pretraining and curated evaluation subsets,
primarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To
demonstrate the utility of the dataset, we introduce a baseline model,
GeoTreeCLIP, which leverages paired remote sensing data and taxonomic text
labels within a vision-language framework pretrained on GlobalGeoTree-6M.
Experimental results show that GeoTreeCLIP achieves substantial improvements in
zero- and few-shot classification on GlobalGeoTree-10kEval over existing
advanced models. By making the dataset, models, and code publicly available, we
aim to establish a benchmark to advance tree species classification and foster
innovation in biodiversity research and ecological applications.

</details>


### [296] [Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets](https://arxiv.org/abs/2505.12532)
*Ahmet Bilican,M. Akın Yılmaz,A. Murat Tekalp,R. Gökberk Cinbiş*

Main category: cs.CV

TL;DR: WaveFT is a novel PEFT method that learns sparse updates in the wavelet domain, outperforming LoRA and others in efficiency and performance, especially in low-parameter regimes.


<details>
  <summary>Details</summary>
Motivation: Adapting large foundation models efficiently is crucial under tight compute and memory constraints, but existing PEFT methods like LoRA lack granularity and effectiveness with few parameters.

Method: WaveFT learns highly sparse updates in the wavelet domain of residual matrices, enabling precise control of trainable parameters and fine-grained capacity adjustment.

Result: WaveFT significantly outperforms LoRA and other PEFT methods in personalized text-to-image generation, achieving better subject fidelity, prompt alignment, and image diversity, especially with low parameter counts.

Conclusion: WaveFT is ideal for extreme parameter-efficient scenarios, offering superior performance and granularity compared to existing methods like LoRA.

Abstract: Efficiently adapting large foundation models is critical, especially with
tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)
methods such as LoRA offer limited granularity and effectiveness in
few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT
method that learns highly sparse updates in the wavelet domain of residual
matrices. WaveFT allows precise control of trainable parameters, offering
fine-grained capacity adjustment and excelling with remarkably low parameter
count, potentially far fewer than LoRA's minimum -- ideal for extreme
parameter-efficient scenarios. In order to demonstrate the effect of the
wavelet transform, we compare WaveFT with a special case, called SHiRA, that
entails applying sparse updates directly in the weight domain. Evaluated on
personalized text-to-image generation using Stable Diffusion XL as baseline,
WaveFT significantly outperforms LoRA and other PEFT methods, especially at low
parameter counts; achieving superior subject fidelity, prompt alignment, and
image diversity.

</details>


### [297] [ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations](https://arxiv.org/abs/2505.12547)
*Florent Chiaroni,Ali Ayub,Ola Ahmad*

Main category: cs.CV

TL;DR: ProMi introduces a training-free, prototype-mixture-based method for few-shot binary segmentation using bounding-box annotations, outperforming baselines and showing real-world applicability in robotics.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotations are costly and time-consuming; ProMi aims to simplify segmentation with coarse bounding-box labels.

Method: ProMi treats the background class as a mixture of distributions, enabling efficient few-shot segmentation without training.

Result: ProMi outperforms existing baselines across datasets and demonstrates effectiveness in real-world robotics tasks.

Conclusion: ProMi is a simple, effective solution for few-shot segmentation, reducing annotation effort while maintaining performance.

Abstract: In robotics applications, few-shot segmentation is crucial because it allows
robots to perform complex tasks with minimal training data, facilitating their
adaptation to diverse, real-world environments. However, pixel-level
annotations of even small amount of images is highly time-consuming and costly.
In this paper, we present a novel few-shot binary segmentation method based on
bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an
efficient prototype-mixture-based method that treats the background class as a
mixture of distributions. Our approach is simple, training-free, and effective,
accommodating coarse annotations with ease. Compared to existing baselines,
ProMi achieves the best results across different datasets with significant
gains, demonstrating its effectiveness. Furthermore, we present qualitative
experiments tailored to real-world mobile robot tasks, demonstrating the
applicability of our approach in such scenarios. Our code:
https://github.com/ThalesGroup/promi.

</details>


### [298] [VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold](https://arxiv.org/abs/2505.12549)
*Dominic Maggio,Hyungtae Lim,Luca Carlone*

Main category: cs.CV

TL;DR: VGGT-SLAM is a dense RGB SLAM system using uncalibrated monocular cameras, optimizing over the SL(4) manifold for improved map quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods using similarity transforms are inadequate for uncalibrated cameras due to reconstruction ambiguity.

Method: Aligns submaps by optimizing over the SL(4) manifold, estimating 15-DOF homography transforms and accounting for loop closures.

Result: Achieves better map quality with long video sequences, overcoming VGGT's GPU limitations.

Conclusion: VGGT-SLAM effectively addresses reconstruction ambiguity and improves performance for uncalibrated cameras.

Abstract: We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally
and globally aligning submaps created from the feed-forward scene
reconstruction approach VGGT using only uncalibrated monocular cameras. While
related works align submaps using similarity transforms (i.e., translation,
rotation, and scale), we show that such approaches are inadequate in the case
of uncalibrated cameras. In particular, we revisit the idea of reconstruction
ambiguity, where given a set of uncalibrated cameras with no assumption on the
camera motion or scene structure, the scene can only be reconstructed up to a
15-degrees-of-freedom projective transformation of the true geometry. This
inspires us to recover a consistent scene reconstruction across submaps by
optimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom
homography transforms between sequential submaps while accounting for potential
loop closure constraints. As verified by extensive experiments, we demonstrate
that VGGT-SLAM achieves improved map quality using long video sequences that
are infeasible for VGGT due to its high GPU requirements.

</details>


### [299] [Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID](https://arxiv.org/abs/2505.12580)
*Priyank Pathak,Yogesh S Rawat*

Main category: cs.CV

TL;DR: The paper introduces RLQ, a framework to improve Clothes Changing Re-Identification (CC-ReID) for low-quality images by combining Coarse Attributes Prediction (CAP) and Task Agnostic Distillation (TAD).


<details>
  <summary>Details</summary>
Motivation: Existing CC-ReID models struggle with low-quality images due to noise in biometric attributes and feature representations, leading to incorrect matches.

Method: RLQ uses CAP to predict coarse external attributes and TAD to enhance internal features via task-agnostic self-supervision and distillation.

Result: RLQ outperforms existing methods by 1.6%-2.9% Top-1 on real-world datasets and shows 5.3%-6% improvement on PRCC.

Conclusion: The proposed RLQ framework effectively addresses the challenges of low-quality images in CC-ReID, demonstrating superior performance on real-world datasets.

Abstract: This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the
real world. Existing works perform well with high-quality (HQ) images, but
struggle with low-quality (LQ) where we can have artifacts like pixelation,
out-of-focus blur, and motion blur. These artifacts introduce noise to not only
external biometric attributes (e.g. pose, body shape, etc.) but also corrupt
the model's internal feature representation. Models usually cluster LQ image
features together, making it difficult to distinguish between them, leading to
incorrect matches. We propose a novel framework Robustness against Low-Quality
(RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse
Attributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in
alternate steps in a novel training mechanism. CAP enriches the model with
external fine-grained attributes via coarse predictions, thereby reducing the
effect of noisy inputs. On the other hand, TAD enhances the model's internal
feature representation by bridging the gap between HQ and LQ features, via an
external dataset through task-agnostic self-supervision and distillation. RLQ
outperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets
like LaST, and DeepChange, while showing consistent improvement of 5.3%-6%
Top-1 on PRCC with competitive performance on LTCC. *The code will be made
public soon.*

</details>


### [300] [Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset](https://arxiv.org/abs/2505.12588)
*Samya Bagchi,Peter Anastasiou,Matthew Tetlow,Tat-Jun Chin,Yasir Latif*

Main category: cs.CV

TL;DR: The paper introduces the e-STURT dataset, the first event-camera dataset for star observations under controlled jitter, aiding the development of jitter compensation algorithms for space missions.


<details>
  <summary>Details</summary>
Motivation: Jitter disrupts spacecraft fine-pointing for optical communication and observation, necessitating high-fidelity datasets for algorithm development.

Method: The dataset uses an event camera and piezoelectric actuator to simulate jitter, capturing ground-truth data with 200 sequences.

Result: The e-STURT dataset is publicly available, providing a foundation for jitter-aware algorithm development.

Conclusion: The dataset addresses a critical need for high-fidelity jitter simulations in space sensing applications.

Abstract: Jitter degrades a spacecraft's fine-pointing ability required for optical
communication, earth observation, and space domain awareness. Development of
jitter estimation and compensation algorithms requires high-fidelity sensor
observations representative of on-board jitter. In this work, we present the
Event-based Star Tracking Under Jitter (e-STURT) dataset -- the first event
camera based dataset of star observations under controlled jitter conditions.
Specialized hardware employed for the dataset emulates an event-camera
undergoing on-board jitter. While the event camera provides asynchronous, high
temporal resolution star observations, systematic and repeatable jitter is
introduced using a micrometer accurate piezoelectric actuator. Various jitter
sources are simulated using distinct frequency bands and utilizing both axes of
motion. Ground-truth jitter is captured in hardware from the piezoelectric
actuator. The resulting dataset consists of 200 sequences and is made publicly
available. This work highlights the dataset generation process, technical
challenges and the resulting limitations. To serve as a baseline, we propose a
high-frequency jitter estimation algorithm that operates directly on the event
stream. The e-STURT dataset will enable the development of jitter aware
algorithms for mission critical event-based space sensing applications.

</details>


### [301] [SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models](https://arxiv.org/abs/2505.12589)
*Bo Liu,Pengfei Qiao,Minhan Ma,Xuange Zhang,Yinan Tang,Peng Xu,Kun Liu,Tongtong Yuan*

Main category: cs.CV

TL;DR: The paper introduces SurveillanceVQA-589K, a large-scale video QA benchmark for surveillance, highlighting gaps in current LVLMs' performance in complex tasks like causal reasoning and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored challenge of understanding surveillance video content due to its complexity, irregular dynamics, and safety-critical nature.

Method: A hybrid annotation pipeline combining human-written captions with LVLM-assisted QA generation, plus a multi-dimensional evaluation protocol.

Result: Evaluation of eight LVLMs shows significant performance gaps, particularly in causal and anomaly-related tasks.

Conclusion: The benchmark serves as a resource for advancing video-language understanding in safety-critical applications like monitoring and decision-making.

Abstract: Understanding surveillance video content remains a critical yet underexplored
challenge in vision-language research, particularly due to its real-world
complexity, irregular event dynamics, and safety-critical implications. In this
work, we introduce SurveillanceVQA-589K, the largest open-ended video question
answering benchmark tailored to the surveillance domain. The dataset comprises
589,380 QA pairs spanning 12 cognitively diverse question types, including
temporal reasoning, causal inference, spatial understanding, and anomaly
interpretation, across both normal and abnormal video scenarios. To construct
the benchmark at scale, we design a hybrid annotation pipeline that combines
temporally aligned human-written captions with Large Vision-Language
Model-assisted QA generation using prompt-based techniques. We also propose a
multi-dimensional evaluation protocol to assess contextual, temporal, and
causal comprehension. We evaluate eight LVLMs under this framework, revealing
significant performance gaps, especially in causal and anomaly-related tasks,
underscoring the limitations of current models in real-world surveillance
contexts. Our benchmark provides a practical and comprehensive resource for
advancing video-language understanding in safety-critical applications such as
intelligent monitoring, incident analysis, and autonomous decision-making.

</details>


### [302] [Learning Cross-Spectral Point Features with Task-Oriented Training](https://arxiv.org/abs/2505.12593)
*Mia Thomas,Trevor Ablett,Jonathan Kelly*

Main category: cs.CV

TL;DR: The paper proposes a method to train a feature network for matching and registration tasks to integrate thermal imagery into UAV navigation systems, achieving low registration error.


<details>
  <summary>Details</summary>
Motivation: Visible-spectrum cameras struggle in low-visibility conditions, while thermal cameras perform well. The work aims to leverage thermal imagery for robust UAV navigation.

Method: The approach trains a feature network on matching and registration tasks using thermal-visible image pairs, feeding network responses into a differentiable registration pipeline.

Result: The model achieves a registration error below 10 pixels for over 75% of estimates on the MultiPoint dataset and works with classical pipelines.

Conclusion: The proposed method effectively integrates thermal imagery into navigation systems, improving performance in low-visibility conditions.

Abstract: Unmanned aerial vehicles (UAVs) enable operations in remote and hazardous
environments, yet the visible-spectrum, camera-based navigation systems often
relied upon by UAVs struggle in low-visibility conditions. Thermal cameras,
which capture long-wave infrared radiation, are able to function effectively in
darkness and smoke, where visible-light cameras fail. This work explores
learned cross-spectral (thermal-visible) point features as a means to integrate
thermal imagery into established camera-based navigation systems. Existing
methods typically train a feature network's detection and description outputs
directly, which often focuses training on image regions where thermal and
visible-spectrum images exhibit similar appearance. Aiming to more fully
utilize the available data, we propose a method to train the feature network on
the tasks of matching and registration. We run our feature network on
thermal-visible image pairs, then feed the network response into a
differentiable registration pipeline. Losses are applied to the matching and
registration estimates of this pipeline. Our selected model, trained on the
task of matching, achieves a registration error (corner error) below 10 pixels
for more than 75% of estimates on the MultiPoint dataset. We further
demonstrate that our model can also be used with a classical pipeline for
matching and registration.

</details>


### [303] [Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding](https://arxiv.org/abs/2505.12605)
*Thong Nguyen,Zhiyuan Hu,Xu Lin,Cong-Duy Nguyen,See-Kiong Ng,Luu Anh Tuan*

Main category: cs.CV

TL;DR: The paper investigates key components affecting temporal understanding in large vision-language models (LVLMs) and proposes a temporal-oriented recipe to improve video understanding.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs rely on implicit temporal understanding for video tasks, limiting their potential. The study aims to identify and address these limitations.

Method: The authors conduct an empirical study to uncover critical components, focusing on the interface between visual encoder and language model, and propose temporal-oriented training and an upscaled interface.

Result: The proposed model significantly outperforms previous LVLMs on standard video understanding tasks.

Conclusion: The study highlights the importance of explicit temporal understanding components in LVLMs and demonstrates the effectiveness of the proposed temporal-oriented recipe.

Abstract: Recent years have witnessed outstanding advances of large vision-language
models (LVLMs). In order to tackle video understanding, most of them depend
upon their implicit temporal understanding capacity. As such, they have not
deciphered important components that contribute to temporal understanding
ability, which might limit the potential of these LVLMs for video
understanding. In this work, we conduct a thorough empirical study to demystify
crucial components that influence the temporal understanding of LVLMs. Our
empirical study reveals that significant impacts are centered around the
intermediate interface between the visual encoder and the large language model.
Building on these insights, we propose a temporal-oriented recipe that
encompasses temporal-oriented training schemes and an upscaled interface. Our
final model developed using our recipe significantly enhances previous LVLMs on
standard video understanding tasks.

</details>


### [304] [Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking](https://arxiv.org/abs/2505.12606)
*Shiyu Xuan,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: Diff-MM is a unified multi-modal tracker using pre-trained Stable Diffusion for feature extraction, outperforming existing methods like OneTracker by 8.3% AUC on TNL2K.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal trackers rely on limited training data, leading to unsatisfactory performance. Diff-MM leverages pre-trained models to overcome this limitation.

Method: Diff-MM uses Stable Diffusion's UNet for feature extraction via a parallel pipeline and introduces sub-module tuning for complementary multi-modal information.

Result: Diff-MM achieves superior performance, e.g., 8.3% higher AUC than OneTracker on TNL2K.

Conclusion: Diff-MM demonstrates the potential of pre-trained models for unified multi-modal tracking, improving performance in complex scenarios.

Abstract: Multi-modal object tracking integrates auxiliary modalities such as depth,
thermal infrared, event flow, and language to provide additional information
beyond RGB images, showing great potential in improving tracking stabilization
in complex scenarios. Existing methods typically start from an RGB-based
tracker and learn to understand auxiliary modalities only from training data.
Constrained by the limited multi-modal training data, the performance of these
methods is unsatisfactory. To alleviate this limitation, this work proposes a
unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding
capability of the pre-trained text-to-image generation model. Diff-MM leverages
the UNet of pre-trained Stable Diffusion as a tracking feature extractor
through the proposed parallel feature extraction pipeline, which enables
pairwise image inputs for object tracking. We further introduce a multi-modal
sub-module tuning method that learns to gain complementary information between
different modalities. By harnessing the extensive prior knowledge in the
generation model, we achieve a unified tracker with uniform parameters for
RGB-N/D/T/E tracking. Experimental results demonstrate the promising
performance of our method compared with recently proposed trackers, e.g., its
AUC outperforms OneTracker by 8.3% on TNL2K.

</details>


### [305] [BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation](https://arxiv.org/abs/2505.12620)
*Haiquan Wen,Yiwei He,Zhenglin Huang,Tianxiao Li,Zihan YU,Xingru Huang,Lu Qi,Baoyuan Wu,Xiangtai Li,Guangliang Cheng*

Main category: cs.CV

TL;DR: The paper introduces GenBuster-200K, a large-scale AI-generated video dataset, and BusterX, a detection framework combining MLLM and reinforcement learning for explainable deepfake detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large-scale AI-generated video datasets and the need for explainable detection methods due to rising misinformation risks.

Method: Proposes GenBuster-200K (200K high-resolution videos) and BusterX, leveraging MLLM and reinforcement learning for detection and explanation.

Result: BusterX outperforms state-of-the-art methods, validated through comparisons and ablation studies.

Conclusion: The work provides a scalable, explainable solution for AI-generated video detection, with plans to release code, models, and datasets.

Abstract: Advances in AI generative models facilitate super-realistic video synthesis,
amplifying misinformation risks via social media and eroding trust in digital
content. Several research works have explored new deepfake detection methods on
AI-generated images to alleviate these risks. However, with the fast
development of video generation models, such as Sora and WanX, there is
currently a lack of large-scale, high-quality AI-generated video datasets for
forgery detection. In addition, existing detection approaches predominantly
treat the task as binary classification, lacking explainability in model
decision-making and failing to provide actionable insights or guidance for the
public. To address these challenges, we propose \textbf{GenBuster-200K}, a
large-scale AI-generated video dataset featuring 200K high-resolution video
clips, diverse latest generative techniques, and real-world scenes. We further
introduce \textbf{BusterX}, a novel AI-generated video detection and
explanation framework leveraging multimodal large language model (MLLM) and
reinforcement learning for authenticity determination and explainable
rationale. To our knowledge, GenBuster-200K is the {\it \textbf{first}}
large-scale, high-quality AI-generated video dataset that incorporates the
latest generative techniques for real-world scenarios. BusterX is the {\it
\textbf{first}} framework to integrate MLLM with reinforcement learning for
explainable AI-generated video detection. Extensive comparisons with
state-of-the-art methods and ablation studies validate the effectiveness and
generalizability of BusterX. The code, models, and datasets will be released.

</details>


### [306] [Degradation-Aware Feature Perturbation for All-in-One Image Restoration](https://arxiv.org/abs/2505.12630)
*Xiangpeng Tian,Xiangyu Liao,Xiao Liu,Meng Li,Chao Ren*

Main category: cs.CV

TL;DR: DFPIR introduces Degradation-aware Feature Perturbations (DFP) to align feature space with shared parameters, improving all-in-one image restoration.


<details>
  <summary>Details</summary>
Motivation: Addressing task interference in unified models due to divergent gradient updates from varied degradation types.

Method: Proposes DFPIR with channel-wise and attention-wise perturbations via Degradation-Guided Perturbation Block (DGPB).

Result: Achieves state-of-the-art performance in tasks like denoising, dehazing, deraining, deblurring, and low-light enhancement.

Conclusion: DFPIR effectively mitigates task interference and enhances performance in diverse image restoration tasks.

Abstract: All-in-one image restoration aims to recover clear images from various
degradation types and levels with a unified model. Nonetheless, the significant
variations among degradation types present challenges for training a universal
model, often resulting in task interference, where the gradient update
directions of different tasks may diverge due to shared parameters. To address
this issue, motivated by the routing strategy, we propose DFPIR, a novel
all-in-one image restorer that introduces Degradation-aware Feature
Perturbations(DFP) to adjust the feature space to align with the unified
parameter space. In this paper, the feature perturbations primarily include
channel-wise perturbations and attention-wise perturbations. Specifically,
channel-wise perturbations are implemented by shuffling the channels in
high-dimensional space guided by degradation types, while attention-wise
perturbations are achieved through selective masking in the attention space. To
achieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)
to implement these two functions, positioned between the encoding and decoding
stages of the encoder-decoder architecture. Extensive experimental results
demonstrate that DFPIR achieves state-of-the-art performance on several
all-in-one image restoration tasks including image denoising, image dehazing,
image deraining, motion deblurring, and low-light image enhancement. Our codes
are available at https://github.com/TxpHome/DFPIR.

</details>


### [307] [Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform](https://arxiv.org/abs/2505.12631)
*Li Lin*

Main category: cs.CV

TL;DR: HaarMoDic, a network using 2D Haar transform, improves 3D human pose prediction by simultaneously accessing spatial and temporal information, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with precise predictions due to ignoring the arbitrariness of human motion sequences in temporal and spatial axes.

Method: Proposes HaarMoDic with a Multi-Resolution Haar (MR-Haar) block, using 2D Haar transform to project motion sequences to higher resolution coordinates.

Result: HaarMoDic achieves better performance than state-of-the-art methods on the Human3.6M dataset in MPJPE.

Conclusion: The MR-Haar block effectively combines spatial and temporal information, enhancing prediction accuracy for complex human motions.

Abstract: The 3D human pose is vital for modern computer vision and computer graphics,
and its prediction has drawn attention in recent years. 3D human pose
prediction aims at forecasting a human's future motion from the previous
sequence. Ignoring that the arbitrariness of human motion sequences has a firm
origin in transition in both temporal and spatial axes limits the performance
of state-of-the-art methods, leading them to struggle with making precise
predictions on complex cases, e.g., arbitrarily posing or greeting. To
alleviate this problem, a network called HaarMoDic is proposed in this paper,
which utilizes the 2D Haar transform to project joints to higher resolution
coordinates where the network can access spatial and temporal information
simultaneously. An ablation study proves that the significant contributing
module within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)
block. Instead of mining in one of two axes or extracting separately, the
MR-Haar block projects whole motion sequences to a mixed-up coordinate in
higher resolution with 2D Haar Transform, allowing the network to give scope to
information from both axes in different resolutions. With the MR-Haar block,
the HaarMoDic network can make predictions referring to a broader range of
information. Experimental results demonstrate that HaarMoDic surpasses
state-of-the-art methods in every testing interval on the Human3.6M dataset in
the Mean Per Joint Position Error (MPJPE) metric.

</details>


### [308] [Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents](https://arxiv.org/abs/2505.12632)
*Yunseok Jang,Yeda Song,Sungryull Sohn,Lajanugen Logeswaran,Tiange Luo,Dong-Ki Kim,Kyunghoon Bae,Honglak Lee*

Main category: cs.CV

TL;DR: MONDAY is a large-scale dataset for mobile OS navigation tasks, derived from YouTube videos, enhancing cross-platform generalization in models. An automated framework for dataset expansion is also introduced.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse, large-scale datasets for training GUI visual agents capable of cross-platform mobile OS navigation.

Method: Developed MONDAY dataset (313K annotated frames from 20K videos) and an automated framework using OCR, UI element detection, and multi-step action identification.

Result: Models pre-trained with MONDAY show 18.11%p performance gain on unseen platforms and robust cross-platform generalization.

Conclusion: MONDAY and the automated framework advance research in mobile OS navigation by providing scalable, diverse data.

Abstract: Recent advancements in Large Language Models (LLMs) and Vision-Language
Models (VLMs) have sparked significant interest in developing GUI visual
agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from
YouTube), a large-scale dataset of 313K annotated frames from 20K instructional
videos capturing diverse real-world mobile OS navigation across multiple
platforms. Models that include MONDAY in their pre-training phases demonstrate
robust cross-platform generalization capabilities, consistently outperforming
models trained on existing single OS datasets while achieving an average
performance gain of 18.11%p on an unseen mobile OS platform. To enable
continuous dataset expansion as mobile platforms evolve, we present an
automated framework that leverages publicly available video content to create
comprehensive task datasets without manual annotation. Our framework comprises
robust OCR-based scene detection (95.04% F1score), near-perfect UI element
detection (99.87% hit ratio), and novel multi-step action identification to
extract reliable action sequences across diverse interface configurations. We
contribute both the MONDAY dataset and our automated collection framework to
facilitate future research in mobile OS navigation.

</details>


### [309] [MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control](https://arxiv.org/abs/2505.12635)
*Mingqi Shao,Feng Xiong,Zhaoxu Sun,Mu Xu*

Main category: cs.CV

TL;DR: MVPainter improves 3D texture generation by addressing alignment, consistency, and quality, using data filtering, ControlNet, and PBR attributes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: 3D texture generation lags behind geometry generation, lacking systematic exploration of key dimensions like alignment and consistency.

Method: Proposes MVPainter with data filtering, augmentation, ControlNet for geometric conditioning, and PBR attribute extraction.

Result: Achieves state-of-the-art performance in reference-texture alignment, geometry-texture consistency, and local texture quality.

Conclusion: MVPainter advances 3D texture generation and is released as an open-source pipeline for reproducibility.

Abstract: Recently, significant advances have been made in 3D object generation.
Building upon the generated geometry, current pipelines typically employ image
diffusion models to generate multi-view RGB images, followed by UV texture
reconstruction through texture baking. While 3D geometry generation has
improved significantly, supported by multiple open-source frameworks, 3D
texture generation remains underexplored. In this work, we systematically
investigate 3D texture generation through the lens of three core dimensions:
reference-texture alignment, geometry-texture consistency, and local texture
quality. To tackle these issues, we propose MVPainter, which employs data
filtering and augmentation strategies to enhance texture fidelity and detail,
and introduces ControlNet-based geometric conditioning to improve
texture-geometry alignment. Furthermore, we extract physically-based rendering
(PBR) attributes from the generated views to produce PBR meshes suitable for
real-world rendering applications. MVPainter achieves state-of-the-art results
across all three dimensions, as demonstrated by human-aligned evaluations. To
facilitate further research and reproducibility, we also release our full
pipeline as an open-source system, including data construction, model
architecture, and evaluation tools.

</details>


### [310] [Single Image Reflection Removal via inter-layer Complementarity](https://arxiv.org/abs/2505.12641)
*Yue Huang,Zi'ang Li,Tianle Hu,Jie Wen,Guanbin Li,Jinglin Zhang,Guoxu Zhou,Xiaozhao Fang*

Main category: cs.CV

TL;DR: The paper proposes improvements to dual-stream architectures for image reflection removal by enhancing inter-layer complementarity and introducing an attention mechanism, achieving better results with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Dual-stream architectures for image reflection removal lack full exploitation of inter-layer complementarity, limiting separation quality.

Method: Introduces an inter-layer complementarity model and an efficient attention mechanism to enhance dual-stream architectures.

Result: Achieves state-of-the-art separation quality on public datasets with reduced computational cost and model complexity.

Conclusion: The proposed method effectively improves dual-stream architectures for reflection removal, balancing performance and efficiency.

Abstract: Although dual-stream architectures have achieved remarkable success in single
image reflection removal, they fail to fully exploit inter-layer
complementarity in their physical modeling and network design, which limits the
quality of image separation. To address this fundamental limitation, we propose
two targeted improvements to enhance dual-stream architectures: First, we
introduce a novel inter-layer complementarity model where low-frequency
components extracted from the residual layer interact with the transmission
layer through dual-stream architecture to enhance inter-layer complementarity.
Meanwhile, high-frequency components from the residual layer provide inverse
modulation to both streams, improving the detail quality of the transmission
layer. Second, we propose an efficient inter-layer complementarity attention
mechanism which first cross-reorganizes dual streams at the channel level to
obtain reorganized streams with inter-layer complementary structures, then
performs attention computation on the reorganized streams to achieve better
inter-layer separation, and finally restores the original stream structure for
output. Experimental results demonstrate that our method achieves
state-of-the-art separation quality on multiple public datasets while
significantly reducing both computational cost and model complexity.

</details>


### [311] [Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency](https://arxiv.org/abs/2505.12644)
*Bo Yang,Hengwei Zhang,Jindong Wang,Yuchen Ren,Chenhao Lin,Chao Shen,Zhengyu Zhao*

Main category: cs.CV

TL;DR: SEA dynamically selects diverse surrogate models to balance transferability and efficiency, outperforming existing attacks.


<details>
  <summary>Details</summary>
Motivation: The trade-off between transferability and efficiency in surrogate ensemble attacks limits performance despite accessible pre-trained models.

Method: Proposes Selective Ensemble Attack (SEA), dynamically selecting diverse models across iterations to decouple within-iteration and cross-iteration diversity.

Result: SEA achieves 8.5% higher transferability than existing attacks under the same efficiency, validated on ImageNet and real-world systems.

Conclusion: SEA adaptively balances transferability and efficiency, opening new possibilities for resource-aware attacks.

Abstract: In surrogate ensemble attacks, using more surrogate models yields higher
transferability but lower resource efficiency. This practical trade-off between
transferability and efficiency has largely limited existing attacks despite
many pre-trained models are easily accessible online. In this paper, we argue
that such a trade-off is caused by an unnecessary common assumption, i.e., all
models should be identical across iterations. By lifting this assumption, we
can use as many surrogates as we want to unleash transferability without
sacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),
which dynamically selects diverse models (from easily accessible pre-trained
models) across iterations based on our new interpretation of decoupling
within-iteration and cross-iteration model diversity.In this way, the number of
within-iteration models is fixed for maintaining efficiency, while only
cross-iteration model diversity is increased for higher transferability.
Experiments on ImageNet demonstrate the superiority of SEA in various
scenarios. For example, when dynamically selecting 4 from 20 accessible models,
SEA yields 8.5% higher transferability than existing attacks under the same
efficiency. The superiority of SEA also generalizes to real-world systems, such
as commercial vision APIs and large vision-language models. Overall, SEA opens
up the possibility of adaptively balancing transferability and efficiency
according to specific resource requirements.

</details>


### [312] [AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use](https://arxiv.org/abs/2505.12650)
*Yaotian Yang,Yiwen Tang,Yizhe Chen,Xiao Chen,Jiangjie Qiu,Hao Xiong,Haoyu Yin,Zhiyao Luo,Yifei Zhang,Sijia Tao,Wentao Li,Qinghua Zhang,Yuqiang Li,Wanli Ouyang,Bin Zhao,Xiaonan Wang,Fei Wei*

Main category: cs.CV

TL;DR: AutoMat automates the conversion of STEM images into atomic crystal structures and predicts properties, outperforming existing tools and models.


<details>
  <summary>Details</summary>
Motivation: Scarcity of accurate atomic structures for machine learning potentials and labor-intensive conversion of STEM images into usable formats.

Method: AutoMat combines denoising, template retrieval, atomic reconstruction, relaxation, and property prediction via MatterSim, orchestrated for efficiency.

Result: Outperforms existing models in 450 structure samples, validated by STEM2Mat-Bench metrics like lattice RMSD and formation energy MAE.

Conclusion: AutoMat bridges microscopy and atomistic simulation, offering a scalable solution for materials science.

Abstract: Machine learning-based interatomic potentials and force fields depend
critically on accurate atomic structures, yet such data are scarce due to the
limited availability of experimentally resolved crystals. Although
atomic-resolution electron microscopy offers a potential source of structural
data, converting these images into simulation-ready formats remains
labor-intensive and error-prone, creating a bottleneck for model training and
validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that
automatically transforms scanning transmission electron microscopy (STEM)
images into atomic crystal structures and predicts their physical properties.
AutoMat combines pattern-adaptive denoising, physics-guided template retrieval,
symmetry-aware atomic reconstruction, fast relaxation and property prediction
via MatterSim, and coordinated orchestration across all stages. We propose the
first dedicated STEM2Mat-Bench for this task and evaluate performance using
lattice RMSD, formation energy MAE, and structure-matching success rate. By
orchestrating external tool calls, AutoMat enables a text-only LLM to
outperform vision-language models in this domain, achieving closed-loop
reasoning throughout the pipeline. In large-scale experiments over 450
structure samples, AutoMat substantially outperforms existing multimodal large
language models and tools. These results validate both AutoMat and
STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic
simulation in materials science.The code and dataset are publicly available at
https://github.com/yyt-2378/AutoMat and
https://huggingface.co/datasets/yaotianvector/STEM2Mat.

</details>


### [313] [SPKLIP: Aligning Spike Video Streams with Natural Language](https://arxiv.org/abs/2505.12656)
*Yongchang Gao,Meiling Jin,Zhaofei Yu,Tiejun Huang,Guozhang Chen*

Main category: cs.CV

TL;DR: SPKLIP is a novel architecture for Spike-VLA, addressing modality mismatch with hierarchical spike feature extraction and spike-text contrastive learning, achieving state-of-the-art performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Current models like CLIP underperform in Spike-VLA due to modality mismatch, necessitating a specialized architecture for better semantic understanding.

Method: SPKLIP uses a hierarchical spike feature extractor for multi-scale temporal dynamics and spike-text contrastive learning. A full-spiking visual encoder variant enhances energy efficiency.

Result: Achieves state-of-the-art performance on benchmark datasets and strong few-shot generalization on a new real-world dataset. Demonstrates high energy efficiency.

Conclusion: SPKLIP advances event-based multimodal research, showing potential for neuromorphic deployment, with code and dataset made available.

Abstract: Spike cameras offer unique sensing capabilities but their sparse,
asynchronous output challenges semantic understanding, especially for Spike
Video-Language Alignment (Spike-VLA) where models like CLIP underperform due to
modality mismatch. We introduce SPKLIP, the first architecture specifically for
Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that
adaptively models multi-scale temporal dynamics in event streams, and uses
spike-text contrastive learning to directly align spike video with language,
enabling effective few-shot learning. A full-spiking visual encoder variant,
integrating SNN components into our pipeline, demonstrates enhanced energy
efficiency. Experiments show state-of-the-art performance on benchmark spike
datasets and strong few-shot generalization on a newly contributed real-world
dataset. SPKLIP's energy efficiency highlights its potential for neuromorphic
deployment, advancing event-based multimodal research. The source code and
dataset are available at [link removed for anonymity].

</details>


### [314] [Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps](https://arxiv.org/abs/2505.12660)
*Ziqi Wen,Jonathan Skaza,Shravan Murlidaran,William Y. Wang,Miguel P. Eckstein*

Main category: cs.CV

TL;DR: The paper introduces F-SUM, a novel image-computable model combining foveated vision and VLMs to predict human scene understanding times, outperforming traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Existing models predict RTs in tasks like target search but lack image-computable predictors for scene understanding. Advances in VLMs and linguistic metrics provide new opportunities.

Method: Proposes F-SUM, integrating foveated vision with VLMs to create spatially resolved maps of scene understanding. Validated using human RTs and saccades.

Result: F-SUM correlates with human RTs (r=0.47), saccades (r=0.51), and description accuracy (r=-0.56), outperforming standard metrics.

Conclusion: F-SUM is a promising metric for predicting human scene understanding times, highlighting the role of foveated vision in comprehension difficulty.

Abstract: Although models exist that predict human response times (RTs) in tasks such
as target search and visual discrimination, the development of image-computable
predictors for scene understanding time remains an open challenge. Recent
advances in vision-language models (VLMs), which can generate scene
descriptions for arbitrary images, combined with the availability of
quantitative metrics for comparing linguistic descriptions, offer a new
opportunity to model human scene understanding. We hypothesize that the primary
bottleneck in human scene understanding and the driving source of variability
in response times across scenes is the interaction between the foveated nature
of the human visual system and the spatial distribution of task-relevant visual
information within an image. Based on this assumption, we propose a novel
image-computable model that integrates foveated vision with VLMs to produce a
spatially resolved map of scene understanding as a function of fixation
location (Foveated Scene Understanding Map, or F-SUM), along with an aggregate
F-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) and
number of saccades (r=0.51) required to comprehend a scene (across 277 scenes).
The F-SUM score also correlates with average (N=16) human description accuracy
(r=-0.56) in time-limited presentations. These correlations significantly
exceed those of standard image-based metrics such as clutter, visual
complexity, and scene ambiguity based on language entropy. Together, our work
introduces a new image-computable metric for predicting human response times in
scene understanding and demonstrates the importance of foveated visual
processing in shaping comprehension difficulty.

</details>


### [315] [Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking](https://arxiv.org/abs/2505.12667)
*Zihan Su,Xuerui Qiu,Hongbin Xu,Tangyu Jiang,Junhao Zhuang,Chun Yuan,Ming Li,Shengfeng He,Fei Richard Yu*

Main category: cs.CV

TL;DR: Safe-Sora is the first framework to embed invisible watermarks in AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet-enhanced Mamba architecture.


<details>
  <summary>Details</summary>
Motivation: The need for reliable copyright protection in AI-generated videos, as current watermarking methods are underexplored in video generation.

Method: Hierarchical coarse-to-fine adaptive matching for watermark placement and a 3D wavelet transform-enhanced Mamba architecture for spatiotemporal fusion.

Result: Achieves state-of-the-art performance in video quality, watermark fidelity, and robustness.

Conclusion: Safe-Sora pioneers efficient and robust watermarking for AI-generated videos, leveraging state space models for the first time in this domain.

Abstract: The explosive growth of generative video models has amplified the demand for
reliable copyright preservation of AI-generated content. Despite its popularity
in image synthesis, invisible generative watermarking remains largely
underexplored in video generation. To address this gap, we propose Safe-Sora,
the first framework to embed graphical watermarks directly into the video
generation process. Motivated by the observation that watermarking performance
is closely tied to the visual similarity between the watermark and cover
content, we introduce a hierarchical coarse-to-fine adaptive matching
mechanism. Specifically, the watermark image is divided into patches, each
assigned to the most visually similar video frame, and further localized to the
optimal spatial region for seamless embedding. To enable spatiotemporal fusion
of watermark patches across video frames, we develop a 3D wavelet
transform-enhanced Mamba architecture with a novel spatiotemporal local
scanning strategy, effectively modeling long-range dependencies during
watermark embedding and retrieval. To the best of our knowledge, this is the
first attempt to apply state space models to watermarking, opening new avenues
for efficient and robust watermark protection. Extensive experiments
demonstrate that Safe-Sora achieves state-of-the-art performance in terms of
video quality, watermark fidelity, and robustness, which is largely attributed
to our proposals. We will release our code upon publication.

</details>


### [316] [TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning](https://arxiv.org/abs/2505.12670)
*Lihong Chen,Hossein Hassani,Soodeh Nikan*

Main category: cs.CV

TL;DR: TS-VLM is a lightweight Vision-Language Model for autonomous driving, using a Text-Guided SoftSort Pooling module to efficiently fuse multi-view data, achieving high performance with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs for autonomous driving are computationally heavy and inefficient in multi-view data integration, limiting real-time deployment.

Method: TS-VLM introduces a Text-Guided SoftSort Pooling (TGSSP) module for dynamic, query-aware multi-view feature fusion without costly attention mechanisms.

Result: TS-VLM outperforms state-of-the-art models on DriveLM (e.g., BLEU-4: 56.82) and reduces computational cost by up to 90%.

Conclusion: TS-VLM offers a practical, efficient solution for real-time autonomous driving applications, balancing performance and computational efficiency.

Abstract: Vision-Language Models (VLMs) have shown remarkable potential in advancing
autonomous driving by leveraging multi-modal fusion in order to enhance scene
perception, reasoning, and decision-making. Despite their potential, existing
models suffer from computational overhead and inefficient integration of
multi-view sensor data that make them impractical for real-time deployment in
safety-critical autonomous driving applications. To address these shortcomings,
this paper is devoted to designing a lightweight VLM called TS-VLM, which
incorporates a novel Text-Guided SoftSort Pooling (TGSSP) module. By resorting
to semantics of the input queries, TGSSP ranks and fuses visual features from
multiple views, enabling dynamic and query-aware multi-view aggregation without
reliance on costly attention mechanisms. This design ensures the query-adaptive
prioritization of semantically related views, which leads to improved
contextual accuracy in multi-view reasoning for autonomous driving. Extensive
evaluations on the DriveLM benchmark demonstrate that, on the one hand, TS-VLM
outperforms state-of-the-art models with a BLEU-4 score of 56.82, METEOR of
41.91, ROUGE-L of 74.64, and CIDEr of 3.39. On the other hand, TS-VLM reduces
computational cost by up to 90%, where the smallest version contains only 20.1
million parameters, making it more practical for real-time deployment in
autonomous vehicles.

</details>


### [317] [Few-Step Diffusion via Score identity Distillation](https://arxiv.org/abs/2505.12674)
*Mingyuan Zhou,Yi Gu,Zhendong Wang*

Main category: cs.CV

TL;DR: SiD introduces a data-free, one-step distillation framework for few-step generation, improving text-image alignment and diversity without relying on real images.


<details>
  <summary>Details</summary>
Motivation: Existing methods for distilling high-resolution T2I models like SDXL rely on real or teacher-synthesized images and face trade-offs between text-image alignment and diversity due to CFG.

Method: SiD optimizes Score identity Distillation, avoiding step-specific networks and integrating into existing pipelines. It introduces Diffusion GAN-based adversarial loss and new guidance strategies (Zero-CFG, Anti-CFG).

Result: Achieves state-of-the-art performance on SDXL at 1024x1024 resolution, improving diversity without sacrificing alignment.

Conclusion: SiD offers a robust, efficient solution for one- and few-step generation, even without real images, with public implementation available.

Abstract: Diffusion distillation has emerged as a promising strategy for accelerating
text-to-image (T2I) diffusion models by distilling a pretrained score network
into a one- or few-step generator. While existing methods have made notable
progress, they often rely on real or teacher-synthesized images to perform well
when distilling high-resolution T2I diffusion models such as Stable Diffusion
XL (SDXL), and their use of classifier-free guidance (CFG) introduces a
persistent trade-off between text-image alignment and generation diversity. We
address these challenges by optimizing Score identity Distillation (SiD) -- a
data-free, one-step distillation framework -- for few-step generation. Backed
by theoretical analysis that justifies matching a uniform mixture of outputs
from all generation steps to the data distribution, our few-step distillation
algorithm avoids step-specific networks and integrates seamlessly into existing
pipelines, achieving state-of-the-art performance on SDXL at 1024x1024
resolution. To mitigate the alignment-diversity trade-off when real text-image
pairs are available, we introduce a Diffusion GAN-based adversarial loss
applied to the uniform mixture and propose two new guidance strategies:
Zero-CFG, which disables CFG in the teacher and removes text conditioning in
the fake score network, and Anti-CFG, which applies negative CFG in the fake
score network. This flexible setup improves diversity without sacrificing
alignment. Comprehensive experiments on SD1.5 and SDXL demonstrate
state-of-the-art performance in both one-step and few-step generation settings,
along with robustness to the absence of real images. Our efficient PyTorch
implementation, along with the resulting one- and few-step distilled
generators, will be released publicly as a separate branch at
https://github.com/mingyuanzhou/SiD-LSG.

</details>


### [318] [CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models](https://arxiv.org/abs/2505.12677)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TL;DR: CURE is a training-free framework for unlearning undesired concepts in diffusion models using a Spectral Eraser for efficient, interpretable suppression.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of existing safety interventions like incomplete removal, jail-breaking susceptibility, and computational inefficiency.

Method: Uses a Spectral Eraser, a closed-form orthogonal projection module based on Singular Value Decomposition, to isolate and remove undesired concepts without retraining.

Result: Achieves efficient, thorough concept removal in 2 seconds with minimal impact on generation ability and enhanced robustness.

Conclusion: CURE offers a fast, effective, and interpretable solution for concept unlearning in diffusion models.

Abstract: As Text-to-Image models continue to evolve, so does the risk of generating
unsafe, copyrighted, or privacy-violating content. Existing safety
interventions - ranging from training data curation and model fine-tuning to
inference-time filtering and guidance - often suffer from incomplete concept
removal, susceptibility to jail-breaking, computational inefficiency, or
collateral damage to unrelated capabilities. In this paper, we introduce CURE,
a training-free concept unlearning framework that operates directly in the
weight space of pre-trained diffusion models, enabling fast, interpretable, and
highly specific suppression of undesired concepts. At the core of our method is
the Spectral Eraser, a closed-form, orthogonal projection module that
identifies discriminative subspaces using Singular Value Decomposition over
token embeddings associated with the concepts to forget and retain.
Intuitively, the Spectral Eraser identifies and isolates features unique to the
undesired concept while preserving safe attributes. This operator is then
applied in a single step update to yield an edited model in which the target
concept is effectively unlearned - without retraining, supervision, or
iterative optimization. To balance the trade-off between filtering toxicity and
preserving unrelated concepts, we further introduce an Expansion Mechanism for
spectral regularization which selectively modulates singular vectors based on
their relative significance to control the strength of forgetting. All the
processes above are in closed-form, guaranteeing extremely efficient erasure in
only $2$ seconds. Benchmarking against prior approaches, CURE achieves a more
efficient and thorough removal for targeted artistic styles, objects,
identities, or explicit content, with minor damage to original generation
ability and demonstrates enhanced robustness against red-teaming.

</details>


### [319] [Mamba-Adaptor: State Space Model Adaptor for Visual Recognition](https://arxiv.org/abs/2505.12685)
*Fei Xie,Jiahao Nie,Yujin Tang,Wenkang Zhang,Hongshen Zhao*

Main category: cs.CV

TL;DR: Mamba-Adaptor improves Mamba models for visual tasks by addressing context access, long-range forgetting, and spatial modeling with two modules: Adaptor-T and Adaptor-S.


<details>
  <summary>Details</summary>
Motivation: Mamba models underperform in visual tasks due to limitations in global context access, long-range forgetting, and weak spatial modeling.

Method: Introduces Adaptor-T for memory augmentation and Adaptor-S for enhanced spatial modeling using multi-scale dilated convolutions.

Result: Achieves state-of-the-art performance on ImageNet and COCO benchmarks.

Conclusion: Mamba-Adaptor is effective as a visual backbone, performance booster, and fine-tuning module.

Abstract: Recent State Space Models (SSM), especially Mamba, have demonstrated
impressive performance in visual modeling and possess superior model
efficiency. However, the application of Mamba to visual tasks suffers inferior
performance due to three main constraints existing in the sequential model: 1)
Casual computing is incapable of accessing global context; 2) Long-range
forgetting when computing the current hidden states; 3) Weak spatial structural
modeling due to the transformed sequential input. To address these issues, we
investigate a simple yet powerful vision task Adaptor for Mamba models, which
consists of two functional modules: Adaptor-T and Adaptor-S. When solving the
hidden states for SSM, we apply a lightweight prediction module Adaptor-T to
select a set of learnable locations as memory augmentations to ease long-range
forgetting issues. Moreover, we leverage Adapator-S, composed of multi-scale
dilated convolutional kernels, to enhance the spatial modeling and introduce
the image inductive bias into the feature output. Both modules can enlarge the
context modeling in casual computing, as the output is enhanced by the
inaccessible features. We explore three usages of Mamba-Adaptor: A general
visual backbone for various vision tasks; A booster module to raise the
performance of pretrained backbones; A highly efficient fine-tuning module that
adapts the base model for transfer learning tasks. Extensive experiments verify
the effectiveness of Mamba-Adaptor in three settings. Notably, our
Mamba-Adaptor achieves state-of the-art performance on the ImageNet and COCO
benchmarks.

</details>


### [320] [TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy](https://arxiv.org/abs/2505.12693)
*Luyao Lei,Shuo Xu,Yifan Bai,Xing Wei*

Main category: cs.CV

TL;DR: TACOcc is a multi-modal fusion framework for 3D occupancy prediction, addressing geometry-semantics mismatch and surface detail loss through adaptive bidirectional retrieval and volume rendering supervision.


<details>
  <summary>Details</summary>
Motivation: The limitations of fixed fusion strategies and sparse annotations in multi-modal 3D occupancy prediction, leading to geometry-semantics mismatch and surface detail loss.

Method: A target-scale adaptive bidirectional symmetric retrieval mechanism for cross-modal feature alignment and an improved volume rendering pipeline for photometric consistency supervision.

Result: Improved fusion accuracy and surface detail reconstruction, validated on nuScenes and SemanticKITTI benchmarks.

Conclusion: TACOcc effectively addresses fusion and detail loss challenges, enhancing 3D semantic occupancy prediction.

Abstract: The performance of multi-modal 3D occupancy prediction is limited by
ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion
strategies and surface detail loss caused by sparse, noisy annotations. The
mismatch stems from the heterogeneous scale and distribution of point cloud and
image features, leading to biased matching under fixed neighborhood fusion. To
address this, we propose a target-scale adaptive, bidirectional symmetric
retrieval mechanism. It expands the neighborhood for large targets to enhance
context awareness and shrinks it for small ones to improve efficiency and
suppress noise, enabling accurate cross-modal feature alignment. This mechanism
explicitly establishes spatial correspondences and improves fusion accuracy.
For surface detail loss, sparse labels provide limited supervision, resulting
in poor predictions for small objects. We introduce an improved volume
rendering pipeline based on 3D Gaussian Splatting, which takes fused features
as input to render images, applies photometric consistency supervision, and
jointly optimizes 2D-3D consistency. This enhances surface detail
reconstruction while suppressing noise propagation. In summary, we propose
TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy
prediction, enhanced by volume rendering supervision. Experiments on the
nuScenes and SemanticKITTI benchmarks validate its effectiveness.

</details>


### [321] [Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation](https://arxiv.org/abs/2505.12702)
*Tianming Liang,Haichao Jiang,Yuting Yang,Chaolei Tan,Shuai Li,Wei-Shi Zheng,Jian-Fang Hu*

Main category: cs.CV

TL;DR: The paper introduces Long-RVOS, a benchmark for long-term referring video object segmentation, addressing limitations of existing datasets by including longer videos with complex scenarios. It proposes ReferMo, a baseline method, and new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS datasets focus on short clips with salient objects, limiting practical applicability. Long-RVOS aims to address this by introducing longer videos with challenges like occlusion and shot changes.

Method: The authors propose ReferMo, integrating motion information for a broader temporal receptive field and using a local-to-global architecture to capture dynamics and dependencies.

Result: Current methods struggle with long videos, but ReferMo shows significant improvements in long-term scenarios.

Conclusion: Long-RVOS and ReferMo aim to advance RVOS research towards more realistic, long-form videos.

Abstract: Referring video object segmentation (RVOS) aims to identify, track and
segment the objects in a video based on language descriptions, which has
received great attention in recent years. However, existing datasets remain
focus on short video clips within several seconds, with salient objects visible
in most frames. To advance the task towards more practical scenarios, we
introduce \textbf{Long-RVOS}, a large-scale benchmark for long-term referring
video object segmentation. Long-RVOS contains 2,000+ videos of an average
duration exceeding 60 seconds, covering a variety of objects that undergo
occlusion, disappearance-reappearance and shot changing. The objects are
manually annotated with three different types of descriptions to individually
evaluate the understanding of static attributes, motion patterns and
spatiotemporal relationships. Moreover, unlike previous benchmarks that rely
solely on the per-frame spatial evaluation, we introduce two new metrics to
assess the temporal and spatiotemporal consistency. We benchmark 6
state-of-the-art methods on Long-RVOS. The results show that current approaches
struggle severely with the long-video challenges. To address this, we further
propose ReferMo, a promising baseline method that integrates motion information
to expand the temporal receptive field, and employs a local-to-global
architecture to capture both short-term dynamics and long-term dependencies.
Despite simplicity, ReferMo achieves significant improvements over current
methods in long-term scenarios. We hope that Long-RVOS and our baseline can
drive future RVOS research towards tackling more realistic and long-form
videos.

</details>


### [322] [SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence](https://arxiv.org/abs/2505.12703)
*Jiabin Chen,Haiping Wang,Jinpeng Li,Yuan Liu,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: SpatialLLM is a novel language model for spatial intelligence tasks in urban scenes, requiring no training or expertise, leveraging pre-trained LLMs for zero-shot analysis.


<details>
  <summary>Details</summary>
Motivation: To simplify spatial intelligence tasks in urban scenes without relying on geographic tools or domain expertise.

Method: Uses structured scene descriptions from raw spatial data to prompt pre-trained LLMs for zero-shot analysis.

Result: Pretrained LLMs accurately perceive spatial data and perform tasks like urban planning and traffic management.

Conclusion: SpatialLLM offers a new approach for urban analysis, highlighting multi-field knowledge, context length, and reasoning as key factors.

Abstract: We propose SpatialLLM, a novel approach advancing spatial intelligence tasks
in complex urban scenes. Unlike previous methods requiring geographic analysis
tools or domain expertise, SpatialLLM is a unified language model directly
addressing various spatial intelligence tasks without any training,
fine-tuning, or expert intervention. The core of SpatialLLM lies in
constructing detailed and structured scene descriptions from raw spatial data
to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show
that, with our designs, pretrained LLMs can accurately perceive spatial
distribution information and enable zero-shot execution of advanced spatial
intelligence tasks, including urban planning, ecological analysis, traffic
management, etc. We argue that multi-field knowledge, context length, and
reasoning ability are key factors influencing LLM performances in urban
analysis. We hope that SpatialLLM will provide a novel viable perspective for
urban intelligent analysis and management. The code and dataset are available
at https://github.com/WHU-USI3DV/SpatialLLM.

</details>


### [323] [Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining](https://arxiv.org/abs/2505.12711)
*Qichen Sun,Zhengrui Guo,Rui Peng,Hao Chen,Jinzhuo Wang*

Main category: cs.CV

TL;DR: ALTER is a tri-modal pretraining framework integrating WSIs, genomics, and pathology reports to address challenges in computational pathology, such as data fusion, missing modalities, and diverse downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Challenges in computational pathology include fusing heterogeneous data, handling missing modalities, and addressing diverse downstream tasks.

Method: Proposes ALTER, an any-to-any tri-modal pretraining framework integrating WSIs, genomics, and pathology reports, with modality-adaptive design.

Result: ALTER achieves superior or comparable performance in clinical tasks like survival prediction, cancer subtyping, gene mutation prediction, and report generation.

Conclusion: ALTER provides a robust, flexible solution for multimodal learning in computational pathology, outperforming or matching state-of-the-art methods.

Abstract: Recent advances in computational pathology and artificial intelligence have
significantly enhanced the utilization of gigapixel whole-slide images and and
additional modalities (e.g., genomics) for pathological diagnosis. Although
deep learning has demonstrated strong potential in pathology, several key
challenges persist: (1) fusing heterogeneous data types requires sophisticated
strategies beyond simple concatenation due to high computational costs; (2)
common scenarios of missing modalities necessitate flexible strategies that
allow the model to learn robustly in the absence of certain modalities; (3) the
downstream tasks in CPath are diverse, ranging from unimodal to multimodal,
cnecessitating a unified model capable of handling all modalities. To address
these challenges, we propose ALTER, an any-to-any tri-modal pretraining
framework that integrates WSIs, genomics, and pathology reports. The term "any"
emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with
any subset of modalities, and its capacity to learn robust, cross-modal
representations beyond WSI-centric approaches. We evaluate ALTER across
extensive clinical tasks including survival prediction, cancer subtyping, gene
mutation prediction, and report generation, achieving superior or comparable
performance to state-of-the-art baselines.

</details>


### [324] [IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo](https://arxiv.org/abs/2505.12714)
*Yinzhe Wang,Yiwen Xiao,Hu Wang,Yiping Xu,Yan Tian*

Main category: cs.CV

TL;DR: IA-MVS improves depth estimation in multi-view stereo by narrowing depth hypotheses per instance and refining them, leveraging intra-instance continuity and a new confidence model.


<details>
  <summary>Details</summary>
Motivation: Existing MVS methods underutilize instance-specific depth coverage and suffer from error accumulation, limiting precision.

Method: Proposes Instance-Adaptive MVS (IA-MVS) with instance-specific depth narrowing, refinement, and a continuity-based filtering mechanism. Introduces a conditional probability-based confidence model.

Result: Achieves state-of-the-art performance on the DTU benchmark.

Conclusion: IA-MVS enhances precision and robustness without extra training, applicable to MVSNet-based models.

Abstract: Multi-view stereo (MVS) models based on progressive depth hypothesis
narrowing have made remarkable advancements. However, existing methods haven't
fully utilized the potential that the depth coverage of individual instances is
smaller than that of the entire scene, which restricts further improvements in
depth estimation precision. Moreover, inevitable deviations in the initial
stage accumulate as the process advances. In this paper, we propose
Instance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation
by narrowing the depth hypothesis range and conducting refinement on each
instance. Additionally, a filtering mechanism based on intra-instance depth
continuity priors is incorporated to boost robustness. Furthermore, recognizing
that existing confidence estimation can degrade IA-MVS performance on point
clouds. We have developed a detailed mathematical model for confidence
estimation based on conditional probability. The proposed method can be widely
applied in models based on MVSNet without imposing extra training burdens. Our
method achieves state-of-the-art performance on the DTU benchmark. The source
code is available at https://github.com/KevinWang73106/IA-MVS.

</details>


### [325] [VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection](https://arxiv.org/abs/2505.12715)
*Aditya Taparia,Noel Ngu,Mario Leiva,Joshua Shay Kricheli,John Corcoran,Nathaniel D. Bastian,Gerardo Simari,Paulo Shakarian,Ransalu Senanayake*

Main category: cs.CV

TL;DR: VLC Fusion uses a Vision-Language Model to dynamically adjust sensor modality weights based on environmental cues, outperforming traditional fusion methods in object detection.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods fail to adapt to subtle environmental and sensor variations, limiting detection performance.

Method: Introduces Vision-Language Conditioned Fusion (VLC Fusion), leveraging a VLM to guide modality weighting using high-level environmental context.

Result: VLC Fusion outperforms baselines on autonomous driving and military datasets, improving accuracy in seen and unseen scenarios.

Conclusion: VLC Fusion effectively addresses adaptive modality fusion, enhancing object detection under varying conditions.

Abstract: Although fusing multiple sensor modalities can enhance object detection
performance, existing fusion approaches often overlook subtle variations in
environmental conditions and sensor inputs. As a result, they struggle to
adaptively weight each modality under such variations. To address this
challenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a
novel fusion framework that leverages a Vision-Language Model (VLM) to
condition the fusion process on nuanced environmental cues. By capturing
high-level environmental context such as as darkness, rain, and camera
blurring, the VLM guides the model to dynamically adjust modality weights based
on the current scene. We evaluate VLC Fusion on real-world autonomous driving
and military target detection datasets that include image, LIDAR, and mid-wave
infrared modalities. Our experiments show that VLC Fusion consistently
outperforms conventional fusion baselines, achieving improved detection
accuracy in both seen and unseen scenarios.

</details>


### [326] [FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks](https://arxiv.org/abs/2505.12728)
*Zihua Wang,Ruibo Li,Haozhe Du,Joey Tianyi Zhou,Yu Zhang,Xu Yang*

Main category: cs.CV

TL;DR: FLASH is a speculative decoding framework for LMMs that accelerates inference by leveraging visual token redundancy and semi-autoregressive decoding, achieving up to 2.68× speed-up.


<details>
  <summary>Details</summary>
Motivation: Slow decoding speeds in LMMs, especially with visual inputs, limit their efficiency. Existing speculative decoding methods overlook visual input properties.

Method: FLASH uses latent-aware token compression and semi-autoregressive decoding to generate multiple tokens per pass, improving draft model efficiency.

Result: FLASH achieves up to 2.68× speed-up on video captioning and 2.55× on visual instruction tuning tasks.

Conclusion: FLASH effectively addresses LMM decoding inefficiencies by optimizing for visual input properties, outperforming prior methods.

Abstract: Large language and multimodal models (LLMs and LMMs) exhibit strong inference
capabilities but are often limited by slow decoding speeds. This challenge is
especially acute in LMMs, where visual inputs typically comprise more tokens
with lower information density than text -- an issue exacerbated by recent
trends toward finer-grained visual tokenizations to boost performance.
Speculative decoding has been effective in accelerating LLM inference by using
a smaller draft model to generate candidate tokens, which are then selectively
verified by the target model, improving speed without sacrificing output
quality. While this strategy has been extended to LMMs, existing methods
largely overlook the unique properties of visual inputs and depend solely on
text-based draft models. In this work, we propose \textbf{FLASH} (Fast
Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework
designed specifically for LMMs, which leverages two key properties of
multimodal data to design the draft model. First, to address redundancy in
visual tokens, we propose a lightweight latent-aware token compression
mechanism. Second, recognizing that visual objects often co-occur within a
scene, we employ a semi-autoregressive decoding strategy to generate multiple
tokens per forward pass. These innovations accelerate draft decoding while
maintaining high acceptance rates, resulting in faster overall inference.
Experiments show that FLASH significantly outperforms prior speculative
decoding approaches in both unimodal and multimodal settings, achieving up to
\textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on
visual instruction tuning tasks compared to the original LMM.

</details>


### [327] [MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning](https://arxiv.org/abs/2505.12742)
*Jinhua Zhang,Wei Long,Minghao Han,Weiyi You,Shuhang Gu*

Main category: cs.CV

TL;DR: MVAR introduces scale and spatial Markov assumptions to reduce redundancy in autoregressive visual generation, achieving efficient training and inference with comparable performance.


<details>
  <summary>Details</summary>
Motivation: Conventional next-token and next-scale prediction methods exhibit scale and spatial redundancy, increasing computational complexity and memory usage.

Method: Proposes MVAR with scale-Markov trajectory (adjacent scale input) and spatial-Markov attention (localized neighborhood focus), reducing attention complexity from O(N^2) to O(Nk).

Result: Reduces GPU memory by 3.0x, enables training on 8 GPUs, and eliminates KV cache during inference while matching or outperforming existing models on ImageNet.

Conclusion: MVAR effectively mitigates redundancy in autoregressive visual modeling, offering a scalable and efficient alternative to traditional methods.

Abstract: Essential to visual generation is efficient modeling of visual data priors.
Conventional next-token prediction methods define the process as learning the
conditional probability distribution of successive tokens. Recently, next-scale
prediction methods redefine the process to learn the distribution over
multi-scale representations, significantly reducing generation latency.
However, these methods condition each scale on all previous scales and require
each token to consider all preceding tokens, exhibiting scale and spatial
redundancy. To better model the distribution by mitigating redundancy, we
propose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive
framework that introduces scale and spatial Markov assumptions to reduce the
complexity of conditional probability modeling. Specifically, we introduce a
scale-Markov trajectory that only takes as input the features of adjacent
preceding scale for next-scale prediction, enabling the adoption of a parallel
training strategy that significantly reduces GPU memory consumption.
Furthermore, we propose spatial-Markov attention, which restricts the attention
of each token to a localized neighborhood of size k at corresponding positions
on adjacent scales, rather than attending to every token across these scales,
for the pursuit of reduced modeling complexity. Building on these improvements,
we reduce the computational complexity of attention calculation from O(N^2) to
O(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating
the need for KV cache during inference. Extensive experiments on ImageNet
demonstrate that MVAR achieves comparable or superior performance with both
small model trained from scratch and large fine-tuned models, while reducing
the average GPU memory footprint by 3.0x.

</details>


### [328] [LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking](https://arxiv.org/abs/2505.12753)
*Martha Teiko Teye,Ori Maoz,Matthias Rottmann*

Main category: cs.CV

TL;DR: A LiDAR-based two-staged transformer model for multi-object tracking, combining a smoother for refining detections and a tracker for maintaining object identities, outperforming baselines on nuScenes and KITTI datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in LiDAR-based tracking due to sparse data and the need for temporal coherence, especially in crowded or fast-moving scenes.

Method: A two-staged approach: a smoother refines detections across a temporal window, and a tracker uses DETR-based attention to associate objects with refined detections.

Result: Online mode outperforms baselines with aMOTA of 0.722 and aMOTP of 0.475 on nuScenes; offline mode improves aMOTP by 3 pp.

Conclusion: The proposed transformer-based model effectively handles LiDAR tracking challenges, demonstrating strong performance in both online and offline modes.

Abstract: Multi-object tracking from LiDAR point clouds presents unique challenges due
to the sparse and irregular nature of the data, compounded by the need for
temporal coherence across frames. Traditional tracking systems often rely on
hand-crafted features and motion models, which can struggle to maintain
consistent object identities in crowded or fast-moving scenes. We present a
lidar-based two-staged DETR inspired transformer; a smoother and tracker. The
smoother stage refines lidar object detections, from any off-the-shelf
detector, across a moving temporal window. The tracker stage uses a DETR-based
attention block to maintain tracks across time by associating tracked objects
with the refined detections using the point cloud as context. The model is
trained on the datasets nuScenes and KITTI in both online and offline (forward
peeking) modes demonstrating strong performance across metrics such as
ID-switch and multiple object tracking accuracy (MOTA). The numerical results
indicate that the online mode outperforms the lidar-only baseline and SOTA
models on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,
while the offline mode provides an additional 3 pp aMOTP

</details>


### [329] [It's not you, it's me -- Global urban visual perception varies across demographics and personalities](https://arxiv.org/abs/2505.12758)
*Matias Quintana,Youlong Gu,Xiucheng Liang,Yujun Hou,Koichi Ito,Yihan Zhu,Mahmoud Abdelrahman,Filip Biljecki*

Main category: cs.CV

TL;DR: The paper introduces the SPECS dataset, analyzing urban streetscape perceptions across diverse demographics and personalities, revealing biases in current models and advocating for localized, demographic-aware urban planning.


<details>
  <summary>Details</summary>
Motivation: Current urban planning approaches often overlook demographic and personality differences, risking bias. The study aims to address this gap by examining how these factors shape streetscape perceptions.

Method: A large-scale survey of 1,000 participants from five countries and 45 nationalities, using street view imagery to assess perceptions across ten indicators, including traditional and new ones.

Result: Significant differences in perception scores were found among demographics and personalities. Machine learning models trained on global data overestimated positive and underestimated negative indicators compared to human responses.

Conclusion: Urban planning should incorporate local demographic and personality insights to avoid biases and better reflect human perceptions.

Abstract: Understanding people's preferences and needs is crucial for urban planning
decisions, yet current approaches often combine them from multi-cultural and
multi-city populations, obscuring important demographic differences and risking
amplifying biases. We conducted a large-scale urban visual perception survey of
streetscapes worldwide using street view imagery, examining how demographics --
including gender, age, income, education, race and ethnicity, and, for the
first time, personality traits -- shape perceptions among 1,000 participants,
with balanced demographics, from five countries and 45 nationalities. This
dataset, introduced as Street Perception Evaluation Considering Socioeconomics
(SPECS), exhibits statistically significant differences in perception scores in
six traditionally used indicators (safe, lively, wealthy, beautiful, boring,
and depressing) and four new ones we propose (live nearby, walk, cycle, green)
among demographics and personalities. We revealed that location-based
sentiments are carried over in people's preferences when comparing urban
streetscapes with other cities. Further, we compared the perception scores
based on where participants and streetscapes are from. We found that an
off-the-shelf machine learning model trained on an existing global perception
dataset tends to overestimate positive indicators and underestimate negative
ones compared to human responses, suggesting that targeted intervention should
consider locals' perception. Our study aspires to rectify the myopic treatment
of street perception, which rarely considers demographics or personality
traits.

</details>


### [330] [Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?](https://arxiv.org/abs/2505.12766)
*Haibin He,Maoyuan Ye,Jing Zhang,Xiantao Cai,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: The paper introduces Reasoning-OCR, a benchmark to evaluate Large Multimodal Models (LMMs) on complex logical reasoning tasks using OCR cues, covering six visual scenarios and 150 questions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on simple OCR tasks, leaving complex reasoning unexplored. The authors aim to fill this gap.

Method: They design Reasoning-OCR with six reasoning challenges across diverse visual scenarios, minimizing specialized knowledge bias.

Result: Evaluation reveals gaps in LMMs' reasoning performance, highlighting the need for improvement.

Conclusion: Reasoning-OCR aims to inspire future research on enhancing LMMs' complex reasoning abilities using OCR cues.

Abstract: Large Multimodal Models (LMMs) have become increasingly versatile,
accompanied by impressive Optical Character Recognition (OCR) related
capabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'
abilities of relatively simple visual question answering, visual-text parsing,
etc. However, the extent to which LMMs can deal with complex logical reasoning
problems based on OCR cues is relatively unexplored. To this end, we introduce
the Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoning
problems based on the cues that can be extracted from rich visual-text.
Reasoning-OCR covers six visual scenarios and encompasses 150 meticulously
designed questions categorized into six reasoning challenges. Additionally,
Reasoning-OCR minimizes the impact of field-specialized knowledge. Our
evaluation offers some insights for proprietary and open-source LMMs in
different reasoning challenges, underscoring the urgent to improve the
reasoning performance. We hope Reasoning-OCR can inspire and facilitate future
research on enhancing complex reasoning ability based on OCR cues.
Reasoning-OCR is publicly available at
https://github.com/Hxyz-123/ReasoningOCR.

</details>


### [331] [Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection](https://arxiv.org/abs/2505.12772)
*Junyi Hu,Tian Bai,Fengyi Wu,Zhengming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: The paper introduces the Pyramid Sparse Transformer (PST), a lightweight feature fusion module that reduces computational complexity while maintaining spatial detail, improving performance in detection and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current attention-based fusion methods are computationally expensive and challenging to implement, especially in resource-constrained environments.

Method: PST uses coarse-to-fine token selection and shared attention parameters to minimize computation. It can be trained with coarse attention and activated at inference for accuracy gains without retraining.

Result: PST improves mAP by 0.9%, 0.5%, and 0.4% on MS COCO with YOLOv11 models and boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0% with ResNet backbones.

Conclusion: PST is an effective, hardware-friendly enhancement for vision models, offering simplicity and performance gains in detection and classification.

Abstract: Feature fusion is critical for high-performance vision models but often
incurs prohibitive complexity. However, prevailing attention-based fusion
methods often involve significant computational complexity and implementation
challenges, limiting their efficiency in resource-constrained environments. To
address these issues, we introduce the Pyramid Sparse Transformer (PST), a
lightweight, plug-and-play module that integrates coarse-to-fine token
selection and shared attention parameters to reduce computation while
preserving spatial detail. PST can be trained using only coarse attention and
seamlessly activated at inference for further accuracy gains without
retraining. When added to state-of-the-art real-time detection models, such as
YOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO
with minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as
backbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,
respectively. These results demonstrate PST's effectiveness as a simple,
hardware-friendly enhancement for both detection and classification tasks.

</details>


### [332] [Enhancing Transformers Through Conditioned Embedded Tokens](https://arxiv.org/abs/2505.12789)
*Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TL;DR: The paper identifies ill-conditioning in transformer attention blocks, proposes a method to improve conditioning via embedded tokens, and demonstrates its effectiveness across tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers' success relies on attention mechanisms, but inherent ill-conditioning hampers training efficiency.

Method: Develops a theoretical framework linking attention block conditioning to embedded tokens and introduces conditioned embedded tokens to improve it.

Result: Significantly mitigates ill-conditioning, leading to stable and efficient training, validated across multiple transformer tasks.

Conclusion: The method improves transformer training efficiency and stability, showing broad applicability.

Abstract: Transformers have transformed modern machine learning, driving breakthroughs
in computer vision, natural language processing, and robotics. At the core of
their success lies the attention mechanism, which enables the modeling of
global dependencies among input tokens. However, we reveal that the attention
block in transformers suffers from inherent ill-conditioning, which hampers
gradient-based optimization and leads to inefficient training. To address this,
we develop a theoretical framework that establishes a direct relationship
between the conditioning of the attention block and that of the embedded
tokenized data. Building on this insight, we introduce conditioned embedded
tokens, a method that systematically modifies the embedded tokens to improve
the conditioning of the attention mechanism. Our analysis demonstrates that
this approach significantly mitigates ill-conditioning, leading to more stable
and efficient training. We validate our methodology across various transformer
architectures, achieving consistent improvements in image classification,
object detection, instance segmentation, and natural language processing,
highlighting its broad applicability and effectiveness.

</details>


### [333] [Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation](https://arxiv.org/abs/2505.12803)
*Jiawen Xu,Odej Kao,Margret Keuper*

Main category: cs.CV

TL;DR: GradMix, a gradient-based data augmentation method, improves open set recognition by masking learned concepts to encourage diverse feature learning, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting novel classes in model inference by learning features relevant for unseen categories from limited data.

Method: Proposes GradMix, which dynamically uses gradient-based attribution maps during training to mask learned concepts, promoting diverse feature learning.

Result: Outperforms state-of-the-art in open set recognition, close set classification, and out-of-distribution detection; enhances model robustness and self-supervised learning performance.

Conclusion: GradMix effectively improves feature diversity and model generalization, demonstrating broad applicability and robustness.

Abstract: Open set recognition (OSR) is devised to address the problem of detecting
novel classes during model inference. Even in recent vision models, this
remains an open issue which is receiving increasing attention. Thereby, a
crucial challenge is to learn features that are relevant for unseen categories
from given data, for which these features might not be discriminative. To
facilitate this process and "optimize to learn" more diverse features, we
propose GradMix, a data augmentation method that dynamically leverages
gradient-based attribution maps of the model during training to mask out
already learned concepts. Thus GradMix encourages the model to learn a more
complete set of representative features from the same data source. Extensive
experiments on open set recognition, close set classification, and
out-of-distribution detection reveal that our method can often outperform the
state-of-the-art. GradMix can further increase model robustness to corruptions
as well as downstream classification performance for self-supervised learning,
indicating its benefit for model generalization.

</details>


### [334] [Rethinking Features-Fused-Pyramid-Neck for Object Detection](https://arxiv.org/abs/2505.12820)
*Hulin Li*

Main category: cs.CV

TL;DR: The paper introduces an independent hierarchy pyramid (IHP) architecture to address feature misalignment in multi-head detectors, proposing soft nearest neighbor interpolation (SNI) and features adaptive selection (ESD) for improved performance.


<details>
  <summary>Details</summary>
Motivation: Feature misalignment in multi-head detectors due to forced point-to-point fusion of hierarchical features.

Method: Designed IHP for features-unfused-pyramid-neck, introduced SNI with weight downscaling, and proposed ESD for feature retention.

Result: Achieved state-of-the-art results on Pascal VOC and MS COCO datasets.

Conclusion: The proposed SA solution effectively addresses feature misalignment, enhancing real-time detection performance.

Abstract: Multi-head detectors typically employ a features-fused-pyramid-neck for
multi-scale detection and are widely adopted in the industry. However, this
approach faces feature misalignment when representations from different
hierarchical levels of the feature pyramid are forcibly fused point-to-point.
To address this issue, we designed an independent hierarchy pyramid (IHP)
architecture to evaluate the effectiveness of the features-unfused-pyramid-neck
for multi-head detectors. Subsequently, we introduced soft nearest neighbor
interpolation (SNI) with a weight downscaling factor to mitigate the impact of
feature fusion at different hierarchies while preserving key textures.
Furthermore, we present a features adaptive selection method for down sampling
in extended spatial windows (ESD) to retain spatial features and enhance
lightweight convolutional techniques (GSConvE). These advancements culminate in
our secondary features alignment solution (SA) for real-time detection,
achieving state-of-the-art results on Pascal VOC and MS COCO. Code will be
released at https://github.com/AlanLi1997/rethinking-fpn. This paper has been
accepted by ECCV2024 and published on Springer Nature.

</details>


### [335] [Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering](https://arxiv.org/abs/2505.12826)
*Jianfeng Cai,Wengang Zhou,Zongmeng Zhang,Jiale Hong,Nianji Zhan,Houqiang Li*

Main category: cs.CV

TL;DR: The paper investigates activation engineering to reduce hallucinations in VideoLLMs, identifying temporal variation as a key factor and proposing a temporal-aware framework for mitigation.


<details>
  <summary>Details</summary>
Motivation: Hallucination in VideoLLMs remains a challenge, and activation engineering's potential for this domain is unexplored.

Method: Systematic investigation of activation engineering factors, leading to a temporal-aware framework for adaptive module manipulation.

Result: The proposed method significantly reduces hallucination across models and benchmarks.

Conclusion: Activation engineering, guided by temporal variation, effectively mitigates hallucinations in VideoLLMs without fine-tuning.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in
video understanding.However, hallucination, where the model generates plausible
yet incorrect outputs, persists as a significant and under-addressed challenge
in the video domain. Among existing solutions, activation engineering has
proven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its
applicability to VideoLLMs remains largely unexplored. In this work, we are the
first to systematically investigate the effectiveness and underlying mechanisms
of activation engineering for mitigating hallucinations in VideoLLMs. We
initially conduct an investigation of the key factors affecting the performance
of activation engineering and find that a model's sensitivity to hallucination
depends on $\textbf{temporal variation}$ rather than task type. Moreover,
selecting appropriate internal modules and dataset for activation engineering
is critical for reducing hallucination. Guided by these findings, we propose a
temporal-aware activation engineering framework for VideoLLMs, which adaptively
identifies and manipulates hallucination-sensitive modules based on the
temporal variation characteristic, substantially mitigating hallucinations
without additional LLM fine-tuning. Experiments across multiple models and
benchmarks demonstrate that our method markedly reduces hallucination in
VideoLLMs, thereby validating the robustness of our findings.

</details>


### [336] [A Study on the Refining Handwritten Font by Mixing Font Styles](https://arxiv.org/abs/2505.12834)
*Avinash Kumar,Kyeolhee Kang,Ammar ul Hassan,Jaeyoung Choi*

Main category: cs.CV

TL;DR: FontFusionGAN (FFGAN) improves handwritten fonts by combining them with printed fonts using GANs, enhancing readability while preserving aesthetics.


<details>
  <summary>Details</summary>
Motivation: Handwritten fonts are expressive but often hard to read. FFGAN aims to merge handwritten and printed fonts for better legibility and visual appeal.

Method: Uses a GAN trained on handwritten and printed fonts to generate hybrid fonts that balance readability and aesthetics.

Result: FFGAN significantly improves readability of handwritten fonts while maintaining their unique style, applicable to various text-image tasks.

Conclusion: FFGAN offers a practical solution for enhancing handwritten fonts, benefiting applications like document creation and aiding individuals with reading/writing difficulties.

Abstract: Handwritten fonts have a distinct expressive character, but they are often
difficult to read due to unclear or inconsistent handwriting. FontFusionGAN
(FFGAN) is a novel method for improving handwritten fonts by combining them
with printed fonts. Our method implements generative adversarial network (GAN)
to generate font that mix the desirable features of handwritten and printed
fonts. By training the GAN on a dataset of handwritten and printed fonts, it
can generate legible and visually appealing font images. We apply our method to
a dataset of handwritten fonts and demonstrate that it significantly enhances
the readability of the original fonts while preserving their unique aesthetic.
Our method has the potential to improve the readability of handwritten fonts,
which would be helpful for a variety of applications including document
creation, letter writing, and assisting individuals with reading and writing
difficulties. In addition to addressing the difficulties of font creation for
languages with complex character sets, our method is applicable to other
text-image-related tasks, such as font attribute control and multilingual font
style transfer.

</details>


### [337] [Accelerate TarFlow Sampling with GS-Jacobi Iteration](https://arxiv.org/abs/2505.12849)
*Ben Liu,Zhen Qin*

Main category: cs.CV

TL;DR: The paper proposes the GS-Jacobi iteration method to accelerate TarFlow's slow sampling process, introducing CRM and IGM metrics to optimize block importance and initial values, achieving significant speed-ups without quality loss.


<details>
  <summary>Details</summary>
Motivation: TarFlow's sampling is slow due to sequential computation in causal attention. The paper aims to optimize this by leveraging block importance and initial value sensitivity.

Method: Uses GS-Jacobi iteration, CRM to identify block convergence difficulty, and IGM to evaluate initial values, optimizing sampling efficiency.

Result: Achieves speed-ups of 4.53x to 5.32x across four TarFlow models without degrading FID scores or image quality.

Conclusion: GS-Jacobi sampling with CRM and IGM effectively accelerates TarFlow while maintaining performance, offering practical improvements for image generation.

Abstract: Image generation models have achieved widespread applications. As an
instance, the TarFlow model combines the transformer architecture with
Normalizing Flow models, achieving state-of-the-art results on multiple
benchmarks. However, due to the causal form of attention requiring sequential
computation, TarFlow's sampling process is extremely slow. In this paper, we
demonstrate that through a series of optimization strategies, TarFlow sampling
can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as
GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow
model have varying importance: a small number of blocks play a major role in
image generation tasks, while other blocks contribute relatively little; some
blocks are sensitive to initial values and prone to numerical overflow, while
others are relatively robust. Based on these two characteristics, we propose
the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM
is used to identify whether a TarFlow block is "simple" (converges in few
iterations) or "tough" (requires more iterations); IGM is used to evaluate
whether the initial value of the iteration is good. Experiments on four TarFlow
models demonstrate that GS-Jacobi sampling can significantly enhance sampling
efficiency while maintaining the quality of generated images (measured by FID),
achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in
Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample
quality. Code and checkpoints are accessible on
https://github.com/encoreus/GS-Jacobi_for_TarFlow

</details>


### [338] [The Way Up: A Dataset for Hold Usage Detection in Sport Climbing](https://arxiv.org/abs/2505.12854)
*Anna Maschek,David C. Schedl*

Main category: cs.CV

TL;DR: A dataset of 22 annotated climbing videos is introduced to address the lack of detailed hold usage annotations, and keypoint-based 2D pose-estimation models are evaluated for detecting hold usage in sport climbing.


<details>
  <summary>Details</summary>
Motivation: No existing climbing dataset with detailed hold usage annotations, limiting applications in climbing-related research and AI-assisted systems.

Method: Keypoint-based 2D pose-estimation models are used to detect hold usage by analyzing joint keypoints and their overlap with climbing holds.

Result: Multiple state-of-the-art models are evaluated, revealing climbing-specific challenges in pose estimation.

Conclusion: The dataset and results provide a foundation for future research in AI-assisted climbing systems, addressing key challenges in climbing-specific pose estimation.

Abstract: Detecting an athlete's position on a route and identifying hold usage are
crucial in various climbing-related applications. However, no climbing dataset
with detailed hold usage annotations exists to our knowledge. To address this
issue, we introduce a dataset of 22 annotated climbing videos, providing
ground-truth labels for hold locations, usage order, and time of use.
Furthermore, we explore the application of keypoint-based 2D pose-estimation
models for detecting hold usage in sport climbing. We determine usage by
analyzing the key points of certain joints and the corresponding overlap with
climbing holds. We evaluate multiple state-of-the-art models and analyze their
accuracy on our dataset, identifying and highlighting climbing-specific
challenges. Our dataset and results highlight key challenges in
climbing-specific pose estimation and establish a foundation for future
research toward AI-assisted systems for sports climbing.

</details>


### [339] [Towards a Universal Image Degradation Model via Content-Degradation Disentanglement](https://arxiv.org/abs/2505.12860)
*Wenbo Yang,Zhongling Wang,Zhou Wang*

Main category: cs.CV

TL;DR: A universal degradation model is proposed to synthesize diverse image degradations without user input, outperforming existing models limited to specific degradations.


<details>
  <summary>Details</summary>
Motivation: Existing models lack generalizability for synthesizing a broad range of degradations, requiring user-provided parameters.

Method: The model disentangles homogeneous and inhomogeneous degradation features automatically, using a disentangle-by-compression method and novel modules for inhomogeneous components.

Result: The model demonstrates accuracy in film-grain simulation and blind image restoration tasks.

Conclusion: The proposed universal degradation model is adaptable and effective for synthesizing complex degradations, with potential applications in image restoration and artistic effects.

Abstract: Image degradation synthesis is highly desirable in a wide variety of
applications ranging from image restoration to simulating artistic effects.
Existing models are designed to generate one specific or a narrow set of
degradations, which often require user-provided degradation parameters. As a
result, they lack the generalizability to synthesize degradations beyond their
initial design or adapt to other applications. Here we propose the first
universal degradation model that can synthesize a broad spectrum of complex and
realistic degradations containing both homogeneous (global) and inhomogeneous
(spatially varying) components. Our model automatically extracts and
disentangles homogeneous and inhomogeneous degradation features, which are
later used for degradation synthesis without user intervention. A
disentangle-by-compression method is proposed to separate degradation
information from images. Two novel modules for extracting and incorporating
inhomogeneous degradations are created to model inhomogeneous components in
complex degradations. We demonstrate the model's accuracy and adaptability in
film-grain simulation and blind image restoration tasks. The demo video, code,
and dataset of this project will be released upon publication at
github.com/yangwenbo99/content-degradation-disentanglement.

</details>


### [340] [Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation](https://arxiv.org/abs/2505.12861)
*Jiaqi Tan,Xu Zheng,Yang Liu*

Main category: cs.CV

TL;DR: RobustSeg, a two-stage framework, enhances multi-modal robustness in semantic segmentation using Hybrid Prototype Distillation and Representation Regularization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multi-modal semantic segmentation (MMSS) caused by dynamic environments, sensor failures, and noise, which create a gap between theory and practice.

Method: A two-stage framework: pre-training a teacher model with complete modalities, then training a student model with modality dropout using Hybrid Prototype Distillation (HPDM) and Representation Regularization (RRM).

Result: Outperforms previous methods on three benchmarks with improvements of +2.76%, +4.56%, and +0.98%.

Conclusion: RobustSeg effectively bridges the gap between theoretical models and practical performance in MMSS, demonstrating superior robustness.

Abstract: Multi-modal semantic segmentation (MMSS) faces significant challenges in
real-world scenarios due to dynamic environments, sensor failures, and noise
interference, creating a gap between theoretical models and practical
performance. To address this, we propose a two-stage framework called
RobustSeg, which enhances multi-modal robustness through two key components:
the Hybrid Prototype Distillation Module (HPDM) and the Representation
Regularization Module (RRM). In the first stage, RobustSeg pre-trains a
multi-modal teacher model using complete modalities. In the second stage, a
student model is trained with random modality dropout while learning from the
teacher via HPDM and RRM. HPDM transforms features into compact prototypes,
enabling cross-modal hybrid knowledge distillation and mitigating bias from
missing modalities. RRM reduces representation discrepancies between the
teacher and student by optimizing functional entropy through the log-Sobolev
inequality. Extensive experiments on three public benchmarks demonstrate that
RobustSeg outperforms previous state-of-the-art methods, achieving improvements
of +2.76%, +4.56%, and +0.98%, respectively. Code is available at:
https://github.com/RobustSeg/RobustSeg.

</details>


### [341] [ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling](https://arxiv.org/abs/2505.12890)
*Ege Özsoy,Chantal Pellegrini,David Bani-Harouni,Kun Yuan,Matthias Keicher,Nassir Navab*

Main category: cs.CV

TL;DR: ORQA is a multimodal model and benchmark for surgical intelligence, unifying diverse OR datasets and signals for holistic OR comprehension.


<details>
  <summary>Details</summary>
Motivation: Existing systems lack scope and generalizability, focusing on single tasks. ORQA aims to advance OR intelligence with a unified approach.

Method: ORQA integrates four public OR datasets, fuses multimodal signals (visual, auditory, structured data), and uses progressive knowledge distillation for optimized models.

Result: ORQA shows strong performance and zero-shot generalization, enabling scalable, unified OR modeling.

Conclusion: ORQA significantly advances multimodal surgical intelligence, with code and data to be released.

Abstract: The real-world complexity of surgeries necessitates surgeons to have deep and
holistic comprehension to ensure precision, safety, and effective
interventions. Computational systems are required to have a similar level of
comprehension within the operating room. Prior works, limited to single-task
efforts like phase recognition or scene graph generation, lack scope and
generalizability. In this work, we introduce ORQA, a novel OR question
answering benchmark and foundational multimodal model to advance OR
intelligence. By unifying all four public OR datasets into a comprehensive
benchmark, we enable our approach to concurrently address a diverse range of OR
challenges. The proposed multimodal large language model fuses diverse OR
signals such as visual, auditory, and structured data, for a holistic modeling
of the OR. Finally, we propose a novel, progressive knowledge distillation
paradigm, to generate a family of models optimized for different speed and
memory requirements. We show the strong performance of ORQA on our proposed
benchmark, and its zero-shot generalization, paving the way for scalable,
unified OR modeling and significantly advancing multimodal surgical
intelligence. We will release our code and data upon acceptance.

</details>


### [342] [EPIC: Explanation of Pretrained Image Classification Networks via Prototype](https://arxiv.org/abs/2505.12897)
*Piotr Borycki,Magdalena Trędowicz,Szymon Janusz,Jacek Tabor,Przemysław Spurek,Arkadiusz Lewicki,Łukasz Struski*

Main category: cs.CV

TL;DR: EPIC bridges post-hoc and ante-hoc XAI methods, offering prototype-based explanations for pre-trained models without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods either provide coarse post-hoc explanations or require specialized ante-hoc architectures, limiting flexibility and applicability.

Method: EPIC combines post-hoc compatibility with prototype-based explanations, working on pre-trained models without modifications.

Result: EPIC replicates the explanatory power of interpretable models and performs well on benchmark datasets like CUB-200-2011 and ImageNet.

Conclusion: EPIC provides intuitive, high-quality explanations, bridging the gap between post-hoc and ante-hoc XAI approaches.

Abstract: Explainable AI (XAI) methods generally fall into two categories. Post-hoc
approaches generate explanations for pre-trained models and are compatible with
various neural network architectures. These methods often use feature
importance visualizations, such as saliency maps, to indicate which input
regions influenced the model's prediction. Unfortunately, they typically offer
a coarse understanding of the model's decision-making process. In contrast,
ante-hoc (inherently explainable) methods rely on specially designed model
architectures trained from scratch. A notable subclass of these methods
provides explanations through prototypes, representative patches extracted from
the training data. However, prototype-based approaches have limitations: they
require dedicated architectures, involve specialized training procedures, and
perform well only on specific datasets. In this work, we propose EPIC
(Explanation of Pretrained Image Classification), a novel approach that bridges
the gap between these two paradigms. Like post-hoc methods, EPIC operates on
pre-trained models without architectural modifications. Simultaneously, it
delivers intuitive, prototype-based explanations inspired by ante-hoc
techniques. To the best of our knowledge, EPIC is the first post-hoc method
capable of fully replicating the core explanatory power of inherently
interpretable models. We evaluate EPIC on benchmark datasets commonly used in
prototype-based explanations, such as CUB-200-2011 and Stanford Cars, alongside
large-scale datasets like ImageNet, typically employed by post-hoc methods.
EPIC uses prototypes to explain model decisions, providing a flexible and
easy-to-understand tool for creating clear, high-quality explanations.

</details>


### [343] [Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach](https://arxiv.org/abs/2505.12903)
*Shiao Wang,Xiao Wang,Liye Jin,Bo Jiang,Lin Zhu,Lan Chen,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces SFTrack, a Slow-Fast Tracking paradigm using event cameras for low-latency object tracking, combining high-precision and efficient modes.


<details>
  <summary>Details</summary>
Motivation: Traditional RGB-based tracking methods struggle with latency and resource constraints. Event cameras offer a promising alternative for low-latency applications.

Method: SFTrack uses graph-based representation learning from event streams and integrates it into two FlashAttention-based backbones (slow and fast trackers). The fast tracker is lightweight, while the slow tracker is high-precision. Both are combined via fine-tuning and knowledge distillation.

Result: Experiments on FE240, COESOT, and EventVOT benchmarks show the method's effectiveness and efficiency in real-world scenarios.

Conclusion: SFTrack provides a flexible, efficient solution for object tracking in varying resource environments, leveraging event cameras and dual-tracker design.

Abstract: Existing tracking algorithms typically rely on low-frame-rate RGB cameras
coupled with computationally intensive deep neural network architectures to
achieve effective tracking. However, such frame-based methods inherently face
challenges in achieving low-latency performance and often fail in
resource-constrained environments. Visual object tracking using bio-inspired
event cameras has emerged as a promising research direction in recent years,
offering distinct advantages for low-latency applications. In this paper, we
propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different
operational requirements, termed SFTrack. The proposed framework supports two
complementary modes, i.e., a high-precision slow tracker for scenarios with
sufficient computational resources, and an efficient fast tracker tailored for
latency-aware, resource-constrained environments. Specifically, our framework
first performs graph-based representation learning from
high-temporal-resolution event streams, and then integrates the learned
graph-structured information into two FlashAttention-based vision backbones,
yielding the slow and fast trackers, respectively. The fast tracker achieves
low latency through a lightweight network design and by producing multiple
bounding box outputs in a single forward pass. Finally, we seamlessly combine
both trackers via supervised fine-tuning and further enhance the fast tracker's
performance through a knowledge distillation strategy. Extensive experiments on
public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the
effectiveness and efficiency of our proposed method across different real-world
scenarios. The source code has been released on
https://github.com/Event-AHU/SlowFast_Event_Track.

</details>


### [344] [Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection](https://arxiv.org/abs/2505.12908)
*Xiao Wang,Yu Jin,Lan Chen,Bo Jiang,Lin Zhu,Yonghong Tian,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: The paper proposes CvHeat-DET, a dynamic graph-based contour-aware heat conduction network for event stream object detection, addressing limitations of CNNs and Transformers in capturing contours and multi-scale features.


<details>
  <summary>Details</summary>
Motivation: Current event stream object detection methods (CNNs/Transformers) lack contour modeling and multi-scale feature exploitation, despite EVS advantages in low-light, high-speed, and low-latency scenarios.

Method: CvHeat-DET uses dynamic graphs to predict thermal diffusivity coefficients in heat conduction models, leveraging event stream contours and hierarchical graph features for multi-scale learning.

Result: Experiments on three benchmark datasets confirm CvHeat-DET's effectiveness in event stream object detection.

Conclusion: CvHeat-DET improves contour modeling and multi-scale feature utilization, validated by benchmarks, with code available on GitHub.

Abstract: Event-based Vision Sensors (EVS) have demonstrated significant advantages
over traditional RGB frame-based cameras in low-light conditions, high-speed
motion capture, and low latency. Consequently, object detection based on EVS
has attracted increasing attention from researchers. Current event stream
object detection algorithms are typically built upon Convolutional Neural
Networks (CNNs) or Transformers, which either capture limited local features
using convolutional filters or incur high computational costs due to the
utilization of self-attention. Recently proposed vision heat conduction
backbone networks have shown a good balance between efficiency and accuracy;
however, these models are not specifically designed for event stream data. They
exhibit weak capability in modeling object contour information and fail to
exploit the benefits of multi-scale features. To address these issues, this
paper proposes a novel dynamic graph induced contour-aware heat conduction
network for event stream based object detection, termed CvHeat-DET. The
proposed model effectively leverages the clear contour information inherent in
event streams to predict the thermal diffusivity coefficients within the heat
conduction model, and integrates hierarchical structural graph features to
enhance feature learning across multiple scales. Extensive experiments on three
benchmark datasets for event stream-based object detection fully validated the
effectiveness of the proposed model. The source code of this paper will be
released on https://github.com/Event-AHU/OpenEvDET.

</details>


### [345] [HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos](https://arxiv.org/abs/2505.12911)
*Simone Alberto Peirone,Francesca Pistilli,Giuseppe Averta*

Main category: cs.CV

TL;DR: HiERO is a weakly-supervised method that leverages hierarchical activity patterns from unscripted videos to improve reasoning about human activities, achieving state-of-the-art performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Human activities are complex and variable, but their underlying hierarchical structure can be extracted from videos to enhance reasoning.

Method: HiERO aligns video clips with narrated descriptions to infer hierarchical contextual, semantic, and temporal reasoning.

Result: HiERO outperforms fully-supervised methods in benchmarks (e.g., +12.5% F1 on EgoProceL) and achieves state-of-the-art performance.

Conclusion: Hierarchical activity knowledge is highly relevant for reasoning tasks in egocentric vision.

Abstract: Human activities are particularly complex and variable, and this makes
challenging for deep learning models to reason about them. However, we note
that such variability does have an underlying structure, composed of a
hierarchy of patterns of related actions. We argue that such structure can
emerge naturally from unscripted videos of human activities, and can be
leveraged to better reason about their content. We present HiERO, a
weakly-supervised method to enrich video segments features with the
corresponding hierarchical activity threads. By aligning video clips with their
narrated descriptions, HiERO infers contextual, semantic and temporal reasoning
with an hierarchical architecture. We prove the potential of our enriched
features with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with
minimal additional training, and in zero-shot for procedure learning tasks
(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art
performance in all the benchmarks, and for procedure learning tasks it
outperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)
in zero shot. Our results prove the relevance of using knowledge of the
hierarchy of human activities for multiple reasoning tasks in egocentric
vision.

</details>


### [346] [Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption](https://arxiv.org/abs/2505.12912)
*Kazuki Adachi,Shin'ya Yamaguchi,Tomoki Hamagami*

Main category: cs.CV

TL;DR: The paper addresses CLIP's vulnerability to sensor degradation, proposing UnInfo, a test-time adaptation method to improve robustness by retaining uniformity in image embeddings.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with distribution shifts like sensor degradation, and collecting new datasets for fine-tuning is costly. Test-time adaptation with unlabeled data is explored as a solution.

Method: Proposes UnInfo, which includes uniformity-aware confidence maximization, information-aware loss balancing, and knowledge distillation from an EMA teacher to address corrupted image embeddings.

Result: UnInfo improves accuracy under sensor degradation by preserving uniformity in image embeddings.

Conclusion: UnInfo effectively adapts CLIP to sensor degradation, enhancing its robustness without requiring labeled data.

Abstract: Pre-trained vision-language models such as contrastive language-image
pre-training (CLIP) have demonstrated a remarkable generalizability, which has
enabled a wide range of applications represented by zero-shot classification.
However, vision-language models still suffer when they face datasets with large
gaps from training ones, i.e., distribution shifts. We found that CLIP is
especially vulnerable to sensor degradation, a type of realistic distribution
shift caused by sensor conditions such as weather, light, or noise. Collecting
a new dataset from a test distribution for fine-tuning highly costs since
sensor degradation occurs unexpectedly and has a range of variety. Thus, we
investigate test-time adaptation (TTA) of zero-shot classification, which
enables on-the-fly adaptation to the test distribution with unlabeled test
data. Existing TTA methods for CLIP mainly focus on modifying image and text
embeddings or predictions to address distribution shifts. Although these
methods can adapt to domain shifts, such as fine-grained labels spaces or
different renditions in input images, they fail to adapt to distribution shifts
caused by sensor degradation. We found that this is because image embeddings
are "corrupted" in terms of uniformity, a measure related to the amount of
information. To make models robust to sensor degradation, we propose a novel
method called uniformity-aware information-balanced TTA (UnInfo). To address
the corruption of image embeddings, we introduce uniformity-aware confidence
maximization, information-aware loss balancing, and knowledge distillation from
the exponential moving average (EMA) teacher. Through experiments, we
demonstrate that our UnInfo improves accuracy under sensor degradation by
retaining information in terms of uniformity.

</details>


### [347] [LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration](https://arxiv.org/abs/2505.12935)
*Di You,Daniel Siromani,Pier Luigi Dragotti*

Main category: cs.CV

TL;DR: The paper introduces a wavelet-inspired invertible neural network (INN) for image restoration (IR) using latent diffusion models (LDMs), addressing challenges like predefined degradation reliance and computational overhead. Two approaches, LatentINDIGO-PixelINN and LatentINDIGO-LatentINN, are proposed, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current LDMs for IR face challenges: reliance on predefined degradations, unstable latent space guidance, and high computational costs from frequent pixel-domain conversions.

Method: A wavelet-inspired INN simulates degradations and reconstructs details. Integrated into LDMs via two approaches: one operating in the pixel domain and another fully in latent space, with regularization to maintain natural image manifold proximity.

Result: The method achieves state-of-the-art performance on synthetic and real-world low-quality images and adapts to arbitrary output sizes.

Conclusion: The proposed INN-based approach effectively addresses key challenges in LDMs for IR, offering improved performance and flexibility.

Abstract: There is a growing interest in the use of latent diffusion models (LDMs) for
image restoration (IR) tasks due to their ability to model effectively the
distribution of natural images. While significant progress has been made, there
are still key challenges that need to be addressed. First, many approaches
depend on a predefined degradation operator, making them ill-suited for complex
or unknown degradations that deviate from standard analytical models. Second,
many methods struggle to provide a stable guidance in the latent space and
finally most methods convert latent representations back to the pixel domain
for guidance at every sampling iteration, which significantly increases
computational and memory overhead. To overcome these limitations, we introduce
a wavelet-inspired invertible neural network (INN) that simulates degradations
through a forward transform and reconstructs lost details via the inverse
transform. We further integrate this design into a latent diffusion pipeline
through two proposed approaches: LatentINDIGO-PixelINN, which operates in the
pixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space
to reduce complexity. Both approaches alternate between updating intermediate
latent variables under the guidance of our INN and refining the INN forward
model to handle unknown degradations. In addition, a regularization step
preserves the proximity of latent variables to the natural image manifold.
Experiments demonstrate that our algorithm achieves state-of-the-art
performance on synthetic and real-world low-quality images, and can be readily
adapted to arbitrary output sizes.

</details>


### [348] [Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection](https://arxiv.org/abs/2505.12966)
*Zihan Xiong,Xiaohua Wu,Lei Chen,Fangqi Lou*

Main category: cs.CV

TL;DR: Proposes MACB-DF, an audio-visual joint learning method using contrastive learning and a novel module to balance modalities, achieving 95.5% accuracy and superior cross-dataset generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of current multimodal deepfake detection methods, particularly unbalanced learning between modalities.

Method: Uses contrastive learning for multi-level and cross-modal fusion, and an orthogonalization-multimodal pareto module to handle gradient conflicts.

Result: Achieves 95.5% average accuracy and significant cross-dataset improvements (8.0% and 7.7% in ACC scores).

Conclusion: MACB-DF effectively balances and exploits modality information, outperforming existing methods in deepfake detection.

Abstract: Advances in computer vision and deep learning have blurred the line between
deepfakes and authentic media, undermining multimedia credibility through
audio-visual forgery. Current multimodal detection methods remain limited by
unbalanced learning between modalities. To tackle this issue, we propose an
Audio-Visual Joint Learning Method (MACB-DF) to better mitigate modality
conflicts and neglect by leveraging contrastive learning to assist in
multi-level and cross-modal fusion, thereby fully balancing and exploiting
information from each modality. Additionally, we designed an
orthogonalization-multimodal pareto module that preserves unimodal information
while addressing gradient conflicts in audio-video encoders caused by differing
optimization targets of the loss functions. Extensive experiments and ablation
studies conducted on mainstream deepfake datasets demonstrate consistent
performance gains of our model across key evaluation metrics, achieving an
average accuracy of 95.5% across multiple datasets. Notably, our method
exhibits superior cross-dataset generalization capabilities, with absolute
improvements of 8.0% and 7.7% in ACC scores over the previous best-performing
approach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb
datasets.

</details>


### [349] [A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation](https://arxiv.org/abs/2505.12998)
*Vinkle Srivastav,Juliette Puel,Jonathan Vappou,Elijah Van Houten,Paolo Cabras,Nicolas Padoy*

Main category: cs.CV

TL;DR: TFUScapes is a large-scale dataset of tFUS simulations through realistic human skulls, paired with DeepTFUS, a deep learning model for estimating pressure fields from CT volumes and transducer positions.


<details>
  <summary>Details</summary>
Motivation: The heterogeneous human skull distorts ultrasound wavefronts, requiring patient-specific corrections. TFUScapes and DeepTFUS aim to enable data-driven solutions for accurate tFUS targeting.

Method: TFUScapes uses a scalable k-Wave solver for simulations. DeepTFUS extends a U-Net with transducer-aware conditioning and advanced fusion techniques, trained with specialized loss functions.

Result: The dataset and model provide high-fidelity wavefield approximations, facilitating research in computational acoustics and neurotechnology.

Conclusion: TFUScapes and DeepTFUS advance tFUS research by offering a public dataset and a deep learning framework for improved targeting accuracy.

Abstract: Transcranial focused ultrasound (tFUS) is an emerging modality for
non-invasive brain stimulation and therapeutic intervention, offering
millimeter-scale spatial precision and the ability to target deep brain
structures. However, the heterogeneous and anisotropic nature of the human
skull introduces significant distortions to the propagating ultrasound
wavefront, which require time-consuming patient-specific planning and
corrections using numerical solvers for accurate targeting. To enable
data-driven approaches in this domain, we introduce TFUScapes, the first
large-scale, high-resolution dataset of tFUS simulations through anatomically
realistic human skulls derived from T1-weighted MRI images. We have developed a
scalable simulation engine pipeline using the k-Wave pseudo-spectral solver,
where each simulation returns a steady-state pressure field generated by a
focused ultrasound transducer placed at realistic scalp locations. In addition
to the dataset, we present DeepTFUS, a deep learning model that estimates
normalized pressure fields directly from input 3D CT volumes and transducer
position. The model extends a U-Net backbone with transducer-aware
conditioning, incorporating Fourier-encoded position embeddings and MLP layers
to create global transducer embeddings. These embeddings are fused with U-Net
encoder features via feature-wise modulation, dynamic convolutions, and
cross-attention mechanisms. The model is trained using a combination of
spatially weighted and gradient-sensitive loss functions, enabling it to
approximate high-fidelity wavefields. The TFUScapes dataset is publicly
released to accelerate research at the intersection of computational acoustics,
neurotechnology, and deep learning. The project page is available at
https://github.com/CAMMA-public/TFUScapes.

</details>


### [350] [Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions](https://arxiv.org/abs/2505.13023)
*Yimao Guo,Zuomin Qu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: Anti-Inpainting is a proactive defense method against diffusion-based malicious image manipulation, designed to protect images under unknown conditions using multi-level feature extraction, semantic-preserving augmentation, and distribution deviation optimization.


<details>
  <summary>Details</summary>
Motivation: Existing proactive defense methods fail against manipulations guided by unknown conditions crafted by malicious users.

Method: Uses a multi-level deep feature extractor, multi-scale semantic-preserving data augmentation, and selection-based distribution deviation optimization.

Result: Effective protection against diffusion-based inpainters under unknown conditions, with robustness against purification methods and transferability across diffusion models.

Conclusion: Anti-Inpainting provides robust and transferable protection against malicious image manipulation under unknown conditions.

Abstract: As diffusion-based malicious image manipulation becomes increasingly
prevalent, multiple proactive defense methods are developed to safeguard images
against unauthorized tampering. However, most proactive defense methods only
can safeguard images against manipulation under known conditions, and fail to
protect images from manipulations guided by tampering conditions crafted by
malicious users. To tackle this issue, we propose Anti-Inpainting, a proactive
defense method that achieves adequate protection under unknown conditions
through a triple mechanism to address this challenge. Specifically, a
multi-level deep feature extractor is presented to obtain intricate features
during the diffusion denoising process to improve protective effectiveness. We
design multi-scale semantic-preserving data augmentation to enhance the
transferability of adversarial perturbations across unknown conditions by
multi-scale transformations while preserving semantic integrity. In addition,
we propose a selection-based distribution deviation optimization strategy to
improve the protection of adversarial perturbation against manipulation under
diverse random seeds. Extensive experiments indicate the proactive defensive
performance of Anti-Inpainting against diffusion-based inpainters guided by
unknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we
also demonstrate the proposed approach's robustness under various image
purification methods and its transferability across different versions of
diffusion models.

</details>


### [351] [Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification](https://arxiv.org/abs/2505.13039)
*Xiao Wu,Xiaoqing Zhang,Zunjie Xiao,Lingxi Hu,Risa Higashita,Jiang Liu*

Main category: cs.CV

TL;DR: The paper introduces ERoHPRF, a method to improve medical image classification by using heterogeneous pyramid receptive fields and expert-like reparameterization, addressing limitations in capturing diverse lesion characteristics and reducing bias.


<details>
  <summary>Details</summary>
Motivation: Current CNN architectures struggle with capturing diverse lesion features and produce biased predictions in medical image classification, posing risks for real-world diagnosis.

Method: ERoHPRF employs heterogeneous pyramid receptive fields and expert-like reparameterization to mimic multi-expert consultation, enhancing feature capture and fairness.

Result: ERoHPRF outperforms state-of-the-art methods in classification performance, fairness, and computational efficiency.

Conclusion: ERoHPRF offers a balanced solution for medical image classification, combining effectiveness, fairness, and efficiency.

Abstract: Efficient convolutional neural network (CNN) architecture designs have
attracted growing research interests. However, they usually apply single
receptive field (RF), small asymmetric RFs, or pyramid RFs to learn different
feature representations, still encountering two significant challenges in
medical image classification tasks: 1) They have limitations in capturing
diverse lesion characteristics efficiently, e.g., tiny, coordination, small and
salient, which have unique roles on results, especially imbalanced medical
image classification. 2) The predictions generated by those CNNs are often
unfair/biased, bringing a high risk by employing them to real-world medical
diagnosis conditions. To tackle these issues, we develop a new concept,
Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields
(ERoHPRF), to simultaneously boost medical image classification performance and
fairness. This concept aims to mimic the multi-expert consultation mode by
applying the well-designed heterogeneous pyramid RF bags to capture different
lesion characteristics effectively via convolution operations with multiple
heterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like
structural reparameterization technique to merge its parameters with the
two-stage strategy, ensuring competitive computation cost and inference speed
through comparisons to a single RF. To manifest the effectiveness and
generalization ability of ERoHPRF, we incorporate it into mainstream efficient
CNN architectures. The extensive experiments show that our method maintains a
better trade-off than state-of-the-art methods in terms of medical image
classification, fairness, and computation overhead. The codes of this paper
will be released soon.

</details>


### [352] [A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation](https://arxiv.org/abs/2505.13043)
*Hao-Ran Yang,Xiaohui Chen,Chuan-Xian Ren*

Main category: cs.CV

TL;DR: The paper introduces a Generalized Label Shift (GLS) perspective to Cross-domain Gaze Estimation (CDGE), addressing domain shift via label and conditional shift correction. A novel framework with importance reweighting and conditional operator discrepancy estimation is proposed, showing superior generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Existing CDGE methods focus on domain-invariant features but are insufficient due to Generalized Label Shift (GLS). The paper aims to address this by modeling cross-domain problems as label and conditional shifts.

Method: A GLS correction framework is introduced, using truncated Gaussian-based importance reweighting for label shift correction. A probability-aware estimation of conditional operator discrepancy is derived for conditional invariant learning.

Result: Extensive experiments demonstrate the method's superior generalization across domains and applicability with various backbone models.

Conclusion: The proposed GLS-based approach effectively addresses domain shift in CDGE, offering improved generalization and model versatility.

Abstract: Aiming to generalize the well-trained gaze estimation model to new target
domains, Cross-domain Gaze Estimation (CDGE) is developed for real-world
application scenarios. Existing CDGE methods typically extract the
domain-invariant features to mitigate domain shift in feature space, which is
proved insufficient by Generalized Label Shift (GLS) theory. In this paper, we
introduce a novel GLS perspective to CDGE and modelize the cross-domain problem
by label and conditional shift problem. A GLS correction framework is presented
and a feasible realization is proposed, in which a importance reweighting
strategy based on truncated Gaussian distribution is introduced to overcome the
continuity challenges in label shift correction. To embed the reweighted source
distribution to conditional invariant learning, we further derive a
probability-aware estimation of conditional operator discrepancy. Extensive
experiments on standard CDGE tasks with different backbone models validate the
superior generalization capability across domain and applicability on various
models of proposed method.

</details>


### [353] [RGB-to-Polarization Estimation: A New Task and Benchmark Study](https://arxiv.org/abs/2505.13050)
*Beibei Lin,Zifeng Yuan,Tingting Chen*

Main category: cs.CV

TL;DR: The paper introduces RGB-to-polarization image estimation, a task to infer polarization data from RGB images, and establishes a benchmark using deep learning models to evaluate performance and guide future research.


<details>
  <summary>Details</summary>
Motivation: Polarization images offer valuable physical information but require costly hardware. The task aims to bypass this by estimating polarization from standard RGB images.

Method: The study leverages existing datasets and evaluates various deep learning models, including restoration-oriented and generative architectures, to benchmark performance.

Result: The benchmark sets the current performance limits and highlights strengths/weaknesses of different model families, like reconstruction vs. generative synthesis.

Conclusion: The work provides a foundational resource for future research and suggests directions for improving polarization estimation from RGB images.

Abstract: Polarization images provide rich physical information that is fundamentally
absent from standard RGB images, benefiting a wide range of computer vision
applications such as reflection separation and material classification.
However, the acquisition of polarization images typically requires additional
optical components, which increases both the cost and the complexity of the
applications. To bridge this gap, we introduce a new task: RGB-to-polarization
image estimation, which aims to infer polarization information directly from
RGB images. In this work, we establish the first comprehensive benchmark for
this task by leveraging existing polarization datasets and evaluating a diverse
set of state-of-the-art deep learning models, including both
restoration-oriented and generative architectures. Through extensive
quantitative and qualitative analysis, our benchmark not only establishes the
current performance ceiling of RGB-to-polarization estimation, but also
systematically reveals the respective strengths and limitations of different
model families -- such as direct reconstruction versus generative synthesis,
and task-specific training versus large-scale pre-training. In addition, we
provide some potential directions for future research on polarization
estimation. This benchmark is intended to serve as a foundational resource to
facilitate the design and evaluation of future methods for polarization
estimation from standard RGB inputs.

</details>


### [354] [3D Visual Illusion Depth Estimation](https://arxiv.org/abs/2505.13061)
*CHengtang Yao,Zhidan Liu,Jiaxi Zeng,Lidong Yu,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: The paper demonstrates that machine vision systems are deceived by 3D visual illusions, similar to humans. A large dataset is used to evaluate depth estimation methods, and a robust framework is proposed to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To investigate how 3D visual illusions affect machine vision systems, particularly in depth estimation tasks.

Method: A dataset of 3k scenes and 200k images is collected. A robust depth estimation framework leverages vision-language models to select reliable depth cues.

Result: State-of-the-art depth estimation methods are fooled by illusions, while the proposed framework outperforms them.

Conclusion: The study highlights vulnerabilities in machine vision and offers a solution to improve robustness against 3D illusions.

Abstract: 3D visual illusion is a perceptual phenomenon where a two-dimensional plane
is manipulated to simulate three-dimensional spatial relationships, making a
flat artwork or object look three-dimensional in the human visual system. In
this paper, we reveal that the machine visual system is also seriously fooled
by 3D visual illusions, including monocular and binocular depth estimation. In
order to explore and analyze the impact of 3D visual illusion on depth
estimation, we collect a large dataset containing almost 3k scenes and 200k
images to train and evaluate SOTA monocular and binocular depth estimation
methods. We also propose a robust depth estimation framework that uses common
sense from a vision-language model to adaptively select reliable depth from
binocular disparity and monocular depth. Experiments show that SOTA monocular,
binocular, and multi-view depth estimation approaches are all fooled by various
3D visual illusions, while our method achieves SOTA performance.

</details>


### [355] [Cross-modal feature fusion for robust point cloud registration with ambiguous geometry](https://arxiv.org/abs/2505.13088)
*Zhaoyi Wang,Shengyu Huang,Jemil Avers Butt,Yuanzhou Cai,Matej Varga,Andreas Wieser*

Main category: cs.CV

TL;DR: CoFF is a novel method for point cloud registration that fuses 3D point cloud and 2D RGB image features to improve alignment, especially in geometrically ambiguous regions.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud registration methods often ignore radiometric information from RGB images, limiting effectiveness in areas where geometry alone is insufficient.

Method: CoFF uses a two-stage fusion of 3D point cloud and 2D image features, including pixel-wise and patch-wise fusion, followed by coarse-to-fine matching.

Result: CoFF achieves state-of-the-art performance, with registration recalls of 95.9% and 81.6% on 3DMatch and 3DLoMatch datasets.

Conclusion: Integrating radiometric and geometric data significantly enhances point cloud registration, as demonstrated by CoFF's superior results.

Abstract: Point cloud registration has seen significant advancements with the
application of deep learning techniques. However, existing approaches often
overlook the potential of integrating radiometric information from RGB images.
This limitation reduces their effectiveness in aligning point clouds pairs,
especially in regions where geometric data alone is insufficient. When used
effectively, radiometric information can enhance the registration process by
providing context that is missing from purely geometric data. In this paper, we
propose CoFF, a novel Cross-modal Feature Fusion method that utilizes both
point cloud geometry and RGB images for pairwise point cloud registration.
Assuming that the co-registration between point clouds and RGB images is
available, CoFF explicitly addresses the challenges where geometric information
alone is unclear, such as in regions with symmetric similarity or planar
structures, through a two-stage fusion of 3D point cloud features and 2D image
features. It incorporates a cross-modal feature fusion module that assigns
pixel-wise image features to 3D input point clouds to enhance learned 3D point
features, and integrates patch-wise image features with superpoint features to
improve the quality of coarse matching. This is followed by a coarse-to-fine
matching module that accurately establishes correspondences using the fused
features. We extensively evaluate CoFF on four common datasets: 3DMatch,
3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In
addition, we assess CoFF on specific subset datasets containing geometrically
ambiguous cases. Our experimental results demonstrate that CoFF achieves
state-of-the-art registration performance across all benchmarks, including
remarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch
and 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)

</details>


### [356] [Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction](https://arxiv.org/abs/2505.13091)
*Yuanbo Wang,Zhaoxuan Zhang,Jiajin Qiu,Dilong Sun,Zhengyu Meng,Xiaopeng Wei,Xin Yang*

Main category: cs.CV

TL;DR: The paper introduces Touch2Shape, a model combining tactile images and diffusion models to improve 3D shape reconstruction by capturing local details and overcoming occlusion/lighting limitations.


<details>
  <summary>Details</summary>
Motivation: Current 3D diffusion models struggle with local details and are limited by occlusion/lighting. Tactile images can address these gaps by capturing local 3D information.

Method: Proposes Touch2Shape, using a touch-conditioned diffusion model with a touch embedding module for compact representation and a fusion module for refinement. Combines diffusion models with reinforcement learning for shape exploration.

Result: Experiments show improved reconstruction quality and performance, validated qualitatively and quantitatively.

Conclusion: Touch2Shape effectively leverages tactile data to enhance 3D shape reconstruction and exploration, outperforming existing methods.

Abstract: Diffusion models have made breakthroughs in 3D generation tasks. Current 3D
diffusion models focus on reconstructing target shape from images or a set of
partial observations. While excelling in global context understanding, they
struggle to capture the local details of complex shapes and limited to the
occlusion and lighting conditions. To overcome these limitations, we utilize
tactile images to capture the local 3D information and propose a Touch2Shape
model, which leverages a touch-conditioned diffusion model to explore and
reconstruct the target shape from touch. For shape reconstruction, we have
developed a touch embedding module to condition the diffusion model in creating
a compact representation and a touch shape fusion module to refine the
reconstructed shape. For shape exploration, we combine the diffusion model with
reinforcement learning to train a policy. This involves using the generated
latent vector from the diffusion model to guide the touch exploration policy
training through a novel reward design. Experiments validate the reconstruction
quality thorough both qualitatively and quantitative analysis, and our touch
exploration policy further boosts reconstruction performance.

</details>


### [357] [Industry-focused Synthetic Segmentation Pre-training](https://arxiv.org/abs/2505.13099)
*Shinichi Mae,Ryosuke Yamada,Hirokatsu Kataoka*

Main category: cs.CV

TL;DR: The paper introduces InsCore, a synthetic dataset for pre-training instance segmentation models, addressing legal and domain gap challenges in industrial applications. It outperforms real-image datasets and SAM with fewer images.


<details>
  <summary>Details</summary>
Motivation: Challenges like legal restrictions on real-image datasets (e.g., ImageNet) and domain gaps in industrial settings motivate the need for a synthetic, annotation-free solution.

Method: Proposes InsCore, a synthetic dataset using formula-driven supervised learning (FDSL) to generate annotated images mimicking industrial data characteristics.

Result: InsCore pre-trained models outperform COCO, ImageNet-21k, and fine-tuned SAM by 6.2 points on average, using only 100k synthetic images.

Conclusion: InsCore offers a practical, license-free foundation for industrial vision tasks, demonstrating superior performance and data efficiency.

Abstract: Pre-training on real-image datasets has been widely proven effective for
improving instance segmentation. However, industrial applications face two key
challenges: (1) legal and ethical restrictions, such as ImageNet's prohibition
of commercial use, and (2) limited transferability due to the domain gap
between web images and industrial imagery. Even recent vision foundation
models, including the segment anything model (SAM), show notable performance
degradation in industrial settings. These challenges raise critical questions:
Can we build a vision foundation model for industrial applications without
relying on real images or manual annotations? And can such models outperform
even fine-tuned SAM on industrial datasets? To address these questions, we
propose the Instance Core Segmentation Dataset (InsCore), a synthetic
pre-training dataset based on formula-driven supervised learning (FDSL).
InsCore generates fully annotated instance segmentation images that reflect key
characteristics of industrial data, including complex occlusions, dense
hierarchical masks, and diverse non-rigid shapes, distinct from typical web
imagery. Unlike previous methods, InsCore requires neither real images nor
human annotations. Experiments on five industrial datasets show that models
pre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as
well as fine-tuned SAM, achieving an average improvement of 6.2 points in
instance segmentation performance. This result is achieved using only 100k
synthetic images, more than 100 times fewer than the 11 million images in SAM's
SA-1B dataset, demonstrating the data efficiency of our approach. These
findings position InsCore as a practical and license-free vision foundation
model for industrial applications.

</details>


### [358] [ARIW-Framework: Adaptive Robust Iterative Watermarking Framework](https://arxiv.org/abs/2505.13101)
*Shaowu Wu,Liting Zeng,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: The paper proposes an adaptive robust iterative watermarking framework (ARIW-Framework) to improve visual quality, robustness, and generalization in digital image copyright protection.


<details>
  <summary>Details</summary>
Motivation: The rise of large models necessitates better copyright protection for generated images, but existing deep learning watermarking techniques have limitations in visual quality, robustness, and generalization.

Method: The ARIW-Framework uses an iterative approach to optimize the encoder for robust residuals, incorporates noise layers and a decoder for robustness weights, and employs a parallel optimization strategy. Image gradients are used to determine embedding strength for better visual quality.

Result: Extensive experiments show the method achieves superior visual quality, robustness, and generalization against noise attacks.

Conclusion: The proposed ARIW-Framework effectively addresses the limitations of current watermarking techniques, offering a high-quality and robust solution for image copyright protection.

Abstract: With the rapid rise of large models, copyright protection for generated image
content has become a critical security challenge. Although deep learning
watermarking techniques offer an effective solution for digital image copyright
protection, they still face limitations in terms of visual quality, robustness
and generalization. To address these issues, this paper proposes an adaptive
robust iterative watermarking framework (ARIW-Framework) that achieves
high-quality watermarked images while maintaining exceptional robustness and
generalization performance. Specifically, we introduce an iterative approach to
optimize the encoder for generating robust residuals. The encoder incorporates
noise layers and a decoder to compute robustness weights for residuals under
various noise attacks. By employing a parallel optimization strategy, the
framework enhances robustness against multiple types of noise attacks.
Furthermore, we leverage image gradients to determine the embedding strength at
each pixel location, significantly improving the visual quality of the
watermarked images. Extensive experiments demonstrate that the proposed method
achieves superior visual quality while exhibiting remarkable robustness and
generalization against noise attacks.

</details>


### [359] [Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection](https://arxiv.org/abs/2505.13123)
*Snehashis Majhi,Giacomo D'Amicantonio,Antitza Dantcheva,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,Egor Bondarev,Francois Bremond*

Main category: cs.CV

TL;DR: PI-VAD introduces a poly-modal framework for video anomaly detection, augmenting RGB features with five additional modalities to improve reliability in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: RGB features alone are insufficient for distinguishing anomalies like shoplifting from similar events, necessitating multi-modal augmentation.

Method: PI-VAD uses five modalities (Pose, Depth, Panoptic masks, optical flow, VLM) and includes two modules for pseudo-modality generation and cross-modal induction.

Result: Achieves state-of-the-art accuracy on three VAD datasets without computational overhead at inference.

Conclusion: PI-VAD demonstrates robust performance by leveraging multi-modal cues, enhancing real-world anomaly detection.

Abstract: Weakly-supervised methods for video anomaly detection (VAD) are
conventionally based merely on RGB spatio-temporal features, which continues to
limit their reliability in real-world scenarios. This is due to the fact that
RGB-features are not sufficiently distinctive in setting apart categories such
as shoplifting from visually similar events. Therefore, towards robust complex
real-world VAD, it is essential to augment RGB spatio-temporal features by
additional modalities. Motivated by this, we introduce the Poly-modal Induced
framework for VAD: "PI-VAD", a novel approach that augments RGB representations
by five additional modalities. Specifically, the modalities include sensitivity
to fine-grained motion (Pose), three dimensional scene and entity
representation (Depth), surrounding objects (Panoptic masks), global motion
(optical flow), as well as language cues (VLM). Each modality represents an
axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two
plug-in modules, namely Pseudo-modality Generation module and Cross Modal
Induction module, which generate modality-specific prototypical representation
and, thereby, induce multi-modal information into RGB cues. These modules
operate by performing anomaly-aware auxiliary tasks and necessitate five
modality backbones -- only during training. Notably, PI-VAD achieves
state-of-the-art accuracy on three prominent VAD datasets encompassing
real-world scenarios, without requiring the computational overhead of five
modality backbones at inference.

</details>


### [360] [Adaptive Image Restoration for Video Surveillance: A Real-Time Approach](https://arxiv.org/abs/2505.13130)
*Muhammad Awais Amin,Adama Ilboudo,Abdul Samad bin Shahid,Amjad Ali,Waqas Haider Khan Bangyal*

Main category: cs.CV

TL;DR: A real-time image restoration solution for video surveillance was developed using transfer learning with ResNet_50 to identify and address image degradation types.


<details>
  <summary>Details</summary>
Motivation: Image degradation negatively impacts automated decision-making in computer vision, and existing solutions lack real-time processing capabilities.

Method: Transfer learning with ResNet_50 was used to develop a model for identifying degradation types and referencing necessary treatments.

Result: The solution is flexible, scalable, and suitable for real-time image restoration in video surveillance.

Conclusion: The study successfully addresses the need for real-time image restoration, offering a practical and adaptable solution.

Abstract: One of the major challenges in the field of computer vision especially for
detection, segmentation, recognition, monitoring, and automated solutions, is
the quality of images. Image degradation, often caused by factors such as rain,
fog, lighting, etc., has a negative impact on automated
decision-making.Furthermore, several image restoration solutions exist,
including restoration models for single degradation and restoration models for
multiple degradations. However, these solutions are not suitable for real-time
processing. In this study, the aim was to develop a real-time image restoration
solution for video surveillance. To achieve this, using transfer learning with
ResNet_50, we developed a model for automatically identifying the types of
degradation present in an image to reference the necessary treatment(s) for
image restoration. Our solution has the advantage of being flexible and
scalable.

</details>


### [361] [Learning to Adapt to Position Bias in Vision Transformer Classifiers](https://arxiv.org/abs/2505.13137)
*Robert-Jan Bruintjes,Jan van Gemert*

Main category: cs.CV

TL;DR: The paper explores the role of position bias in image classification, proposing Position-SHAP to measure it and Auto-PE to optimize position embeddings based on dataset bias.


<details>
  <summary>Details</summary>
Motivation: Position information can be both arbitrary (due to translation invariance) and critical (e.g., sky being up). The study aims to understand its impact on Vision Transformers.

Method: Introduces Position-SHAP to measure position bias and Auto-PE, a single-parameter extension for position embeddings, to adapt to dataset-specific needs.

Result: Finds varying levels of position bias across datasets; Auto-PE improves or matches accuracy when combined with existing position embeddings.

Conclusion: Position bias significantly affects classifier performance, and Auto-PE offers a flexible solution to optimize position embeddings.

Abstract: How discriminative position information is for image classification depends
on the data. On the one hand, the camera position is arbitrary and objects can
appear anywhere in the image, arguing for translation invariance. At the same
time, position information is key for exploiting capture/center bias, and scene
layout, e.g.: the sky is up. We show that position bias, the level to which a
dataset is more easily solved when positional information on input features is
used, plays a crucial role in the performance of Vision Transformers image
classifiers. To investigate, we propose Position-SHAP, a direct measure of
position bias by extending SHAP to work with position embeddings. We show
various levels of position bias in different datasets, and find that the
optimal choice of position embedding depends on the position bias apparent in
the dataset. We therefore propose Auto-PE, a single-parameter position
embedding extension, which allows the position embedding to modulate its norm,
enabling the unlearning of position information. Auto-PE combines with existing
PEs to match or improve accuracy on classification datasets.

</details>


### [362] [CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow](https://arxiv.org/abs/2505.13140)
*Takahiro Maeda,Jinkun Cao,Norimichi Ukita,Kris Kitani*

Main category: cs.CV

TL;DR: CacheFlow introduces a fast, flow-based method for 3D human motion prediction by precomputing and caching results, achieving significant speed improvements without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of existing density estimation techniques for 3D human motion prediction, which often exceed the predicted time horizon in inference time.

Method: Uses an unconditional flow-based generative model to transform a Gaussian mixture into future motion density, precomputing results for efficiency. A lightweight model then maps historical trajectories to samples.

Result: Achieves inference in ~1ms, 4x faster than VAE and 30x faster than diffusion methods, with improved density estimation and comparable prediction accuracy on Human3.6M and AMASS.

Conclusion: CacheFlow offers a faster, efficient alternative for 3D human motion prediction without compromising accuracy or expressiveness.

Abstract: Many density estimation techniques for 3D human motion prediction require a
significant amount of inference time, often exceeding the duration of the
predicted time horizon. To address the need for faster density estimation for
3D human motion prediction, we introduce a novel flow-based method for human
motion prediction called CacheFlow. Unlike previous conditional generative
models that suffer from time efficiency, CacheFlow takes advantage of an
unconditional flow-based generative model that transforms a Gaussian mixture
into the density of future motions. The results of the computation of the
flow-based generative model can be precomputed and cached. Then, for
conditional prediction, we seek a mapping from historical trajectories to
samples in the Gaussian mixture. This mapping can be done by a much more
lightweight model, thus saving significant computation overhead compared to a
typical conditional flow model. In such a two-stage fashion and by caching
results from the slow flow model computation, we build our CacheFlow without
loss of prediction accuracy and model expressiveness. This inference process is
completed in approximately one millisecond, making it 4 times faster than
previous VAE methods and 30 times faster than previous diffusion-based methods
on standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our
method demonstrates improved density estimation accuracy and comparable
prediction accuracy to a SOTA method on Human3.6M. Our code and models will be
publicly available.

</details>


### [363] [FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching](https://arxiv.org/abs/2505.13174)
*Alp Eren Sari,Paolo Favaro*

Main category: cs.CV

TL;DR: FlowCut is a three-stage unsupervised method for video instance segmentation, using pseudo-labels to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of labeled video datasets for unsupervised video instance segmentation by creating high-quality pseudo-labels.

Method: A three-stage framework: (1) generate pseudo-instance masks using image and optical flow features, (2) construct consistent short video segments, (3) train a model on YouTubeVIS-2021.

Result: Achieves state-of-the-art performance on YouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks.

Conclusion: FlowCut successfully demonstrates the viability of using pseudo-labels for unsupervised video instance segmentation.

Abstract: We propose FlowCut, a simple and capable method for unsupervised video
instance segmentation consisting of a three-stage framework to construct a
high-quality video dataset with pseudo labels. To our knowledge, our work is
the first attempt to curate a video dataset with pseudo-labels for unsupervised
video instance segmentation. In the first stage, we generate pseudo-instance
masks by exploiting the affinities of features from both images and optical
flows. In the second stage, we construct short video segments containing
high-quality, consistent pseudo-instance masks by temporally matching them
across the frames. In the third stage, we use the YouTubeVIS-2021 video dataset
to extract our training instance segmentation set, and then train a video
segmentation model. FlowCut achieves state-of-the-art performance on the
YouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks.

</details>


### [364] [Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision](https://arxiv.org/abs/2505.13191)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyoshi*

Main category: cs.CV

TL;DR: MRAM introduces a hierarchical hard attention model mimicking human vision, outperforming RAM, DRAM, and CNNs in image classification with more human-like attention dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing hard attention models (RAM, DRAM) fail to replicate the hierarchy of human vision, leading to unnatural attention behaviors (overly fixational or saccadic).

Method: MRAM decouples glimpse location generation and task execution into two recurrent layers to model human visual hierarchy.

Result: MRAM achieves balanced fixation-saccadic behavior and outperforms CNN, RAM, and DRAM on image classification benchmarks.

Conclusion: MRAM successfully models human visual hierarchy, improving both attention dynamics and task performance.

Abstract: Inspired by foveal vision, hard attention models promise interpretability and
parameter economy. However, existing models like the Recurrent Model of Visual
Attention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the
hierarchy of human vision system, that compromise on the visual exploration
dynamics. As a result, they tend to produce attention that are either overly
fixational or excessively saccadic, diverging from human eye movement behavior.
In this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a
novel hard attention framework that explicitly models the neural hierarchy of
human visual processing. By decoupling the function of glimpse location
generation and task execution in two recurrent layers, MRAM emergent a balanced
behavior between fixation and saccadic movement. Our results show that MRAM not
only achieves more human-like attention dynamics, but also consistently
outperforms CNN, RAM and DRAM baselines on standard image classification
benchmarks.

</details>


### [365] [MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects](https://arxiv.org/abs/2505.13201)
*Yuzhen Chen,Hojun Son,Arpan Kusari*

Main category: cs.CV

TL;DR: The paper introduces MatPredict, a dataset combining synthetic objects with diverse material properties to improve material property inference from images for consumer robotics.


<details>
  <summary>Details</summary>
Motivation: Enhancing the ability to identify complex objects in indoor environments by determining material properties from camera images, valuable for consumer robotics.

Method: Combines Replica dataset's synthetic objects with MatSynth's material properties, rendering 3D meshes with varied materials, lighting, and camera placements. Benchmarks neural network models for material property inference.

Result: Generated 18 objects with 14 materials, providing variability in lighting and camera placement. Demonstrated performance of neural networks in inferring material properties.

Conclusion: MatPredict enhances realism in simulations, improving model training for consumer robotics perception. The dataset and code are publicly available.

Abstract: Determining material properties from camera images can expand the ability to
identify complex objects in indoor environments, which is valuable for consumer
robotics applications. To support this, we introduce MatPredict, a dataset that
combines the high-quality synthetic objects from Replica dataset with MatSynth
dataset's material properties classes - to create objects with diverse material
properties. We select 3D meshes of specific foreground objects and render them
with different material properties. In total, we generate \textbf{18} commonly
occurring objects with \textbf{14} different materials. We showcase how we
provide variability in terms of lighting and camera placement for these
objects. Next, we provide a benchmark for inferring material properties from
visual images using these perturbed models in the scene, discussing the
specific neural network models involved and their performance based on
different image comparison metrics. By accurately simulating light interactions
with different materials, we can enhance realism, which is crucial for training
models effectively through large-scale simulations. This research aims to
revolutionize perception in consumer robotics. The dataset is provided
\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is
provided \href{https://github.com/arpan-kusari/MatPredict}{here}.

</details>


### [366] [MAGI-1: Autoregressive Video Generation at Scale](https://arxiv.org/abs/2505.13211)
*Sand. ai,Hansi Teng,Hongyu Jia,Lei Sun,Lingzhi Li,Maolin Li,Mingqiu Tang,Shuai Han,Tianning Zhang,W. Q. Zhang,Weifeng Luo,Xiaoyang Kang,Yuchen Sun,Yue Cao,Yunpeng Huang,Yutong Lin,Yuxin Fang,Zewei Tao,Zheng Zhang,Zhongshu Wang,Zixun Liu,Dai Shi,Guoli Su,Hanwen Sun,Hong Pan,Jie Wang,Jiexin Sheng,Min Cui,Min Hu,Ming Yan,Shucheng Yin,Siran Zhang,Tingting Liu,Xianping Yin,Xiaoyu Yang,Xin Song,Xuan Hu,Yankai Zhang,Yuqiao Li*

Main category: cs.CV

TL;DR: MAGI-1 is a scalable, autoregressive world model for video generation, achieving high temporal consistency and controllable output via chunk-wise prompting.


<details>
  <summary>Details</summary>
Motivation: To enable causal temporal modeling and streaming video generation with high consistency and scalability.

Method: Autoregressive prediction of video chunks with denoising training and algorithmic innovations.

Result: Strong performance on I2V tasks, constant peak inference cost, and support for large context lengths (up to 4M tokens).

Conclusion: MAGI-1 demonstrates robust, scalable video generation with real-time deployment capabilities.

Abstract: We present MAGI-1, a world model that generates videos by autoregressively
predicting a sequence of video chunks, defined as fixed-length segments of
consecutive frames. Trained to denoise per-chunk noise that increases
monotonically over time, MAGI-1 enables causal temporal modeling and naturally
supports streaming generation. It achieves strong performance on image-to-video
(I2V) tasks conditioned on text instructions, providing high temporal
consistency and scalability, which are made possible by several algorithmic
innovations and a dedicated infrastructure stack. MAGI-1 facilitates
controllable generation via chunk-wise prompting and supports real-time,
memory-efficient deployment by maintaining constant peak inference cost,
regardless of video length. The largest variant of MAGI-1 comprises 24 billion
parameters and supports context lengths of up to 4 million tokens,
demonstrating the scalability and robustness of our approach. The code and
models are available at https://github.com/SandAI-org/MAGI-1 and
https://github.com/SandAI-org/MagiAttention. The product can be accessed at
https://sand.ai.

</details>


### [367] [RB-SCD: A New Benchmark for Semantic Change Detection of Roads and Bridges in Traffic Scenes](https://arxiv.org/abs/2505.13212)
*Qingling Shu,Sibao Chen,Zhihui You,Wei Lu,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces the RB-SCD dataset for road and bridge semantic change detection and proposes the MFDCD framework, which integrates multimodal frequency-domain features for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained semantic change detection due to insufficient annotated datasets in traffic scenarios.

Method: Proposes the MFDCD framework, combining visual and textual features in the frequency domain using Dynamic Frequency Coupler and Textual Frequency Filter.

Result: MFDCD outperforms on the RB-SCD dataset and three public benchmarks, demonstrating its effectiveness.

Conclusion: The RB-SCD dataset and MFDCD framework advance semantic change detection in urban and traffic scenarios.

Abstract: Accurate detection of changes in roads and bridges, such as construction,
renovation, and demolition, is essential for urban planning and traffic
management. However, existing methods often struggle to extract fine-grained
semantic change information due to the lack of high-quality annotated datasets
in traffic scenarios. To address this, we introduce the Road and Bridge
Semantic Change Detection (RB-SCD) dataset, a comprehensive benchmark
comprising 260 pairs of high-resolution remote sensing images from diverse
cities and countries. RB-SCD captures 11 types of semantic changes across
varied road and bridge structures, enabling detailed structural and functional
analysis. Building on this dataset, we propose a novel framework, Multimodal
Frequency-Driven Change Detector (MFDCD), which integrates multimodal features
in the frequency domain. MFDCD includes a Dynamic Frequency Coupler (DFC) that
fuses hierarchical visual features with wavelet-based frequency components, and
a Textual Frequency Filter (TFF) that transforms CLIP-derived textual features
into the frequency domain and applies graph-based filtering. Experimental
results on RB-SCD and three public benchmarks demonstrate the effectiveness of
our approach.

</details>


### [368] [Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation](https://arxiv.org/abs/2505.13215)
*Seungjun Oh,Younggeun Lee,Hyejin Jeon,Eunbyung Park*

Main category: cs.CV

TL;DR: Hybrid 3D-4D Gaussian Splatting (3D-4DGS) improves efficiency by using 3D Gaussians for static regions and 4D Gaussians for dynamic elements, reducing computational overhead while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing 4D Gaussian Splatting methods are computationally expensive and inefficient due to redundant 4D Gaussian allocation in static regions.

Method: The proposed 3D-4DGS framework starts with a 4D Gaussian representation and iteratively converts static regions to 3D Gaussians, reserving 4D Gaussians for dynamic elements.

Result: The method achieves faster training times and maintains or improves visual quality compared to baseline 4D Gaussian Splatting.

Conclusion: 3D-4DGS offers a more efficient and effective approach for dynamic 3D scene reconstruction by optimizing Gaussian representation.

Abstract: Recent advancements in dynamic 3D scene reconstruction have shown promising
results, enabling high-fidelity 3D novel view synthesis with improved temporal
consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an
appealing approach due to its ability to model high-fidelity spatial and
temporal variations. However, existing methods suffer from substantial
computational and memory overhead due to the redundant allocation of 4D
Gaussians to static regions, which can also degrade image quality. In this
work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework
that adaptively represents static regions with 3D Gaussians while reserving 4D
Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian
representation and iteratively converts temporally invariant Gaussians into 3D,
significantly reducing the number of parameters and improving computational
efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,
capturing complex motions with high fidelity. Our approach achieves
significantly faster training times compared to baseline 4D Gaussian Splatting
methods while maintaining or improving the visual quality.

</details>


### [369] [Swin DiT: Diffusion Transformer using Pseudo Shifted Windows](https://arxiv.org/abs/2505.13219)
*Jiafu Wu,Yabiao Wang,Jian Li,Jinlong Peng,Yun Cao,Chengjie Wang,Jiangning Zhang*

Main category: cs.CV

TL;DR: Swin-DiT improves image generation efficiency by reducing global computation redundancy with PSWA and PCCA, outperforming DiT-XL/2 with 54% better FID and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Traditional DiTs face high computational costs and redundancy in global information modeling for high-resolution images, with attention mechanisms suffering from low-frequency inertia.

Method: Proposes Pseudo Shifted Window Attention (PSWA) for efficient global-local interaction and Progressive Coverage Channel Allocation (PCCA) for high-order attention similarity. Introduces Swin-DiT models.

Result: Swin-DiT-L achieves a 54% improvement in FID over DiT-XL/2 with reduced computational requirements.

Conclusion: Swin-DiT offers a more efficient and effective alternative to traditional DiTs for image generation.

Abstract: Diffusion Transformers (DiTs) achieve remarkable performance within the
domain of image generation through the incorporation of the transformer
architecture. Conventionally, DiTs are constructed by stacking serial isotropic
global information modeling transformers, which face significant computational
cost when processing high-resolution images. We empirically analyze that latent
space image generation does not exhibit a strong dependence on global
information as traditionally assumed. Most of the layers in the model
demonstrate redundancy in global computation. In addition, conventional
attention mechanisms exhibit low-frequency inertia issues. To address these
issues, we propose \textbf{P}seudo \textbf{S}hifted \textbf{W}indow
\textbf{A}ttention (PSWA), which fundamentally mitigates global model
redundancy. PSWA achieves intermediate global-local information interaction
through window attention, while employing a high-frequency bridging branch to
simulate shifted window operations, supplementing appropriate global and
high-frequency information. Furthermore, we propose the Progressive Coverage
Channel Allocation(PCCA) strategy that captures high-order attention similarity
without additional computational cost. Building upon all of them, we propose a
series of Pseudo \textbf{S}hifted \textbf{Win}dow DiTs (\textbf{Swin DiT}),
accompanied by extensive experiments demonstrating their superior performance.
For example, our proposed Swin-DiT-L achieves a 54%$\uparrow$ FID improvement
over DiT-XL/2 while requiring less computational.
https://github.com/wujiafu007/Swin-DiT

</details>


### [370] [Automatic Complementary Separation Pruning Toward Lightweight CNNs](https://arxiv.org/abs/2505.13225)
*David Levin,Gonen Singer*

Main category: cs.CV

TL;DR: ACSP is an automated pruning method for CNNs combining structured and activation-based pruning, optimizing component removal while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To create a fully automated, efficient pruning method that reduces redundancy and computational costs without manual intervention.

Method: ACSP integrates structured and activation-based pruning, uses a graph space for component separation, and employs clustering and knee-finding algorithms.

Result: ACSP achieves competitive accuracy on VGG-16, ResNet-50, and MobileNet-V2 across datasets like CIFAR and ImageNet, reducing computational costs.

Conclusion: ACSP is a scalable, practical solution for real-world deployment, eliminating the need for manual pruning volume definition.

Abstract: In this paper, we present Automatic Complementary Separation Pruning (ACSP),
a novel and fully automated pruning method for convolutional neural networks.
ACSP integrates the strengths of both structured pruning and activation-based
pruning, enabling the efficient removal of entire components such as neurons
and channels while leveraging activations to identify and retain the most
relevant components. Our approach is designed specifically for supervised
learning tasks, where we construct a graph space that encodes the separation
capabilities of each component with respect to all class pairs. By employing
complementary selection principles and utilizing a clustering algorithm, ACSP
ensures that the selected components maintain diverse and complementary
separation capabilities, reducing redundancy and maintaining high network
performance. The method automatically determines the optimal subset of
components in each layer, utilizing a knee-finding algorithm to select the
minimal subset that preserves performance without requiring user-defined
pruning volumes. Extensive experiments on multiple architectures, including
VGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,
and ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared
to other methods while significantly reducing computational costs. This fully
automated approach not only enhances scalability but also makes ACSP especially
practical for real-world deployment by eliminating the need for manually
defining the pruning volume.

</details>


### [371] [From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection](https://arxiv.org/abs/2505.13233)
*Lincan Cai,Jingxuan Kang,Shuang Li,Wenxuan Ma,Binhui Xie,Zhida Qin,Jian Liang*

Main category: cs.CV

TL;DR: The paper proposes ABS, an attention-based selection method for VLMs to improve zero-shot performance by addressing issues from random augmentations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Random visual augmentations in VLMs can introduce artifacts and over-emphasize local details, harming global semantic understanding.

Method: ABS uses attention-guided cropping in images and feature space, plus soft matching for better LLM description alignment.

Result: ABS achieves top performance in out-of-distribution generalization and zero-shot tasks, rivaling few-shot methods.

Conclusion: ABS is a training-free solution that enhances VLMs by balancing local and global semantics, outperforming existing methods.

Abstract: Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive
zero-shot capabilities on downstream tasks. Prior research highlights the
crucial role of visual augmentation techniques, like random cropping, in
alignment with fine-grained class descriptions generated by large language
models (LLMs), significantly enhancing zero-shot performance by incorporating
multi-view information. However, the inherent randomness of these augmentations
can inevitably introduce background artifacts and cause models to overly focus
on local details, compromising global semantic understanding. To address these
issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election
(\textbf{ABS}) method from local details to global context, which applies
attention-guided cropping in both raw images and feature space, supplement
global semantic information through strategic feature selection. Additionally,
we introduce a soft matching technique to effectively filter LLM descriptions
for better alignment. \textbf{ABS} achieves state-of-the-art performance on
out-of-distribution generalization and zero-shot classification tasks. Notably,
\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation
methods. Our code is available at
\href{https://github.com/BIT-DA/ABS}{\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.

</details>


### [372] [WriteViT: Handwritten Text Generation with Vision Transformer](https://arxiv.org/abs/2505.13235)
*Dang Hoai Nam,Huynh Tong Dang Khoa,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: WriteViT is a one-shot handwritten text synthesis framework using Vision Transformers (ViT) to generalize handwriting styles, especially effective for Vietnamese and English in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Humans generalize handwriting styles easily, but machines struggle, especially with subtle cues in low-data scenarios. This gap inspired WriteViT.

Method: WriteViT uses a ViT-based Writer Identifier for style embeddings, a multi-scale generator with Transformer encoder-decoder blocks and conditional positional encoding (CPE), and a lightweight ViT-based recognizer.

Result: WriteViT produces high-quality, style-consistent handwriting and maintains strong recognition performance, especially for Vietnamese and English.

Conclusion: Transformer-based designs like WriteViT show promise for multilingual handwriting generation and efficient style adaptation.

Abstract: Humans can quickly generalize handwriting styles from a single example by
intuitively separating content from style. Machines, however, struggle with
this task, especially in low-data settings, often missing subtle spatial and
stylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot
handwritten text synthesis framework that incorporates Vision Transformers
(ViT), a family of models that have shown strong performance across various
computer vision tasks. WriteViT integrates a ViT-based Writer Identifier for
extracting style embeddings, a multi-scale generator built with Transformer
encoder-decoder blocks enhanced by conditional positional encoding (CPE), and a
lightweight ViT-based recognizer. While previous methods typically rely on CNNs
or CRNNs, our design leverages transformers in key components to better capture
both fine-grained stroke details and higher-level style information. Although
handwritten text synthesis has been widely explored, its application to
Vietnamese -- a language rich in diacritics and complex typography -- remains
limited. Experiments on Vietnamese and English datasets demonstrate that
WriteViT produces high-quality, style-consistent handwriting while maintaining
strong recognition performance in low-resource scenarios. These results
highlight the promise of transformer-based designs for multilingual handwriting
generation and efficient style adaptation.

</details>


### [373] [Joint Depth and Reflectivity Estimation using Single-Photon LiDAR](https://arxiv.org/abs/2505.13250)
*Hashan K. Weerasooriya,Prateek Chennuri,Weijian Zhang,Istvan Gyongy,Stanley H. Chan*

Main category: cs.CV

TL;DR: The paper introduces a joint estimation method for depth and reflectivity in SP-LiDAR, outperforming existing approaches in dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: Existing SP-LiDAR methods recover depth and reflectivity separately or sequentially, and conventional 3D histograms are ineffective for dynamic scenes.

Method: Proposes 'SPLiDER', a novel reconstruction method that jointly estimates depth and reflectivity by exploiting their mutual correlation.

Result: Outperforms existing methods on synthetic and real SP-LiDAR data, achieving superior joint reconstruction quality.

Conclusion: Joint estimation of depth and reflectivity is beneficial in dynamic scenes, and SPLiDER effectively enhances signal recovery.

Abstract: Single-Photon Light Detection and Ranging (SP-LiDAR is emerging as a leading
technology for long-range, high-precision 3D vision tasks. In SP-LiDAR,
timestamps encode two complementary pieces of information: pulse travel time
(depth) and the number of photons reflected by the object (reflectivity).
Existing SP-LiDAR reconstruction methods typically recover depth and
reflectivity separately or sequentially use one modality to estimate the other.
Moreover, the conventional 3D histogram construction is effective mainly for
slow-moving or stationary scenes. In dynamic scenes, however, it is more
efficient and effective to directly process the timestamps. In this paper, we
introduce an estimation method to simultaneously recover both depth and
reflectivity in fast-moving scenes. We offer two contributions: (1) A
theoretical analysis demonstrating the mutual correlation between depth and
reflectivity and the conditions under which joint estimation becomes
beneficial. (2) A novel reconstruction method, "SPLiDER", which exploits the
shared information to enhance signal recovery. On both synthetic and real
SP-LiDAR data, our method outperforms existing approaches, achieving superior
joint reconstruction quality.

</details>


### [374] [Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning](https://arxiv.org/abs/2505.13261)
*Mingrui Chen,Haogeng Liu,Hao Liang,Huaibo Huang,Wentao Zhang,Ran He*

Main category: cs.CV

TL;DR: The paper explores how modeling problem difficulty improves reinforcement learning fine-tuning for multimodal reasoning, using offline data curation, online advantage differentiation, and difficulty hints to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To understand how prior knowledge of problem difficulty shapes the effectiveness of reinforcement learning in fine-tuning for multimodal reasoning tasks.

Method: 1. Offline data curation to filter prompts by difficulty. 2. Online advantage differentiation for adaptive learning. 3. Difficulty hints in the second training stage for complex samples.

Result: Significant performance improvements on multimodal mathematical reasoning benchmarks with minimal training data (2K+0.6K).

Conclusion: Explicitly modeling difficulty enhances reinforcement learning fine-tuning, leading to better performance in multimodal reasoning tasks.

Abstract: In this work, we investigate how explicitly modeling problem's difficulty
prior information shapes the effectiveness of reinforcement learning based
fine-tuning for multimodal reasoning. Our exploration mainly comprises of
following three perspective: First, through offline data curation, we analyze
the U-shaped difficulty distribution of two given datasets using the base model
by multi-round sampling, and then filter out prompts that are either too simple
or extremely difficult to provide meaningful gradients and perform subsequent
two-stage training. Second, we implement an online advantage differentiation,
computing group-wise empirical accuracy as a difficulty proxy to adaptively
reweight advantages estimation, providing stronger learning signals for more
challenging problems. Finally, we introduce difficulty hints as explicit
prompts for more complex samples in the second training stage, encouraging the
model to calibrate its reasoning depth and perform reflective validation
checks. Our comprehensive approach demonstrates significant performances across
various multi-modal mathematical reasoning benchmarks with only 2K+0.6K
two-stage training data.

</details>


### [375] [DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection](https://arxiv.org/abs/2505.13266)
*Yehao Liu,Xiaosu Xu,Zijian Wang,Yiqing Yao*

Main category: cs.CV

TL;DR: A depth-aware BEV feature transformation method for 3D lane detection, integrating depth estimation to improve accuracy without relying on flat ground assumptions.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of traditional BEV feature construction from FV images, which lack depth and rely on flat ground assumptions, by leveraging monocular depth estimation.

Method: Proposes a depth-aware BEV feature transformation with modules for feature extraction (Depth Net), height dimension reduction, and fusion of FV and depth features.

Result: Achieves comparable performance to state-of-the-art methods on Apollo and OpenLane datasets.

Conclusion: The method effectively integrates depth estimation into BEV feature construction, enhancing 3D lane detection accuracy.

Abstract: 3D Lane detection plays an important role in autonomous driving. Recent
advances primarily build Birds-Eye-View (BEV) feature from front-view (FV)
images to perceive 3D information of Lane more effectively. However,
constructing accurate BEV information from FV image is limited due to the
lacking of depth information, causing previous works often rely heavily on the
assumption of a flat ground plane. Leveraging monocular depth estimation to
assist in constructing BEV features is less constrained, but existing methods
struggle to effectively integrate the two tasks. To address the above issue, in
this paper, an accurate 3D lane detection method based on depth-aware BEV
feature transtormation is proposed. In detail, an effective feature extraction
module is designed, in which a Depth Net is integrated to obtain the vital
depth information for 3D perception, thereby simplifying the complexity of view
transformation. Subquently a feature reduce module is proposed to reduce height
dimension of FV features and depth features, thereby enables effective fusion
of crucial FV features and depth features. Then a fusion module is designed to
build BEV feature from prime FV feature and depth information. The proposed
method performs comparably with state-of-the-art methods on both synthetic
Apollo, realistic OpenLane datasets.

</details>


### [376] [Event-Driven Dynamic Scene Depth Completion](https://arxiv.org/abs/2505.13279)
*Zhiqiang Yan,Jianhao Jiao,Zhengxue Wang,Gim Hee Lee*

Main category: cs.CV

TL;DR: EventDC is a novel event-driven depth completion framework for dynamic scenes, using event cameras to improve alignment and depth estimation.


<details>
  <summary>Details</summary>
Motivation: Depth completion in dynamic scenes is challenging due to motion-induced degradation of RGB and LiDAR inputs. Event cameras offer high temporal resolution and motion sensitivity, providing complementary data.

Method: EventDC includes Event-Modulated Alignment (EMA) for feature alignment and Local Depth Filtering (LDF) for depth refinement, both leveraging event data. It also uses two loss terms for global alignment and local depth recovery.

Result: EventDC outperforms existing methods, demonstrated through extensive experiments on a new benchmark of real-world and synthetic datasets.

Conclusion: EventDC effectively addresses depth completion in dynamic scenes, setting a new standard and enabling future research with its benchmark.

Abstract: Depth completion in dynamic scenes poses significant challenges due to rapid
ego-motion and object motion, which can severely degrade the quality of input
modalities such as RGB images and LiDAR measurements. Conventional RGB-D
sensors often struggle to align precisely and capture reliable depth under such
conditions. In contrast, event cameras with their high temporal resolution and
sensitivity to motion at the pixel level provide complementary cues that are
%particularly beneficial in dynamic environments.To this end, we propose
EventDC, the first event-driven depth completion framework. It consists of two
key components: Event-Modulated Alignment (EMA) and Local Depth Filtering
(LDF). Both modules adaptively learn the two fundamental components of
convolution operations: offsets and weights conditioned on motion-sensitive
event streams. In the encoder, EMA leverages events to modulate the sampling
positions of RGB-D features to achieve pixel redistribution for improved
alignment and fusion. In the decoder, LDF refines depth estimations around
moving objects by learning motion-aware masks from events. Additionally,
EventDC incorporates two loss terms to further benefit global alignment and
enhance local depth recovery. Moreover, we establish the first benchmark for
event-based depth completion comprising one real-world and two synthetic
datasets to facilitate future research. Extensive experiments on this benchmark
demonstrate the superiority of our EventDC.

</details>


### [377] [Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts](https://arxiv.org/abs/2505.13281)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TL;DR: The paper investigates whether geometric and topological (GT) concepts in humans are innate or learned, using computer vision models. Transformer-based models align closely with human performance, while vision-language models underperform and deviate from human profiles.


<details>
  <summary>Details</summary>
Motivation: To explore if GT concepts are learned through everyday interaction rather than being innate, using ML models as a testbed.

Method: Evaluated three types of models (CNNs, transformers, vision-language) on an odd-one-out task testing 43 GT concepts.

Result: Transformer models outperformed young children and aligned with human performance, while vision-language models underperformed and misaligned.

Conclusion: Supports the learning account for GT concepts and highlights potential drawbacks of naive multimodality in models.

Abstract: With the rapid improvement of machine learning (ML) models, cognitive
scientists are increasingly asking about their alignment with how humans think.
Here, we ask this question for computer vision models and human sensitivity to
geometric and topological (GT) concepts. Under the core knowledge account,
these concepts are innate and supported by dedicated neural circuitry. In this
work, we investigate an alternative explanation, that GT concepts are learned
``for free'' through everyday interaction with the environment. We do so using
computer visions models, which are trained on large image datasets. We build on
prior studies to investigate the overall performance and human alignment of
three classes of models -- convolutional neural networks (CNNs),
transformer-based models, and vision-language models -- on an odd-one-out task
testing 43 GT concepts spanning seven classes. Transformer-based models achieve
the highest overall accuracy, surpassing that of young children. They also show
strong alignment with children's performance, finding the same classes of
concepts easy vs. difficult. By contrast, vision-language models underperform
their vision-only counterparts and deviate further from human profiles,
indicating that na\"ive multimodality might compromise abstract geometric
sensitivity. These findings support the use of computer vision models to
evaluate the sufficiency of the learning account for explaining human
sensitivity to GT concepts, while also suggesting that integrating linguistic
and visual representations might have unpredicted deleterious consequences.

</details>


### [378] [DD-Ranking: Rethinking the Evaluation of Dataset Distillation](https://arxiv.org/abs/2505.13300)
*Zekai Li,Xinhao Zhong,Samir Khaki,Zhiyuan Liang,Yuhao Zhou,Mingjia Shi,Ziqiao Wang,Xuanlei Zhao,Wangbo Zhao,Ziheng Qin,Mengxuan Wu,Pengfei Zhou,Haonan Wang,David Junhao Zhang,Jia-Wei Liu,Shaobo Wang,Dai Liu,Linfeng Zhang,Guang Li,Kun Wang,Zheng Zhu,Zhiheng Ma,Joey Tianyi Zhou,Jiancheng Lv,Yaochu Jin,Peihao Wang,Kaipeng Zhang,Lingjuan Lyu,Yiran Huang,Zeynep Akata,Zhiwei Deng,Xindi Wu,George Cazenavette,Yuzhang Shang,Justin Cui,Jindong Gu,Qian Zheng,Hao Ye,Shuo Wang,Xiaobo Wang,Yan Yan,Angela Yao,Mike Zheng Shou,Tianlong Chen,Hakan Bilen,Baharan Mirzasoleiman,Manolis Kellis,Konstantinos N. Plataniotis,Zhangyang Wang,Bo Zhao,Yang You,Kai Wang*

Main category: cs.CV

TL;DR: The paper critiques current evaluation metrics for dataset distillation (DD) methods, proposing DD-Ranking as a unified framework with new metrics for fairer assessment.


<details>
  <summary>Details</summary>
Motivation: Current DD evaluation relies on accuracy, which may misrepresent performance due to auxiliary techniques, not the distilled dataset's inherent quality.

Method: Introduces DD-Ranking, a framework with new metrics to evaluate true performance improvements in DD methods.

Result: Empirical findings show current metrics are misleading; DD-Ranking provides a fairer standard.

Conclusion: DD-Ranking offers a more comprehensive and fair evaluation for future DD research.

Abstract: In recent years, dataset distillation has provided a reliable solution for
data compression, where models trained on the resulting smaller synthetic
datasets achieve performance comparable to those trained on the original
datasets. To further improve the performance of synthetic datasets, various
training pipelines and optimization objectives have been proposed, greatly
advancing the field of dataset distillation. Recent decoupled dataset
distillation methods introduce soft labels and stronger data augmentation
during the post-evaluation phase and scale dataset distillation up to larger
datasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy
still a reliable metric to fairly evaluate dataset distillation methods? Our
empirical findings suggest that the performance improvements of these methods
often stem from additional techniques rather than the inherent quality of the
images themselves, with even randomly sampled images achieving superior
results. Such misaligned evaluation settings severely hinder the development of
DD. Therefore, we propose DD-Ranking, a unified evaluation framework, along
with new general evaluation metrics to uncover the true performance
improvements achieved by different methods. By refocusing on the actual
information enhancement of distilled datasets, DD-Ranking provides a more
comprehensive and fair evaluation standard for future research advancements.

</details>


### [379] [GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval](https://arxiv.org/abs/2505.13306)
*Chengsong Sun,Weiping Li,Xiang Li,Yuankun Liu,Lianlei Shan*

Main category: cs.CV

TL;DR: The paper proposes GCRDP, a method for few-shot cross-modal retrieval, addressing biases in latent semantic space using GMM and contrastive learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Few-shot cross-modal retrieval struggles with sparse data and biases (intra-modal and inter-modal), hindering accuracy.

Method: GCRDP uses Gaussian Mixture Model (GMM) for multi-peak distribution and multi-positive sample contrastive learning, with a new cross-modal alignment strategy.

Result: Superior performance on four benchmark datasets compared to six state-of-the-art methods.

Conclusion: GCRDP effectively addresses biases and improves cross-modal retrieval accuracy in few-shot settings.

Abstract: Few-shot cross-modal retrieval focuses on learning cross-modal
representations with limited training samples, enabling the model to handle
unseen classes during inference. Unlike traditional cross-modal retrieval
tasks, which assume that both training and testing data share the same class
distribution, few-shot retrieval involves data with sparse representations
across modalities. Existing methods often fail to adequately model the
multi-peak distribution of few-shot cross-modal data, resulting in two main
biases in the latent semantic space: intra-modal bias, where sparse samples
fail to capture intra-class diversity, and inter-modal bias, where
misalignments between image and text distributions exacerbate the semantic gap.
These biases hinder retrieval accuracy. To address these issues, we propose a
novel method, GCRDP, for few-shot cross-modal retrieval. This approach
effectively captures the complex multi-peak distribution of data using a
Gaussian Mixture Model (GMM) and incorporates a multi-positive sample
contrastive learning mechanism for comprehensive feature modeling.
Additionally, we introduce a new strategy for cross-modal semantic alignment,
which constrains the relative distances between image and text feature
distributions, thereby improving the accuracy of cross-modal representations.
We validate our approach through extensive experiments on four benchmark
datasets, demonstrating superior performance over six state-of-the-art methods.

</details>


### [380] [eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks](https://arxiv.org/abs/2505.13309)
*Jad Mansour,Sebastian Realpe,Hayat Rajani,Michele Grimaldi,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: The paper introduces eStonefish-scenes, a synthetic event-based optical flow dataset for underwater applications, addressing the lack of diverse real-world datasets. It also provides a data generation pipeline and eWiz, a library for processing event-based data.


<details>
  <summary>Details</summary>
Motivation: Existing event-based datasets for optical flow are limited in diversity and scalability, especially for underwater applications, hindering the use of event-based vision with AUVs.

Method: The authors create eStonefish-scenes using the Stonefish simulator, offering a customizable underwater environment generation pipeline and a scene generator for realistic reef seabeds. They also develop eWiz, a library for event-based data processing.

Result: The work provides a scalable synthetic dataset and tools (eWiz) for event-based data handling, enabling dynamic underwater scenarios like fish schools and obstacle avoidance.

Conclusion: The synthetic dataset and tools bridge the gap in underwater event-based vision research, facilitating advancements in robotics and AUV applications.

Abstract: The combined use of event-based vision and Spiking Neural Networks (SNNs) is
expected to significantly impact robotics, particularly in tasks like visual
odometry and obstacle avoidance. While existing real-world event-based datasets
for optical flow prediction, typically captured with Unmanned Aerial Vehicles
(UAVs), offer valuable insights, they are limited in diversity, scalability,
and are challenging to collect. Moreover, there is a notable lack of labelled
datasets for underwater applications, which hinders the integration of
event-based vision with Autonomous Underwater Vehicles (AUVs). To address this,
synthetic datasets could provide a scalable solution while bridging the gap
between simulation and reality. In this work, we introduce eStonefish-scenes, a
synthetic event-based optical flow dataset based on the Stonefish simulator.
Along with the dataset, we present a data generation pipeline that enables the
creation of customizable underwater environments. This pipeline allows for
simulating dynamic scenarios, such as biologically inspired schools of fish
exhibiting realistic motion patterns, including obstacle avoidance and reactive
navigation around corals. Additionally, we introduce a scene generator that can
build realistic reef seabeds by randomly distributing coral across the terrain.
To streamline data accessibility, we present eWiz, a comprehensive library
designed for processing event-based data, offering tools for data loading,
augmentation, visualization, encoding, and training data generation, along with
loss functions and performance metrics.

</details>


### [381] [Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates](https://arxiv.org/abs/2505.13316)
*Gabriele Spadaro,Alberto Presta,Jhony H. Giraldo,Marco Grangetto,Wei Hu,Giuseppe Valenzise,Attilio Fiandrotti,Enzo Tartaglione*

Main category: cs.CV

TL;DR: The paper proposes a DDPM-based method (DDPM-PCC) for efficient low-bit-rate point cloud compression, outperforming existing techniques in rate-distortion performance.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud compression methods focus on high-fidelity reconstruction, requiring high bit-rates, which is inefficient for bandwidth-constrained applications.

Method: The authors use a Denoising Diffusion Probabilistic Model (DDPM) with a PointNet encoder and learnable vector quantizer to achieve low bit-rates while preserving quality.

Result: Experiments on ShapeNet and ModelNet40 show improved rate-distortion performance at low bit-rates compared to standard and state-of-the-art methods.

Conclusion: DDPM-PCC is an effective solution for low-bit-rate point cloud compression, with publicly released code for further research.

Abstract: Efficient compression of low-bit-rate point clouds is critical for
bandwidth-constrained applications. However, existing techniques mainly focus
on high-fidelity reconstruction, requiring many bits for compression. This
paper proposes a "Denoising Diffusion Probabilistic Model" (DDPM) architecture
for point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder
produces the condition vector for the generation, which is then quantized via a
learnable vector quantizer. This configuration allows to achieve a low bitrates
while preserving quality. Experiments on ShapeNet and ModelNet40 show improved
rate-distortion at low rates compared to standardized and state-of-the-art
approaches. We publicly released the code at
https://github.com/EIDOSLAB/DDPM-PCC.

</details>


### [382] [VesselGPT: Autoregressive Modeling of Vascular Geometry](https://arxiv.org/abs/2505.13318)
*Paula Feldman,Martin Sinnona,Viviana Siless,Claudio Delrieux,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: The paper introduces an autoregressive method using VQ-VAE and GPT-2 to synthesize anatomical trees, achieving high-fidelity reconstruction and preserving morphological details.


<details>
  <summary>Details</summary>
Motivation: Anatomical trees' complex geometry makes accurate representation challenging, and the paper leverages large language models to address this.

Method: The approach embeds vessel structures into a discrete vocabulary with VQ-VAE and models generation autoregressively with GPT-2, using B-spline for cross-sections.

Result: The method captures intricate geometries and branching patterns, enabling realistic vascular tree synthesis with high-fidelity reconstruction.

Conclusion: This is the first autoregressive method for blood vessel generation, with code, data, and models to be made available.

Abstract: Anatomical trees are critical for clinical diagnosis and treatment planning,
yet their complex and diverse geometry make accurate representation a
significant challenge. Motivated by the latest advances in large language
models, we introduce an autoregressive method for synthesizing anatomical
trees. Our approach first embeds vessel structures into a learned discrete
vocabulary using a VQ-VAE architecture, then models their generation
autoregressively with a GPT-2 model. This method effectively captures intricate
geometries and branching patterns, enabling realistic vascular tree synthesis.
Comprehensive qualitative and quantitative evaluations reveal that our
technique achieves high-fidelity tree reconstruction with compact discrete
representations. Moreover, our B-spline representation of vessel cross-sections
preserves critical morphological details that are often overlooked in previous'
methods parameterizations. To the best of our knowledge, this work is the first
to generate blood vessels in an autoregressive manner. Code, data, and trained
models will be made available.

</details>


### [383] [Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning](https://arxiv.org/abs/2505.13327)
*Ajian Liu,Haocheng Yuan,Xiao Guo,Hui Ma,Wanyi Zhuang,Changtao Miao,Yan Hong,Chuanbiao Song,Jun Lan,Qi Chu,Tao Gong,Yanyan Liang,Weiqiang Wang,Jun Wan,Xiaoming Liu,Zhen Lei*

Main category: cs.CV

TL;DR: The paper proposes a unified approach (HiPTune) and dataset (UniAttackData+) to detect both physical and digital face attacks, addressing limitations in current methods and datasets.


<details>
  <summary>Details</summary>
Motivation: Current face attack detection models are trained separately for physical and digital attacks, making them vulnerable to unknown threats and complicating deployment. A unified solution is needed.

Method: Introduces UniAttackData+, a comprehensive dataset, and HiPTune, a framework using hierarchical prompt tuning and dynamic prompt integration for adaptive classification.

Result: Experiments on 12 datasets demonstrate the framework's effectiveness, inspiring further research in unified attack detection.

Conclusion: The proposed unified model and dataset address gaps in face attack detection, offering a robust solution for diverse threats.

Abstract: Presentation Attack Detection and Face Forgery Detection are designed to
protect face data from physical media-based Presentation Attacks and digital
editing-based DeepFakes respectively. But separate training of these two models
makes them vulnerable to unknown attacks and burdens deployment environments.
The lack of a Unified Face Attack Detection model to handle both types of
attacks is mainly due to two factors. First, there's a lack of adequate
benchmarks for models to explore. Existing UAD datasets have limited attack
types and samples, restricting the model's ability to address advanced threats.
To address this, we propose UniAttackDataPlus (UniAttackData+), the most
extensive and sophisticated collection of forgery techniques to date. It
includes 2,875 identities and their 54 kinds of falsified samples, totaling
697,347 videos. Second, there's a lack of a reliable classification criterion.
Current methods try to find an arbitrary criterion within the same semantic
space, which fails when encountering diverse attacks. So, we present a novel
Visual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that
adaptively explores multiple classification criteria from different semantic
spaces. We build a Visual Prompt Tree to explore various classification rules
hierarchically. Then, by adaptively pruning the prompts, the model can select
the most suitable prompts to guide the encoder to extract discriminative
features at different levels in a coarse-to-fine way. Finally, to help the
model understand the classification criteria in visual space, we propose a
Dynamically Prompt Integration module to project the visual prompts to the text
encoder for more accurate semantics. Experiments on 12 datasets have shown the
potential to inspire further innovations in the UAD field.

</details>


### [384] [RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers](https://arxiv.org/abs/2505.13344)
*Ahmet Berke Gokmen,Yigit Ekin,Bahri Batuhan Bilecen,Aysegul Dundar*

Main category: cs.CV

TL;DR: RoPECraft is a training-free video motion transfer method for diffusion transformers, modifying rotary positional embeddings (RoPE) to encode motion, outperforming recent methods.


<details>
  <summary>Details</summary>
Motivation: To enable motion transfer in videos without training, leveraging RoPE modifications for efficient and high-quality results.

Method: Extracts optical flow from a reference video, warps RoPE tensors, optimizes embeddings via flow-matching, and regularizes using Fourier phase components.

Result: Outperforms recent methods qualitatively and quantitatively on benchmarks.

Conclusion: RoPECraft effectively transfers motion in videos without training, achieving superior performance.

Abstract: We propose RoPECraft, a training-free video motion transfer method for
diffusion transformers that operates solely by modifying their rotary
positional embeddings (RoPE). We first extract dense optical flow from a
reference video, and utilize the resulting motion offsets to warp the
complex-exponential tensors of RoPE, effectively encoding motion into the
generation process. These embeddings are then further optimized during
denoising time steps via trajectory alignment between the predicted and target
velocities using a flow-matching objective. To keep the output faithful to the
text prompt and prevent duplicate generations, we incorporate a regularization
term based on the phase components of the reference video's Fourier transform,
projecting the phase angles onto a smooth manifold to suppress high-frequency
artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all
recently published methods, both qualitatively and quantitatively.

</details>


### [385] [Faster Video Diffusion with Trainable Sparse Attention](https://arxiv.org/abs/2505.13389)
*Peiyuan Zhang,Haofeng Huang,Yongqi Chen,Will Lin,Zhengzhong Liu,Ion Stoica,Eric P. Xing,Hao Zhang*

Main category: cs.CV

TL;DR: VSA introduces a trainable, hardware-efficient sparse attention method for video diffusion transformers, reducing computational costs without performance loss.


<details>
  <summary>Details</summary>
Motivation: Quadratic 3D attention in video diffusion transformers is computationally expensive, with most attention mass concentrated on few positions.

Method: VSA uses a two-stage approach: coarse pooling into tiles to identify critical tokens, and fine token-level attention within tiles, optimized for hardware efficiency.

Result: VSA reduces training FLOPS by 2.53×, speeds up attention by 6×, and lowers generation time from 31s to 18s with comparable quality.

Conclusion: Trainable sparse attention (VSA) is a practical alternative to full attention, enabling further scaling of video diffusion models.

Abstract: Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models.

</details>


### [386] [FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning](https://arxiv.org/abs/2505.13419)
*Zhuozhao Hu,Kaishen Yuan,Xin Liu,Zitong Yu,Yuan Zong,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: The paper introduces FEALLM, a Multimodal Large Language Model (MLLM) for Facial Emotion Analysis (FEA), addressing challenges like interpretability and generalization. It includes a new dataset (FEA Instruction Dataset) and benchmark (FEABench) to improve FEA performance.


<details>
  <summary>Details</summary>
Motivation: Traditional FEA methods lack interpretability and generalization. MLLMs show promise but struggle with FEA due to dataset limitations and inability to model relationships between facial expressions (FEs) and action units (AUs).

Method: The authors create a specialized FEA Instruction Dataset and FEABench benchmark. They propose FEALLM, an MLLM architecture designed to capture detailed facial information and model FE-AU relationships.

Result: FEALLM performs strongly on FEABench and generalizes well in zero-shot evaluations on datasets like RAF-DB, AffectNet, BP4D, and DISFA.

Conclusion: FEALLM and the new dataset/benchmark enhance FEA by improving interpretability, generalization, and performance, making it a robust solution for affective computing.

Abstract: Facial Emotion Analysis (FEA) plays a crucial role in visual affective
computing, aiming to infer a person's emotional state based on facial data.
Scientifically, facial expressions (FEs) result from the coordinated movement
of facial muscles, which can be decomposed into specific action units (AUs)
that provide detailed emotional insights. However, traditional methods often
struggle with limited interpretability, constrained generalization and
reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have
shown exceptional performance in various visual tasks, while they still face
significant challenges in FEA due to the lack of specialized datasets and their
inability to capture the intricate relationships between FEs and AUs. To
address these issues, we introduce a novel FEA Instruction Dataset that
provides accurate and aligned FE and AU descriptions and establishes causal
reasoning relationships between them, followed by constructing a new benchmark,
FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to
capture more detailed facial information, enhancing its capability in FEA
tasks. Our model demonstrates strong performance on FEABench and impressive
generalization capability through zero-shot evaluation on various datasets,
including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and
effectiveness in FEA tasks. The dataset and code will be available at
https://github.com/953206211/FEALLM.

</details>


### [387] [G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning](https://arxiv.org/abs/2505.13426)
*Liang Chen,Hongcheng Gao,Tianyu Liu,Zhiqi Huang,Flood Sung,Xinyu Zhou,Yuxin Wu,Baobao Chang*

Main category: cs.CV

TL;DR: VLM-Gym is introduced to bridge the gap between Vision-Language Models' (VLMs) multimodal capabilities and decision-making in interactive environments. G1 models, trained with perception-enhanced RL, outperform proprietary models like Claude-3.7-Sonnet-Thinking.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle to apply their multimodal skills in decision-making tasks, limiting their use as autonomous agents in interactive environments like games.

Method: Developed VLM-Gym, a scalable RL environment for training VLMs. G0 models use pure RL, while G1 models incorporate perception-enhanced cold starts before RL fine-tuning.

Result: G1 models consistently outperform G0 and proprietary models, with perception and reasoning abilities mutually improving during RL training.

Conclusion: VLM-Gym and G1 models demonstrate the potential of VLMs as interactive agents, with open-source release to encourage further research.

Abstract: Vision-Language Models (VLMs) excel in many direct multimodal tasks but
struggle to translate this prowess into effective decision-making within
interactive, visually rich environments like games. This ``knowing-doing'' gap
significantly limits their potential as autonomous agents, as leading VLMs
often performing badly in simple games. To address this, we introduce VLM-Gym,
a curated reinforcement learning (RL) environment featuring diverse visual
games with unified interfaces and adjustable, compositional difficulty,
specifically designed for scalable multi-game parallel training. Leveraging
VLM-Gym, we train G0 models using pure RL-driven self-evolution, which
demonstrate emergent perception and reasoning patterns. To further mitigate
challenges arising from game diversity, we develop G1 models. G1 incorporates a
perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models
consistently surpass their teacher across all games and outperform leading
proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals
an intriguing finding: perception and reasoning abilities mutually bootstrap
each other throughout the RL training process. Source code including VLM-Gym
and RL training are released at https://github.com/chenllliang/G1 to foster
future research in advancing VLMs as capable interactive agents.

</details>


### [388] [Understanding Complexity in VideoQA via Visual Program Generation](https://arxiv.org/abs/2505.13429)
*Cristobal Eyzaguirre,Igor Vasiljevic,Achal Dave,Jiajun Wu,Rares Andrei Ambrus,Thomas Kollar,Juan Carlos Niebles,Pavel Tokmakov*

Main category: cs.CV

TL;DR: A data-driven method for analyzing query complexity in VideoQA, using code generation to predict question difficulty, outperforming human estimates and enabling automatic benchmark creation.


<details>
  <summary>Details</summary>
Motivation: Humans struggle to predict question difficulty for models; an automated approach is needed to improve benchmark design.

Method: Leverages code generation for VideoQA, using code complexity as a proxy for question difficulty, and proposes an algorithm to estimate complexity.

Result: The method correlates better with model performance than human estimates and generates a benchmark 1.9x harder than NExT-QA.

Conclusion: The approach effectively predicts question difficulty and scales to new models, enhancing benchmark design.

Abstract: We propose a data-driven approach to analyzing query complexity in Video
Question Answering (VideoQA). Previous efforts in benchmark design have relied
on human expertise to design challenging questions, yet we experimentally show
that humans struggle to predict which questions are difficult for machine
learning models. Our automatic approach leverages recent advances in code
generation for visual question answering, using the complexity of generated
code as a proxy for question difficulty. We demonstrate that this measure
correlates significantly better with model performance than human estimates. To
operationalize this insight, we propose an algorithm for estimating question
complexity from code. It identifies fine-grained primitives that correlate with
the hardest questions for any given set of models, making it easy to scale to
new approaches in the future. Finally, to further illustrate the utility of our
method, we extend it to automatically generate complex questions, constructing
a new benchmark that is 1.9 times harder than the popular NExT-QA.

</details>


### [389] [KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture](https://arxiv.org/abs/2505.13436)
*R. James Cotton*

Main category: cs.CV

TL;DR: The paper explores using imitation learning on a biomechanical model to infer inverse dynamics (e.g., joint torques, muscle activations) from movement data, demonstrating its potential for clinical movement analysis.


<details>
  <summary>Details</summary>
Motivation: To improve movement science and rehabilitation by enabling detailed analysis of movement impairments and interventions, including early detection of neurological conditions or fall risk.

Method: Applied imitation learning to a biomechanical model using a large dataset of movements from able-bodied and impaired individuals, focusing on accurate biomechanical modeling and clinical metrics.

Result: The imitation learning policy, KinTwin, accurately replicates kinematics and infers clinically meaningful joint torques and muscle activations.

Conclusion: Imitation learning can enable high-quality movement analysis for clinical practice, offering detailed insights into movement impairments.

Abstract: Broader access to high-quality movement analysis could greatly benefit
movement science and rehabilitation, such as allowing more detailed
characterization of movement impairments and responses to interventions, or
even enabling early detection of new neurological conditions or fall risk.
While emerging technologies are making it easier to capture kinematics with
biomechanical models, or how joint angles change over time, inferring the
underlying physics that give rise to these movements, including ground reaction
forces, joint torques, or even muscle activations, is still challenging. Here
we explore whether imitation learning applied to a biomechanical model from a
large dataset of movements from able-bodied and impaired individuals can learn
to compute these inverse dynamics. Although imitation learning in human pose
estimation has seen great interest in recent years, our work differences in
several ways: we focus on using an accurate biomechanical model instead of
models adopted for computer vision, we test it on a dataset that contains
participants with impaired movements, we reported detailed tracking metrics
relevant for the clinical measurement of movement including joint angles and
ground contact events, and finally we apply imitation learning to a
muscle-driven neuromusculoskeletal model. We show that our imitation learning
policy, KinTwin, can accurately replicate the kinematics of a wide range of
movements, including those with assistive devices or therapist assistance, and
that it can infer clinically meaningful differences in joint torques and muscle
activations. Our work demonstrates the potential for using imitation learning
to enable high-quality movement analysis in clinical practice.

</details>


### [390] [FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance](https://arxiv.org/abs/2505.13437)
*Dian Shao,Mingfei Shi,Shengda Xu,Haodong Chen,Yongle Huang,Binglu Wang*

Main category: cs.CV

TL;DR: FinePhys is a framework for generating fine-grained human actions by combining physics-based motion re-estimation with data-driven 3D poses, outperforming baselines in plausibility and naturalness.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with synthesizing physically plausible human actions, especially fine-grained ones like gymnastics routines.

Method: FinePhys estimates 2D poses, lifts them to 3D via in-context learning, and refines them using physics-based motion re-estimation governed by Euler-Lagrange equations.

Result: FinePhys outperforms baselines on FineGym subsets (FX-JUMP, FX-TURN, FX-SALTO), generating more natural and plausible actions.

Conclusion: FinePhys effectively bridges the gap in fine-grained human action generation by integrating physics and data-driven approaches.

Abstract: Despite significant advances in video generation, synthesizing physically
plausible human actions remains a persistent challenge, particularly in
modeling fine-grained semantics and complex temporal dynamics. For instance,
generating gymnastics routines such as "switch leap with 0.5 turn" poses
substantial difficulties for current methods, often yielding unsatisfactory
results. To bridge this gap, we propose FinePhys, a Fine-grained human action
generation framework that incorporates Physics to obtain effective skeletal
guidance. Specifically, FinePhys first estimates 2D poses in an online manner
and then performs 2D-to-3D dimension lifting via in-context learning. To
mitigate the instability and limited interpretability of purely data-driven 3D
poses, we further introduce a physics-based motion re-estimation module
governed by Euler-Lagrange equations, calculating joint accelerations via
bidirectional temporal updating. The physically predicted 3D poses are then
fused with data-driven ones, offering multi-scale 2D heatmap guidance for the
diffusion process. Evaluated on three fine-grained action subsets from FineGym
(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms
competitive baselines. Comprehensive qualitative results further demonstrate
FinePhys's ability to generate more natural and plausible fine-grained human
actions.

</details>


### [391] [VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2505.13439)
*Huawei Lin,Tong Geng,Zhaozhuo Xu,Weijie Zhao*

Main category: cs.CV

TL;DR: VTBench is introduced to evaluate visual tokenizers (VTs) in image generation, revealing continuous VAEs outperform discrete VTs in reconstruction and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack isolation of VT performance, leading to gaps in understanding their impact on AR models.

Method: VTBench systematically evaluates VTs across three tasks (Image Reconstruction, Detail Preservation, Text Preservation) using diverse scenarios and metrics.

Result: Continuous VAEs outperform discrete VTs in retaining spatial structure and semantic detail, while discrete VTs degrade reconstructions and lose fine-grained textures.

Conclusion: The study highlights the need for better discrete VTs and releases VTBench to support future research.

Abstract: Autoregressive (AR) models have recently shown strong performance in image
generation, where a critical component is the visual tokenizer (VT) that maps
continuous pixel inputs to discrete token sequences. The quality of the VT
largely defines the upper bound of AR model performance. However, current
discrete VTs fall significantly behind continuous variational autoencoders
(VAEs), leading to degraded image reconstructions and poor preservation of
details and text. Existing benchmarks focus on end-to-end generation quality,
without isolating VT performance. To address this gap, we introduce VTBench, a
comprehensive benchmark that systematically evaluates VTs across three core
tasks: Image Reconstruction, Detail Preservation, and Text Preservation, and
covers a diverse range of evaluation scenarios. We systematically assess
state-of-the-art VTs using a set of metrics to evaluate the quality of
reconstructed images. Our findings reveal that continuous VAEs produce superior
visual representations compared to discrete VTs, particularly in retaining
spatial structure and semantic detail. In contrast, the degraded
representations produced by discrete VTs often lead to distorted
reconstructions, loss of fine-grained textures, and failures in preserving text
and object integrity. Furthermore, we conduct experiments on GPT-4o image
generation and discuss its potential AR nature, offering new insights into the
role of visual tokenization. We release our benchmark and codebase publicly to
support further research and call on the community to develop strong,
general-purpose open-source VTs.

</details>


### [392] [Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos](https://arxiv.org/abs/2505.13440)
*Ruoyu Wang,Yi Ma,Shenghua Gao*

Main category: cs.CV

TL;DR: A two-stage method for novel view synthesis and reconstruction from uncalibrated data, avoiding reliance on camera parameters or priors. Stage 1 learns implicit scene representation, while Stage 2 refines it with explicit 3D Gaussian primitives.


<details>
  <summary>Details</summary>
Motivation: To enable self-supervised training on uncalibrated videos or multi-view images without needing camera parameters or geometric priors.

Method: Two-stage training: Stage 1 learns implicit scene and latent camera features; Stage 2 refines with explicit 3D Gaussian primitives and rendering/depth losses.

Result: Achieves high-quality novel view synthesis and accurate camera pose estimation without supervision.

Conclusion: The two-stage approach is effective, combining implicit learning and explicit refinement for robust 3D consistency.

Abstract: Currently almost all state-of-the-art novel view synthesis and reconstruction
models rely on calibrated cameras or additional geometric priors for training.
These prerequisites significantly limit their applicability to massive
uncalibrated data. To alleviate this requirement and unlock the potential for
self-supervised training on large-scale uncalibrated videos, we propose a novel
two-stage strategy to train a view synthesis model from only raw video frames
or multi-view images, without providing camera parameters or other priors. In
the first stage, we learn to reconstruct the scene implicitly in a latent space
without relying on any explicit 3D representation. Specifically, we predict
per-frame latent camera and scene context features, and employ a view synthesis
model as a proxy for explicit rendering. This pretraining stage substantially
reduces the optimization complexity and encourages the network to learn the
underlying 3D consistency in a self-supervised manner. The learned latent
camera and implicit scene representation have a large gap compared with the
real 3D world. To reduce this gap, we introduce the second stage training by
explicitly predicting 3D Gaussian primitives. We additionally apply explicit
Gaussian Splatting rendering loss and depth projection loss to align the
learned latent representations with physically grounded 3D geometry. In this
way, Stage 1 provides a strong initialization and Stage 2 enforces 3D
consistency - the two stages are complementary and mutually beneficial.
Extensive experiments demonstrate the effectiveness of our approach, achieving
high-quality novel view synthesis and accurate camera pose estimation, compared
to methods that employ supervision with calibration, pose, or depth
information. The code is available at https://github.com/Dwawayu/Pensieve.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [393] [MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation](https://arxiv.org/abs/2505.11797)
*Hancan Zhu,Jinhao Chen,Guanghua He*

Main category: eess.IV

TL;DR: MedVKAN integrates Mamba and KAN for efficient medical image segmentation, outperforming CNNs and Transformers with near-linear complexity and enhanced nonlinear expressiveness.


<details>
  <summary>Details</summary>
Motivation: Address limitations of CNNs (limited receptive fields) and Transformers (quadratic complexity) in medical image segmentation.

Method: Propose MedVKAN, combining Mamba (for long-range dependencies) and KAN (for nonlinear expressiveness), with EFC-KAN and VKAN modules.

Result: State-of-the-art performance on four datasets and second on one, validating efficiency and effectiveness.

Conclusion: MedVKAN offers a computationally efficient and innovative framework for medical image segmentation, leveraging Mamba and KAN.

Abstract: Medical image segmentation relies heavily on convolutional neural networks
(CNNs) and Transformer-based models. However, CNNs are constrained by limited
receptive fields, while Transformers suffer from scalability challenges due to
their quadratic computational complexity. To address these limitations, recent
advances have explored alternative architectures. The state-space model Mamba
offers near-linear complexity while capturing long-range dependencies, and the
Kolmogorov-Arnold Network (KAN) enhances nonlinear expressiveness by replacing
fixed activation functions with learnable ones. Building on these strengths, we
propose MedVKAN, an efficient feature extraction model integrating Mamba and
KAN. Specifically, we introduce the EFC-KAN module, which enhances KAN with
convolutional operations to improve local pixel interaction. We further design
the VKAN module, integrating Mamba with EFC-KAN as a replacement for
Transformer modules, significantly improving feature extraction. Extensive
experiments on five public medical image segmentation datasets show that
MedVKAN achieves state-of-the-art performance on four datasets and ranks second
on the remaining one. These results validate the potential of Mamba and KAN for
medical image segmentation while introducing an innovative and computationally
efficient feature extraction framework. The code is available at:
https://github.com/beginner-cjh/MedVKAN.

</details>


### [394] [Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy](https://arxiv.org/abs/2505.11832)
*Yuxiang Lai,Jike Zhong,Vanessa Su,Xiaofeng Yang*

Main category: eess.IV

TL;DR: The paper proposes an autoregressive model for predicting organ motion in radiotherapy, outperforming existing methods by capturing patient-specific motion patterns from 4D CT scans.


<details>
  <summary>Details</summary>
Motivation: Existing methods for organ motion prediction rely on PCA, which is limited by registration quality and struggles with periodic dynamics. The paper aims to improve prediction by leveraging autoregressive techniques from NLP.

Method: The approach reformulates organ motion prediction as an autoregressive process, using 4D CT scans to predict future motion phases based on prior patterns.

Result: The method outperforms benchmarks in predicting lung and heart motion, validated on datasets from 50 and 20 patients with over 1,300 CT phases.

Conclusion: The autoregressive model enhances pre-treatment planning, enabling more precise radiation delivery by better capturing motion dynamics.

Abstract: Radiotherapy often involves a prolonged treatment period. During this time,
patients may experience organ motion due to breathing and other physiological
factors. Predicting and modeling this motion before treatment is crucial for
ensuring precise radiation delivery. However, existing pre-treatment organ
motion prediction methods primarily rely on deformation analysis using
principal component analysis (PCA), which is highly dependent on registration
quality and struggles to capture periodic temporal dynamics for motion
modeling.In this paper, we observe that organ motion prediction closely
resembles an autoregressive process, a technique widely used in natural
language processing (NLP). Autoregressive models predict the next token based
on previous inputs, naturally aligning with our objective of predicting future
organ motion phases. Building on this insight, we reformulate organ motion
prediction as an autoregressive process to better capture patient-specific
motion patterns. Specifically, we acquire 4D CT scans for each patient before
treatment, with each sequence comprising multiple 3D CT phases. These phases
are fed into the autoregressive model to predict future phases based on prior
phase motion patterns. We evaluate our method on a real-world test set of 4D CT
scans from 50 patients who underwent radiotherapy at our institution and a
public dataset containing 4D CT scans from 20 patients (some with multiple
scans), totaling over 1,300 3D CT phases. The performance in predicting the
motion of the lung and heart surpasses existing benchmarks, demonstrating its
effectiveness in capturing motion dynamics from CT images. These results
highlight the potential of our method to improve pre-treatment planning in
radiotherapy, enabling more precise and adaptive radiation delivery.

</details>


### [395] [Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation](https://arxiv.org/abs/2505.11909)
*Pengfei Lyu,Pak-Hei Yeung,Xiaosheng Yu,Jing Xia,Jianning Chi,Chengdong Wu,Jagath C. Rajapakse*

Main category: eess.IV

TL;DR: LowBridge is a model-agnostic UDA framework for cross-modal medical image segmentation, leveraging shared low-level features like edges to adapt domains without supervision, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle cross-modal medical image segmentation by addressing domain shifts between modalities, focusing on unsupervised domain adaptation (UDA) to avoid costly labeled data.

Method: LowBridge trains a generative model to recover source images from edges, then a segmentation model on generated images. At test time, target edges generate source-style images for segmentation.

Result: Outperforms 11 UDA methods across datasets, showing robustness and model-agnostic flexibility.

Conclusion: LowBridge's simplicity and effectiveness suggest it can integrate with advanced models for future improvements in cross-modal segmentation.

Abstract: This paper addresses the task of cross-modal medical image segmentation by
exploring unsupervised domain adaptation (UDA) approaches. We propose a
model-agnostic UDA framework, LowBridge, which builds on a simple observation
that cross-modal images share some similar low-level features (e.g., edges) as
they are depicting the same structures. Specifically, we first train a
generative model to recover the source images from their edge features,
followed by training a segmentation model on the generated source images,
separately. At test time, edge features from the target images are input to the
pretrained generative model to generate source-style target domain images,
which are then segmented using the pretrained segmentation network. Despite its
simplicity, extensive experiments on various publicly available datasets
demonstrate that \proposed achieves state-of-the-art performance, outperforming
eleven existing UDA approaches under different settings. Notably, further
ablation studies show that \proposed is agnostic to different types of
generative and segmentation models, suggesting its potential to be seamlessly
plugged with the most advanced models to achieve even more outstanding results
in the future. The code is available at https://github.com/JoshuaLPF/LowBridge.

</details>


### [396] [Joint Manifold Learning and Optimal Transport for Dynamic Imaging](https://arxiv.org/abs/2505.11913)
*Sven Dummer,Puru Vaish,Christoph Brune*

Main category: eess.IV

TL;DR: The paper proposes integrating low-dimensionality assumptions of the image manifold with optimal transport (OT) regularization to improve dynamic imaging, addressing data scarcity and time-point limitations.


<details>
  <summary>Details</summary>
Motivation: Dynamic imaging faces challenges due to limited time-series data and time points, hindering pattern learning. Existing methods either ignore temporal priors or lack integration of multiple time series.

Method: A latent model representation of the image manifold is proposed, ensuring consistency with time-series data and OT priors. This combines low-dimensionality assumptions with OT regularization.

Result: The integration enhances OT interpolations with latent models and improves the use of OT priors in latent models, addressing data scarcity.

Conclusion: Combining low-dimensionality assumptions with OT regularization offers a robust solution for dynamic imaging, leveraging both temporal and multi-series information.

Abstract: Dynamic imaging is critical for understanding and visualizing dynamic
biological processes in medicine and cell biology. These applications often
encounter the challenge of a limited amount of time series data and time
points, which hinders learning meaningful patterns. Regularization methods
provide valuable prior knowledge to address this challenge, enabling the
extraction of relevant information despite the scarcity of time-series data and
time points. In particular, low-dimensionality assumptions on the image
manifold address sample scarcity, while time progression models, such as
optimal transport (OT), provide priors on image development to mitigate the
lack of time points. Existing approaches using low-dimensionality assumptions
disregard a temporal prior but leverage information from multiple time series.
OT-prior methods, however, incorporate the temporal prior but regularize only
individual time series, ignoring information from other time series of the same
image modality. In this work, we investigate the effect of integrating a
low-dimensionality assumption of the underlying image manifold with an OT
regularizer for time-evolving images. In particular, we propose a latent model
representation of the underlying image manifold and promote consistency between
this representation, the time series data, and the OT prior on the
time-evolving images. We discuss the advantages of enriching OT interpolations
with latent models and integrating OT priors into latent models.

</details>


### [397] [Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis](https://arxiv.org/abs/2505.12061)
*Samuel T. M. Ball*

Main category: eess.IV

TL;DR: Bayesian CNNs improve retinal OCT segmentation by providing uncertainty maps, enhancing clinical reliability and performance.


<details>
  <summary>Details</summary>
Motivation: Manual retinal layer delineation in OCT is time-consuming and biased. Existing deep learning lacks uncertainty estimation, risking 'confidently wrong' results.

Method: Bayesian convolutional neural networks (BCNNs) segment an OCT dataset (35 scans) with uncertainty maps for segmentation and layer thicknesses.

Result: Achieved 95.65% Dice score, outperforming prior work, and identified uncertain samples with artefacts.

Conclusion: BCNNs enhance clinical applicability, robustness, and performance in OCT segmentation.

Abstract: Optical Coherence Tomography (OCT) provides valuable insights in
ophthalmology, cardiology, and neurology due to high-resolution,
cross-sectional images of the retina. One critical task for ophthalmologists
using OCT is delineation of retinal layers within scans. This process is
time-consuming and prone to human bias, affecting the accuracy and reliability
of diagnoses. Previous efforts to automate delineation using deep learning face
challenges in uptake from clinicians and statisticians due to the absence of
uncertainty estimation, leading to "confidently wrong" models via
hallucinations. In this study, we address these challenges by applying Bayesian
convolutional neural networks (BCNNs) to segment an openly available OCT
imaging dataset containing 35 human retina OCTs split between healthy controls
and patients with multiple sclerosis. Our findings demonstrate that Bayesian
models can be used to provide uncertainty maps of the segmentation, which can
further be used to identify highly uncertain samples that exhibit recording
artefacts such as noise or miscalibration at inference time. Our method also
allows for uncertainty-estimation for important secondary measurements such as
layer thicknesses, that are medically relevant for patients. We show that these
features come in addition to greater performance compared to similar work over
all delineations; with an overall Dice score of 95.65%. Our work brings greater
clinical applicability, statistical robustness, and performance to retinal OCT
segmentation.

</details>


### [398] [NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results](https://arxiv.org/abs/2505.12089)
*Sangmin Lee,Eunpil Park,Angel Canelo,Hyunhee Park,Youngjo Kim,Hyung-Ju Chun,Xin Jin,Chongyi Li,Chun-Le Guo,Radu Timofte,Qi Wu,Tianheng Qiu,Yuchun Dong,Shenglin Ding,Guanghua Pan,Weiyu Zhou,Tao Hu,Yixu Feng,Duwei Dai,Yu Cao,Peng Wu,Wei Dong,Yanning Zhang,Qingsen Yan,Simon J. Larsen,Ruixuan Jiang,Senyan Xu,Xingbo Wang,Xin Lu,Marcos V. Conde,Javier Abad-Hernandez,Alvaro Garcıa-Lara,Daniel Feijoo,Alvaro Garcıa,Zeyu Xiao,Zhuoyuan Li*

Main category: eess.IV

TL;DR: The paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration Challenge, highlighting its goal to improve multi-frame HDR and restoration techniques using a novel RAW dataset. Top solutions met strict efficiency constraints, with the best achieving 43.22 dB PSNR.


<details>
  <summary>Details</summary>
Motivation: To advance efficient multi-frame HDR and restoration techniques by encouraging innovative solutions under strict computational constraints.

Method: Participants developed solutions for fusing noisy, misaligned RAW frames with varying exposures, adhering to limits of 30M parameters and 4.0T FLOPs.

Result: Six teams submitted valid solutions; the top approach achieved 43.22 dB PSNR, demonstrating the effectiveness of novel methods.

Conclusion: The challenge successfully fostered advancements in efficient burst HDR and restoration, providing a benchmark for future research.

Abstract: This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration
Challenge, which aims to advance efficient multi-frame high dynamic range (HDR)
and restoration techniques. The challenge is based on a novel RAW multi-frame
fusion dataset, comprising nine noisy and misaligned RAW frames with various
exposure levels per scene. Participants were tasked with developing solutions
capable of effectively fusing these frames while adhering to strict efficiency
constraints: fewer than 30 million model parameters and a computational budget
under 4.0 trillion FLOPs. A total of 217 participants registered, with six
teams finally submitting valid solutions. The top-performing approach achieved
a PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.
This paper provides a comprehensive overview of the challenge, compares the
proposed solutions, and serves as a valuable reference for researchers and
practitioners in efficient burst HDR and restoration.

</details>


### [399] [HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology](https://arxiv.org/abs/2505.12120)
*Dmitry Nechaev,Alexey Pchelnikov,Ekaterina Ivanova*

Main category: eess.IV

TL;DR: The paper introduces HISTAI, a large-scale, multimodal WSI dataset with extensive clinical metadata to address limitations in current DP datasets.


<details>
  <summary>Details</summary>
Motivation: Existing WSI datasets lack scale, diversity, and clinical metadata, hindering AI model robustness and generalizability.

Method: The HISTAI dataset includes over 60,000 slides from diverse tissues, paired with detailed clinical metadata and annotations.

Result: HISTAI provides a comprehensive resource to enhance AI model performance in computational pathology.

Conclusion: HISTAI aims to foster innovation and reproducibility in DP by addressing gaps in current datasets.

Abstract: Recent advancements in Digital Pathology (DP), particularly through
artificial intelligence and Foundation Models, have underscored the importance
of large-scale, diverse, and richly annotated datasets. Despite their critical
role, publicly available Whole Slide Image (WSI) datasets often lack sufficient
scale, tissue diversity, and comprehensive clinical metadata, limiting the
robustness and generalizability of AI models. In response, we introduce the
HISTAI dataset, a large, multimodal, open-access WSI collection comprising over
60,000 slides from various tissue types. Each case in the HISTAI dataset is
accompanied by extensive clinical metadata, including diagnosis, demographic
information, detailed pathological annotations, and standardized diagnostic
coding. The dataset aims to fill gaps identified in existing resources,
promoting innovation, reproducibility, and the development of clinically
relevant computational pathology solutions. The dataset can be accessed at
https://github.com/HistAI/HISTAI.

</details>


### [400] [CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction](https://arxiv.org/abs/2505.12203)
*Zhiting Zheng,Shuqi Wu,Wen Ding*

Main category: eess.IV

TL;DR: CTLformer, a hybrid model combining convolutional and transformer architectures, improves LDCT denoising with multi-scale and dynamic attention mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LDCT images suffer from significant noise, reducing diagnostic accuracy. Current methods struggle with multi-scale feature fusion and diverse noise patterns.

Method: CTLformer integrates convolutional layers and transformer architecture, featuring multi-scale attention (Token2Token and self-attention modules) and dynamic attention control for adaptive noise handling.

Result: CTLformer outperforms existing methods on the AAPM Mayo Clinic dataset, enhancing denoising performance and model efficiency.

Conclusion: CTLformer offers an efficient LDCT denoising solution with potential for broader medical image analysis, especially in complex noise scenarios.

Abstract: Low-dose CT (LDCT) images are often accompanied by significant noise, which
negatively impacts image quality and subsequent diagnostic accuracy. To address
the challenges of multi-scale feature fusion and diverse noise distribution
patterns in LDCT denoising, this paper introduces an innovative model,
CTLformer, which combines convolutional structures with transformer
architecture. Two key innovations are proposed: a multi-scale attention
mechanism and a dynamic attention control mechanism. The multi-scale attention
mechanism, implemented through the Token2Token mechanism and self-attention
interaction modules, effectively captures both fine details and global
structures at different scales, enhancing relevant features and suppressing
noise. The dynamic attention control mechanism adapts the attention
distribution based on the noise characteristics of the input image, focusing on
high-noise regions while preserving details in low-noise areas, thereby
enhancing robustness and improving denoising performance. Furthermore,
CTLformer integrates convolutional layers for efficient feature extraction and
uses overlapping inference to mitigate boundary artifacts, further
strengthening its denoising capability. Experimental results on the 2016
National Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset
demonstrate that CTLformer significantly outperforms existing methods in both
denoising performance and model efficiency, greatly improving the quality of
LDCT images. The proposed CTLformer not only provides an efficient solution for
LDCT denoising but also shows broad potential in medical image analysis,
especially for clinical applications dealing with complex noise patterns.

</details>


### [401] [PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning](https://arxiv.org/abs/2505.12233)
*Yeonkyung Lee,Woojung Han,Youngjun Jun,Hyeonmin Kim,Jungkyung Cho,Seong Jae Hwang*

Main category: eess.IV

TL;DR: PRETI is a retinal foundation model that integrates metadata-aware learning with self-supervised representation learning, improving diagnostic performance by leveraging widely available metadata and patient-level data pairs.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on costly clinical reports and leverage widely available metadata (e.g., age, gender) for better retinal disease analysis.

Method: Proposes Learnable Metadata Embedding (LME) for dynamic metadata refinement, patient-level data pairs for robustness, and Retina-Aware Adaptive Masking (RAAM) for optimized image representation.

Result: PRETI achieves state-of-the-art performance in diverse disease and biomarker predictions, demonstrating superior diagnostic capabilities.

Conclusion: Metadata-guided foundation models like PRETI are crucial for advancing retinal disease analysis, offering a cost-effective and robust alternative to report supervision.

Abstract: Retinal foundation models have significantly advanced retinal image analysis
by leveraging self-supervised learning to reduce dependence on labeled data
while achieving strong generalization. Many recent approaches enhance retinal
image understanding using report supervision, but obtaining clinical reports is
often costly and challenging. In contrast, metadata (e.g., age, gender) is
widely available and serves as a valuable resource for analyzing disease
progression. To effectively incorporate patient-specific information, we
propose PRETI, a retinal foundation model that integrates metadata-aware
learning with robust self-supervised representation learning. We introduce
Learnable Metadata Embedding (LME), which dynamically refines metadata
representations. Additionally, we construct patient-level data pairs,
associating images from the same individual to improve robustness against
non-clinical variations. To further optimize retinal image representation, we
propose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively
applies masking within the retinal region and dynamically adjusts the masking
ratio during training. PRETI captures both global structures and fine-grained
pathological details, resulting in superior diagnostic performance. Extensive
experiments demonstrate that PRETI achieves state-of-the-art results across
diverse diseases and biomarker predictions using in-house and public data,
indicating the importance of metadata-guided foundation models in retinal
disease analysis. Our code and pretrained model are available at
https://github.com/MICV-yonsei/PRETI

</details>


### [402] [Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans](https://arxiv.org/abs/2505.12298)
*Amal Lahchim,Lazar Davic*

Main category: eess.IV

TL;DR: A robust CNN-based method for segmenting infected lung regions in COVID-19 CT scans, using a modified U-Net with attention mechanisms, achieving high accuracy (Dice: 0.8658, IoU: 0.8316).


<details>
  <summary>Details</summary>
Motivation: To improve automatic segmentation of infected lung regions in COVID-19 CT scans for better diagnosis and treatment planning.

Method: Modified U-Net architecture with attention mechanisms, data augmentation, and postprocessing techniques.

Result: Achieved superior performance (Dice: 0.8658, IoU: 0.8316) compared to other methods.

Conclusion: The method is effective; future work includes dataset expansion, 3D segmentation, and clinical deployment.

Abstract: In this study, we propose a robust methodology for automatic segmentation of
infected lung regions in COVID-19 CT scans using convolutional neural networks.
The approach is based on a modified U-Net architecture enhanced with attention
mechanisms, data augmentation, and postprocessing techniques. It achieved a
Dice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.
The dataset was sourced from public repositories and augmented for diversity.
Results demonstrate superior segmentation performance. Future work includes
expanding the dataset, exploring 3D segmentation, and preparing the model for
clinical deployment.

</details>


### [403] [Mutual Evidential Deep Learning for Medical Image Segmentation](https://arxiv.org/abs/2505.12418)
*Yuanpeng He,Yali Bi,Lijian Li,Chi-Man Pun,Wenpin Jiao,Zhi Jin*

Main category: eess.IV

TL;DR: The paper proposes a mutual evidential deep learning (MEDL) framework to improve pseudo-label reliability in semi-supervised medical segmentation by leveraging complementary evidence and uncertainty-driven learning.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised methods suffer from biases due to low-quality pseudo-labels, as they fail to assess pseudo-label reliability from diverse sources.

Method: MEDL introduces diverse network architectures for complementary evidence, uses class-aware evidential fusion, and employs an asymptotic Fisher information-based learning strategy for pseudo-label reliability.

Result: Extensive experiments on five datasets show MEDL achieves state-of-the-art performance.

Conclusion: MEDL effectively addresses pseudo-label reliability issues and outperforms existing methods in semi-supervised medical segmentation.

Abstract: Existing semi-supervised medical segmentation co-learning frameworks have
realized that model performance can be diminished by the biases in model
recognition caused by low-quality pseudo-labels. Due to the averaging nature of
their pseudo-label integration strategy, they fail to explore the reliability
of pseudo-labels from different sources. In this paper, we propose a mutual
evidential deep learning (MEDL) framework that offers a potentially viable
solution for pseudo-label generation in semi-supervised learning from two
perspectives. First, we introduce networks with different architectures to
generate complementary evidence for unlabeled samples and adopt an improved
class-aware evidential fusion to guide the confident synthesis of evidential
predictions sourced from diverse architectural networks. Second, utilizing the
uncertainty in the fused evidence, we design an asymptotic Fisher
information-based evidential learning strategy. This strategy enables the model
to initially focus on unlabeled samples with more reliable pseudo-labels,
gradually shifting attention to samples with lower-quality pseudo-labels while
avoiding over-penalization of mislabeled classes in high data uncertainty
samples. Additionally, for labeled data, we continue to adopt an
uncertainty-driven asymptotic learning strategy, gradually guiding the model to
focus on challenging voxels. Extensive experiments on five mainstream datasets
have demonstrated that MEDL achieves state-of-the-art performance.

</details>


### [404] [FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction](https://arxiv.org/abs/2505.12552)
*Junliang Ye,Lei Wang,Md Zakir Hossain*

Main category: eess.IV

TL;DR: FreqSelect, a lightweight module, selectively filters spatial-frequency bands in fMRI data to improve image reconstruction by emphasizing predictive frequencies and suppressing noise, enhancing both performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in the mismatch between rich visual stimuli and noisy, low-resolution fMRI signals, with current models treating all spatial-frequency components uniformly, limiting effectiveness.

Method: Introduces FreqSelect, an adaptive module that dynamically filters spatial-frequency bands before encoding, integrating into VAE-diffusion pipelines without extra supervision.

Result: FreqSelect improves reconstruction quality on the Natural Scenes dataset across metrics and provides interpretable insights into brain frequency representation.

Conclusion: FreqSelect generalizes across subjects and scenes, offering a principled approach to enhance decoding accuracy and neuroscientific interpretability, with potential for other neuroimaging modalities.

Abstract: Reconstructing natural images from functional magnetic resonance imaging
(fMRI) data remains a core challenge in natural decoding due to the mismatch
between the richness of visual stimuli and the noisy, low resolution nature of
fMRI signals. While recent two-stage models, combining deep variational
autoencoders (VAEs) with diffusion models, have advanced this task, they treat
all spatial-frequency components of the input equally. This uniform treatment
forces the model to extract meaning features and suppress irrelevant noise
simultaneously, limiting its effectiveness. We introduce FreqSelect, a
lightweight, adaptive module that selectively filters spatial-frequency bands
before encoding. By dynamically emphasizing frequencies that are most
predictive of brain activity and suppressing those that are uninformative,
FreqSelect acts as a content-aware gate between image features and natural
data. It integrates seamlessly into standard very deep VAE-diffusion pipelines
and requires no additional supervision. Evaluated on the Natural Scenes
dataset, FreqSelect consistently improves reconstruction quality across both
low- and high-level metrics. Beyond performance gains, the learned
frequency-selection patterns offer interpretable insights into how different
visual frequencies are represented in the brain. Our method generalizes across
subjects and scenes, and holds promise for extension to other neuroimaging
modalities, offering a principled approach to enhancing both decoding accuracy
and neuroscientific interpretability.

</details>


### [405] [The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems](https://arxiv.org/abs/2505.12836)
*Muhamed Kuric,Martin Zach,Andreas Habring,Michael Unser,Thomas Pock*

Main category: eess.IV

TL;DR: The paper introduces a Gaussian latent machine for sampling from product-of-experts models in Bayesian imaging, unifying existing methods and offering efficient Gibbs sampling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sampling from complex prior and posterior distributions in Bayesian imaging by proposing a unified approach.

Method: Lifts the model into a Gaussian latent machine, enabling a general sampling framework that includes two-block Gibbs sampling and direct sampling.

Result: The proposed method is efficient and effective across various Bayesian imaging problems, as demonstrated by numerical experiments.

Conclusion: The Gaussian latent machine provides a versatile and efficient sampling solution for Bayesian imaging, generalizing existing algorithms.

Abstract: We consider the problem of sampling from a product-of-experts-type model that
encompasses many standard prior and posterior distributions commonly found in
Bayesian imaging. We show that this model can be easily lifted into a novel
latent variable model, which we refer to as a Gaussian latent machine. This
leads to a general sampling approach that unifies and generalizes many existing
sampling algorithms in the literature. Most notably, it yields a highly
efficient and effective two-block Gibbs sampling approach in the general case,
while also specializing to direct sampling algorithms in particular cases.
Finally, we present detailed numerical experiments that demonstrate the
efficiency and effectiveness of our proposed sampling approach across a wide
range of prior and posterior sampling problems from Bayesian imaging.

</details>


### [406] [RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions](https://arxiv.org/abs/2505.12887)
*Junzhi Ning,Cheng Tang,Kaijin Zhou,Diping Song,Lihao Liu,Ming Hu,Wei Li,Yanzhou Su,Tianbing Li,Jiyao Liu,Yejin,Sheng Zhang,Yuanfeng Ji,Junjun He*

Main category: eess.IV

TL;DR: The paper introduces RetinaLogos-1400k, a synthetic dataset of 1.4 million Colour Fundus Photographs (CFPs) generated using LLMs, and a three-step training framework (RetinaLogos) for fine-grained control over retinal images, improving disease grading and detection.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality labelled retinal imaging data limits machine learning progress in ophthalmology. Existing methods for synthesizing CFPs are constrained by predefined labels and fail to capture diverse anatomical structures.

Method: The authors create RetinaLogos-1400k using LLMs to describe retinal conditions and structures. They then employ a three-step training framework (RetinaLogos) for fine-grained semantic control over synthetic images.

Result: 62.07% of synthetic images are indistinguishable from real ones by ophthalmologists. The synthetic data improves accuracy by 10%-25% in diabetic retinopathy grading and glaucoma detection.

Conclusion: RetinaLogos-1400k and the RetinaLogos framework provide a scalable solution to augment ophthalmic datasets, addressing data scarcity and improving model performance.

Abstract: The scarcity of high-quality, labelled retinal imaging data, which presents a
significant challenge in the development of machine learning models for
ophthalmology, hinders progress in the field. To synthesise Colour Fundus
Photographs (CFPs), existing methods primarily relying on predefined disease
labels face significant limitations. However, current methods remain limited,
thus failing to generate images for broader categories with diverse and
fine-grained anatomical structures. To overcome these challenges, we first
introduce an innovative pipeline that creates a large-scale, synthetic
Caption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.
Specifically, RetinaLogos-1400k uses large language models (LLMs) to describe
retinal conditions and key structures, such as optic disc configuration,
vascular distribution, nerve fibre layers, and pathological features.
Furthermore, based on this dataset, we employ a novel three-step training
framework, called RetinaLogos, which enables fine-grained semantic control over
retinal images and accurately captures different stages of disease progression,
subtle anatomical variations, and specific lesion types. Extensive experiments
demonstrate state-of-the-art performance across multiple datasets, with 62.07%
of text-driven synthetic images indistinguishable from real ones by
ophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in
diabetic retinopathy grading and glaucoma detection, thereby providing a
scalable solution to augment ophthalmic datasets.

</details>


### [407] [Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies](https://arxiv.org/abs/2505.12963)
*Maksim I. Ivanov,Olga E. Mendybaeva,Yuri E. Karyakin,Igor N. Glukhikh,Aleksey V. Lebedev*

Main category: eess.IV

TL;DR: The paper investigates AI for TMJ pathology diagnosis, focusing on articular disc segmentation in MRI images. It compares neural networks (U-Net, YOLOv8n, YOLOv11n, Roboflow) and highlights Roboflow's potential.


<details>
  <summary>Details</summary>
Motivation: High prevalence of TMJ pathologies and the need for faster, more accurate diagnosis drive the research. Existing tools like Diagnocat and MandSeg are inadequate for articular disc analysis.

Method: An original dataset of 94 MRI images was created and augmented. Neural networks (U-Net, YOLOv8n, YOLOv11n, Roboflow) were trained and evaluated using metrics like Dice Score and Precision.

Result: Roboflow showed the best performance for TMJ segmentation.

Conclusion: The study confirms Roboflow's potential for TMJ diagnosis. Future work includes developing algorithms for jaw distance measurement and disc position analysis.

Abstract: This article explores the use of artificial intelligence for the diagnosis of
pathologies of the temporomandibular joint (TMJ), in particular, for the
segmentation of the articular disc on MRI images. The relevance of the work is
due to the high prevalence of TMJ pathologies, as well as the need to improve
the accuracy and speed of diagnosis in medical institutions. During the study,
the existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,
are not suitable for studying the articular disc due to the orientation towards
bone structures. To solve the problem, an original dataset was collected from
94 images with the classes "temporomandibular joint" and "jaw". To increase the
amount of data, augmentation methods were used. After that, the models of
U-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and
compared. The evaluation was carried out according to the Dice Score,
Precision, Sensitivity, Specificity, and Mean Average Precision metrics. The
results confirm the potential of using the Roboflow model for segmentation of
the temporomandibular joint. In the future, it is planned to develop an
algorithm for measuring the distance between the jaws and determining the
position of the articular disc, which will improve the diagnosis of TMJ
pathologies.

</details>


### [408] [Enhancing Diffusion-Weighted Images (DWI) for Diffusion MRI: Is it Enough without Non-Diffusion-Weighted B=0 Reference?](https://arxiv.org/abs/2505.12978)
*Yinzhe Wu,Jiahao Huang,Fanwen Wang,Mengze Gao,Congyu Liao,Guang Yang,Kawin Setsompop*

Main category: eess.IV

TL;DR: The paper introduces a novel ratio loss for improving diffusion MRI (dMRI) super-resolution by addressing the divergence in the ratio between generated diffusion-weighted images (DWIs) and b=0 images, enhancing the accuracy of diffusion metrics.


<details>
  <summary>Details</summary>
Motivation: High-resolution dMRI is challenging due to SNR and acquisition time trade-offs. Conventional methods optimize DWIs without considering their relationship with b=0 images, which is critical for accurate diffusion metrics.

Method: The study proposes a ratio loss, defined as the MSE loss between predicted and ground-truth log of DWI/b=0 ratios, to improve convergence and accuracy.

Result: Incorporating the ratio loss reduces ratio MSE, slightly improves PSNR of DWIs, and better preserves b=0 ratio-based features for diffusion metrics.

Conclusion: The ratio loss enhances dMRI super-resolution and ensures more accurate derivation of diffusion metrics.

Abstract: Diffusion MRI (dMRI) is essential for studying brain microstructure, but
high-resolution imaging remains challenging due to the inherent trade-offs
between acquisition time and signal-to-noise ratio (SNR). Conventional methods
often optimize only the diffusion-weighted images (DWIs) without considering
their relationship with the non-diffusion-weighted (b=0) reference images.
However, calculating diffusion metrics, such as the apparent diffusion
coefficient (ADC) and diffusion tensor with its derived metrics like fractional
anisotropy (FA) and mean diffusivity (MD), relies on the ratio between each DWI
and the b=0 image, which is crucial for clinical observation and diagnostics.
In this study, we demonstrate that solely enhancing DWIs using a conventional
pixel-wise mean squared error (MSE) loss is insufficient, as the error in ratio
between generated DWIs and b=0 diverges. We propose a novel ratio loss, defined
as the MSE loss between the predicted and ground-truth log of DWI/b=0 ratios.
Our results show that incorporating the ratio loss significantly improves the
convergence of this ratio error, achieving lower ratio MSE and slightly
enhancing the peak signal-to-noise ratio (PSNR) of generated DWIs. This leads
to improved dMRI super-resolution and better preservation of b=0 ratio-based
features for the derivation of diffusion metrics.

</details>


### [409] [A generalisable head MRI defacing pipeline: Evaluation on 2,566 meningioma scans](https://arxiv.org/abs/2505.12999)
*Lorena Garcia-Foncillas Macias,Aaron Kujawa,Aya Elshalakany,Jonathan Shapey,Tom Vercauteren*

Main category: eess.IV

TL;DR: A robust MRI defacing pipeline ensures patient privacy while preserving brain anatomy, achieving a 99.92% success rate on clinical scans.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing MRI defacing methods, which often fail to fully deface or degrade brain tissue.

Method: Integrates atlas-based registration with brain masking for high-resolution MRI defacing.

Result: Achieved a 99.92% success rate on 2,566 clinical scans and excellent anatomical preservation (Dice coefficient: 0.9975 ± 0.0023).

Conclusion: The proposed pipeline is reliable and generalizable, with publicly available source code.

Abstract: Reliable MRI defacing techniques to safeguard patient privacy while
preserving brain anatomy are critical for research collaboration. Existing
methods often struggle with incomplete defacing or degradation of brain tissue
regions. We present a robust, generalisable defacing pipeline for
high-resolution MRI that integrates atlas-based registration with brain
masking. Our method was evaluated on 2,566 heterogeneous clinical scans for
meningioma and achieved a 99.92 per cent success rate (2,564/2,566) upon visual
inspection. Excellent anatomical preservation is demonstrated with a Dice
similarity coefficient of 0.9975 plus or minus 0.0023 between brain masks
automatically extracted from the original and defaced volumes. Source code is
available at https://github.com/cai4cai/defacing_pipeline.

</details>


### [410] [Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model](https://arxiv.org/abs/2505.13152)
*Jonas Brenig,Radu Timofte*

Main category: eess.IV

TL;DR: A hybrid compression scheme combines a decoder network with a diffusion model to improve fidelity (PSNR) while maintaining perceptual quality (LPIPS, FID), outperforming CDC in image and video compression.


<details>
  <summary>Details</summary>
Motivation: Diffusion models excel in perceptual quality but lag in fidelity compared to traditional methods. The goal is to enhance fidelity without sacrificing perceptual gains.

Method: A decoder network generates an initial image optimized for distortion, followed by a diffusion model refining the reconstruction for perceptual quality by predicting residuals.

Result: Achieves up to +2dB PSNR improvement over CDC while maintaining comparable LPIPS and FID scores. Extends successfully to video compression.

Conclusion: The hybrid approach effectively balances fidelity and perceptual quality, offering a scalable solution for image and video compression.

Abstract: Denoising diffusion models achieved impressive results on several image
generation tasks often outperforming GAN based models. Recently, the generative
capabilities of diffusion models have been employed for perceptual image
compression, such as in CDC. A major drawback of these diffusion-based methods
is that, while producing impressive perceptual quality images they are dropping
in fidelity/increasing the distortion to the original uncompressed images when
compared with other traditional or learned image compression schemes aiming for
fidelity. In this paper, we propose a hybrid compression scheme optimized for
perceptual quality, extending the approach of the CDC model with a decoder
network in order to reduce the impact on distortion metrics such as PSNR. After
using the decoder network to generate an initial image, optimized for
distortion, the latent conditioned diffusion model refines the reconstruction
for perceptual quality by predicting the residual. On standard benchmarks, we
achieve up to +2dB PSNR fidelity improvements while maintaining comparable
LPIPS and FID perceptual scores when compared with CDC. Additionally, the
approach is easily extensible to video compression, where we achieve similar
results.

</details>


### [411] [GuidedMorph: Two-Stage Deformable Registration for Breast MRI](https://arxiv.org/abs/2505.13414)
*Yaqian Chen,Hanxue Gu,Haoyu Dong,Qihang Li,Yuwen Chen,Nicholas Konz,Lin Li,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: GuidedMorph, a two-stage registration framework, improves breast MR image alignment by focusing on dense tissue details, outperforming baselines in accuracy and structural preservation.


<details>
  <summary>Details</summary>
Motivation: Accurate breast MR image registration is crucial for cancer detection and treatment but is challenged by dense tissue complexity and non-rigid deformations.

Method: Proposes GuidedMorph with a dual-stage approach: global structure alignment and dense tissue tracking using DSTN and EDT-based warping. Works with VoxelMorph/TransMorph backbones.

Result: Outperforms baselines with 13.01% dense tissue Dice, 3.13% breast Dice, and 1.21% breast SSIM improvements on ISPY2 and internal datasets.

Conclusion: GuidedMorph offers a versatile, accurate solution for breast MR registration, enhancing tumor tracking and treatment planning.

Abstract: Accurately registering breast MR images from different time points enables
the alignment of anatomical structures and tracking of tumor progression,
supporting more effective breast cancer detection, diagnosis, and treatment
planning. However, the complexity of dense tissue and its highly non-rigid
nature pose challenges for conventional registration methods, which primarily
focus on aligning general structures while overlooking intricate internal
details. To address this, we propose \textbf{GuidedMorph}, a novel two-stage
registration framework designed to better align dense tissue. In addition to a
single-scale network for global structure alignment, we introduce a framework
that utilizes dense tissue information to track breast movement. The learned
transformation fields are fused by introducing the Dual Spatial Transformer
Network (DSTN), improving overall alignment accuracy. A novel warping method
based on the Euclidean distance transform (EDT) is also proposed to accurately
warp the registered dense tissue and breast masks, preserving fine structural
details during deformation. The framework supports paradigms that require
external segmentation models and with image data only. It also operates
effectively with the VoxelMorph and TransMorph backbones, offering a versatile
solution for breast registration. We validate our method on ISPY2 and internal
dataset, demonstrating superior performance in dense tissue, overall breast
alignment, and breast structural similarity index measure (SSIM), with notable
improvements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and
1.21% in breast SSIM compared to the best learning-based baseline.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [412] [Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems](https://arxiv.org/abs/2505.11535)
*Yuhang Wang,Hao Zhou*

Main category: cs.RO

TL;DR: LKAlert is a novel system using VLM to predict and explain LKA failures 1-3 seconds in advance, enhancing driver trust and situational awareness. It includes a benchmark dataset (OpenLKA-Alert) and a methodological framework for VLM-based black-box behavior prediction.


<details>
  <summary>Details</summary>
Motivation: Address the unpredictability and opacity of Lane Keeping Assist (LKA) systems, which limit driver trust and anticipation.

Method: LKAlert processes dash-cam video and CAN data, integrating surrogate lane segmentation features from an interpretable model. It combines surrogate feature guidance with LoRA for VLM-based prediction.

Result: Predicts LKA failures with 69.8% accuracy and 58.6% F1-score, generates high-quality explanations (71.7 ROUGE-L), and operates at 2 Hz.

Conclusion: LKAlert improves LKA safety and usability, offering a scalable paradigm for human-centered supervision of black-box automation.

Abstract: Lane Keeping Assist systems, while increasingly prevalent, often suffer from
unpredictable real-world failures, largely due to their opaque, black-box
nature, which limits driver anticipation and trust. To bridge the gap between
automated assistance and effective human oversight, we present LKAlert, a novel
supervisory alert system that leverages VLM to forecast potential LKA risk 1-3
seconds in advance. LKAlert processes dash-cam video and CAN data, integrating
surrogate lane segmentation features from a parallel interpretable model as
automated guiding attention. Unlike traditional binary classifiers, LKAlert
issues both predictive alert and concise natural language explanation,
enhancing driver situational awareness and trust. To support the development
and evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark
dataset designed for predictive and explainable LKA failure warnings. It
contains synchronized multimodal inputs and human-authored justifications
across annotated temporal windows. We further contribute a generalizable
methodological framework for VLM-based black-box behavior prediction, combining
surrogate feature guidance with LoRA. This framework enables VLM to reason over
structured visual context without altering its vision backbone, making it
broadly applicable to other complex, opaque systems requiring interpretable
oversight. Empirical results correctly predicts upcoming LKA failures with
69.8% accuracy and a 58.6\% F1-score. The system also generates high-quality
textual explanations for drivers (71.7 ROUGE-L) and operates efficiently at
approximately 2 Hz, confirming its suitability for real-time, in-vehicle use.
Our findings establish LKAlert as a practical solution for enhancing the safety
and usability of current ADAS and offer a scalable paradigm for applying VLMs
to human-centered supervision of black-box automation.

</details>


### [413] [GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation](https://arxiv.org/abs/2505.11865)
*Teli Ma,Jia Zheng,Zifan Wang,Ziyao Gao,Jiaming Zhou,Junwei Liang*

Main category: cs.RO

TL;DR: The paper introduces HOVA-500K, a large-scale affordance-annotated dataset, and GLOVER++, a framework for transferring affordance knowledge from human demonstrations to robotic tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address challenges in transferring manipulation skills from human demonstrations due to lack of large-scale datasets and insufficient exploration of affordances in diverse contexts.

Method: Introduces HOVA-500K dataset and GLOVER++, a global-to-local affordance training framework for open-vocabulary reasoning tasks.

Result: GLOVER++ achieves state-of-the-art results on HOVA-500K and demonstrates strong generalization in robotic manipulation tasks.

Conclusion: HOVA-500K and GLOVER++ bridge the gap between human demonstrations and robotic manipulation capabilities.

Abstract: Learning manipulation skills from human demonstration videos offers a
promising path toward generalizable and interpretable robotic
intelligence-particularly through the lens of actionable affordances. However,
transferring such knowledge remains challenging due to: 1) a lack of
large-scale datasets with precise affordance annotations, and 2) insufficient
exploration of affordances in diverse manipulation contexts. To address these
gaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset
comprising 500,000 images across 1,726 object categories and 675 actions. We
also release a standardized benchmarking suite for multi-modal affordance
reasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local
affordance training framework that effectively transfers actionable affordance
knowledge from human demonstrations to downstream open-vocabulary reasoning
tasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark
and demonstrates strong generalization across diverse downstream robotic
manipulation tasks. By explicitly modeling actionable affordances, GLOVER++
facilitates robust transfer across scenes, modalities, and tasks. We hope that
HOVA-500K and the GLOVER++ framework will serve as valuable resources for
bridging the gap between human demonstrations and robotic manipulation
capabilities.

</details>


### [414] [Experimental Study on Automatically Assembling Custom Catering Packages With a 3-DOF Delta Robot Using Deep Learning Methods](https://arxiv.org/abs/2505.11879)
*Reihaneh Yourdkhani,Arash Tavoosian,Navid Asadi Khomami,Mehdi Tale Masouleh*

Main category: cs.RO

TL;DR: The paper presents an automated packing system using a Delta robot with a deep learning approach, achieving over 80% success in grasping catering packages.


<details>
  <summary>Details</summary>
Motivation: To advance robotic packaging automation by addressing the challenge of automated packing with a novel deep learning and geometrical approach.

Method: Uses YOLOV5 for object detection, FastSAM for segmentation, and a geometrical method with eigenvectors to calculate grasp points for a 3-DOF Delta robot.

Result: Achieves real-time detection and autonomous packing with an over 80% success rate in grasping.

Conclusion: The study significantly advances robotic packaging automation, demonstrating practical applicability.

Abstract: This paper introduces a pioneering experimental study on the automated
packing of a catering package using a two-fingered gripper affixed to a
3-degree-of-freedom Delta parallel robot. A distinctive contribution lies in
the application of a deep learning approach to tackle this challenge. A custom
dataset, comprising 1,500 images, is meticulously curated for this endeavor,
representing a noteworthy initiative as the first dataset focusing on
Persian-manufactured products. The study employs the YOLOV5 model for object
detection, followed by segmentation using the FastSAM model. Subsequently,
rotation angle calculation is facilitated with segmentation masks, and a
rotated rectangle encapsulating the object is generated. This rectangle forms
the basis for calculating two grasp points using a novel geometrical approach
involving eigenvectors. An extensive experimental study validates the proposed
model, where all pertinent information is seamlessly transmitted to the 3-DOF
Delta parallel robot. The proposed algorithm ensures real-time detection,
calibration, and the fully autonomous packing process of a catering package,
boasting an impressive over 80\% success rate in automatic grasping. This study
marks a significant stride in advancing the capabilities of robotic systems for
practical applications in packaging automation.

</details>


### [415] [Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning](https://arxiv.org/abs/2505.12278)
*Zhengyi Luo,Chen Tessler,Toru Lin,Ye Yuan,Tairan He,Wenli Xiao,Yunrong Guo,Gal Chechik,Kris Kitani,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: PDC is a vision-driven framework for dexterous humanoid control, enabling tasks like object search and manipulation using egocentric vision, without relying on privileged state info.


<details>
  <summary>Details</summary>
Motivation: Human behavior is shaped by visual perception; PDC aims to replicate this for dexterous control in simulated humanoids.

Method: PDC uses egocentric vision for task specification and trains policies with reinforcement learning from scratch.

Result: The framework successfully performs household tasks and exhibits emergent behaviors like active search.

Conclusion: Vision-driven control induces human-like behaviors, closing the perception-action loop for robotics and AI.

Abstract: Human behavior is fundamentally shaped by visual perception -- our ability to
interact with the world depends on actively gathering relevant information and
adapting our movements accordingly. Behaviors like searching for objects,
reaching, and hand-eye coordination naturally emerge from the structure of our
sensory system. Inspired by these principles, we introduce Perceptive Dexterous
Control (PDC), a framework for vision-driven dexterous whole-body control with
simulated humanoids. PDC operates solely on egocentric vision for task
specification, enabling object search, target placement, and skill selection
through visual cues, without relying on privileged state information (e.g., 3D
object positions and geometries). This perception-as-interface paradigm enables
learning a single policy to perform multiple household tasks, including
reaching, grasping, placing, and articulated object manipulation. We also show
that training from scratch with reinforcement learning can produce emergent
behaviors such as active search. These results demonstrate how vision-driven
control and complex tasks induce human-like behaviors and can serve as the key
ingredients in closing the perception-action loop for animation, robotics, and
embodied AI.

</details>


### [416] [Structureless VIO](https://arxiv.org/abs/2505.12337)
*Junlin Song,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: The paper proposes a structureless VIO method that removes the visual map, improving efficiency and accuracy compared to traditional structure-based VIO.


<details>
  <summary>Details</summary>
Motivation: The tight coupling of localization and mapping in VO/VIO creates a chicken-and-egg problem, and map-free solutions are underexplored.

Method: A novel structureless VIO framework is introduced, eliminating the need for a visual map.

Result: The structureless VIO outperforms structure-based VIO in computational efficiency and accuracy.

Conclusion: The proposed method offers a viable alternative to traditional VIO by decoupling localization from mapping.

Abstract: Visual odometry (VO) is typically considered as a chicken-and-egg problem, as
the localization and mapping modules are tightly-coupled. The estimation of
visual map relies on accurate localization information. Meanwhile, localization
requires precise map points to provide motion constraints. This classical
design principle is naturally inherited by visual-inertial odometry (VIO).
Efficient localization solution that does not require a map has not been fully
investigated. To this end, we propose a novel structureless VIO, where the
visual map is removed from the odometry framework. Experimental results
demonstrated that, compared to the structure-based VIO baseline, our
structureless VIO not only substantially improves computational efficiency but
also has advantages in accuracy.

</details>


### [417] [TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation](https://arxiv.org/abs/2505.12748)
*Hangyu Li,Qin Zhao,Haoran Xu,Xinyu Jiang,Qingwei Ben,Feiyu Jia,Haoyu Zhao,Liang Xu,Jia Zeng,Hanqing Wang,Bo Dai,Junting Dong,Jiangmiao Pang*

Main category: cs.RO

TL;DR: TeleOpBench is a simulator-centric benchmark for bimanual dexterous teleoperation, introducing 30 task environments and evaluating four teleoperation modalities with a common protocol.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified benchmark for fair, reproducible comparison of diverse teleoperation systems in embodied-robot learning.

Method: Developed TeleOpBench with 30 task environments and implemented four teleoperation modalities (MoCap, VR device, exoskeletons, monocular vision tracking), evaluating them with a common protocol.

Result: Strong correlation between simulator and hardware performance in 10 held-out tasks, validating TeleOpBench's external validity.

Conclusion: TeleOpBench provides a common yardstick for teleoperation research and an extensible platform for future innovation.

Abstract: Teleoperation is a cornerstone of embodied-robot learning, and bimanual
dexterous teleoperation in particular provides rich demonstrations that are
difficult to obtain with fully autonomous systems. While recent studies have
proposed diverse hardware pipelines-ranging from inertial motion-capture gloves
to exoskeletons and vision-based interfaces-there is still no unified benchmark
that enables fair, reproducible comparison of these systems. In this paper, we
introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual
dexterous teleoperation. TeleOpBench contains 30 high-fidelity task
environments that span pick-and-place, tool use, and collaborative
manipulation, covering a broad spectrum of kinematic and force-interaction
difficulty. Within this benchmark we implement four representative
teleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand
exoskeletons, and (iv) monocular vision tracking-and evaluate them with a
common protocol and metric suite. To validate that performance in simulation is
predictive of real-world behavior, we conduct mirrored experiments on a
physical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10
held-out tasks we observe a strong correlation between simulator and hardware
performance, confirming the external validity of TeleOpBench. TeleOpBench
establishes a common yardstick for teleoperation research and provides an
extensible platform for future algorithmic and hardware innovation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [418] [Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals](https://arxiv.org/abs/2505.12114)
*Dena F. Mujtaba,Nihar R. Mahapatra*

Main category: cs.HC

TL;DR: The paper introduces a counterfactual-based framework using GANs to evaluate and quantify bias in AI-driven personality assessments, addressing ethical concerns in hiring.


<details>
  <summary>Details</summary>
Motivation: AI-enhanced personality assessments in hiring can amplify biases from training data, leading to discriminatory outcomes based on protected attributes like gender, ethnicity, and age.

Method: The approach uses GANs to generate counterfactual representations of job applicants by altering protected attributes, enabling fairness analysis without model access. It supports multimodal evaluation (visual, audio, textual).

Result: Applied to a personality prediction model, the method reveals significant disparities across demographic groups and validates effectiveness with a protected attribute classifier.

Conclusion: The framework provides a scalable tool for fairness auditing in black-box AI hiring systems, emphasizing the importance of counterfactual approaches for ethical transparency.

Abstract: AI-enhanced personality assessments are increasingly shaping hiring
decisions, using affective computing to predict traits from the Big Five
(OCEAN) model. However, integrating AI into these assessments raises ethical
concerns, especially around bias amplification rooted in training data. These
biases can lead to discriminatory outcomes based on protected attributes like
gender, ethnicity, and age. To address this, we introduce a
counterfactual-based framework to systematically evaluate and quantify bias in
AI-driven personality assessments. Our approach employs generative adversarial
networks (GANs) to generate counterfactual representations of job applicants by
altering protected attributes, enabling fairness analysis without access to the
underlying model. Unlike traditional bias assessments that focus on unimodal or
static data, our method supports multimodal evaluation-spanning visual, audio,
and textual features. This comprehensive approach is particularly important in
high-stakes applications like hiring, where third-party vendors often provide
AI systems as black boxes. Applied to a state-of-the-art personality prediction
model, our method reveals significant disparities across demographic groups. We
also validate our framework using a protected attribute classifier to confirm
the effectiveness of our counterfactual generation. This work provides a
scalable tool for fairness auditing of commercial AI hiring platforms,
especially in black-box settings where training data and model internals are
inaccessible. Our results highlight the importance of counterfactual approaches
in improving ethical transparency in affective computing.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [419] [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/abs/2505.13393)
*Christopher K. Frantz*

Main category: cs.MA

TL;DR: The paper introduces IG Parser, a tool for qualitative content analysis of institutions, using IG Script syntax to encode and transform natural language for diverse analytical techniques.


<details>
  <summary>Details</summary>
Motivation: To facilitate rigorous analysis of formal and informal institutions governing social systems by automating their encoding and transformation.

Method: The IG Parser employs IG Script, a syntax based on Institutional Grammar 2.0, to encode natural language and automate transformation for downstream analysis.

Result: The tool supports diverse analytical techniques through its syntax and automated processes, illustrated with examples.

Conclusion: IG Parser enhances institutional analysis by providing a robust, automated framework for encoding and analyzing institutions.

Abstract: This article provides an overview of IG Parser, a software that facilitates
qualitative content analysis of formal (e.g., legal) rules or informal (e.g.,
socio-normative) norms, and strategies (such as conventions) -- referred to as
\emph{institutions} -- that govern social systems and operate configurally to
describe \emph{institutional systems}. To this end, the IG Parser employs a
distinctive syntax that ensures rigorous encoding of natural language, while
automating the transformation into various formats that support the downstream
analysis using diverse analytical techniques. The conceptual core of the IG
Parser is an associated syntax, IG Script, that operationalizes the conceptual
foundations of the Institutional Grammar, and more specifically Institutional
Grammar 2.0, an analytical paradigm for institutional analysis. This article
presents the IG Parser, including its conceptual foundations, syntactic
specification of IG Script, alongside architectural principles. This
introduction is augmented with selective illustrative examples that highlight
the use and benefit associated with the tool.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [420] [OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography](https://arxiv.org/abs/2505.12261)
*Hanchen Wang,Yixuan Wu,Yinan Feng,Peng Jin,Shihang Feng,Yiming Mao,James Wiskin,Baris Turkbey,Peter A. Pinto,Bradford J. Wood,Songting Luo,Yinpeng Chen,Emad Boctor,Youzuo Lin*

Main category: physics.med-ph

TL;DR: The paper introduces OpenPros, a large-scale benchmark dataset for limited-view prostate ultrasound computed tomography (USCT), aiming to improve early detection of prostate cancer by advancing deep learning methods for high-resolution imaging.


<details>
  <summary>Details</summary>
Motivation: Prostate cancer is common and deadly, but current ultrasound methods lack sensitivity, especially for anterior tumors. USCT offers quantitative tissue characterization but faces challenges in clinical implementation due to limited-angle acquisition.

Method: The authors created OpenPros, a dataset with 280,000 paired samples of 2D speed-of-sound phantoms and ultrasound full-waveform data, derived from 3D digital prostate models based on clinical MRI/CT scans and ex vivo measurements. Simulations used finite-difference time-domain and Runge-Kutta solvers.

Result: Deep learning methods outperformed physics-based approaches in efficiency and accuracy but still fell short of clinical standards for high-resolution images.

Conclusion: OpenPros is released publicly to foster development of better machine learning algorithms for clinically usable, high-resolution prostate ultrasound imaging.

Abstract: Prostate cancer is one of the most common and lethal cancers among men,
making its early detection critically important. Although ultrasound imaging
offers greater accessibility and cost-effectiveness compared to MRI,
traditional transrectal ultrasound methods suffer from low sensitivity,
especially in detecting anteriorly located tumors. Ultrasound computed
tomography provides quantitative tissue characterization, but its clinical
implementation faces significant challenges, particularly under anatomically
constrained limited-angle acquisition conditions specific to prostate imaging.
To address these unmet needs, we introduce OpenPros, the first large-scale
benchmark dataset explicitly developed for limited-view prostate USCT. Our
dataset includes over 280,000 paired samples of realistic 2D speed-of-sound
(SOS) phantoms and corresponding ultrasound full-waveform data, generated from
anatomically accurate 3D digital prostate models derived from real clinical
MRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.
Simulations are conducted under clinically realistic configurations using
advanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,
both provided as open-source components. Through comprehensive baseline
experiments, we demonstrate that state-of-the-art deep learning methods surpass
traditional physics-based approaches in both inference efficiency and
reconstruction accuracy. Nevertheless, current deep learning models still fall
short of delivering clinically acceptable high-resolution images with
sufficient accuracy. By publicly releasing OpenPros, we aim to encourage the
development of advanced machine learning algorithms capable of bridging this
performance gap and producing clinically usable, high-resolution, and highly
accurate prostate ultrasound images. The dataset is publicly accessible at
https://open-pros.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [421] [Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
*Peter Chen,Xiaopeng Li,Ziniu Li,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: The paper addresses the issue of GRPO stalling in all-negative-sample groups by introducing a framework for response diversity using AI feedback, validated empirically across various model sizes and benchmarks.


<details>
  <summary>Details</summary>
Motivation: GRPO's inefficiency in updating policies when all sampled responses in a group are incorrect (all-negative-sample groups) hinders learning progress.

Method: Proposes a framework to diversify responses in all-negative-sample groups using AI feedback, supported by theoretical analysis.

Result: Empirical validation shows improved performance across model sizes (7B, 14B, 32B) in offline and online settings with 10 benchmarks.

Conclusion: Learning from all-negative-sample groups is feasible and beneficial, advancing insights in RL for LLMs.

Abstract: Reinforcement learning (RL) has demonstrated significant success in enhancing
reasoning capabilities in large language models (LLMs). One of the most widely
used RL methods is Group Relative Policy Optimization
(GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and
success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls
when all sampled responses in a group are incorrect -- referred to as an
\emph{all-negative-sample} group -- as it fails to update the policy, hindering
learning progress. The contributions of this paper are two-fold. First, we
propose a simple yet effective framework that introduces response diversity
within all-negative-sample groups in GRPO using AI feedback. We also provide a
theoretical analysis, via a stylized model, showing how this diversification
improves learning dynamics. Second, we empirically validate our approach,
showing the improved performance across various model sizes (7B, 14B, 32B) in
both offline and online learning settings with 10 benchmarks, including base
and distilled variants. Our findings highlight that learning from
all-negative-sample groups is not only feasible but beneficial, advancing
recent insights from \citet{Xiong-2025-Minimalist}.

</details>


### [422] [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
*Xilong Wang,John Bloch,Zedian Shao,Yuepeng Hu,Shuyan Zhou,Neil Zhenqiang Gong*

Main category: cs.LG

TL;DR: EnvInjection is a new attack method for MLLM-based web agents that adds pixel perturbations to webpages, inducing target actions by approximating a non-differentiable mapping with a neural network.


<details>
  <summary>Details</summary>
Motivation: Existing attacks on MLLM-based web agents lack effectiveness, stealthiness, or practicality.

Method: EnvInjection perturbs raw pixel values of webpages, formulates the task as an optimization problem, and uses a neural network to approximate the non-differentiable mapping for gradient backpropagation.

Result: EnvInjection outperforms existing baselines in effectiveness across multiple webpage datasets.

Conclusion: EnvInjection addresses limitations of prior attacks, offering a practical and effective solution for manipulating web agents.

Abstract: Multi-modal large language model (MLLM)-based web agents interact with
webpage environments by generating actions based on screenshots of the
webpages. Environmental prompt injection attacks manipulate the environment to
induce the web agent to perform a specific, attacker-chosen action--referred to
as the target action. However, existing attacks suffer from limited
effectiveness or stealthiness, or are impractical in real-world settings. In
this work, we propose EnvInjection, a new attack that addresses these
limitations. Our attack adds a perturbation to the raw pixel values of the
rendered webpage, which can be implemented by modifying the webpage's source
code. After these perturbed pixels are mapped into a screenshot, the
perturbation induces the web agent to perform the target action. We formulate
the task of finding the perturbation as an optimization problem. A key
challenge in solving this problem is that the mapping between raw pixel values
and screenshot is non-differentiable, making it difficult to backpropagate
gradients to the perturbation. To overcome this, we train a neural network to
approximate the mapping and apply projected gradient descent to solve the
reformulated optimization problem. Extensive evaluation on multiple webpage
datasets shows that EnvInjection is highly effective and significantly
outperforms existing baselines.

</details>


### [423] [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
*Harshil Vejendla,Haizhou Shi,Yibin Wang,Tunyu Zhang,Huan Zhang,Hao Wang*

Main category: cs.LG

TL;DR: The paper proposes a method to eliminate test-time sampling for LLM uncertainty estimation by distilling a Bayesian LLM's confidence into a non-Bayesian student model, achieving efficiency gains without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Bayesian methods for LLM uncertainty estimation are inefficient due to multiple sampling iterations during inference, limiting practical deployment.

Method: Distill the confidence of a Bayesian LLM into a non-Bayesian student model by minimizing divergence between their predictive distributions, using only the training dataset.

Result: The method achieves N-times more efficient uncertainty estimation, with performance comparable or superior to Bayesian LLMs.

Conclusion: The distillation technique generalizes uncertainty estimation from training to test data efficiently, offering a practical alternative to Bayesian methods.

Abstract: Recent advances in uncertainty estimation for Large Language Models (LLMs)
during downstream adaptation have addressed key challenges of reliability and
simplicity. However, existing Bayesian methods typically require multiple
sampling iterations during inference, creating significant efficiency issues
that limit practical deployment. In this paper, we investigate the possibility
of eliminating the need for test-time sampling for LLM uncertainty estimation.
Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned
confidence into a non-Bayesian student LLM by minimizing the divergence between
their predictive distributions. Unlike typical calibration methods, our
distillation is carried out solely on the training dataset without the need of
an additional validation dataset. This simple yet effective approach achieves
N-times more efficient uncertainty estimation during testing, where N is the
number of samples traditionally required by Bayesian LLMs. Our extensive
experiments demonstrate that uncertainty estimation capabilities on training
data can successfully generalize to unseen test data through our distillation
technique, consistently producing results comparable to (or even better than)
state-of-the-art Bayesian LLMs.

</details>


### [424] [Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
*Tunyu Zhang,Haizhou Shi,Yibin Wang,Hengyi Wang,Xiaoxiao He,Zhuowei Li,Haoxian Chen,Ligong Han,Kai Xu,Huan Zhang,Dimitris Metaxas,Hao Wang*

Main category: cs.LG

TL;DR: A token-level uncertainty estimation framework for LLMs improves self-assessment and generation quality in mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs produce inconsistent output quality, making it hard to trust responses in complex tasks like multi-step reasoning.

Method: Introduces low-rank random weight perturbation during decoding to estimate token-level uncertainties, aggregated for semantic uncertainty.

Result: Token-level uncertainty metrics strongly correlate with answer correctness and robustness, outperforming existing methods.

Conclusion: Effective uncertainty estimation enhances LLM reasoning performance and reliability.

Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities,
their output quality remains inconsistent across various application scenarios,
making it difficult to identify trustworthy responses, especially in complex
tasks requiring multi-step reasoning. In this paper, we propose a token-level
uncertainty estimation framework to enable LLMs to self-assess and self-improve
their generation quality in mathematical reasoning. Specifically, we introduce
low-rank random weight perturbation to LLM decoding, generating predictive
distributions that we use to estimate token-level uncertainties. We then
aggregate these uncertainties to reflect semantic uncertainty of the generated
sequences. Experiments on mathematical reasoning datasets of varying difficulty
demonstrate that our token-level uncertainty metrics strongly correlate with
answer correctness and model robustness. Additionally, we explore using
uncertainty to directly enhance the model's reasoning performance through
multiple generations and the particle filtering algorithm. Our approach
consistently outperforms existing uncertainty estimation methods, establishing
effective uncertainty estimation as a valuable tool for both evaluating and
improving reasoning generation in LLMs.

</details>


### [425] [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
*David Chanin,Tomáš Dulka,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: Sparse autoencoders (SAEs) fail to maintain monosemanticity when trained on correlated features with fewer dimensions than true features, a problem termed 'feature hedging.' This issue is studied theoretically and empirically, and an improved SAE variant is proposed.


<details>
  <summary>Details</summary>
Motivation: To understand why SAEs underperform supervised baselines and address the issue of feature merging in correlated settings.

Method: Theoretical analysis in toy models and empirical study of SAEs in LLMs, leading to a proposed improved variant of matryoshka SAEs.

Result: Feature hedging is identified as a key problem in SAEs, causing loss of interpretability and performance.

Conclusion: Feature hedging is a fundamental issue in SAEs, but awareness of it could drive future improvements for better LLM interpretation.

Abstract: It is assumed that sparse autoencoders (SAEs) decompose polysemantic
activations into interpretable linear directions, as long as the activations
are composed of sparse linear combinations of underlying features. However, we
find that if an SAE is more narrow than the number of underlying "true
features" on which it is trained, and there is correlation between features,
the SAE will merge components of correlated features together, thus destroying
monosemanticity. In LLM SAEs, these two conditions are almost certainly true.
This phenomenon, which we call feature hedging, is caused by SAE reconstruction
loss, and is more severe the narrower the SAE. In this work, we introduce the
problem of feature hedging and study it both theoretically in toy models and
empirically in SAEs trained on LLMs. We suspect that feature hedging may be one
of the core reasons that SAEs consistently underperform supervised baselines.
Finally, we use our understanding of feature hedging to propose an improved
variant of matryoshka SAEs. Our work shows there remain fundamental issues with
SAEs, but we are hopeful that that highlighting feature hedging will catalyze
future advances that allow SAEs to achieve their full potential of interpreting
LLMs at scale.

</details>


### [426] [Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors](https://arxiv.org/abs/2505.11770)
*Jing Huang,Junyi Tao,Thomas Icard,Diyi Yang,Christopher Potts*

Main category: cs.LG

TL;DR: The paper demonstrates that interpretability techniques can predict neural network behavior on out-of-distribution tasks by leveraging causal mechanisms, outperforming causal-agnostic methods.


<details>
  <summary>Details</summary>
Motivation: To determine if interpretability techniques can predict model behavior on out-of-distribution examples, focusing on language models.

Method: Proposes two methods: counterfactual simulation (checking causal variables) and value probing (using variable values for predictions).

Result: Both methods achieve high AUC-ROC in-distribution and outperform causal-agnostic methods in out-of-distribution settings.

Conclusion: Internal causal analysis is a novel and valuable tool for predicting language model behavior, especially in challenging scenarios.

Abstract: Interpretability research now offers a variety of techniques for identifying
abstract internal mechanisms in neural networks. Can such techniques be used to
predict how models will behave on out-of-distribution examples? In this work,
we provide a positive answer to this question. Through a diverse set of
language modeling tasks--including symbol manipulation, knowledge retrieval,
and instruction following--we show that the most robust features for
correctness prediction are those that play a distinctive causal role in the
model's behavior. Specifically, we propose two methods that leverage causal
mechanisms to predict the correctness of model outputs: counterfactual
simulation (checking whether key causal variables are realized) and value
probing (using the values of those variables to make predictions). Both achieve
high AUC-ROC in distribution and outperform methods that rely on
causal-agnostic features in out-of-distribution settings, where predicting
model behaviors is more crucial. Our work thus highlights a novel and
significant application for internal causal analysis of language models.

</details>


### [427] [VenusX: Unlocking Fine-Grained Functional Understanding of Proteins](https://arxiv.org/abs/2505.11812)
*Yang Tan,Wenrui Gou,Bozitao Zhong,Liang Hong,Huiqun Yu,Bingxin Zhou*

Main category: cs.LG

TL;DR: VenusX is a large-scale benchmark for fine-grained protein functional annotation and pairing at residue, fragment, and domain levels, addressing the need for detailed functional understanding beyond protein-level predictions.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models excel at protein-level predictions but lack granularity for understanding functional mechanisms. VenusX fills this gap by providing a detailed benchmark for residue, fragment, and domain-level annotations.

Method: VenusX includes three task categories (residue-level binary classification, fragment-level multi-class classification, and pairwise functional similarity scoring) with over 878,000 samples from databases like InterPro, BioLiP, and SAbDab. It uses mixed-family and cross-family splits at three sequence identity thresholds for robust evaluation.

Result: The benchmark evaluates diverse models (protein language models, sequence-structure hybrids, etc.) across all datasets and settings, providing a comprehensive performance comparison.

Conclusion: VenusX offers a detailed, scalable benchmark for fine-grained protein function analysis, supporting future research with publicly available code and data.

Abstract: Deep learning models have driven significant progress in predicting protein
function and interactions at the protein level. While these advancements have
been invaluable for many biological applications such as enzyme engineering and
function annotation, a more detailed perspective is essential for understanding
protein functional mechanisms and evaluating the biological knowledge captured
by models. To address this demand, we introduce VenusX, the first large-scale
benchmark for fine-grained functional annotation and function-based protein
pairing at the residue, fragment, and domain levels. VenusX comprises three
major task categories across six types of annotations, including residue-level
binary classification, fragment-level multi-class classification, and pairwise
functional similarity scoring for identifying critical active sites, binding
sites, conserved sites, motifs, domains, and epitopes. The benchmark features
over 878,000 samples curated from major open-source databases such as InterPro,
BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three
sequence identity thresholds, our benchmark enables a comprehensive assessment
of model performance on both in-distribution and out-of-distribution scenarios.
For baseline evaluation, we assess a diverse set of popular and open-source
models, including pre-trained protein language models, sequence-structure
hybrids, structure-based methods, and alignment-based techniques. Their
performance is reported across all benchmark datasets and evaluation settings
using multiple metrics, offering a thorough comparison and a strong foundation
for future research. Code and data are publicly available at
https://github.com/ai4protein/VenusX.

</details>


### [428] [Concept-Guided Interpretability via Neural Chunking](https://arxiv.org/abs/2505.11576)
*Shuchen Wu,Stephan Alaniz,Shyamgopal Karthik,Peter Dayan,Eric Schulz,Zeynep Akata*

Main category: cs.LG

TL;DR: The paper challenges the view of neural networks as black boxes by proposing the Reflection Hypothesis, which suggests their activity mirrors training data patterns. It introduces three methods (DSC, PA, UCD) to extract interpretable units from neural dynamics, demonstrating their effectiveness across model sizes and architectures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding neural networks by revealing patterns in their activity that reflect training data, moving beyond the black-box perception.

Method: Proposes three cognitively-inspired methods: Discrete Sequence Chunking (DSC), Population Averaging (PA), and Unsupervised Chunk Discovery (UCD) to segment neural dynamics into interpretable units.

Result: Shows robust correspondence between extracted entities and concepts, and demonstrates that inducing these entities alters network behavior. Effective across RNNs and LLMs.

Conclusion: Offers a new interpretability direction by combining cognitive principles and data structure, transforming neural networks from black boxes into understandable systems.

Abstract: Neural networks are often black boxes, reflecting the significant challenge
of understanding their internal workings. We propose a different perspective
that challenges the prevailing view: rather than being inscrutable, neural
networks exhibit patterns in their raw population activity that mirror
regularities in the training data. We refer to this as the Reflection
Hypothesis and provide evidence for this phenomenon in both simple recurrent
neural networks (RNNs) and complex large language models (LLMs). Building on
this insight, we propose to leverage cognitively-inspired methods of chunking
to segment high-dimensional neural population dynamics into interpretable units
that reflect underlying concepts. We propose three methods to extract these
emerging entities, complementing each other based on label availability and
dimensionality. Discrete sequence chunking (DSC) creates a dictionary of
entities; population averaging (PA) extracts recurring entities that correspond
to known labels; and unsupervised chunk discovery (UCD) can be used when labels
are absent. We demonstrate the effectiveness of these methods in extracting
entities across varying model sizes, ranging from inducing compositionality in
RNNs to uncovering recurring neural population states in large models with
diverse architectures, and illustrate their advantage over other methods.
Throughout, we observe a robust correspondence between the extracted entities
and concrete or abstract concepts. Artificially inducing the extracted entities
in neural populations effectively alters the network's generation of associated
concepts. Our work points to a new direction for interpretability, one that
harnesses both cognitive principles and the structure of naturalistic data to
reveal the hidden computations of complex learning systems, gradually
transforming them from black boxes into systems we can begin to understand.

</details>


### [429] [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
*Chi-Min Chan,Chunpu Xu,Jiaming Ji,Zhen Ye,Pengcheng Wen,Chunyang Jiang,Yaodong Yang,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: The paper introduces J1-7B, a model fine-tuned on reflection-enhanced datasets and trained with RL, outperforming previous LLM-as-a-Judge methods by 4.8% and showing a 5.1% stronger scaling trend under STTS.


<details>
  <summary>Details</summary>
Motivation: The shift from model training to evaluation quality in AI research highlights the need for interpretable and scalable evaluation methods, addressing the limitations of traditional reward models.

Method: J1-7B is fine-tuned on reflection-enhanced datasets via rejection-sampling and trained with RL using verifiable rewards. STTS is applied at inference for performance improvement.

Result: J1-7B outperforms prior LLM-as-a-Judge by 4.8% and shows a 5.1% stronger scaling trend under STTS. Key findings reveal scaling trends emerge primarily during RL training.

Conclusion: The study demonstrates the effectiveness of RL training in acquiring STTS capability, advancing interpretable and scalable AI evaluation methods.

Abstract: The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.

</details>


### [430] [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594)
*Jintao Zhang,Jia Wei,Pengle Zhang,Xiaoming Xu,Haofeng Huang,Haoxu Wang,Kai Jiang,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: The paper introduces FP4 and 8-bit attention methods to improve efficiency in both inference and training tasks, achieving significant speedups and maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the quadratic time complexity of attention mechanisms by leveraging low-bit computation for enhanced efficiency in both inference and training.

Method: 1. Uses FP4 Tensor Cores in Blackwell GPUs for faster attention computation. 2. Designs an 8-bit attention mechanism for training tasks, including forward and backward propagation.

Result: FP4 attention achieves a 5x speedup over FlashAttention on RTX5090. 8-bit attention works losslessly in fine-tuning but shows slower convergence in pretraining.

Conclusion: Low-bit attention is viable for inference and fine-tuning, though pretraining requires further optimization. The methods are plug-and-play and code is publicly available.

Abstract: The efficiency of attention is important due to its quadratic time
complexity. We enhance the efficiency of attention through two key
contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to
accelerate attention computation. Our implementation achieves 1038 TOPS on
RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.
Experiments show that our FP4 attention can accelerate inference of various
models in a plug-and-play way. Second, we pioneer low-bit attention to training
tasks. Existing low-bit attention works like FlashAttention3 and SageAttention
focus only on inference. However, the efficiency of training large models is
also important. To explore whether low-bit attention can be effectively applied
to training tasks, we design an accurate and efficient 8-bit attention for both
forward and backward propagation. Experiments indicate that 8-bit attention
achieves lossless performance in fine-tuning tasks but exhibits slower
convergence in pretraining tasks. The code will be available at
https://github.com/thu-ml/SageAttention.

</details>


### [431] [Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach](https://arxiv.org/abs/2505.11645)
*Jinzhou Cao,Xiangxu Wang,Jiashi Chen,Wei Tu,Zhenhui Li,Xindong Yang,Tianhong Zhao,Qingquan Li*

Main category: cs.LG

TL;DR: SemiGTX is a semi-supervised graph learning framework for sectoral economic mapping, integrating geospatial data and achieving high R2 scores in GDP prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook semi-supervised learning in data-scarce scenarios and lack unified multi-task frameworks for comprehensive economic analysis.

Method: SemiGTX uses fusion encoding for geospatial data, a semi-information loss function, and multi-task learning to map GDP across sectors.

Result: Achieves R2 scores of 0.93, 0.96, and 0.94 for primary, secondary, and tertiary sectors, with cross-regional generality.

Conclusion: SemiGTX enhances economic monitoring and forecasting through explainable, integrated urban data analysis.

Abstract: Fine-grained economic mapping through urban representation learning has
emerged as a crucial tool for evidence-based economic decisions. While existing
methods primarily rely on supervised or unsupervised approaches, they often
overlook semi-supervised learning in data-scarce scenarios and lack unified
multi-task frameworks for comprehensive sectoral economic analysis. To address
these gaps, we propose SemiGTX, an explainable semi-supervised graph learning
framework for sectoral economic mapping. The framework is designed with
dedicated fusion encoding modules for various geospatial data modalities,
seamlessly integrating them into a cohesive graph structure. It introduces a
semi-information loss function that combines spatial self-supervision with
locally masked supervised regression, enabling more informative and effective
region representations. Through multi-task learning, SemiGTX concurrently maps
GDP across primary, secondary, and tertiary sectors within a unified model.
Extensive experiments conducted in the Pearl River Delta region of China
demonstrate the model's superior performance compared to existing methods,
achieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and
tertiary sectors, respectively. Cross-regional experiments in Beijing and
Chengdu further illustrate its generality. Systematic analysis reveals how
different data modalities influence model predictions, enhancing explainability
while providing valuable insights for regional development planning. This
representation learning framework advances regional economic monitoring through
diverse urban data integration, providing a robust foundation for precise
economic forecasting.

</details>


### [432] [Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling](https://arxiv.org/abs/2505.12225)
*Jizhou Guo,Zhaomin Wu,Philip S. Yu*

Main category: cs.LG

TL;DR: ELHSR is a parameter-efficient reward model using LLM hidden states, outperforming baselines with minimal parameters and computational cost.


<details>
  <summary>Details</summary>
Motivation: Current reward models for LLMs are computationally expensive and parameter-heavy, limiting practicality.

Method: Introduces ELHSR, leveraging LLM hidden states for efficiency, requiring few training samples and minimal parameters.

Result: ELHSR outperforms baselines with <0.005% parameters, achieves efficiency gains, and works well with logits or combined with traditional models.

Conclusion: ELHSR offers a scalable, efficient solution for reward modeling, applicable even to closed-source LLMs.

Abstract: High-quality reward models are crucial for unlocking the reasoning potential
of large language models (LLMs), with best-of-N voting demonstrating
significant performance gains. However, current reward models, which typically
operate on the textual output of LLMs, are computationally expensive and
parameter-heavy, limiting their real-world applications. We introduce the
Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly
parameter-efficient approach that leverages the rich information embedded in
LLM hidden states to address these issues. ELHSR systematically outperform
baselines with less than 0.005% of the parameters of baselines, requiring only
a few samples for training. ELHSR also achieves orders-of-magnitude efficiency
improvement with significantly less time and fewer FLOPs per sample than
baseline reward models. Moreover, ELHSR exhibits robust performance even when
trained only on logits, extending its applicability to some closed-source LLMs.
In addition, ELHSR can also be combined with traditional reward models to
achieve additional performance gains.

</details>


### [433] [MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging](https://arxiv.org/abs/2505.11883)
*Zihuan Qiu,Yi Xu,Chiyuan He,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: MINGLE is a novel framework for test-time continual model merging, addressing parameter interference and adaptability issues with dynamic adaptation and null-space constraints.


<details>
  <summary>Details</summary>
Motivation: Current continual model merging methods suffer from parameter interference and limited adaptability to evolving test distributions, leading to catastrophic forgetting and ineffective adaptation.

Method: MINGLE uses test-time adaptation with unlabeled samples, a mixture-of-experts architecture, Null-Space Constrained Gating, and an Adaptive Relaxation Strategy.

Result: MINGLE reduces forgetting, improves robustness, and outperforms state-of-the-art methods by 7-9% on average.

Conclusion: MINGLE effectively addresses key challenges in continual model merging, offering scalable and efficient adaptation with minimal forgetting.

Abstract: Continual model merging integrates independently fine-tuned models
sequentially without access to original training data, providing a scalable and
efficient solution to continual learning. However, current methods still face
critical challenges, notably parameter interference among tasks and limited
adaptability to evolving test distributions. The former causes catastrophic
forgetting of integrated tasks, while the latter hinders effective adaptation
to new tasks. To address these, we propose MINGLE, a novel framework for
test-time continual model merging, which leverages test-time adaptation using a
small set of unlabeled test samples from the current task to dynamically guide
the merging process. MINGLE employs a mixture-of-experts architecture composed
of parameter-efficient, low-rank experts, enabling efficient adaptation and
improving robustness to distribution shifts. To mitigate catastrophic
forgetting, we propose Null-Space Constrained Gating, which restricts gating
updates to subspaces orthogonal to prior task representations. This suppresses
activations on old task inputs and preserves model behavior on past tasks. To
further balance stability and adaptability, we design an Adaptive Relaxation
Strategy, which dynamically adjusts the constraint strength based on
interference signals captured during test-time adaptation. Extensive
experiments on standard continual merging benchmarks demonstrate that MINGLE
achieves robust generalization, reduces forgetting significantly, and
consistently surpasses previous state-of-the-art methods by 7-9\% on average
across diverse task orders.

</details>


### [434] [Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation](https://arxiv.org/abs/2505.11998)
*Prashant Shivaram Bhat,Shakib Yazdani,Elahe Arani,Bahram Zonooz*

Main category: cs.LG

TL;DR: PEARL is a rehearsal-free CL framework using dynamic rank allocation for LoRA to address catastrophic forgetting, outperforming baselines across multiple architectures.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting in CL undermines consolidated knowledge when learning new tasks, and current LoRA-based methods are sensitive to rank selection.

Method: PEARL dynamically allocates ranks for LoRA components based on task proximity to reference weights in parameter space.

Result: PEARL outperforms baselines across ResNet, Separable Convolutional Network, and Vision Transformer in various CL scenarios.

Conclusion: PEARL effectively mitigates catastrophic forgetting with adaptive rank allocation, demonstrating superior performance in CL.

Abstract: Catastrophic forgetting has remained a critical challenge for deep neural
networks in Continual Learning (CL) as it undermines consolidated knowledge
when learning new tasks. Parameter efficient fine tuning CL techniques are
gaining traction for their effectiveness in addressing catastrophic forgetting
with a lightweight training schedule while avoiding degradation of consolidated
knowledge in pre-trained models. However, low rank adapters (LoRA) in these
approaches are highly sensitive to rank selection which can lead to sub-optimal
resource allocation and performance. To this end, we introduce PEARL, a
rehearsal-free CL framework that entails dynamic rank allocation for LoRA
components during CL training. Specifically, PEARL leverages reference task
weights and adaptively determines the rank of task-specific LoRA components
based on the current tasks' proximity to reference task weights in parameter
space. To demonstrate the versatility of PEARL, we evaluate it across three
vision architectures (ResNet, Separable Convolutional Network and Vision
Transformer) and a multitude of CL scenarios, and show that PEARL outperforms
all considered baselines by a large margin.

</details>


### [435] [UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection](https://arxiv.org/abs/2505.12457)
*Yang Zhao,Kai Xiong,Xiao Ding,Li Du,YangouOuyang,Zhouhao Sun,Jiannan Guan,Wenbin Zhang,Bin Liu,Dong Hu,Bing Qin,Ting Liu*

Main category: cs.LG

TL;DR: UFO-RL, a novel framework, uses efficient single-pass uncertainty estimation to select data within LLMs' potential comprehension zone, reducing training time by up to 16x while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Scaling RL for LLMs is computationally expensive due to multi-sampling for policy optimization and evaluation. The Zone of Proximal Development (ZPD) theory suggests LLMs learn best from data within their comprehension zone.

Method: UFO-RL introduces a single-pass uncertainty estimation method to identify informative data instances, avoiding conventional multi-sampling. It selects data within the estimated ZPD for training.

Result: Training with 10% of UFO-RL-selected data matches or exceeds full-data performance, reducing training time by up to 16x and improving stability and generalization.

Conclusion: UFO-RL provides a practical, efficient strategy for scaling RL fine-tuning of LLMs by focusing on valuable data.

Abstract: Scaling RL for LLMs is computationally expensive, largely due to
multi-sampling for policy optimization and evaluation, making efficient data
selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,
we hypothesize LLMs learn best from data within their potential comprehension
zone. Addressing the limitation of conventional, computationally intensive
multi-sampling methods for data assessment, we introduce UFO-RL. This novel
framework uses a computationally efficient single-pass uncertainty estimation
to identify informative data instances, achieving up to 185x faster data
evaluation. UFO-RL leverages this metric to select data within the estimated
ZPD for training. Experiments show that training with just 10% of data selected
by UFO-RL yields performance comparable to or surpassing full-data training,
reducing overall training time by up to 16x while enhancing stability and
generalization. UFO-RL offers a practical and highly efficient strategy for
scaling RL fine-tuning of LLMs by focusing learning on valuable data.

</details>


### [436] [Enhancing Latent Computation in Transformers with Latent Tokens](https://arxiv.org/abs/2505.12629)
*Yuchang Sun,Yanxi Chen,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: The paper introduces latent tokens, dummy tokens that enhance Transformer-based LLMs by steering autoregressive decoding via attention, improving performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM performance and adaptability, especially in out-of-distribution scenarios, using auxiliary tokens without significant infrastructure changes.

Method: Proposes latent tokens—non-interpretable dummy tokens integrated into pre-trained Transformers, trained efficiently, and applied flexibly during inference.

Result: Latent tokens outperform baselines, particularly in out-of-distribution generalization, confirming their effectiveness.

Conclusion: Latent tokens offer a lightweight, efficient way to improve LLM adaptability and performance, with potential for broader applications.

Abstract: Augmenting large language models (LLMs) with auxiliary tokens has emerged as
a promising strategy for enhancing model performance. In this work, we
introduce a lightweight method termed latent tokens; these are dummy tokens
that may be non-interpretable in natural language but steer the autoregressive
decoding process of a Transformer-based LLM via the attention mechanism. The
proposed latent tokens can be seamlessly integrated with a pre-trained
Transformer, trained in a parameter-efficient manner, and applied flexibly at
inference time, while adding minimal complexity overhead to the existing
infrastructure of standard Transformers. We propose several hypotheses about
the underlying mechanisms of latent tokens and design synthetic tasks
accordingly to verify them. Numerical results confirm that the proposed method
noticeably outperforms the baselines, particularly in the out-of-distribution
generalization scenarios, highlighting its potential in improving the
adaptability of LLMs.

</details>


### [437] [Model alignment using inter-modal bridges](https://arxiv.org/abs/2505.12322)
*Ali Gholamzadeh,Noor Sajid*

Main category: cs.LG

TL;DR: A semi-supervised method for aligning models across modalities using conditional flow matching, achieving performance comparable to end-to-end training with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations in model reuse across modalities due to representation misalignment, especially with scarce paired data.

Method: Uses conditional flow matching to align latent spaces via optimal transport or memory-efficient alignment with labeled exemplars.

Result: Matches end-to-end trained models on tasks like object recognition and image generation, even with <20% labeled data.

Conclusion: Provides a data-efficient solution for inter-modal alignment with minimal supervision.

Abstract: Foundation models have demonstrated remarkable performance across modalities
such as language and vision. However, model reuse across distinct modalities
(e.g., text and vision) remains limited due to the difficulty of aligning
internal representations. Existing methods require extensive paired training
data or are constrained to specific domains. We introduce a semi-supervised
approach for model alignment via conditional flow matching. The conditional
flow between latent spaces of different modalities (e.g., text-to-image or
biological-to-artificial neuronal activity) can be learned in two settings:
($1$) solving a (balanced or unbalanced) optimal transport problem with an
inter-space bridge cost, and ($2$) performing memory-efficient alignment using
labelled exemplars. Despite being constrained by the original models' capacity,
our method--under both settings--matches downstream task performance of
end-to-end trained models on object recognition and image generation tasks
across MNIST, ImageNet, and \cite{majaj2015simple} datasets, particularly when
labelled training data is scarce ($<20\%$). Our method provides a
data-efficient solution for inter-modal model alignment with minimal
supervision.

</details>


### [438] [Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization](https://arxiv.org/abs/2505.12763)
*Sunghwan Kim,Dongjin Kang,Taeyoon Kwon,Hyungjoo Chae,Dongha Lee,Jinyoung Yeo*

Main category: cs.LG

TL;DR: The paper addresses the weak correlation between reward model benchmarks and optimized policy performance, proposing better evaluation designs focusing on reward overoptimization.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for reward models inadequately assess their true capabilities, necessitating improved evaluation methods.

Method: The study explores evaluation designs through reward overoptimization, analyzing alignment with human preferences and learning signal dynamics.

Result: Key findings include minimizing response differences, requiring multiple comparisons, and sourcing diverse responses. High overoptimization correlation may reduce downstream performance correlation.

Conclusion: Benchmarks should use overoptimization as a tool, not the goal, to reliably assess reward models.

Abstract: Reward models (RMs) play a crucial role in reinforcement learning from human
feedback (RLHF), aligning model behavior with human preferences. However,
existing benchmarks for reward models show a weak correlation with the
performance of optimized policies, suggesting that they fail to accurately
assess the true capabilities of RMs. To bridge this gap, we explore several
evaluation designs through the lens of reward overoptimization\textemdash a
phenomenon that captures both how well the reward model aligns with human
preferences and the dynamics of the learning signal it provides to the policy.
The results highlight three key findings on how to construct a reliable
benchmark: (i) it is important to minimize differences between chosen and
rejected responses beyond correctness, (ii) evaluating reward models requires
multiple comparisons across a wide range of chosen and rejected responses, and
(iii) given that reward models encounter responses with diverse
representations, responses should be sourced from a variety of models. However,
we also observe that a extremely high correlation with degree of
overoptimization leads to comparatively lower correlation with certain
downstream performance. Thus, when designing a benchmark, it is desirable to
use the degree of overoptimization as a useful tool, rather than the end goal.

</details>


### [439] [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
*Zheng Wu,Pengzhou Cheng,Zongru Wu,Lingzhong Dong,Zhuosheng Zhang*

Main category: cs.LG

TL;DR: The paper introduces GEM, a method for detecting out-of-distribution (OOD) instructions in GUI agents by fitting a Gaussian mixture model to input embedding distances, improving accuracy by 23.70% over baselines.


<details>
  <summary>Details</summary>
Motivation: GUI agents struggle with OOD instructions, leading to task breakdowns or security risks, necessitating better OOD detection methods.

Method: Proposes GEM, which leverages the clustering pattern of in-distribution inputs and fits a Gaussian mixture model to embedding distances.

Result: Achieves a 23.70% accuracy improvement over baselines across eight datasets and shows strong generalization with nine backbones.

Conclusion: GEM effectively detects OOD instructions in GUI agents, enhancing reliability and security.

Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing
paradigm for human-computer interaction, capable of automatically executing
user instructions to operate intelligent terminal devices. However, when
encountering out-of-distribution (OOD) instructions that violate environmental
constraints or exceed the current capabilities of agents, GUI agents may suffer
task breakdowns or even pose security threats. Therefore, effective OOD
detection for GUI agents is essential. Traditional OOD detection methods
perform suboptimally in this domain due to the complex embedding space and
evolving GUI environments. In this work, we observe that the in-distribution
input semantic space of GUI agents exhibits a clustering pattern with respect
to the distance from the centroid. Based on the finding, we propose GEM, a
novel method based on fitting a Gaussian mixture model over input embedding
distances extracted from the GUI Agent that reflect its capability boundary.
Evaluated on eight datasets spanning smartphones, computers, and web browsers,
our method achieves an average accuracy improvement of 23.70\% over the
best-performing baseline. Analysis verifies the generalization ability of our
method through experiments on nine different backbones. The codes are available
at https://github.com/Wuzheng02/GEM-OODforGUIagents.

</details>


### [440] [Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models](https://arxiv.org/abs/2505.12343)
*Kai Tang,Jinhao You,Xiuqi Ge,Hanze Li,Yichen Guo,Xiande Huang*

Main category: cs.LG

TL;DR: DCLA is a novel decoding method for LVLMs that mitigates hallucinations by enforcing inter-layer consistency without retraining or external resources.


<details>
  <summary>Details</summary>
Motivation: LVLMs often generate inconsistent content (hallucinations), and existing mitigation methods are unstable and hyperparameter-sensitive.

Method: DCLA aggregates representations from previous layers to create a dynamic semantic reference, correcting deviated layers for consistency.

Result: DCLA reduces hallucinations in benchmarks like MME and POPE, improving LVLM reliability and performance.

Conclusion: DCLA offers a robust, training-free solution for hallucination mitigation in LVLMs.

Abstract: Despite the impressive capabilities of Large Vision-Language Models (LVLMs),
they remain susceptible to hallucinations-generating content that is
inconsistent with the input image. Existing training-free hallucination
mitigation methods often suffer from unstable performance and high sensitivity
to hyperparameter settings, limiting their practicality and broader adoption.
In this paper, we propose a novel decoding mechanism, Decoding with Inter-layer
Consistency via Layer Aggregation (DCLA), which requires no retraining,
fine-tuning, or access to external knowledge bases. Specifically, our approach
constructs a dynamic semantic reference by aggregating representations from
previous layers, and corrects semantically deviated layers to enforce
inter-layer consistency. The method allows DCLA to robustly mitigate
hallucinations across multiple LVLMs. Experiments on hallucination benchmarks
such as MME and POPE demonstrate that DCLA effectively reduces hallucinations
while enhancing the reliability and performance of LVLMs.

</details>


### [441] [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)
*Zi Liang,Haibo Hu,Qingqing Ye,Yaxin Xiao,Ronghua Li*

Main category: cs.LG

TL;DR: LoRA's low-rank structure is analyzed for robustness against data poisoning and backdoor attacks, showing better resistance to backdoor attacks but higher vulnerability to untargeted data poisoning.


<details>
  <summary>Details</summary>
Motivation: To investigate the security implications of LoRA's low-rank structure during fine-tuning, particularly its robustness against training-time attacks like data poisoning and backdoors.

Method: Proposes an analytical framework using neural tangent kernel and information theory to model LoRA's training dynamics and its vulnerability to attacks.

Result: LoRA is more robust to backdoor attacks but more vulnerable to untargeted data poisoning compared to full fine-tuning.

Conclusion: The study highlights LoRA's security trade-offs, providing insights for safer deployment in fine-tuning LLMs.

Abstract: Low rank adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large language models (LLMs) thanks to its superb efficiency gains
over previous methods. While extensive studies have examined the performance
and structural properties of LoRA, its behavior upon training-time attacks
remain underexplored, posing significant security risks. In this paper, we
theoretically investigate the security implications of LoRA's low-rank
structure during fine-tuning, in the context of its robustness against data
poisoning and backdoor attacks. We propose an analytical framework that models
LoRA's training dynamics, employs the neural tangent kernel to simplify the
analysis of the training process, and applies information theory to establish
connections between LoRA's low rank structure and its vulnerability against
training-time attacks. Our analysis indicates that LoRA exhibits better
robustness to backdoor attacks than full fine-tuning, while becomes more
vulnerable to untargeted data poisoning due to its over-simplified information
geometry. Extensive experimental evaluations have corroborated our theoretical
findings.

</details>


### [442] [STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference](https://arxiv.org/abs/2505.12359)
*Yichen Guo,Hanze Li,Zonghao Zhang,Jinhao You,Kai Tang,Xiande Huang*

Main category: cs.LG

TL;DR: STAR is a training-free, two-stage token pruning framework for LVLMs that reduces computational overhead while preserving performance by leveraging visual self-attention and cross-modal attention.


<details>
  <summary>Details</summary>
Motivation: Existing single-stage token pruning methods in LVLMs overlook broader information flow, causing performance degradation under high pruning ratios.

Method: STAR uses a two-stage approach: early-stage pruning via visual self-attention for redundant low-level features and later-stage pruning via cross-modal attention for task-irrelevant tokens.

Result: STAR achieves significant computational cost reduction while maintaining or improving performance across multiple LVLM benchmarks.

Conclusion: STAR offers a holistic, efficient solution for token pruning in LVLMs, balancing speed and accuracy.

Abstract: Although large vision-language models (LVLMs) leverage rich visual token
representations to achieve strong performance on multimodal tasks, these tokens
also introduce significant computational overhead during inference. Existing
training-free token pruning methods typically adopt a single-stage strategy,
focusing either on visual self-attention or visual-textual cross-attention.
However, such localized perspectives often overlook the broader information
flow across the model, leading to substantial performance degradation,
especially under high pruning ratios. In this work, we propose STAR (Stage-wise
Attention-guided token Reduction), a training-free, plug-and-play framework
that approaches token pruning from a global perspective. Instead of pruning at
a single point, STAR performs attention-guided reduction in two complementary
stages: an early-stage pruning based on visual self-attention to remove
redundant low-level features, and a later-stage pruning guided by cross-modal
attention to discard task-irrelevant tokens. This holistic approach allows STAR
to significantly reduce computational cost while better preserving
task-critical information. Extensive experiments across multiple LVLM
architectures and benchmarks show that STAR achieves strong acceleration while
maintaining comparable, and in some cases even improved performance.

</details>


### [443] [Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning](https://arxiv.org/abs/2505.12432)
*Zirun Guo,Minjie Hong,Tao Jin*

Main category: cs.LG

TL;DR: Observe-R1 is a framework enhancing multimodal large language models (MLLMs) via RL, using a gradual learning paradigm and a structured dataset (NeuraLadder). It improves reasoning and visual abilities with multimodal constraints and reward systems, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored challenges of adapting RL to multimodal data and formats, aiming to enhance MLLMs' reasoning capabilities.

Method: Introduces Observe-R1 with a gradual learning paradigm, NeuraLadder dataset, multimodal format constraints, bonus rewards for concise answers, and dynamic weighting for training prioritization.

Result: Outperforms larger reasoning models on benchmarks, achieving clearer and more concise reasoning chains, validated by ablation studies.

Conclusion: Observe-R1 demonstrates robustness and generalization, with plans to release the dataset and code.

Abstract: Reinforcement Learning (RL) has shown promise in improving the reasoning
abilities of Large Language Models (LLMs). However, the specific challenges of
adapting RL to multimodal data and formats remain relatively unexplored. In
this work, we present Observe-R1, a novel framework aimed at enhancing the
reasoning capabilities of multimodal large language models (MLLMs). We draw
inspirations from human learning progression--from simple to complex and easy
to difficult, and propose a gradual learning paradigm for MLLMs. To this end,
we construct the NeuraLadder dataset, which is organized and sampled according
to the difficulty and complexity of data samples for RL training. To tackle
multimodal tasks, we introduce a multimodal format constraint that encourages
careful observation of images, resulting in enhanced visual abilities and
clearer and more structured responses. Additionally, we implement a bonus
reward system that favors concise, correct answers within a length constraint,
alongside a dynamic weighting mechanism that prioritizes uncertain and
medium-difficulty problems, ensuring that more informative samples have a
greater impact on training. Our experiments with the Qwen2.5-VL-3B and
Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that
Observe-R1 outperforms a series of larger reasoning models on both reasoning
and general benchmarks, achieving superior clarity and conciseness in reasoning
chains. Ablation studies validate the effectiveness of our strategies,
highlighting the robustness and generalization of our approach. The dataset and
code will be released at https://github.com/zrguo/Observe-R1.

</details>


### [444] [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/abs/2505.12938)
*Uri Dalal,Meirav Segal,Zvika Ben-Haim,Dan Lahav,Omer Nevo*

Main category: cs.LG

TL;DR: The paper introduces a "Variator" agent that leverages LLMs' inconsistency to improve Pass@k performance by generating task variants and submitting solutions for each.


<details>
  <summary>Details</summary>
Motivation: LLMs show inconsistent performance with minor input changes, which the paper aims to exploit rather than fix.

Method: A task-agnostic "Variator" agent generates k variants of a task and submits one solution per variant, validated theoretically and empirically.

Result: The method outperforms baselines on the APPS dataset and shows inconsistency persists in frontier models.

Conclusion: The approach remains relevant for future models, highlighting the utility of inconsistency.

Abstract: Large language models (LLMs) achieve impressive abilities in numerous
domains, but exhibit inconsistent performance in response to minor input
changes. Rather than view this as a drawback, in this paper we introduce a
novel method for leveraging models' inconsistency to boost Pass@k performance.
Specifically, we present a "Variator" agent that generates k variants of a
given task and submits one candidate solution for each one. Our variant
generation approach is applicable to a wide range of domains as it is task
agnostic and compatible with free-form inputs. We demonstrate the efficacy of
our agent theoretically using a probabilistic model of the inconsistency
effect, and show empirically that it outperforms the baseline on the APPS
dataset. Furthermore, we establish that inconsistency persists even in frontier
reasoning models across coding and cybersecurity domains, suggesting our method
is likely to remain relevant for future model generations.

</details>


### [445] [Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning](https://arxiv.org/abs/2505.12477)
*Hugues Van Assel,Mark Ibrahim,Tommaso Biancalani,Aviv Regev,Randall Balestriero*

Main category: cs.LG

TL;DR: The paper compares reconstruction and joint embedding in SSL, showing joint embedding is better when irrelevant features are prominent due to weaker alignment requirements.


<details>
  <summary>Details</summary>
Motivation: To clarify the trade-offs between reconstruction and joint embedding in SSL and provide guidelines for choosing between them.

Method: Leveraging closed-form solutions to analyze the impact of view generation (e.g., data augmentation) on learned representations.

Result: Joint embedding methods require weaker alignment conditions than reconstruction, making them preferable for datasets with large irrelevant features.

Conclusion: Joint embedding is empirically superior in challenging real-world datasets due to its alignment flexibility.

Abstract: Reconstruction and joint embedding have emerged as two leading paradigms in
Self Supervised Learning (SSL). Reconstruction methods focus on recovering the
original sample from a different view in input space. On the other hand, joint
embedding methods align the representations of different views in latent space.
Both approaches offer compelling advantages, yet practitioners lack clear
guidelines for choosing between them. In this work, we unveil the core
mechanisms that distinguish each paradigm. By leveraging closed form solutions
for both approaches, we precisely characterize how the view generation process,
e.g. data augmentation, impacts the learned representations. We then
demonstrate that, unlike supervised learning, both SSL paradigms require a
minimal alignment between augmentations and irrelevant features to achieve
asymptotic optimality with increasing sample size. Our findings indicate that
in scenarios where these irrelevant features have a large magnitude, joint
embedding methods are preferable because they impose a strictly weaker
alignment condition compared to reconstruction based methods. These results not
only clarify the trade offs between the two paradigms but also substantiate the
empirical success of joint embedding approaches on real world challenging
datasets.

</details>


### [446] [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
*Baohao Liao,Hanze Dong,Yuhui Xu,Doyen Sahoo,Christof Monz,Junnan Li,Caiming Xiong*

Main category: cs.LG

TL;DR: Fractured Sampling improves LLM reasoning efficiency by interpolating between full Chain-of-Thought (CoT) and solution-only sampling, reducing token costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current CoT methods incur high token costs, limiting deployment in latency-sensitive settings. Truncated CoT shows promise but lacks a unified framework.

Method: Introduces Fractured Sampling, a strategy balancing full CoT and solution-only sampling along three axes: reasoning trajectories, solutions per trajectory, and truncation depth.

Result: Demonstrates superior accuracy-cost trade-offs across benchmarks, with log-linear scaling gains in Pass@k versus token budget.

Conclusion: Fractured Sampling optimizes computation allocation for efficient and scalable LLM reasoning.

Abstract: Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning.

</details>


### [447] [Scalable Strategies for Continual Learning with Replay](https://arxiv.org/abs/2505.12512)
*Truman Hickok*

Main category: cs.LG

TL;DR: The paper addresses scalability in continual learning by integrating low rank adaptation, consolidation (a phasic replay approach), and sequential merging, outperforming standalone methods.


<details>
  <summary>Details</summary>
Motivation: Future deep learning models require efficient continual learning methods to handle bidirectional transfer and reconcile new information with past knowledge, but current replay-based approaches are unscalable.

Method: The paper applies low rank adaptation, introduces consolidation for reduced replay samples, and proposes sequential merging tailored for continual learning.

Result: The combined strategies outperform standalone variants, achieving up to 55% fewer replay samples for the same performance.

Conclusion: The developed scalable toolset synergistically enhances continual learning, bridging gaps between replay and multi-task fine-tuning techniques.

Abstract: Future deep learning models will be distinguished by systems that perpetually
learn through interaction, imagination, and cooperation, blurring the line
between training and inference. This makes continual learning a critical
challenge, as methods that efficiently maximize bidirectional transfer across
learning trajectories will be essential. Replay is on track to play a
foundational role in continual learning, allowing models to directly reconcile
new information with past knowledge. In practice, however, replay is quite
unscalable, doubling the cost of continual learning when applied naively.
Moreover, the continual learning literature has not fully synchronized with the
multi-task fine-tuning literature, having not fully integrated highly scalable
techniques like model merging and low rank adaptation into a replay-enabled
toolset that can produce a unified model in the face of many sequential tasks.
In this paper, we begin by applying and analyzing low rank adaptation in a
continual learning setting. Next, we introduce consolidation, a phasic approach
to replay which leads to up to 55\% less replay samples being needed for a
given performance target. Then, we propose sequential merging, an offshoot of
task arithmetic which is tailored to the continual learning setting and is
shown to work well in combination with replay. Finally, we demonstrate that the
developed strategies can operate synergistically, resulting in a highly
scalable toolset that outperforms standalone variants.

</details>


### [448] [An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification](https://arxiv.org/abs/2505.12581)
*Lucas M. Dorneles,Luan Fonseca Garcia,Joel Luís Carbonera*

Main category: cs.LG

TL;DR: The paper proposes a method to quantitatively analyze how data augmentation affects patterns learned by neural networks in image classification, using class activation maps and metrics.


<details>
  <summary>Details</summary>
Motivation: There's limited research on how data augmentation impacts the patterns learned by neural networks in complex datasets.

Method: Uses class activation maps to measure pixel importance and compares models trained with different augmentation strategies.

Result: Experiments show data augmentation effects can be analyzed and reveal distinct impact profiles on models.

Conclusion: The methodology successfully quantifies data augmentation effects, aiding deeper understanding of model behavior.

Abstract: Neural networks have become increasingly popular in the last few years as an
effective tool for the task of image classification due to the impressive
performance they have achieved on this task. In image classification tasks, it
is common to use data augmentation strategies to increase the robustness of
trained networks to changes in the input images and to avoid overfitting.
Although data augmentation is a widely adopted technique, the literature lacks
a body of research analyzing the effects data augmentation methods have on the
patterns learned by neural network models working on complex datasets. The
primary objective of this work is to propose a methodology and set of metrics
that may allow a quantitative approach to analyzing the effects of data
augmentation in convolutional networks applied to image classification. An
important tool used in the proposed approach lies in the concept of class
activation maps for said models, which allow us to identify and measure the
importance these models assign to each individual pixel in an image when
executing the classification task. From these maps, we may then extract metrics
over the similarities and differences between maps generated by these models
trained on a given dataset with different data augmentation strategies.
Experiments made using this methodology suggest that the effects of these data
augmentation techniques not only can be analyzed in this way but also allow us
to identify different impact profiles over the trained models.

</details>


### [449] [Two out of Three (ToT): using self-consistency to make robust predictions](https://arxiv.org/abs/2505.12642)
*Jung Hoon Lee,Sujith Vijayan*

Main category: cs.LG

TL;DR: The paper proposes an algorithm (ToT) to improve DL model robustness by allowing abstention when uncertain, inspired by human brain sensitivity to conflicting information.


<details>
  <summary>Details</summary>
Motivation: DL models lack interpretability, posing risks in high-stakes domains; the goal is to enhance decision robustness.

Method: The ToT algorithm generates two alternative predictions alongside the original to determine when to abstain.

Result: ToT helps DL models avoid uncertain decisions, improving reliability.

Conclusion: The ToT algorithm enhances DL model robustness by enabling abstention in uncertain scenarios.

Abstract: Deep learning (DL) can automatically construct intelligent agents, deep
neural networks (alternatively, DL models), that can outperform humans in
certain tasks. However, the operating principles of DL remain poorly
understood, making its decisions incomprehensible. As a result, it poses a
great risk to deploy DL in high-stakes domains in which mistakes or errors may
lead to critical consequences. Here, we aim to develop an algorithm that can
help DL models make more robust decisions by allowing them to abstain from
answering when they are uncertain. Our algorithm, named `Two out of Three
(ToT)', is inspired by the sensitivity of the human brain to conflicting
information. ToT creates two alternative predictions in addition to the
original model prediction and uses the alternative predictions to decide
whether it should provide an answer or not.

</details>


### [450] [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
*Guangda Liu,Chengwei Li,Zhenyu Ning,Jing Lin,Yiwu Yao,Danning Ke,Minyi Guo,Jieru Zhao*

Main category: cs.LG

TL;DR: FreeKV is a framework optimizing KV cache efficiency in LLMs, combining algorithmic and system improvements to achieve near-lossless accuracy and significant speedup.


<details>
  <summary>Details</summary>
Motivation: Long contexts in LLMs create deployment challenges due to the growing KV cache size, with existing methods compromising accuracy or efficiency.

Method: FreeKV uses speculative retrieval and fine-grained correction (algorithm) and hybrid KV layouts with double-buffered recall (system).

Result: Achieves near-lossless accuracy and up to 13x speedup over state-of-the-art KV retrieval methods.

Conclusion: FreeKV effectively balances accuracy and efficiency for long-context LLM deployment.

Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding
context windows to support increasingly demanding applications. However, long
contexts pose significant deployment challenges, primarily due to the KV cache
whose size grows proportionally with context length. While KV cache compression
methods are proposed to address this issue, KV dropping methods incur
considerable accuracy loss, and KV retrieval methods suffer from significant
efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization
framework to enhance KV retrieval efficiency while preserving accuracy. On the
algorithm side, FreeKV introduces speculative retrieval to shift the KV
selection and recall processes out of the critical path, combined with
fine-grained correction to ensure accuracy. On the system side, FreeKV employs
hybrid KV layouts across CPU and GPU memory to eliminate fragmented data
transfers, and leverages double-buffered streamed recall to further improve
efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy
across various scenarios and models, delivering up to 13$\times$ speedup
compared to SOTA KV retrieval methods.

</details>


### [451] [On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning](https://arxiv.org/abs/2505.12681)
*Hana Satou,Alan Mitkiy*

Main category: cs.LG

TL;DR: The paper explores adversarial data augmentation (ADA) to improve robustness and adaptivity in transfer learning, proposing a unified framework that integrates ADA with consistency regularization and domain-invariant learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of transfer learning across domains with distribution shift by leveraging adversarial perturbations constructively.

Method: Proposes a unified framework combining ADA, consistency regularization, and domain-invariant representation learning, tested on datasets like VisDA, DomainNet, and Office-Home.

Result: Demonstrates improved target-domain performance in unsupervised and few-shot settings, showing adversarial examples can enhance domain generalization.

Conclusion: Adversarial learning can be constructive, turning perturbations into a regularizing force for better cross-domain transferability.

Abstract: Transfer learning across domains with distribution shift remains a
fundamental challenge in building robust and adaptable machine learning
systems. While adversarial perturbations are traditionally viewed as threats
that expose model vulnerabilities, recent studies suggest that they can also
serve as constructive tools for data augmentation. In this work, we
systematically investigate the role of adversarial data augmentation (ADA) in
enhancing both robustness and adaptivity in transfer learning settings. We
analyze how adversarial examples, when used strategically during training,
improve domain generalization by enriching decision boundaries and reducing
overfitting to source-domain-specific features. We further propose a unified
framework that integrates ADA with consistency regularization and
domain-invariant representation learning. Extensive experiments across multiple
benchmark datasets -- including VisDA, DomainNet, and Office-Home --
demonstrate that our method consistently improves target-domain performance
under both unsupervised and few-shot domain adaptation settings. Our results
highlight a constructive perspective of adversarial learning, transforming
perturbation from a destructive attack into a regularizing force for
cross-domain transferability.

</details>


### [452] [Structure-based Anomaly Detection and Clustering](https://arxiv.org/abs/2505.12751)
*Filippo Leveni*

Main category: cs.LG

TL;DR: The paper introduces unsupervised methods for anomaly detection in structured and streaming data, including Preference Isolation Forest (PIF) for structured data and Online-iForest for streaming data, outperforming existing techniques. It also proposes MultiLink for structure-based clustering and enhances malware detection with MaxLogit.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is critical in healthcare, manufacturing, and cybersecurity, but existing methods struggle with structured and streaming data. The thesis aims to address these gaps.

Method: Proposes PIF for structured data (Voronoi-iForest and RuzHash-iForest variants) and Sliding-PIF for streaming data. Introduces MultiLink for clustering and Online-iForest for evolving streams. Enhances malware detection with MaxLogit.

Result: Methods outperform existing techniques on synthetic and real datasets. Online-iForest matches offline accuracy without retraining. MaxLogit improves malware detection in production.

Conclusion: The proposed methods advance anomaly detection in structured and streaming data, offering scalability, robustness, and real-time efficiency, with practical applications in cybersecurity.

Abstract: Anomaly detection is a fundamental problem in domains such as healthcare,
manufacturing, and cybersecurity. This thesis proposes new unsupervised methods
for anomaly detection in both structured and streaming data settings. In the
first part, we focus on structure-based anomaly detection, where normal data
follows low-dimensional manifolds while anomalies deviate from them. We
introduce Preference Isolation Forest (PIF), which embeds data into a
high-dimensional preference space via manifold fitting, and isolates outliers
using two variants: Voronoi-iForest, based on geometric distances, and
RuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also
propose Sliding-PIF, which captures local manifold information for streaming
scenarios. Our methods outperform existing techniques on synthetic and real
datasets. We extend this to structure-based clustering with MultiLink, a novel
method for recovering multiple geometric model families in noisy data.
MultiLink merges clusters via a model-aware linkage strategy, enabling robust
multi-class structure recovery. It offers key advantages over existing
approaches, such as speed, reduced sensitivity to thresholds, and improved
robustness to poor initial sampling. The second part of the thesis addresses
online anomaly detection in evolving data streams. We propose Online Isolation
Forest (Online-iForest), which uses adaptive, multi-resolution histograms and
dynamically updates tree structures to track changes over time. It avoids
retraining while achieving accuracy comparable to offline models, with superior
efficiency for real-time applications. Finally, we tackle anomaly detection in
cybersecurity via open-set recognition for malware classification. We enhance a
Gradient Boosting classifier with MaxLogit to detect unseen malware families, a
method now integrated into Cleafy's production system.

</details>


### [453] [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space](https://arxiv.org/abs/2505.13308)
*Hengli Li,Chenxi Li,Tong Wu,Xuekai Zhu,Yuxuan Wang,Zhaoxin Yu,Eric Hanchen Jiang,Song-Chun Zhu,Zixia Jia,Ying Nian Wu,Zilong Zheng*

Main category: cs.LG

TL;DR: LatentSeek enhances LLM reasoning via test-time latent space adaptation, outperforming baselines like Chain-of-Thought prompting and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in LLM reasoning, such as catastrophic forgetting and limited training data, by leveraging latent space for test-time scaling.

Method: LatentSeek uses policy gradient to iteratively update latent representations with self-generated rewards, evaluated on benchmarks like GSM8K and MATH-500.

Result: LatentSeek consistently outperforms baselines, converges quickly, and benefits from additional iterations.

Conclusion: LatentSeek is a lightweight, scalable solution for improving LLM reasoning, highlighting the potential of test-time scaling in latent space.

Abstract: Reasoning ability, a core component of human intelligence, continues to pose
a significant challenge for Large Language Models (LLMs) in the pursuit of AGI.
Although model performance has improved under the training scaling law,
significant challenges remain, particularly with respect to training
algorithms, such as catastrophic forgetting, and the limited availability of
novel training data. As an alternative, test-time scaling enhances reasoning
performance by increasing test-time computation without parameter updating.
Unlike prior methods in this paradigm focused on token space, we propose
leveraging latent space for more effective reasoning and better adherence to
the test-time scaling law. We introduce LatentSeek, a novel framework that
enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)
within the model's latent space. Specifically, LatentSeek leverages policy
gradient to iteratively update latent representations, guided by self-generated
reward signals. LatentSeek is evaluated on a range of reasoning benchmarks,
including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.
Results show that LatentSeek consistently outperforms strong baselines, such as
Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our
analysis demonstrates that LatentSeek is highly efficient, typically converging
within a few iterations for problems of average complexity, while also
benefiting from additional iterations, thereby highlighting the potential of
test-time scaling in the latent space. These findings position LatentSeek as a
lightweight, scalable, and effective solution for enhancing the reasoning
capabilities of LLMs.

</details>


### [454] [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
*Matan Abudy,Orr Well,Emmanuel Chemla,Roni Katzir,Nur Lan*

Main category: cs.LG

TL;DR: Standard regularization methods (L1, L2, or none) push expressive neural networks away from perfect solutions, while MDL-based regularization selects perfect solutions by balancing model complexity and data fit.


<details>
  <summary>Details</summary>
Motivation: To address the issue of expressive neural networks converging to approximations instead of perfect solutions due to standard regularization methods.

Method: Compare standard regularization (L1, L2, none) with MDL-based regularization on formal languages, analyzing their impact on solution convergence.

Result: MDL-based regularization selects perfect solutions over approximations, unlike standard methods, and provides better inductive bias for generalization.

Conclusion: MDL is a theoretically grounded regularization method that counteracts overfitting and promotes generalization, outperforming traditional techniques.

Abstract: State-of-the-art neural networks can be trained to become remarkable
solutions to many problems. But while these architectures can express symbolic,
perfect solutions, trained models often arrive at approximations instead. We
show that the choice of regularization method plays a crucial role: when
trained on formal languages with standard regularization ($L_1$, $L_2$, or
none), expressive architectures not only fail to converge to correct solutions
but are actively pushed away from perfect initializations. In contrast,
applying the Minimum Description Length (MDL) principle to balance model
complexity with data fit provides a theoretically grounded regularization
method. Using MDL, perfect solutions are selected over approximations,
independently of the optimization algorithm. We propose that unlike existing
regularization techniques, MDL introduces the appropriate inductive bias to
effectively counteract overfitting and promote generalization.

</details>


### [455] [TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks](https://arxiv.org/abs/2505.12884)
*Yuanze Hu,Zhaoxin Fan,Xinyu Wang,Gen Li,Ye Qiu,Zhichao Yang,Wenjun Wu,Kejian Wu,Yifan Sun,Xiaotie Deng,Jin Dong*

Main category: cs.LG

TL;DR: TinyAlign improves alignment in lightweight VLMs by retrieving context to enhance multimodal inputs, achieving better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Lightweight VLMs face alignment bottlenecks due to limited language model capacity, reducing effective mutual information and alignment quality.

Method: Proposes TinyAlign, a framework using retrieval-augmented generation to enrich multimodal inputs and improve alignment.

Result: TinyAlign reduces training loss, speeds convergence, and boosts performance, achieving baseline results with 40% less data.

Conclusion: TinyAlign offers a practical solution for lightweight VLMs and provides theoretical insights into alignment bottlenecks.

Abstract: Lightweight Vision-Language Models (VLMs) are indispensable for
resource-constrained applications. The prevailing approach to aligning vision
and language models involves freezing both the vision encoder and the language
model while training small connector modules. However, this strategy heavily
depends on the intrinsic capabilities of the language model, which can be
suboptimal for lightweight models with limited representational capacity. In
this work, we investigate this alignment bottleneck through the lens of mutual
information, demonstrating that the constrained capacity of the language model
inherently limits the Effective Mutual Information (EMI) between multimodal
inputs and outputs, thereby compromising alignment quality. To address this
challenge, we propose TinyAlign, a novel framework inspired by
Retrieval-Augmented Generation, which strategically retrieves relevant context
from a memory bank to enrich multimodal inputs and enhance their alignment.
Extensive empirical evaluations reveal that TinyAlign significantly reduces
training loss, accelerates convergence, and enhances task performance.
Remarkably, it allows models to achieve baseline-level performance with only
40\% of the fine-tuning data, highlighting exceptional data efficiency. Our
work thus offers a practical pathway for developing more capable lightweight
VLMs while introducing a fresh theoretical lens to better understand and
address alignment bottlenecks in constrained multimodal systems.

</details>


### [456] [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
*Sifeng Shang,Jiayi Zhou,Chenyu Lin,Minxian Li,Kaiyang Zhou*

Main category: cs.LG

TL;DR: The paper introduces Quantized Zeroth-order Optimization (QZO) to reduce GPU memory usage for training large language models by minimizing memory for weights, gradients, and optimizer states.


<details>
  <summary>Details</summary>
Motivation: GPU memory is a bottleneck for adapting large language models to downstream tasks, prompting the need for memory-efficient training methods.

Method: The authors propose QZO, which uses zeroth-order optimization to eliminate gradients and optimizer states, and model quantization (e.g., bfloat16 to int4) to reduce memory for weights. QZO perturbs the continuous quantization scale for gradient estimation and employs directional derivative clipping for stability.

Result: QZO reduces total memory cost by over 18× for 4-bit LLMs and enables fine-tuning models like Llama-2-13B and Stable Diffusion 3.5 Large on a single 24GB GPU.

Conclusion: QZO provides a scalable and efficient solution for memory-limited training of large models, demonstrating significant memory savings and practical applicability.

Abstract: As the size of large language models grows exponentially, GPU memory has
become a bottleneck for adapting these models to downstream tasks. In this
paper, we aim to push the limits of memory-efficient training by minimizing
memory usage on model weights, gradients, and optimizer states, within a
unified framework. Our idea is to eliminate both gradients and optimizer states
using zeroth-order optimization, which approximates gradients by perturbing
weights during forward passes to identify gradient directions. To minimize
memory usage on weights, we employ model quantization, e.g., converting from
bfloat16 to int4. However, directly applying zeroth-order optimization to
quantized weights is infeasible due to the precision gap between discrete
weights and continuous gradients, which would otherwise require de-quantization
and re-quantization. To overcome this challenge, we propose Quantized
Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous
quantization scale for gradient estimation and uses a directional derivative
clipping method to stabilize training. QZO is orthogonal to both scalar-based
and codebook-based post-training quantization methods. Compared to
full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by
more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and
Stable Diffusion 3.5 Large within a single 24GB GPU.

</details>


### [457] [CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs](https://arxiv.org/abs/2505.12944)
*Jan Hagnberger,Daniel Musekamp,Mathias Niepert*

Main category: cs.LG

TL;DR: CALM-PDE is a novel model class for solving PDEs in compressed latent space, using continuous convolution for memory efficiency and handling irregular domains.


<details>
  <summary>Details</summary>
Motivation: Existing neural surrogate models for PDEs either use memory-heavy Transformers or are limited to regular discretizations. CALM-PDE aims to address these limitations.

Method: CALM-PDE employs a continuous convolution-based encoder-decoder with an epsilon-neighborhood-constrained kernel and adaptive query points.

Result: CALM-PDE performs competitively or better than baselines, with significant memory and inference time improvements over Transformer-based methods.

Conclusion: CALM-PDE offers an efficient and flexible solution for solving PDEs in irregular domains, balancing performance and computational cost.

Abstract: Solving time-dependent Partial Differential Equations (PDEs) using a densely
discretized spatial domain is a fundamental problem in various scientific and
engineering disciplines, including modeling climate phenomena and fluid
dynamics. However, performing these computations directly in the physical space
often incurs significant computational costs. To address this issue, several
neural surrogate models have been developed that operate in a compressed latent
space to solve the PDE. While these approaches reduce computational complexity,
they often use Transformer-based attention mechanisms to handle irregularly
sampled domains, resulting in increased memory consumption. In contrast,
convolutional neural networks allow memory-efficient encoding and decoding but
are limited to regular discretizations. Motivated by these considerations, we
propose CALM-PDE, a model class that efficiently solves arbitrarily discretized
PDEs in a compressed latent space. We introduce a novel continuous
convolution-based encoder-decoder architecture that uses an
epsilon-neighborhood-constrained kernel and learns to apply the convolution
operator to adaptive and optimized query points. We demonstrate the
effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and
irregularly sampled spatial domains. CALM-PDE is competitive with or
outperforms existing baseline methods while offering significant improvements
in memory and inference time efficiency compared to Transformer-based methods.

</details>


### [458] [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
*Penghui Qi,Zichen Liu,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: AnytimeReasoner optimizes LLM reasoning by improving token efficiency and flexibility under varying budgets, using verifiable dense rewards and decoupled policy optimization.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLMs optimize only final performance under fixed token budgets, hindering efficiency and flexibility.

Method: Truncates reasoning to fit sampled budgets, introduces verifiable dense rewards, and decouples thinking/summary policies. Uses BRPO for variance reduction.

Result: Outperforms GRPO in mathematical reasoning tasks across all budgets, improving training and token efficiency.

Conclusion: AnytimeReasoner enhances reasoning flexibility and efficiency, validated by superior performance in empirical tests.

Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.

</details>


### [459] [Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning](https://arxiv.org/abs/2505.13081)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.LG

TL;DR: The paper addresses detrimental concept drift in multi-modal large language models (MLLMs) during reinforcement fine-tuning (RFT), proposing Counterfactual Preference Optimization (CPO) to stabilize RFT by decoupling beneficial adaptations from harmful drift.


<details>
  <summary>Details</summary>
Motivation: The study identifies harmful biases introduced by unpredictable reasoning token distribution shifts in MLLMs during RFT, aiming to bridge concept drift theory with RFT processes.

Method: The authors formalize chain-of-thought reasoning as non-stationary distributions and introduce CPO, a counterfact-aware RFT method using concept graphs and LLM experts to generate counterfactual reasoning trajectories.

Result: CPO demonstrates superior robustness, generalization, and coordination in RFT, validated through extensive experiments. A large-scale dataset, CXR-CounterFact (CCF), is also contributed.

Conclusion: The paper successfully stabilizes RFT in non-stationary environments, particularly in the medical domain, and provides a public dataset and code for further research.

Abstract: This paper uncovers a critical yet overlooked phenomenon in multi-modal large
language models (MLLMs): detrimental concept drift within chain-of-thought
(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where
reasoning token distributions evolve unpredictably, thereby introducing
significant biases in final predictions. To address this, we are pioneers in
establishing the theoretical bridge between concept drift theory and RFT
processes by formalizing CoT's autoregressive token streams as non-stationary
distributions undergoing arbitrary temporal shifts. Leveraging this framework,
we propose a novel counterfact-aware RFT that systematically decouples
beneficial distribution adaptation from harmful concept drift through concept
graph-empowered LLM experts generating counterfactual reasoning trajectories.
Our solution, Counterfactual Preference Optimization (CPO), enables stable RFT
in non-stationary environments, particularly within the medical domain, through
custom-tuning of counterfactual-aware preference alignment. Extensive
experiments demonstrate our superior performance of robustness, generalization
and coordination within RFT. Besides, we also contributed a large-scale dataset
CXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual
reasoning trajectories derived from MIMIC-CXR. Our code and data are public.

</details>


### [460] [RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization](https://arxiv.org/abs/2505.13289)
*Alonso Urbano,David W. Romero,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: RECON discovers intrinsic symmetry distributions in data, aligning arbitrary reference frames into a common pose for better equivariant modeling.


<details>
  <summary>Details</summary>
Motivation: Existing equivariant networks assume fixed symmetries, degrading performance when data symmetries differ.

Method: RECON uses class-pose decompositions and data-driven normalization to align reference frames.

Result: Effective symmetry discovery demonstrated on 2D images and extended to 3D transformation groups.

Conclusion: RECON enables more flexible equivariant modeling by discovering and aligning intrinsic symmetries.

Abstract: Real-world data often exhibits unknown or approximate symmetries, yet
existing equivariant networks must commit to a fixed transformation group prior
to training, e.g., continuous $SO(2)$ rotations. This mismatch degrades
performance when the actual data symmetries differ from those in the
transformation group. We introduce RECON, a framework to discover each input's
intrinsic symmetry distribution from unlabeled data. RECON leverages class-pose
decompositions and applies a data-driven normalization to align arbitrary
reference frames into a common natural pose, yielding directly comparable and
interpretable symmetry descriptors. We demonstrate effective symmetry discovery
on 2D image benchmarks and -- for the first time -- extend it to 3D
transformation groups, paving the way towards more flexible equivariant
modeling.

</details>


### [461] [Mean Flows for One-step Generative Modeling](https://arxiv.org/abs/2505.13447)
*Zhengyang Geng,Mingyang Deng,Xingjian Bai,J. Zico Kolter,Kaiming He*

Main category: cs.LG

TL;DR: The paper introduces MeanFlow, a one-step generative modeling framework using average velocity for flow fields, outperforming previous methods with an FID of 3.43 on ImageNet 256x256.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between one-step and multi-step generative models by proposing a principled approach using average velocity.

Method: Derives an identity between average and instantaneous velocities to guide neural network training, eliminating the need for pre-training or curriculum learning.

Result: Achieves an FID of 3.43 on ImageNet 256x256, surpassing prior one-step diffusion/flow models.

Conclusion: MeanFlow narrows the gap between one-step and multi-step models, encouraging future research on foundational improvements.

Abstract: We propose a principled and effective framework for one-step generative
modeling. We introduce the notion of average velocity to characterize flow
fields, in contrast to instantaneous velocity modeled by Flow Matching methods.
A well-defined identity between average and instantaneous velocities is derived
and used to guide neural network training. Our method, termed the MeanFlow
model, is self-contained and requires no pre-training, distillation, or
curriculum learning. MeanFlow demonstrates strong empirical performance: it
achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet
256x256 trained from scratch, significantly outperforming previous
state-of-the-art one-step diffusion/flow models. Our study substantially
narrows the gap between one-step diffusion/flow models and their multi-step
predecessors, and we hope it will motivate future research to revisit the
foundations of these powerful models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [462] [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/abs/2505.11611)
*Bofan Gong,Shiyang Lai,Dawn Song*

Main category: cs.AI

TL;DR: The paper explores polysemanticity in neural networks, demonstrating its implications for model safety and how it can be exploited for interventions.


<details>
  <summary>Details</summary>
Motivation: Polysemanticity poses challenges for interpretability and safety in language models, yet its broader implications are not well understood.

Method: The study uses sparse autoencoders to analyze polysemantic structures in small models (Pythia-70M, GPT-2-Small) and tests interventions on larger models (LLaMA3.1-8B-Instruct, Gemma-2-9B-Instruct).

Result: A consistent polysemantic topology is found across models, enabling effective interventions that generalize to larger, black-box models.

Conclusion: The findings suggest a stable, transferable polysemantic structure that may persist across architectures and training regimes, highlighting risks and opportunities for model safety.

Abstract: Polysemanticity -- where individual neurons encode multiple unrelated
features -- is a well-known characteristic of large neural networks and remains
a central challenge in the interpretability of language models. At the same
time, its implications for model safety are also poorly understood. Leveraging
recent advances in sparse autoencoders, we investigate the polysemantic
structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their
vulnerability to targeted, covert interventions at the prompt, feature, token,
and neuron levels. Our analysis reveals a consistent polysemantic topology
shared across both models. Strikingly, we demonstrate that this structure can
be exploited to mount effective interventions on two larger, black-box
instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These
findings suggest not only the generalizability of the interventions but also
point to a stable and transferable polysemantic structure that could
potentially persist across architectures and training regimes.

</details>


### [463] [Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions](https://arxiv.org/abs/2505.11614)
*Jian-Qiao Zhu,Hanbo Xie,Dilip Arumugam,Robert C. Wilson,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: Pretrained LLMs are used for cognitive modeling, achieving both accurate predictions and interpretable explanations of human risky choices via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between predictive performance and interpretability in cognitive models, leveraging LLMs for dual-purpose modeling.

Method: Reinforcement learning with outcome-based rewards to guide LLMs in generating explicit reasoning traces for human risky choices.

Result: High-quality explanations and strong quantitative predictions of human decisions.

Conclusion: LLMs can effectively serve as dual-purpose cognitive models, combining prediction and interpretability.

Abstract: A central goal of cognitive modeling is to develop models that not only
predict human behavior but also provide insight into the underlying cognitive
mechanisms. While neural network models trained on large-scale behavioral data
often achieve strong predictive performance, they typically fall short in
offering interpretable explanations of the cognitive processes they capture. In
this work, we explore the potential of pretrained large language models (LLMs)
to serve as dual-purpose cognitive models--capable of both accurate prediction
and interpretable explanation in natural language. Specifically, we employ
reinforcement learning with outcome-based rewards to guide LLMs toward
generating explicit reasoning traces for explaining human risky choices. Our
findings demonstrate that this approach produces high-quality explanations
alongside strong quantitative predictions of human decisions.

</details>


### [464] [Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity](https://arxiv.org/abs/2505.11861)
*Qi Zhou,Jie Zhang,Dongxia Wang,Qiang Liu,Tianlin Li,Jin Song Dong,Wenhai Wang,Qing Guo*

Main category: cs.AI

TL;DR: Fair-PP is a synthetic dataset for personalized preferences in LLMs, addressing gaps in existing datasets by incorporating social equity. It uses GPT-4o-mini for role-playing, yielding 238,623 records, and introduces an automated framework, LLM analysis, and a reweighting method for preference alignment.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack personalization and social equity in human preference feedback, which is costly to collect. Fair-PP aims to fill this gap.

Method: Fair-PP leverages GPT-4o-mini for role-playing based on social survey data, creating a dataset with 28 social groups and 98 equity topics. It also introduces an automated framework and a reweighting method for alignment.

Result: The method outperforms baselines, providing a fine-grained dataset and enabling alignment with target personas while diverging from others.

Conclusion: Fair-PP offers a scalable solution for personalized preference alignment in LLMs, enhancing social equity and outperforming existing methods.

Abstract: Human preference plays a crucial role in the refinement of large language
models (LLMs). However, collecting human preference feedback is costly and most
existing datasets neglect the correlation between personalization and
preferences. To address this issue, we introduce Fair-PP, a synthetic dataset
of personalized preferences targeting social equity, derived from real-world
social survey data, which includes 28 social groups, 98 equity topics, and 5
personal preference dimensions. Leveraging GPT-4o-mini, we engage in
role-playing based on seven representative persona portrayals guided by
existing social survey data, yielding a total of 238,623 preference records.
Through Fair-PP, we also contribute (i) An automated framework for generating
preference data, along with a more fine-grained dataset of personalized
preferences; (ii) analysis of the positioning of the existing mainstream LLMs
across five major global regions within the personalized preference space; and
(iii) a sample reweighting method for personalized preference alignment,
enabling alignment with a target persona while maximizing the divergence from
other personas. Empirical experiments show our method outperforms the
baselines.

</details>


### [465] [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](https://arxiv.org/abs/2505.12039)
*Renqi Chen,Haoyang Su,Shixiang Tang,Zhenfei Yin,Qi Wu,Hui Li,Ye Sun,Nanqing Dong,Wanli Ouyang,Philip Torr*

Main category: cs.AI

TL;DR: The paper discusses how AI can revolutionize the Science of Science (SoS) by automating large-scale pattern discovery, overcoming limitations of traditional methods, and accelerating research progress.


<details>
  <summary>Details</summary>
Motivation: Traditional SoS methods are limited in handling modern research complexity; AI offers transformative potential for deeper insights and efficiency.

Method: The paper proposes integrating AI with SoS, including a multi-agent system to simulate research societies and replicate real-world patterns.

Result: AI demonstrates superior capability over traditional methods in uncovering research patterns and enhancing SoS research.

Conclusion: AI integration in SoS holds promise but faces challenges; future work should address limitations and expand applications.

Abstract: The Science of Science (SoS) explores the mechanisms underlying scientific
discovery, and offers valuable insights for enhancing scientific efficiency and
fostering innovation. Traditional approaches often rely on simplistic
assumptions and basic statistical tools, such as linear regression and
rule-based simulations, which struggle to capture the complexity and scale of
modern research ecosystems. The advent of artificial intelligence (AI) presents
a transformative opportunity for the next generation of SoS, enabling the
automation of large-scale pattern discovery and uncovering insights previously
unattainable. This paper offers a forward-looking perspective on the
integration of Science of Science with AI for automated research pattern
discovery and highlights key open challenges that could greatly benefit from
AI. We outline the advantages of AI over traditional methods, discuss potential
limitations, and propose pathways to overcome them. Additionally, we present a
preliminary multi-agent system as an illustrative example to simulate research
societies, showcasing AI's ability to replicate real-world research patterns
and accelerate progress in Science of Science research.

</details>


### [466] [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058)
*Vincent Koc*

Main category: cs.AI

TL;DR: TQB++ is a lightweight, multilingual QA benchmark for LLMs, designed for quick, low-cost testing to catch errors early in development.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for fast, efficient QA in LLM pipelines, avoiding delays from heavyweight benchmarks.

Method: Combines a 52-item English gold set with a synthetic-data generator for custom multilingual packs, supporting various tools.

Result: Enables rapid detection of errors like prompt-template issues or tokenizer drift, adding minimal latency.

Conclusion: TQB++ provides a resource-efficient QA solution for generative-AI workflows, released to improve continuous testing.

Abstract: Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual
smoke-test suite designed to give large-language-model (LLM) pipelines a
unit-test style safety net dataset that runs in seconds with minimal cost. Born
out of the tight feedback-loop demands building the Comet Opik
prompt-optimization SDK, where waiting on heavyweight benchmarks breaks
developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with
a tiny synthetic-data generator pypi package built on provider-agnostic
LiteLLM. The generator lets practitioners mint their own tiny packs in any
language, domain, or difficulty, while ten ready-made packs already cover
Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,
Spanish, and Turkish. Every dataset ships with Croissant metadata and
plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so
teams can drop deterministic micro-benchmarks directly into pull-request gates,
prompt-engineering loops, and production dashboards without touching GPU
budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet
reliably flags prompt-template errors, tokenizer drift, and fine-tuning
side-effects long before full-scale suites like MMLU or BIG-Bench would finish
configuring. The entire framework is released to accelerate continuous,
resource-efficient quality assurance across the generative-AI ecosystem.

</details>


### [467] [Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents](https://arxiv.org/abs/2505.12065)
*Tiannuo Yang,Zebin Yao,Bowen Jin,Lixiao Cui,Yusen Li,Gang Wang,Xiaoguang Liu*

Main category: cs.AI

TL;DR: SearchAgent-X improves efficiency in LLM-based search agents by addressing retrieval bottlenecks and system inefficiencies, achieving higher throughput and lower latency.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based search agents face efficiency issues due to interleaved reasoning and retrieval, including retrieval overhead and improper scheduling.

Method: SearchAgent-X uses high-recall approximate retrieval, priority-aware scheduling, and non-stall retrieval to optimize performance.

Result: SearchAgent-X outperforms state-of-the-art systems, achieving up to 3.4× higher throughput and 5× lower latency.

Conclusion: SearchAgent-X provides a high-efficiency framework for LLM-based search agents without compromising generation quality.

Abstract: Large Language Model (LLM)-based search agents have shown remarkable
capabilities in solving complex tasks by dynamically decomposing problems and
addressing them through interleaved reasoning and retrieval. However, this
interleaved paradigm introduces substantial efficiency bottlenecks. First, we
observe that both highly accurate and overly approximate retrieval methods
degrade system efficiency: exact search incurs significant retrieval overhead,
while coarse retrieval requires additional reasoning steps during generation.
Second, we identify inefficiencies in system design, including improper
scheduling and frequent retrieval stalls, which lead to cascading latency --
where even minor delays in retrieval amplify end-to-end inference time. To
address these challenges, we introduce SearchAgent-X, a high-efficiency
inference framework for LLM-based search agents. SearchAgent-X leverages
high-recall approximate retrieval and incorporates two key techniques:
priority-aware scheduling and non-stall retrieval. Extensive experiments
demonstrate that SearchAgent-X consistently outperforms state-of-the-art
systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving
up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without
compromising generation quality. SearchAgent-X is available at
https://github.com/tiannuo-yang/SearchAgent-X.

</details>


### [468] [LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs](https://arxiv.org/abs/2505.12135)
*Omar Choukrani,Idriss Malek,Daniil Orel,Zhuohan Xie,Zangir Iklassov,Martin Takáč,Salem Lahlou*

Main category: cs.AI

TL;DR: A new benchmark suite, LLM-BabyBench, evaluates LLMs on grounded intelligence tasks like predicting action consequences, planning, and decomposing instructions.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' planning and reasoning in interactive environments for developing capable AI agents.

Method: Uses a textual adaptation of BabyAI grid world to create datasets (Predict, Plan, Decompose) from expert agent interactions, with standardized evaluation metrics.

Result: Initial results highlight challenges in grounded reasoning tasks.

Conclusion: The benchmark suite and tools are publicly available to facilitate reproducible LLM assessments.

Abstract: Assessing the capacity of Large Language Models (LLMs) to plan and reason
within the constraints of interactive environments is crucial for developing
capable AI agents. We introduce $\textbf{LLM-BabyBench}$, a new benchmark suite
designed specifically for this purpose. Built upon a textual adaptation of the
procedurally generated BabyAI grid world, this suite evaluates LLMs on three
fundamental aspects of grounded intelligence: (1) predicting the consequences
of actions on the environment state ($\textbf{Predict}$ task), (2) generating
sequences of low-level actions to achieve specified objectives ($\textbf{Plan}$
task), and (3) decomposing high-level instructions into coherent subgoal
sequences ($\textbf{Decompose}$ task). We detail the methodology for generating
the three corresponding datasets ($\texttt{LLM-BabyBench-Predict}$,
$\texttt{-Plan}$, $\texttt{-Decompose}$) by extracting structured information
from an expert agent operating within the text-based environment. Furthermore,
we provide a standardized evaluation harness and metrics, including environment
interaction for validating generated plans, to facilitate reproducible
assessment of diverse LLMs. Initial baseline results highlight the challenges
posed by these grounded reasoning tasks. The benchmark suite, datasets, data
generation code, and evaluation code are made publicly available
($\href{https://github.com/choukrani/llm-babybench}{\text{GitHub}}$,
$\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\text{HuggingFace}}$).

</details>


### [469] [Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering](https://arxiv.org/abs/2505.12189)
*Marco Valentino,Geonhee Kim,Dhairya Dalal,Zhixue Zhao,André Freitas*

Main category: cs.AI

TL;DR: The paper addresses LLMs' tendency to confuse plausibility with logical validity, proposing activation steering to mitigate biases and improve formal reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs often conflate plausibility with logical validity, leading to biased inferences, which undermines their reliability in applications requiring rigorous logic.

Method: The study uses a controlled syllogistic reasoning dataset, identifies layers for formal/material inference, and tests contrastive activation steering (static and dynamic, including K-CAST).

Result: Contrastive steering controls content biases linearly, with dynamic methods (e.g., K-CAST) improving formal reasoning accuracy by up to 15%. Steering is robust to prompts and minimally affects language modeling.

Conclusion: Activation steering enhances LLM robustness in formal reasoning, offering a scalable solution for reducing biases and improving logical consistency.

Abstract: Large language models (LLMs) frequently demonstrate reasoning limitations,
often conflating content plausibility (i.e., material inference) with logical
validity (i.e., formal inference). This can result in biased inferences, where
plausible arguments are incorrectly deemed logically valid or vice versa.
Mitigating this limitation is critical, as it undermines the trustworthiness
and generalizability of LLMs in applications that demand rigorous logical
consistency. This paper investigates the problem of mitigating content biases
on formal reasoning through activation steering. Specifically, we curate a
controlled syllogistic reasoning dataset to disentangle formal validity from
content plausibility. After localising the layers responsible for formal and
material inference, we investigate contrastive activation steering methods for
test-time interventions. An extensive empirical analysis on different LLMs
reveals that contrastive steering consistently supports linear control over
content biases. However, we observe that a static approach is insufficient for
improving all the tested models. We then leverage the possibility to control
content effects by dynamically determining the value of the steering parameters
via fine-grained conditional methods. We found that conditional steering is
effective on unresponsive models, achieving up to 15% absolute improvement in
formal reasoning accuracy with a newly introduced kNN-based method (K-CAST).
Finally, additional experiments reveal that steering for content effects is
robust to prompt variations, incurs minimal side effects on language modeling
capabilities, and can partially generalize to out-of-distribution reasoning
tasks. Practically, this paper demonstrates that activation-level interventions
can offer a scalable strategy for enhancing the robustness of LLMs,
contributing towards more systematic and unbiased formal reasoning.

</details>


### [470] [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
*Danlong Yuan,Tian Xie,Shaohan Huang,Zhuocheng Gong,Huishuai Zhang,Chong Luo,Furu Wei,Dongyan Zhao*

Main category: cs.AI

TL;DR: The paper introduces three reward designs in reinforcement learning to reduce response length in large reasoning models without extra training stages, achieving shorter paths and maintained/improved performance.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models have high memory and time costs due to long reasoning paths, and existing methods require additional training.

Method: Integrates three critical reward designs into the reinforcement learning process of large reasoning models.

Result: Reduces response length by 40% in logic reasoning (with 14% performance gain) and 33% in math problems (performance preserved).

Conclusion: The proposed method effectively shortens reasoning paths without extra training, enhancing efficiency and performance.

Abstract: Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated
remarkable performance on reasoning tasks but often incur a long reasoning path
with significant memory and time costs. Existing methods primarily aim to
shorten reasoning paths by introducing additional training data and stages. In
this paper, we propose three critical reward designs integrated directly into
the reinforcement learning process of large reasoning models, which reduce the
response length without extra training stages. Experiments on four settings
show that our method significantly decreases response length while maintaining
or even improving performance. Specifically, in a logic reasoning setting, we
achieve a 40% reduction in response length averaged by steps alongside a 14%
gain in performance. For math problems, we reduce response length averaged by
steps by 33% while preserving performance.

</details>


### [471] [Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge](https://arxiv.org/abs/2505.12301)
*Luyu Chen,Zeyu Zhang,Haoran Tan,Quanyu Dai,Hao Yang,Zhenhua Dong,Xu Chen*

Main category: cs.AI

TL;DR: A novel training framework aligns LLM-generated judgment distributions with human distributions, improving evaluation reliability and robustness.


<details>
  <summary>Details</summary>
Motivation: Single-point evaluations in LLMs overlook human diversity and uncertainty, causing information loss and reduced reliability.

Method: Proposes a distributional alignment objective using KL divergence, cross-entropy regularization, and adversarial training for robustness.

Result: Outperforms closed-source LLMs and single-point methods in alignment quality, accuracy, and robustness.

Conclusion: The framework enhances LLM evaluations by better aligning with human judgment diversity and uncertainty.

Abstract: LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,
offering significant efficiency and flexibility compared to human judgments.
However, previous methods primarily rely on single-point evaluations,
overlooking the inherent diversity and uncertainty in human evaluations. This
approach leads to information loss and decreases the reliability of
evaluations. To address this limitation, we propose a novel training framework
that explicitly aligns the LLM-generated judgment distribution with empirical
human distributions. Specifically, we propose a distributional alignment
objective based on KL divergence, combined with an auxiliary cross-entropy
regularization to stabilize the training process. Furthermore, considering that
empirical distributions may derive from limited human annotations, we
incorporate adversarial training to enhance model robustness against
distribution perturbations. Extensive experiments across various LLM backbones
and evaluation tasks demonstrate that our framework significantly outperforms
existing closed-source LLMs and conventional single-point alignment methods,
with improved alignment quality, evaluation accuracy, and robustness.

</details>


### [472] [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
*Yinghao Zhu,Ziyi He,Haoran Hu,Xiaochen Zheng,Xichen Zhang,Zixiang Wang,Junyi Gao,Liantao Ma,Lequan Yu*

Main category: cs.AI

TL;DR: The paper introduces MedAgentBoard, a benchmark for evaluating multi-agent collaboration, single-LLM, and conventional methods in medical tasks, revealing nuanced performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalizable evaluations and rigorous comparisons in multi-agent collaboration for medical tasks.

Method: Developed MedAgentBoard, a benchmark covering four medical task categories, and conducted extensive experiments comparing multi-agent, single-LLM, and conventional approaches.

Result: Multi-agent collaboration shows benefits in specific scenarios but does not consistently outperform single LLMs or conventional methods.

Conclusion: Task-specific, evidence-based AI solutions are crucial, and the complexity of multi-agent collaboration must be justified by performance gains.

Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.

</details>


### [473] [mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model](https://arxiv.org/abs/2505.12565)
*Carl Edwards,Chi Han,Gawon Lee,Thao Nguyen,Bowen Jin,Chetan Kumar Prasad,Sara Szymkuć,Bartosz A. Grzybowski,Ying Diao,Jiawei Han,Ge Liu,Hao Peng,Martin D. Burke,Heng Ji*

Main category: cs.AI

TL;DR: The paper proposes mCLM, a modular Chemical-Language Model that tokenizes molecules into functional building blocks to improve drug discovery by ensuring synthesizability and enhancing molecular functions.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle to propose novel, synthesizable drug-like molecules due to atom-level encoding limitations. The paper suggests decomposing molecules into functional building blocks for better performance.

Method: Introduces mCLM, which tokenizes molecules into functional building blocks and learns a bilingual model combining natural language descriptions and molecule building blocks.

Result: mCLM significantly improves 5 out of 6 critical chemical functions in FDA-approved drugs and enhances FDA-rejected drugs through iterative reasoning.

Conclusion: mCLM advances drug discovery by generating synthesizable molecules and improving molecular functions through functional building block reasoning.

Abstract: Despite their ability to understand chemical knowledge and accurately
generate sequential representations, large language models (LLMs) remain
limited in their capacity to propose novel molecules with drug-like properties.
In addition, the molecules that LLMs propose can often be challenging to make
in the lab. To more effectively enable the discovery of functional small
molecules, LLMs need to learn a molecular language. However, LLMs are currently
limited by encoding molecules from atoms. In this paper, we argue that just
like tokenizing texts into (sub-)word tokens instead of characters, molecules
should be decomposed and reassembled at the level of functional building
blocks, i.e., parts of molecules that bring unique functions and serve as
effective building blocks for real-world automated laboratory synthesis. This
motivates us to propose mCLM, a modular Chemical-Language Model tokenizing
molecules into building blocks and learning a bilingual language model of both
natural language descriptions of functions and molecule building blocks. By
reasoning on such functional building blocks, mCLM guarantees to generate
efficiently synthesizable molecules thanks to recent progress in block-based
chemistry, while also improving the functions of molecules in a principled
manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of
significantly improving 5 out of 6 chemical functions critical to determining
drug potentials. More importantly, mCLM can reason on multiple functions and
improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to
greatly improve their shortcomings.

</details>


### [474] [Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities](https://arxiv.org/abs/2505.12680)
*Haoyu Zhao,Yihan Geng,Shange Tang,Yong Lin,Bohan Lyu,Hongzhou Lin,Chi Jin,Sanjeev Arora*

Main category: cs.AI

TL;DR: The paper evaluates whether LLM-based proof assistants understand mathematical structure like humans, focusing on inequalities. It introduces Ineq-Comp, a benchmark for compositional reasoning, and finds current provers struggle, revealing a gap in AI's generalization compared to human intuition.


<details>
  <summary>Details</summary>
Motivation: To assess if LLM-based proof assistants truly grasp mathematical structure, particularly in handling compositional reasoning in inequalities, a fundamental mathematical tool.

Method: Introduces Ineq-Comp, a benchmark of elementary inequalities transformed through systematic operations (e.g., variable duplication, algebraic rewriting). Tests provers like Goedel, STP, and DeepSeek-Prover-V2-7B.

Result: Most provers struggle with compositional reasoning, even with contextual proofs. DeepSeek-Prover-V2-7B performs better but still shows a 20% drop in performance.

Conclusion: Current AI provers lag behind human intuition in compositional reasoning, highlighting a significant gap in generalization.

Abstract: LLM-based formal proof assistants (e.g., in Lean) hold great promise for
automating mathematical discovery. But beyond syntactic correctness, do these
systems truly understand mathematical structure as humans do? We investigate
this question through the lens of mathematical inequalities -- a fundamental
tool across many domains. While modern provers can solve basic inequalities, we
probe their ability to handle human-intuitive compositionality. We introduce
Ineq-Comp, a benchmark built from elementary inequalities through systematic
transformations, including variable duplication, algebraic rewriting, and
multi-step composition. Although these problems remain easy for humans, we find
that most provers -- including Goedel, STP, and Kimina-7B -- struggle
significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly
because it is trained to decompose the problems into sub-problems -- but still
suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor
for all models even when formal proofs of the constituent parts are provided in
context, revealing that the source of weakness is indeed in compositional
reasoning. Our results expose a persisting gap between the generalization
behavior of current AI provers and human mathematical intuition.

</details>


### [475] [Bullying the Machine: How Personas Increase LLM Vulnerability](https://arxiv.org/abs/2505.12692)
*Ziwei Xu,Udit Sanghi,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: Persona conditioning in LLMs can increase safety risks under bullying, with certain personality traits and tactics like gaslighting being particularly effective.


<details>
  <summary>Details</summary>
Motivation: To investigate how persona conditioning in LLMs affects their safety when subjected to bullying tactics.

Method: A simulation framework where an attacker LLM uses bullying tactics on a victim LLM conditioned with Big Five personality traits.

Result: Certain personas (e.g., low agreeableness or conscientiousness) increase susceptibility to unsafe outputs, with emotional/sarcastic tactics like gaslighting being most effective.

Conclusion: Persona-driven interactions introduce new safety risks, necessitating persona-aware safety evaluation and alignment strategies.

Abstract: Large Language Models (LLMs) are increasingly deployed in interactions where
they are prompted to adopt personas. This paper investigates whether such
persona conditioning affects model safety under bullying, an adversarial
manipulation that applies psychological pressures in order to force the victim
to comply to the attacker. We introduce a simulation framework in which an
attacker LLM engages a victim LLM using psychologically grounded bullying
tactics, while the victim adopts personas aligned with the Big Five personality
traits. Experiments using multiple open-source LLMs and a wide range of
adversarial goals reveal that certain persona configurations -- such as
weakened agreeableness or conscientiousness -- significantly increase victim's
susceptibility to unsafe outputs. Bullying tactics involving emotional or
sarcastic manipulation, such as gaslighting and ridicule, are particularly
effective. These findings suggest that persona-driven interaction introduces a
novel vector for safety risks in LLMs and highlight the need for persona-aware
safety evaluation and alignment strategies.

</details>


### [476] [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)
*Zhongxiang Sun,Qipeng Wang,Haoyu Wang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: The paper introduces the Reasoning Score to detect Reasoning Hallucination in LRMs, proposes the RHD framework for detection, and GRPO-R for mitigation, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Reasoning Hallucination—logically coherent but factually incorrect reasoning in LRMs—which is harder to detect and more harmful than traditional hallucinations.

Method: Proposes the Reasoning Score to quantify reasoning depth, analyzes patterns in ReTruthQA, introduces the RHD framework for detection, and GRPO-R for mitigation via reinforcement learning.

Result: Identifies two key hallucination patterns, achieves state-of-the-art detection performance, and reduces hallucination rates with GRPO-R.

Conclusion: The Reasoning Score and GRPO-R effectively detect and mitigate Reasoning Hallucination, improving reasoning quality in LRMs.

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in
multi-step reasoning tasks. However, alongside these successes, a more
deceptive form of model error has emerged--Reasoning Hallucination--where
logically coherent but factually incorrect reasoning traces lead to persuasive
yet faulty conclusions. Unlike traditional hallucinations, these errors are
embedded within structured reasoning, making them more difficult to detect and
potentially more harmful. In this work, we investigate reasoning hallucinations
from a mechanistic perspective. We propose the Reasoning Score, which
quantifies the depth of reasoning by measuring the divergence between logits
obtained from projecting late layers of LRMs to the vocabulary space,
effectively distinguishing shallow pattern-matching from genuine deep
reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA
dataset and identify two key reasoning hallucination patterns: early-stage
fluctuation in reasoning depth and incorrect backtracking to flawed prior
steps. These insights motivate our Reasoning Hallucination Detection (RHD)
framework, which achieves state-of-the-art performance across multiple domains.
To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced
reinforcement learning algorithm that incorporates step-level deep reasoning
rewards via potential-based shaping. Our theoretical analysis establishes
stronger generalization guarantees, and experiments demonstrate improved
reasoning quality and reduced hallucination rates.

</details>


### [477] [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
*Shaohang Wei,Wei Li,Feifan Song,Wen Luo,Tianyi Zhuang,Haochen Tan,Zhijiang Guo,Houfeng Wang*

Main category: cs.AI

TL;DR: The paper introduces TIME, a multi-level benchmark for temporal reasoning in LLMs, addressing real-world challenges like intensive temporal information, fast-changing dynamics, and complex dependencies. It includes 38,522 QA pairs across 3 sub-datasets and evaluates reasoning models.


<details>
  <summary>Details</summary>
Motivation: Existing works overlook real-world temporal reasoning challenges, prompting the creation of TIME to bridge this gap.

Method: Proposes TIME, a benchmark with 3 sub-datasets (TIME-Wiki, TIME-News, TIME-Dial) and 11 sub-tasks, tested on reasoning and non-reasoning models.

Result: Extensive experiments analyze temporal reasoning performance and test-time scaling effects. TIME-Lite, a human-annotated subset, is released.

Conclusion: TIME advances temporal reasoning research by providing a comprehensive benchmark and tools for standardized evaluation.

Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend
the real world. However, existing works neglect the real-world challenges for
temporal reasoning: (1) intensive temporal information, (2) fast-changing event
dynamics, and (3) complex temporal dependencies in social interactions. To
bridge this gap, we propose a multi-level benchmark TIME, designed for temporal
reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3
levels with 11 fine-grained sub-tasks. This benchmark encompasses 3
sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,
and TIME-Dial. We conduct extensive experiments on reasoning models and
non-reasoning models. And we conducted an in-depth analysis of temporal
reasoning performance across diverse real-world scenarios and tasks, and
summarized the impact of test-time scaling on temporal reasoning capabilities.
Additionally, we release TIME-Lite, a human-annotated subset to foster future
research and standardized evaluation in temporal reasoning. The code is
available at https://github.com/sylvain-wei/TIME , and the dataset is available
at https://huggingface.co/datasets/SylvainWei/TIME .

</details>


### [478] [LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs](https://arxiv.org/abs/2505.13098)
*Lars-Peter Meyer,Johannes Frey,Desiree Heim,Felix Brei,Claus Stadler,Kurt Junghanns,Michael Martin*

Main category: cs.AI

TL;DR: The paper introduces LLM-KG-Bench 3.0, a framework for evaluating LLMs in Semantic Web and Knowledge Graph Engineering, featuring enhanced flexibility, updated tasks, and extensive model support.


<details>
  <summary>Details</summary>
Motivation: To assess and compare LLMs' capabilities in handling Knowledge Graphs and Semantic Web tasks without manual evaluation.

Method: The LLM-KG-Bench framework includes automated evaluation tasks, an updated API, and support for various LLMs, tested on over 30 models.

Result: A comprehensive dataset and model cards showcasing LLMs' performance in RDF, SPARQL, Turtle, and JSON-LD tasks.

Conclusion: LLM-KG-Bench 3.0 effectively evaluates LLMs for Knowledge Graph tasks, providing insights into their strengths and limitations.

Abstract: Current Large Language Models (LLMs) can assist developing program code
beside many other things, but can they support working with Knowledge Graphs
(KGs) as well? Which LLM is offering the best capabilities in the field of
Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to
determine without checking many answers manually? The LLM-KG-Bench framework in
Version 3.0 is designed to answer these questions. It consists of an extensible
set of tasks for automated evaluation of LLM answers and covers different
aspects of working with semantic technologies. In this paper the LLM-KG-Bench
framework is presented in Version 3 along with a dataset of prompts, answers
and evaluations generated with it and several state-of-the-art LLMs.
Significant enhancements have been made to the framework since its initial
release, including an updated task API that offers greater flexibility in
handling evaluation tasks, revised tasks, and extended support for various open
models through the vllm library, among other improvements. A comprehensive
dataset has been generated using more than 30 contemporary open and proprietary
LLMs, enabling the creation of exemplary model cards that demonstrate the
models' capabilities in working with RDF and SPARQL, as well as comparing their
performance on Turtle and JSON-LD RDF serialization tasks.

</details>


### [479] [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/abs/2505.13126)
*Liancheng Gong,Wang Zhu,Jesse Thomason,Li Zhang*

Main category: cs.AI

TL;DR: PDDLego+ is a zero-shot framework for formalizing partially observable environments into PDDL, improving performance and robustness without needing prior trajectories.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of partially observable environments, where existing methods fail due to incomplete information.

Method: Proposes PDDLego+, an iterative framework for formalizing, planning, growing, and refining PDDL representations without relying on existing trajectories.

Result: Achieves superior performance and robustness in textual simulated environments, with interpretable domain knowledge benefiting future tasks.

Conclusion: PDDLego+ effectively tackles partially observable environments, demonstrating scalability and interpretability.

Abstract: In planning, using LLMs not to predict plans but to formalize an environment
into the Planning Domain Definition Language (PDDL) has been shown to greatly
improve performance and control. While most work focused on fully observable
environments, we tackle the more realistic and challenging partially observable
environments where existing methods are incapacitated by the lack of complete
information. We propose PDDLego+, a framework to iteratively formalize, plan,
grow, and refine PDDL representations in a zero-shot manner, without needing
access to any existing trajectories. On two textual simulated environments, we
show that PDDLego+ not only achieves superior performance, but also shows
robustness against problem complexity. We also show that the domain knowledge
captured after a successful trial is interpretable and benefits future tasks.

</details>


### [480] [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
*Tianbao Xie,Jiaqi Deng,Xiaochuan Li,Junlin Yang,Haoyuan Wu,Jixuan Chen,Wenjing Hu,Xinyuan Wang,Yuhui Xu,Zekun Wang,Yiheng Xu,Junli Wang,Doyen Sahoo,Tao Yu,Caiming Xiong*

Main category: cs.AI

TL;DR: The paper introduces OSWorld-G, a benchmark for GUI grounding, and Jedi, a large dataset, to address oversimplified tasks in current benchmarks. Their models outperform existing methods and improve agent capabilities.


<details>
  <summary>Details</summary>
Motivation: Current GUI grounding benchmarks oversimplify tasks, lacking real-world complexity like software commonsense and layout understanding.

Method: Developed OSWorld-G (564 annotated samples) and Jedi (4M examples) for diverse tasks. Trained multi-scale models on Jedi.

Result: Models outperformed existing methods on benchmarks (ScreenSpot-v2, ScreenSpot-Pro, OSWorld-G) and improved agent performance from 5% to 27% on OSWorld.

Conclusion: Specialized data for interface elements enables generalization. All resources are open-sourced.

Abstract: Graphical user interface (GUI) grounding, the ability to map natural language
instructions to specific actions on graphical user interfaces, remains a
critical bottleneck in computer use agent development. Current benchmarks
oversimplify grounding tasks as short referring expressions, failing to capture
the complexity of real-world interactions that require software commonsense,
layout understanding, and fine-grained manipulation capabilities. To address
these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising
564 finely annotated samples across diverse task types including text matching,
element recognition, layout understanding, and precise manipulation.
Additionally, we synthesize and release the largest computer use grounding
dataset Jedi, which contains 4 million examples through multi-perspective
decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its
effectiveness by outperforming existing approaches on ScreenSpot-v2,
ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved
grounding with Jedi directly enhances agentic capabilities of general
foundation models on complex computer tasks, improving from 5% to 27% on
OSWorld. Through detailed ablation studies, we identify key factors
contributing to grounding performance and verify that combining specialized
data for different interface elements enables compositional generalization to
novel interfaces. All benchmark, data, checkpoints, and code are open-sourced
and available at https://osworld-grounding.github.io.

</details>


### [481] [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition](https://arxiv.org/abs/2505.13380)
*Nam V. Nguyen,Huy Nguyen,Quang Pham,Van Nguyen,Savitha Ramasamy,Nhat Ho*

Main category: cs.AI

TL;DR: The paper introduces CompeteSMoE, a novel routing mechanism for sparse mixture of experts (SMoE) that improves training efficiency and performance by routing tokens to experts with the highest neural response.


<details>
  <summary>Details</summary>
Motivation: Traditional SMoE training faces challenges due to suboptimal routing, where experts performing computations don't directly influence routing. The goal is to enhance routing efficiency and model performance.

Method: The authors propose a competition mechanism for routing tokens to the most responsive experts, supported by theoretical analysis of its sample efficiency. They develop CompeteSMoE, an algorithm integrating this mechanism into SMoE training.

Result: Empirical evaluations on visual instruction tuning and language pre-training tasks show CompeteSMoE outperforms state-of-the-art SMoE strategies in efficacy, robustness, and scalability.

Conclusion: CompeteSMoE offers a simple yet effective solution to improve SMoE training, with demonstrated advantages over existing methods. The implementation is publicly available.

Abstract: Sparse mixture of experts (SMoE) offers an appealing solution to scale up the
model complexity beyond the mean of increasing the network's depth or width.
However, we argue that effective SMoE training remains challenging because of
the suboptimal routing process where experts that perform computation do not
directly contribute to the routing process. In this work, we propose
competition, a novel mechanism to route tokens to experts with the highest
neural response. Theoretically, we show that the competition mechanism enjoys a
better sample efficiency than the traditional softmax routing. Furthermore, we
develop CompeteSMoE, a simple yet effective algorithm to train large language
models by deploying a router to learn the competition policy, thus enjoying
strong performances at a low training overhead. Our extensive empirical
evaluations on both the visual instruction tuning and language pre-training
tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE
compared to state-of-the-art SMoE strategies. We have made the implementation
available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an
improved version of the previous study at arXiv:2402.02526

</details>


### [482] [CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process](https://arxiv.org/abs/2505.13408)
*Jinhe Bi,Danqi Yan,Yifan Wang,Wenke Huang,Haokun Chen,Guancheng Wan,Mang Ye,Xun Xiao,Hinrich Schuetze,Volker Tresp,Yunpu Ma*

Main category: cs.AI

TL;DR: The paper introduces a CoT-Kinetics energy equation to evaluate the soundness of reasoning trajectories in Large Reasoning Models (LRMs), improving confidence assessment of derived answers.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing LRM outputs inadequately reflect the causal relationship between reasoning and answers, leading to unsatisfactory confidence judgments.

Method: The authors propose a CoT-Kinetics energy equation, inspired by classical mechanics, to model token state transformations and assign scalar scores for reasoning soundness.

Result: The CoT-Kinetics energy equation provides a refined measure of LRM output quality, beyond binary correctness judgments.

Conclusion: The novel approach enhances the evaluation of reasoning soundness in LRMs, enabling more accurate confidence assessments of derived answers.

Abstract: Recent Large Reasoning Models significantly improve the reasoning ability of
Large Language Models by learning to reason, exhibiting the promising
performance in solving complex tasks. LRMs solve tasks that require complex
reasoning by explicitly generating reasoning trajectories together with
answers. Nevertheless, judging the quality of such an output answer is not easy
because only considering the correctness of the answer is not enough and the
soundness of the reasoning trajectory part matters as well. Logically, if the
soundness of the reasoning part is poor, even if the answer is correct, the
confidence of the derived answer should be low. Existing methods did consider
jointly assessing the overall output answer by taking into account the
reasoning part, however, their capability is still not satisfactory as the
causal relationship of the reasoning to the concluded answer cannot properly
reflected. In this paper, inspired by classical mechanics, we present a novel
approach towards establishing a CoT-Kinetics energy equation. Specifically, our
CoT-Kinetics energy equation formulates the token state transformation process,
which is regulated by LRM internal transformer layers, as like a particle
kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy
assigns a scalar score to evaluate specifically the soundness of the reasoning
phase, telling how confident the derived answer could be given the evaluated
reasoning. As such, the LRM's overall output quality can be accurately
measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.

</details>


### [483] [Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.13445)
*Xiaoyuan Liu,Tian Liang,Zhiwei He,Jiahao Xu,Wenxuan Wang,Pinjia He,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: RISE is an online RL framework enhancing LLMs' reasoning and self-verification simultaneously, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of superficial self-reflection in LLMs, where models fail to robustly verify their outputs.

Method: RISE integrates problem-solving and self-verification training within a single RL process, using verifiable rewards for feedback.

Result: Improves problem-solving accuracy and fosters strong self-verification skills, with more frequent and accurate self-verification behaviors.

Conclusion: RISE is a flexible and effective approach for developing robust and self-aware reasoning models.

Abstract: Large Language Models (LLMs) show great promise in complex reasoning, with
Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement
strategy. However, a prevalent issue is ``superficial self-reflection'', where
models fail to robustly verify their own outputs. We introduce RISE
(Reinforcing Reasoning with Self-Verification), a novel online RL framework
designed to tackle this. RISE explicitly and simultaneously trains an LLM to
improve both its problem-solving and self-verification abilities within a
single, integrated RL process. The core mechanism involves leveraging
verifiable rewards from an outcome verifier to provide on-the-fly feedback for
both solution generation and self-verification tasks. In each iteration, the
model generates solutions, then critiques its own on-policy generated
solutions, with both trajectories contributing to the policy update. Extensive
experiments on diverse mathematical reasoning benchmarks show that RISE
consistently improves model's problem-solving accuracy while concurrently
fostering strong self-verification skills. Our analyses highlight the
advantages of online verification and the benefits of increased verification
compute. Additionally, RISE models exhibit more frequent and accurate
self-verification behaviors during reasoning. These advantages reinforce RISE
as a flexible and effective path towards developing more robust and self-aware
reasoners.

</details>


### [484] [StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment](https://arxiv.org/abs/2505.13232)
*Younghyun Kim,Jongheon Jeong,Sangkyung Kwak,Kyungmin Lee,Juho Lee,Jinwoo Shin*

Main category: cs.AI

TL;DR: StarFT is a framework for fine-tuning zero-shot models to prevent learning spurious features, improving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning zero-shot models on smaller datasets can lead to learning spurious features, degrading robustness.

Method: StarFT introduces a regularization aligning output distributions for spuriosity-injected labels with the original model, using language models to generate confounding descriptions.

Result: StarFT improves worst-group and average accuracy by 14.30% and 3.02% in the Waterbirds scenario, outperforming baselines.

Conclusion: StarFT enhances robustness and generalization in fine-tuned zero-shot models, addressing spurious feature learning.

Abstract: Learning robust representations from data often requires scale, which has led
to the success of recent zero-shot models such as CLIP. However, the obtained
robustness can easily be deteriorated when these models are fine-tuned on other
downstream tasks (e.g., of smaller scales). Previous works often interpret this
phenomenon in the context of domain shift, developing fine-tuning methods that
aim to preserve the original domain as much as possible. However, in a
different context, fine-tuned models with limited data are also prone to
learning features that are spurious to humans, such as background or texture.
In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a
novel framework for fine-tuning zero-shot models to enhance robustness by
preventing them from learning spuriosity. We introduce a regularization that
aligns the output distribution for spuriosity-injected labels with the original
zero-shot model, ensuring that the model is not induced to extract irrelevant
features further from these descriptions.We leverage recent language models to
get such spuriosity-injected labels by generating alternative textual
descriptions that highlight potentially confounding features.Extensive
experiments validate the robust generalization of StarFT and its emerging
properties: zero-shot group robustness and improved zero-shot classification.
Notably, StarFT boosts both worst-group and average accuracy by 14.30% and
3.02%, respectively, in the Waterbirds group shift scenario, where other robust
fine-tuning baselines show even degraded performance.

</details>


### [485] [Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks](https://arxiv.org/abs/2505.13391)
*Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: The paper introduces PoNG, a neural architecture for abstract visual reasoning, focusing on improving generalization in o.o.d. scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of model generalization in abstract visual reasoning tasks, especially in out-of-distribution (o.o.d.) setups.

Method: Proposes the Pathways of Normalized Group Convolution (PoNG) model, utilizing group convolution, normalization, and parallel design.

Result: PoNG demonstrates strong generalization, outperforming existing methods on benchmarks like Raven's Progressive Matrices.

Conclusion: PoNG advances generalization in AVR tasks, showing promise for handling diverse test distributions.

Abstract: The abstract visual reasoning (AVR) domain presents a diverse suite of
analogy-based tasks devoted to studying model generalization. Recent years have
brought dynamic progress in the field, particularly in i.i.d. scenarios, in
which models are trained and evaluated on the same data distributions.
Nevertheless, o.o.d. setups that assess model generalization to new test
distributions remain challenging even for the most recent models. To advance
generalization in AVR tasks, we present the Pathways of Normalized Group
Convolution model (PoNG), a novel neural architecture that features group
convolution, normalization, and a parallel design. We consider a wide set of
AVR benchmarks, including Raven's Progressive Matrices and visual analogy
problems with both synthetic and real-world images. The experiments demonstrate
strong generalization capabilities of the proposed model, which in several
settings outperforms the existing literature methods.

</details>


### [486] [MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision](https://arxiv.org/abs/2505.13427)
*Lingxiao Du,Fanqing Meng,Zongkai Liu,Zhixiang Zhou,Ping Luo,Qiaosheng Zhang,Wenqi Shao*

Main category: cs.AI

TL;DR: MM-PRM, a process reward model, improves multimodal reasoning by providing fine-grained supervision over intermediate steps, leveraging automated annotations and achieving significant benchmark improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of fine-grained supervision in MLLMs for complex multi-step reasoning, which often leads to inconsistent or partially correct solutions.

Method: Proposes MM-PRM, trained via an automated framework using MM-Policy and MM-K12 dataset, with step-level annotations generated via MCTS. Evaluated in Best-of-N inference.

Result: Significant improvements on in-domain (MM-K12) and out-of-domain (OlympiadBench, MathVista) benchmarks. Soft labels, smaller learning rates, and path diversity enhance PRM performance.

Conclusion: Process supervision via MM-PRM enhances logical robustness in multimodal reasoning, with released codes and data for broader use.

Abstract: While Multimodal Large Language Models (MLLMs) have achieved impressive
progress in vision-language understanding, they still struggle with complex
multi-step reasoning, often producing logically inconsistent or partially
correct solutions. A key limitation lies in the lack of fine-grained
supervision over intermediate reasoning steps. To address this, we propose
MM-PRM, a process reward model trained within a fully automated, scalable
framework. We first build MM-Policy, a strong multimodal model trained on
diverse mathematical reasoning data. Then, we construct MM-K12, a curated
dataset of 10,000 multimodal math problems with verifiable answers, which
serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based
pipeline, we generate over 700k step-level annotations without human labeling.
The resulting PRM is used to score candidate reasoning paths in the Best-of-N
inference setup and achieves significant improvements across both in-domain
(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)
benchmarks. Further analysis confirms the effectiveness of soft labels, smaller
learning rates, and path diversity in optimizing PRM performance. MM-PRM
demonstrates that process supervision is a powerful tool for enhancing the
logical robustness of multimodal reasoning systems. We release all our codes
and data at https://github.com/ModalMinds/MM-PRM.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [487] [BrainNetMLP: An Efficient and Effective Baseline for Functional Brain Network Classification](https://arxiv.org/abs/2505.11538)
*Jiacheng Hou,Zhenjie Song,Ercan Engin Kuruoglu*

Main category: q-bio.NC

TL;DR: The paper proposes BrainNetMLP, a pure MLP-based method for functional brain network classification, achieving state-of-the-art performance with efficient computation.


<details>
  <summary>Details</summary>
Motivation: Despite the complexity of existing deep learning models, performance gains are limited, prompting a revisit of simpler architectures like MLP.

Method: BrainNetMLP uses a dual-branch MLP structure to capture spatial connectivity and spectral information for spatiotemporal feature fusion.

Result: Evaluated on HCP and ABIDE datasets, BrainNetMLP matches or outperforms complex models, proving MLP's efficacy.

Conclusion: MLP-based models like BrainNetMLP offer efficient, effective alternatives for functional brain network classification.

Abstract: Recent studies have made great progress in functional brain network
classification by modeling the brain as a network of Regions of Interest (ROIs)
and leveraging their connections to understand brain functionality and diagnose
mental disorders. Various deep learning architectures, including Convolutional
Neural Networks, Graph Neural Networks, and the recent Transformer, have been
developed. However, despite the increasing complexity of these models, the
performance gain has not been as salient. This raises a question: Does
increasing model complexity necessarily lead to higher classification accuracy?
In this paper, we revisit the simplest deep learning architecture, the
Multi-Layer Perceptron (MLP), and propose a pure MLP-based method, named
BrainNetMLP, for functional brain network classification, which capitalizes on
the advantages of MLP, including efficient computation and fewer parameters.
Moreover, BrainNetMLP incorporates a dual-branch structure to jointly capture
both spatial connectivity and spectral information, enabling precise
spatiotemporal feature fusion. We evaluate our proposed BrainNetMLP on two
public and popular brain network classification datasets, the Human Connectome
Project (HCP) and the Autism Brain Imaging Data Exchange (ABIDE). Experimental
results demonstrate pure MLP-based methods can achieve state-of-the-art
performance, revealing the potential of MLP-based models as more efficient yet
effective alternatives in functional brain network classification. The code
will be available at https://github.com/JayceonHo/BrainNetMLP.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [488] [ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems](https://arxiv.org/abs/2505.11572)
*Anand Rai,Satyam Rahangdale,Utkarsh Anand,Animesh Mukherjee*

Main category: cs.SD

TL;DR: The paper introduces ASR-FAIRBENCH, a leaderboard to evaluate ASR models for accuracy and fairness, using a fairness-adjusted score (FAAS) alongside traditional metrics like WER.


<details>
  <summary>Details</summary>
Motivation: Address performance disparities in ASR systems across diverse demographic groups.

Method: Uses Meta's Fair-Speech dataset and a mixed-effects Poisson regression model to compute a fairness score, combined with WER for FAAS.

Result: Reveals significant performance disparities in state-of-the-art ASR models across demographics.

Conclusion: Proposes a benchmark to encourage more inclusive ASR technologies.

Abstract: Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday
applications, yet significant disparities in performance across diverse
demographic groups persist. In this work, we introduce the ASR-FAIRBENCH
leaderboard which is designed to assess both the accuracy and equity of ASR
models in real-time. Leveraging the Meta's Fair-Speech dataset, which captures
diverse demographic characteristics, we employ a mixed-effects Poisson
regression model to derive an overall fairness score. This score is integrated
with traditional metrics like Word Error Rate (WER) to compute the Fairness
Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our
approach reveals significant performance disparities in SOTA ASR models across
demographic groups and offers a benchmark to drive the development of more
inclusive ASR technologies.

</details>


### [489] [VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning](https://arxiv.org/abs/2505.12332)
*Qianyue Hu,Junyan Wu,Wei Lu,Xiangyang Luo*

Main category: cs.SD

TL;DR: VoiceCloak is a proactive defense framework designed to disrupt unauthorized voice cloning by Diffusion Models (DMs), targeting speaker identity obfuscation and perceptual quality degradation.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for traditional VC models are incompatible with DMs due to their complex generative mechanisms, necessitating a new approach.

Method: VoiceCloak introduces adversarial perturbations to reference audio, distorts speaker identity embeddings, disrupts conditional guidance processes, and employs noise-guided semantic corruption.

Result: VoiceCloak achieves a high defense success rate against unauthorized VC by DMs.

Conclusion: VoiceCloak effectively bridges the gap in proactive defense for DMs, protecting against malicious misuse of voice cloning.

Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice
cloning (VC), while they also increase the risk of malicious misuse. Existing
proactive defenses designed for traditional VC models aim to disrupt the
forgery process, but they have been proven incompatible with DMs due to the
intricate generative mechanisms of diffusion. To bridge this gap, we introduce
VoiceCloak, a multi-dimensional proactive defense framework with the goal of
obfuscating speaker identity and degrading perceptual quality in potential
unauthorized VC. To achieve these goals, we conduct a focused analysis to
identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt
the cloning process by introducing adversarial perturbations into the reference
audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets
speaker identity by distorting representation learning embeddings to maximize
identity variation, which is guided by auditory perception principles.
Additionally, VoiceCloak disrupts crucial conditional guidance processes,
particularly attention context, thereby preventing the alignment of vocal
characteristics that are essential for achieving convincing cloning. Then, to
address the second objective, VoiceCloak introduces score magnitude
amplification to actively steer the reverse trajectory away from the generation
of high-quality speech. Noise-guided semantic corruption is further employed to
disrupt structural speech semantics captured by DMs, degrading output quality.
Extensive experiments highlight VoiceCloak's outstanding defense success rate
against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak
are available at https://voice-cloak.github.io/VoiceCloak/.

</details>


### [490] [MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix](https://arxiv.org/abs/2505.13032)
*Ziyang Ma,Yinghao Ma,Yanqiao Zhu,Chen Yang,Yi-Wen Chao,Ruiyang Xu,Wenxi Chen,Yuanzhe Chen,Zhuo Chen,Jian Cong,Kai Li,Keliang Li,Siyou Li,Xinfeng Li,Xiquan Li,Zheng Lian,Yuzhe Liang,Minghao Liu,Zhikang Niu,Tianrui Wang,Yuping Wang,Yuxuan Wang,Yihao Wu,Guanrou Yang,Jianwei Yu,Ruibin Yuan,Zhisheng Zheng,Ziya Zhou,Haina Zhu,Wei Xue,Emmanouil Benetos,Kai Yu,Eng-Siong Chng,Xie Chen*

Main category: cs.SD

TL;DR: MMAR is a new benchmark for evaluating deep reasoning in Audio-Language Models (ALMs) across diverse real-world audio scenarios, featuring 1,000 high-quality audio-question-answer triplets categorized into four reasoning layers. It includes Chain-of-Thought annotations and challenges models with graduate-level tasks, revealing current limitations in reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are limited to specific audio domains (sound, music, speech). MMAR aims to broaden evaluation to diverse real-world scenarios and deeper reasoning layers.

Method: MMAR comprises 1,000 curated audio-question-answer triplets from internet videos, hierarchically categorized into Signal, Perception, Semantic, and Cultural reasoning layers. Each question includes a Chain-of-Thought rationale.

Result: Evaluation of models (LALMs, LARMs, OLMs, LLMs, LRMs) shows MMAR's challenging nature, exposing limitations in current models' reasoning and understanding.

Conclusion: MMAR serves as a catalyst for advancing research in audio reasoning, highlighting gaps and encouraging future improvements in ALMs.

Abstract: We introduce MMAR, a new benchmark designed to evaluate the deep reasoning
capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary
tasks. MMAR comprises 1,000 meticulously curated audio-question-answer
triplets, collected from real-world internet videos and refined through
iterative error corrections and quality checks to ensure high quality. Unlike
existing benchmarks that are limited to specific domains of sound, music, or
speech, MMAR extends them to a broad spectrum of real-world audio scenarios,
including mixed-modality combinations of sound, music, and speech. Each
question in MMAR is hierarchically categorized across four reasoning layers:
Signal, Perception, Semantic, and Cultural, with additional sub-categories
within each layer to reflect task diversity and complexity. To further foster
research in this area, we annotate every question with a Chain-of-Thought (CoT)
rationale to promote future advancements in audio reasoning. Each item in the
benchmark demands multi-step deep reasoning beyond surface-level understanding.
Moreover, a part of the questions requires graduate-level perceptual and
domain-specific knowledge, elevating the benchmark's difficulty and depth. We
evaluate MMAR using a broad set of models, including Large Audio-Language
Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models
(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with
audio caption inputs. The performance of these models on MMAR highlights the
benchmark's challenging nature, and our analysis further reveals critical
limitations of understanding and reasoning capabilities among current models.
We hope MMAR will serve as a catalyst for future advances in this important but
little-explored area.

</details>


### [491] [Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio](https://arxiv.org/abs/2505.12863)
*Jongmin Jung,Dongmin Kim,Sihun Lee,Seola Cho,Hyungjoon Soh,Irmak Bukey,Chris Donahue,Dasaem Jeong*

Main category: cs.SD

TL;DR: A unified model for multimodal music translation tasks using a large-scale dataset and tokenization framework achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Past work on music modality translation relies on specialized models for individual tasks, lacking a general-purpose approach.

Method: Proposes a large-scale dataset (1,300+ hours of paired audio-score data) and a unified tokenization framework for discretizing modalities, enabling a single Transformer model for multiple tasks.

Result: Reduces symbol error rate for optical music recognition to 13.67% and achieves breakthroughs like score-image-conditioned audio generation.

Conclusion: The unified multitask model outperforms single-task baselines, demonstrating viability and advancing cross-modal music generation.

Abstract: Music exists in various modalities, such as score images, symbolic scores,
MIDI, and audio. Translations between each modality are established as core
tasks of music information retrieval, such as automatic music transcription
(audio-to-MIDI) and optical music recognition (score image to symbolic score).
However, most past work on multimodal translation trains specialized models on
individual translation tasks. In this paper, we propose a unified approach,
where we train a general-purpose model on many translation tasks
simultaneously. Two key factors make this unified approach viable: a new
large-scale dataset and the tokenization of each modality. Firstly, we propose
a new dataset that consists of more than 1,300 hours of paired audio-score
image data collected from YouTube videos, which is an order of magnitude larger
than any existing music modal translation datasets. Secondly, our unified
tokenization framework discretizes score images, audio, MIDI, and MusicXML into
a sequence of tokens, enabling a single encoder-decoder Transformer to tackle
multiple cross-modal translation as one coherent sequence-to-sequence task.
Experimental results confirm that our unified multitask model improves upon
single-task baselines in several key areas, notably reducing the symbol error
rate for optical music recognition from 24.58% to a state-of-the-art 13.67%,
while similarly substantial improvements are observed across the other
translation tasks. Notably, our approach achieves the first successful
score-image-conditioned audio generation, marking a significant breakthrough in
cross-modal music generation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [492] [Deep Unrolled Meta-Learning for Multi-Coil and Multi-Modality MRI with Adaptive Optimization](https://arxiv.org/abs/2505.11518)
*Merham Fouladvand,Peuroly Batra*

Main category: math.OC

TL;DR: A deep meta-learning framework for MRI combines multi-coil reconstruction and cross-modality synthesis, outperforming conventional methods in PSNR and SSIM under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of conventional methods in handling undersampled data and missing modalities in MRI.

Method: Unrolls a convergent optimization algorithm into a neural network, integrating meta-learning for adaptation to unseen sampling patterns and modalities.

Result: Significant improvements in PSNR and SSIM, especially with aggressive undersampling and domain shifts.

Conclusion: The framework synergizes unrolled optimization, meta-learning, and modality fusion, providing a scalable solution for clinical MRI.

Abstract: We propose a unified deep meta-learning framework for accelerated magnetic
resonance imaging (MRI) that jointly addresses multi-coil reconstruction and
cross-modality synthesis. Motivated by the limitations of conventional methods
in handling undersampled data and missing modalities, our approach unrolls a
provably convergent optimization algorithm into a structured neural network
architecture. Each phase of the network mimics a step of an adaptive
forward-backward scheme with extrapolation, enabling the model to incorporate
both data fidelity and nonconvex regularization in a principled manner. To
enhance generalization across different acquisition settings, we integrate
meta-learning, which enables the model to rapidly adapt to unseen sampling
patterns and modality combinations using task-specific meta-knowledge. The
proposed method is evaluated on the open source datasets, showing significant
improvements in PSNR and SSIM over conventional supervised learning, especially
under aggressive undersampling and domain shifts. Our results demonstrate the
synergy of unrolled optimization, task-aware meta-learning, and modality
fusion, offering a scalable and generalizable solution for real-world clinical
MRI reconstruction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [493] [Introduction to Analytical Software Engineering Design Paradigm](https://arxiv.org/abs/2505.11979)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TL;DR: The paper introduces Analytical Software Engineering (ASE), a new paradigm to address the complexities in modern software systems, evaluated through BSS and ODR frameworks for design pattern detection and code refactoring.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to handle the growing complexity of software systems, necessitating a new approach for tasks like design pattern detection and code refactoring.

Method: ASE is proposed as a novel paradigm, evaluated using BSS (for design pattern detection) and ODR (for optimized code refactoring).

Result: BSS provides a compact, language-agnostic code representation, while ODR optimizes refactoring with heuristic algorithms, reducing computational overhead.

Conclusion: ASE offers a structured solution for complex software challenges, paving the way for future research in software metrics.

Abstract: As modern software systems expand in scale and complexity, the challenges
associated with their modeling and formulation grow increasingly intricate.
Traditional approaches often fall short in effectively addressing these
complexities, particularly in tasks such as design pattern detection for
maintenance and assessment, as well as code refactoring for optimization and
long-term sustainability. This growing inadequacy underscores the need for a
paradigm shift in how such challenges are approached and resolved. This paper
presents Analytical Software Engineering (ASE), a novel design paradigm aimed
at balancing abstraction, tool accessibility, compatibility, and scalability.
ASE enables effective modeling and resolution of complex software engineering
problems. The paradigm is evaluated through two frameworks
Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),
both developed in accordance with ASE principles. BSS offers a compact,
language-agnostic representation of codebases to facilitate precise design
pattern detection. ODR unifies artifact and solution representations to
optimize code refactoring via heuristic algorithms while eliminating iterative
computational overhead. By providing a structured approach to software design
challenges, ASE lays the groundwork for future research in encoding and
analyzing complex software metrics.

</details>


### [494] [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
*Sen Fang,Weiyuan Ding,Bowen Xu*

Main category: cs.SE

TL;DR: EVALOOP is a novel framework for assessing LLM robustness in programming by leveraging self-consistency loops, revealing performance drops and misalignment with initial accuracy.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs in programming focus on static benchmarks, neglecting robustness. Adversarial attacks are inconsistent, necessitating a unified evaluation method.

Method: EVALOOP uses a self-consistency feedback loop: an LLM generates output (e.g., code), which is then used as input to produce new output (e.g., summarization), repeated to assess robustness.

Result: Testing 16 LLMs, EVALOOP showed a 5.01%-19.31% drop in pass@1 performance over ten loops, with robustness not always correlating with initial performance (e.g., GPT-3.5-Turbo vs. DeepSeek-V2).

Conclusion: EVALOOP provides a unified, attack-free method to evaluate LLM robustness, highlighting discrepancies between initial performance and sustained robustness.

Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is
crucial for their effective use in software engineering. Current evaluations,
however, predominantly measure the accuracy of generated code on static
benchmarks, neglecting the critical aspect of model robustness during
programming tasks. While adversarial attacks offer insights on model
robustness, their effectiveness is limited and evaluation could be constrained.
Current adversarial attack methods for robustness evaluation yield inconsistent
results, struggling to provide a unified evaluation across different LLMs. We
introduce EVALOOP, a novel assessment framework that evaluate the robustness
from a self-consistency perspective, i.e., leveraging the natural duality
inherent in popular software engineering tasks, e.g., code generation and code
summarization. EVALOOP initiates a self-contained feedback loop: an LLM
generates output (e.g., code) from an input (e.g., natural language
specification), and then use the generated output as the input to produce a new
output (e.g., summarizes that code into a new specification). EVALOOP repeats
the process to assess the effectiveness of EVALOOP in each loop. This cyclical
strategy intrinsically evaluates robustness without rely on any external attack
setups, providing a unified metric to evaluate LLMs' robustness in programming.
We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found
that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1
performance within ten loops. Intriguingly, robustness does not always align
with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,
despite superior initial code generation compared to DeepSeek-V2, demonstrated
lower robustness over repeated evaluation loop.

</details>


### [495] [AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models](https://arxiv.org/abs/2505.12900)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Jianyuan Liang,Haoyue Jiao,Yaxian Qing,Xiaopu Zhang,Xu Li,Zhipeng Gui,Xuefeng Guan,Longgang Xiang*

Main category: cs.SE

TL;DR: AutoGEEval is a multimodal, automated evaluation framework for geospatial code generation on Google Earth Engine, featuring a benchmark suite and comprehensive analysis of 18 LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized tools for evaluating geospatial code generation, especially on the Google Earth Engine platform.

Method: Proposes AutoGEEval, integrating question generation and answer verification, with a benchmark suite of 1325 test cases across 26 GEE data types.

Result: Evaluated 18 LLMs, revealing performance traits and optimization pathways for GEE code generation.

Conclusion: Provides a unified protocol for developing and assessing geospatial code generation models, advancing automated natural language to domain-specific code translation.

Abstract: Geospatial code generation is emerging as a key direction in the integration
of artificial intelligence and geoscientific analysis. However, there remains a
lack of standardized tools for automatic evaluation in this domain. To address
this gap, we propose AutoGEEval, the first multimodal, unit-level automated
evaluation framework for geospatial code generation tasks on the Google Earth
Engine (GEE) platform powered by large language models (LLMs). Built upon the
GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)
comprising 1325 test cases that span 26 GEE data types. The framework
integrates both question generation and answer verification components to
enable an end-to-end automated evaluation pipeline-from function invocation to
execution validation. AutoGEEval supports multidimensional quantitative
analysis of model outputs in terms of accuracy, resource consumption, execution
efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including
general-purpose, reasoning-augmented, code-centric, and geoscience-specialized
models-revealing their performance characteristics and potential optimization
pathways in GEE code generation. This work provides a unified protocol and
foundational resource for the development and assessment of geospatial code
generation models, advancing the frontier of automated natural language to
domain-specific code translation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [496] [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
*Chih-Kai Yang,Neo Ho,Yen-Ting Piao,Hung-yi Lee*

Main category: eess.AS

TL;DR: SAKURA benchmark evaluates multi-hop reasoning in large audio-language models (LALMs), revealing their struggle to integrate speech/audio information despite correct extraction.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook multi-hop reasoning in LALMs, a critical aspect of their reasoning abilities.

Method: Introduces SAKURA, a benchmark to systematically assess LALMs' multi-hop reasoning using speech and audio data.

Result: LALMs fail to effectively integrate speech/audio representations for multi-hop reasoning, even with correct information extraction.

Conclusion: The study highlights a fundamental challenge in multimodal reasoning, providing insights for future research.

Abstract: Large audio-language models (LALMs) extend the large language models with
multimodal understanding in speech, audio, etc. While their performances on
speech and audio-processing tasks are extensively studied, their reasoning
abilities remain underexplored. Particularly, their multi-hop reasoning, the
ability to recall and integrate multiple facts, lacks systematic evaluation.
Existing benchmarks focus on general speech and audio-processing tasks,
conversational abilities, and fairness but overlook this aspect. To bridge this
gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning
based on speech and audio information. Results show that LALMs struggle to
integrate speech/audio representations for multi-hop reasoning, even when they
extract the relevant information correctly, highlighting a fundamental
challenge in multimodal reasoning. Our findings expose a critical limitation in
LALMs, offering insights and resources for future research.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [497] [Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories](https://arxiv.org/abs/2505.12373)
*Kapil Dev*

Main category: cs.GR

TL;DR: A large-scale study of human aesthetic preferences for 3D shapes, using empirical data and interpretable models to uncover universal and category-specific trends.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical grounding in computational models of 3D aesthetics and provide actionable insights for designers.

Method: Collected 22,301 pairwise comparisons via Amazon Mechanical Turk, applied the Bradley-Terry model for latent scores, and used Random Forests with SHAP analysis to identify influential geometric features.

Result: Identified universal and domain-specific geometric drivers of aesthetic preference (e.g., symmetry, curvature, compactness).

Conclusion: Bridges computational aesthetics and cognitive science, offering practical design guidance and a reproducible dataset.

Abstract: Human aesthetic preferences for 3D shapes are central to industrial design,
virtual reality, and consumer product development. However, most computational
models of 3D aesthetics lack empirical grounding in large-scale human
judgments, limiting their practical relevance. We present a large-scale study
of human preferences. We collected 22,301 pairwise comparisons across five
object categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon
Mechanical Turk. Building on a previously published
dataset~\cite{dev2020learning}, we introduce new non-linear modeling and
cross-category analysis to uncover the geometric drivers of aesthetic
preference. We apply the Bradley-Terry model to infer latent aesthetic scores
and use Random Forests with SHAP analysis to identify and interpret the most
influential geometric features (e.g., symmetry, curvature, compactness). Our
cross-category analysis reveals both universal principles and domain-specific
trends in aesthetic preferences. We focus on human interpretable geometric
features to ensure model transparency and actionable design insights, rather
than relying on black-box deep learning approaches. Our findings bridge
computational aesthetics and cognitive science, providing practical guidance
for designers and a publicly available dataset to support reproducibility. This
work advances the understanding of 3D shape aesthetics through a human-centric,
data-driven framework.

</details>


### [498] [UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes](https://arxiv.org/abs/2505.12774)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Ajmal Mian*

Main category: cs.GR

TL;DR: UniHM is a unified motion language model using diffusion-based generation for scene-aware human motion synthesis, supporting Text-to-Motion and Text-to-HOI in 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with scene-aware motion generation due to motion tokenization limitations, leading to information loss and poor context capture.

Method: Proposes UniHM with mixed-motion representation, LFQ-VAE for better quantization, and an enriched Lingo dataset.

Result: UniHM performs comparably on OMOMO for text-to-HOI and competitively on HumanML3D for text-to-motion.

Conclusion: UniHM addresses scene-aware motion synthesis challenges effectively with its novel contributions.

Abstract: Human motion synthesis in complex scenes presents a fundamental challenge,
extending beyond conventional Text-to-Motion tasks by requiring the integration
of diverse modalities such as static environments, movable objects, natural
language prompts, and spatial waypoints. Existing language-conditioned motion
models often struggle with scene-aware motion generation due to limitations in
motion tokenization, which leads to information loss and fails to capture the
continuous, context-dependent nature of 3D human movement. To address these
issues, we propose UniHM, a unified motion language model that leverages
diffusion-based generation for synthesizing scene-aware human motion. UniHM is
the first framework to support both Text-to-Motion and Text-to-Human-Object
Interaction (HOI) in complex 3D scenes. Our approach introduces three key
contributions: (1) a mixed-motion representation that fuses continuous 6DoF
motion with discrete local motion tokens to improve motion realism; (2) a novel
Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in
both reconstruction accuracy and generative performance; and (3) an enriched
version of the Lingo dataset augmented with HumanML3D annotations, providing
stronger supervision for scene-specific motion learning. Experimental results
demonstrate that UniHM achieves comparative performance on the OMOMO benchmark
for text-to-HOI synthesis and yields competitive results on HumanML3D for
general text-conditioned motion generation.

</details>


### [499] [AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2505.12782)
*Kai Zhang,Xingyu Chen,Xiaofeng Zhang*

Main category: cs.GR

TL;DR: AdaToken-3D is a framework that optimizes 3D Large Multimodal Models (LMMs) by dynamically pruning redundant spatial tokens, improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Current 3D LMMs suffer from inefficiencies like excessive computational overhead and redundant information flows due to heterogeneous mechanisms between spatial and visual tokens.

Method: The proposed AdaToken-3D framework dynamically prunes redundant tokens by analyzing spatial contributions and quantifying token-level information flows via attention pattern mining.

Result: Experiments show AdaToken-3D achieves 21% faster inference speed and 63% FLOPs reduction while maintaining task accuracy. Over 60% of spatial tokens contribute minimally to predictions.

Conclusion: AdaToken-3D provides an efficient solution for 3D LMMs and reveals insights into redundancy patterns, laying foundations for future multimodal learning research.

Abstract: Large Multimodal Models (LMMs) have become a pivotal research focus in deep
learning, demonstrating remarkable capabilities in 3D scene understanding.
However, current 3D LMMs employing thousands of spatial tokens for multimodal
reasoning suffer from critical inefficiencies: excessive computational overhead
and redundant information flows. Unlike 2D VLMs processing single images, 3D
LMMs exhibit inherent architectural redundancy due to the heterogeneous
mechanisms between spatial tokens and visual tokens. To address this challenge,
we propose AdaToken-3D, an adaptive spatial token optimization framework that
dynamically prunes redundant tokens through spatial contribution analysis. Our
method automatically tailors pruning strategies to different 3D LMM
architectures by quantifying token-level information flows via attention
pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)
demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%
FLOPs reduction while maintaining original task accuracy. Beyond efficiency
gains, this work systematically investigates redundancy patterns in multimodal
spatial information flows through quantitative token interaction analysis. Our
findings reveal that over 60\% of spatial tokens contribute minimally ($<$5\%)
to the final predictions, establishing theoretical foundations for efficient 3D
multimodal learning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [500] [TARGET: Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/abs/2505.11545)
*Xingyu Ji,Parker Glenn,Aditya G. Parameswaran,Madelon Hulsebos*

Main category: cs.IR

TL;DR: TARGET is a benchmark for evaluating table retrieval in generative tasks, showing dense embedding-based retrievers outperform BM25, with performance varying across datasets and metadata.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving the right tables for analytical queries or tasks in structured data, given the rise of natural language interfaces and retrieval-augmented generation.

Method: Introduces TARGET, a benchmark to evaluate table retrieval performance of different retrievers, including dense embedding-based and BM25, analyzing their impact on downstream tasks.

Result: Dense embedding-based retrievers significantly outperform BM25, with performance sensitive to metadata and varying across datasets and tasks.

Conclusion: TARGET provides a valuable benchmark for table retrieval, highlighting the superiority of dense embedding methods and the impact of metadata on performance.

Abstract: The data landscape is rich with structured data, often of high value to
organizations, driving important applications in data analysis and machine
learning. Recent progress in representation learning and generative models for
such data has led to the development of natural language interfaces to
structured data, including those leveraging text-to-SQL. Contextualizing
interactions, either through conversational interfaces or agentic components,
in structured data through retrieval-augmented generation can provide
substantial benefits in the form of freshness, accuracy, and comprehensiveness
of answers. The key question is: how do we retrieve the right table(s) for the
analytical query or task at hand? To this end, we introduce TARGET: a benchmark
for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the
retrieval performance of different retrievers in isolation, as well as their
impact on downstream tasks. We find that dense embedding-based retrievers far
outperform a BM25 baseline which is less effective than it is for retrieval
over unstructured text. We also surface the sensitivity of retrievers across
various metadata (e.g., missing table titles), and demonstrate a stark
variation of retrieval performance across datasets and tasks. TARGET is
available at https://target-benchmark.github.io.

</details>


### [501] [MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark](https://arxiv.org/abs/2505.11651)
*Radek Osmulsk,Gabriel de Souza P. Moreira,Ronay Ak,Mengyao Xu,Benedikt Schifferer,Even Oldridge*

Main category: cs.IR

TL;DR: MIRACL-VISION is a multilingual visual document retrieval benchmark addressing gaps in current benchmarks by covering 18 languages and reducing corpus size for efficiency. It reveals performance gaps between visual and text-based models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for visual document retrieval are limited to English, use synthetic questions, and have small corpora. MIRACL-VISION aims to address these limitations.

Method: Extends the MIRACL dataset, uses human-annotated questions, and eliminates 'easy' negatives to optimize corpus size. Evaluates using text and image models.

Result: Visual models underperform text-based models by up to 59.7% in multilingual settings and 12.1% in English.

Conclusion: MIRACL-VISION provides a challenging, multilingual benchmark to improve visual document retrieval models.

Abstract: Document retrieval is an important task for search and Retrieval-Augmented
Generation (RAG) applications. Large Language Models (LLMs) have contributed to
improving the accuracy of text-based document retrieval. However, documents
with complex layout and visual elements like tables, charts and infographics
are not perfectly represented in textual format. Recently, image-based document
retrieval pipelines have become popular, which use visual large language models
(VLMs) to retrieve relevant page images given a query. Current evaluation
benchmarks on visual document retrieval are limited, as they primarily focus
only English language, rely on synthetically generated questions and offer a
small corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual
document retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and
is an extension of the MIRACL dataset, a popular benchmark to evaluate
text-based multilingual retrieval pipelines. MIRACL was built using a
human-intensive annotation process to generate high-quality questions. In order
to reduce MIRACL-VISION corpus size to make evaluation more compute friendly
while keeping the datasets challenging, we have designed a method for
eliminating the "easy" negatives from the corpus. We conducted extensive
experiments comparing MIRACL-VISION with other benchmarks, using popular public
text and image models. We observe a gap in state-of-the-art VLM-based embedding
models on multilingual capabilities, with up to 59.7% lower retrieval accuracy
than a text-based retrieval models. Even for the English language, the visual
models retrieval accuracy is 12.1% lower compared to text-based models.
MIRACL-VISION is a challenging, representative, multilingual evaluation
benchmark for visual retrieval pipelines and will help the community build
robust models for document retrieval.

</details>


### [502] [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
*Guangyuan Ma,Yongliang Ma,Xuanrui Gou,Zhenpeng Su,Ming Zhou,Songlin Hu*

Main category: cs.IR

TL;DR: LightRetriever proposes a lightweight query encoder for LLM-based hybrid retrieval, achieving significant speedups while retaining most performance.


<details>
  <summary>Details</summary>
Motivation: LLM-based retrieval is powerful but slow for real-time queries due to heavy LLM encoding. LightRetriever addresses this by reducing query encoding workload.

Method: Uses a full-sized LLM for document encoding but replaces query encoding with an embedding lookup, drastically reducing computational load.

Result: Achieves 1000x speedup with GPU and 20x without, retaining 95% of full-sized LLM performance on benchmarks.

Conclusion: LightRetriever offers a practical solution for efficient LLM-based retrieval without sacrificing much performance.

Abstract: Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional sparse
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU acceleration, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [503] [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
*Kerry Xiao,Amy Zang*

Main category: econ.GN

TL;DR: The paper explores how vague language in analyst reports conveys useful information about future payoffs, influencing forecast errors and revisions, especially under uncertainty or analyst workload.


<details>
  <summary>Details</summary>
Motivation: To understand the role of vague language in conveying subjective expectations about future payoffs, where quantification is impractical.

Method: Empirical analysis of analyst reports, focusing on textual tone and its predictive power for forecast errors and revisions.

Result: Textual tone predicts forecast errors and revisions, with stronger effects under vaguer language, higher uncertainty, and busier analysts.

Conclusion: Vague language in analyst reports communicates valuable information that numerical forecasts alone cannot capture.

Abstract: People in the real world often possess vague knowledge of future payoffs, for
which quantification is not feasible or desirable. We argue that language, with
differing ability to convey vague information, plays an important but less
known-role in subjective expectations. Empirically, we find that in their
reports, analysts include useful information in linguistic expressions but not
numerical forecasts. Specifically, the textual tone of analyst reports has
predictive power for forecast errors and subsequent revisions in numerical
forecasts, and this relation becomes stronger when analyst's language is
vaguer, when uncertainty is higher, and when analysts are busier. Overall, our
theory and evidence suggest that some useful information is vaguely known and
only communicated through language.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [504] [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
*Liwen Wang,Wenxuan Wang,Shuai Wang,Zongjie Li,Zhenlan Ji,Zongyi Lyu,Daoyuan Wu,Shing-Chi Cheung*

Main category: cs.CR

TL;DR: MASLEAK is a black-box attack framework that extracts sensitive information from Multi-Agent Systems (MAS) with high success rates, revealing proprietary components like system prompts and architecture.


<details>
  <summary>Details</summary>
Motivation: The complexity of MAS raises concerns about intellectual property (IP) protection, prompting the need to study vulnerabilities like MASLEAK.

Method: MASLEAK crafts adversarial queries to elicit and propagate responses from MAS agents, revealing proprietary details without prior knowledge of the system.

Result: MASLEAK achieves 87% success for system prompts/task instructions and 92% for system architecture, tested on synthetic and real-world MAS applications.

Conclusion: The findings highlight MAS vulnerabilities, urging the development of defenses against such attacks.

Abstract: The rapid advancement of Large Language Models (LLMs) has led to the
emergence of Multi-Agent Systems (MAS) to perform complex tasks through
collaboration. However, the intricate nature of MAS, including their
architecture and agent interactions, raises significant concerns regarding
intellectual property (IP) protection. In this paper, we introduce MASLEAK, a
novel attack framework designed to extract sensitive information from MAS
applications. MASLEAK targets a practical, black-box setting, where the
adversary has no prior knowledge of the MAS architecture or agent
configurations. The adversary can only interact with the MAS through its public
API, submitting attack query $q$ and observing outputs from the final agent.
Inspired by how computer worms propagate and infect vulnerable network hosts,
MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain
responses from each MAS agent that reveal a full set of proprietary components,
including the number of agents, system topology, system prompts, task
instructions, and tool usages. We construct the first synthetic dataset of MAS
applications with 810 applications and also evaluate MASLEAK against real-world
MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in
extracting MAS IP, with an average attack success rate of 87% for system
prompts and task instructions, and 92% for system architecture in most cases.
We conclude by discussing the implications of our findings and the potential
defenses.

</details>


### [505] [Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)
*Sayon Palit,Daniel Woods*

Main category: cs.CR

TL;DR: The paper evaluates LLM security tools, finding baseline models like ChatGPT-3.5-Turbo ineffective due to high false positives. Lakera Guard and ProtectAI LLM Guard perform best, balancing usability and performance. Recommendations include transparency, better detection, and user awareness.


<details>
  <summary>Details</summary>
Motivation: LLMs in critical systems face security threats like data leaks. Lack of formal evaluation of security tools prompted this study.

Method: Comparative analysis of 7 LLM security tools using a benchmark dataset of malicious prompts, tested against ChatGPT-3.5-Turbo.

Result: Baseline model had high false positives; Lakera Guard and ProtectAI LLM Guard performed best.

Conclusion: Recommendations: transparency for closed-source tools, improved detection, open-source engagement, user awareness, and better metrics.

Abstract: Large Language Models (LLMs) are increasingly integrated into critical
systems in industries like healthcare and finance. Users can often submit
queries to LLM-enabled chatbots, some of which can enrich responses with
information retrieved from internal databases storing sensitive data. This
gives rise to a range of attacks in which a user submits a malicious query and
the LLM-system outputs a response that creates harm to the owner, such as
leaking internal data or creating legal liability by harming a third-party.
While security tools are being developed to counter these threats, there is
little formal evaluation of their effectiveness and usability. This study
addresses this gap by conducting a thorough comparative analysis of LLM
security tools. We identified 13 solutions (9 closed-source, 4 open-source),
but only 7 were evaluated due to a lack of participation by proprietary model
owners.To evaluate, we built a benchmark dataset of malicious prompts, and
evaluate these tools performance against a baseline LLM model
(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many
false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard
emerged as the best overall tools showcasing the tradeoff between usability and
performance. The study concluded with recommendations for greater transparency
among closed source providers, improved context-aware detections, enhanced
open-source engagement, increased user awareness, and the adoption of more
representative performance metrics.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [506] [Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion](https://arxiv.org/abs/2505.12051)
*Yinghui Zhang,Tailin Chen,Yuchen Zhang,Zeyu Fu*

Main category: cs.MM

TL;DR: CMFusion is a multimodal hate video detection model that outperforms existing methods by effectively integrating temporal dynamics and modality-wise interactions.


<details>
  <summary>Details</summary>
Motivation: The rise of video platforms has increased the spread of harmful content like hate videos, which are hard to detect due to their implicit nature. Current unimodal methods are inadequate, and multimodal approaches often fail to integrate temporal and modality interactions.

Method: CMFusion uses pre-trained models to extract text, audio, and video features, employs a temporal cross-attention mechanism, and processes features with channel-wise and modality-wise fusion modules.

Result: CMFusion outperforms five baselines in accuracy, precision, recall, and F1 score, validated by ablation studies and parameter analyses.

Conclusion: CMFusion is effective for hate video detection, with source codes to be publicly released.

Abstract: The rapid rise of video content on platforms such as TikTok and YouTube has
transformed information dissemination, but it has also facilitated the spread
of harmful content, particularly hate videos. Despite significant efforts to
combat hate speech, detecting these videos remains challenging due to their
often implicit nature. Current detection methods primarily rely on unimodal
approaches, which inadequately capture the complementary features across
different modalities. While multimodal techniques offer a broader perspective,
many fail to effectively integrate temporal dynamics and modality-wise
interactions essential for identifying nuanced hate content. In this paper, we
present CMFusion, an enhanced multimodal hate video detection model utilizing a
novel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts
features from text, audio, and video modalities using pre-trained models and
then incorporates a temporal cross-attention mechanism to capture dependencies
between video and audio streams. The learned features are then processed by
channel-wise and modality-wise fusion modules to obtain informative
representations of videos. Our extensive experiments on a real-world dataset
demonstrate that CMFusion significantly outperforms five widely used baselines
in terms of accuracy, precision, recall, and F1 score. Comprehensive ablation
studies and parameter analyses further validate our design choices,
highlighting the model's effectiveness in detecting hate videos. The source
codes will be made publicly available at https://github.com/EvelynZ10/cmfusion.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [507] [Efficient Generation of Parameterised Quantum Circuits from Large Texts](https://arxiv.org/abs/2505.13208)
*Colin Krawchuk,Nikhil Khatri,Neil John Ortega,Dimitri Kartsaklis*

Main category: quant-ph

TL;DR: The paper introduces DisCoCirc, a novel quantum framework for NLP that encodes entire documents as quantum circuits, improving interpretability and compositionality.


<details>
  <summary>Details</summary>
Motivation: Traditional quantum-classical NLP models rely on classical neural networks, limiting their potential. DisCoCirc aims to overcome this by directly encoding texts into quantum circuits.

Method: The methodology converts large-scale texts into quantum circuits using tree-like pregroup diagrams, leveraging compositional parallels between language and quantum mechanics.

Result: The approach efficiently encodes syntactic and discourse relationships in long texts (up to 6410 words) into quantum circuits.

Conclusion: The system is integrated into the open-source quantum NLP package lambeq Gen II, making it accessible to the community.

Abstract: Quantum approaches to natural language processing (NLP) are redefining how
linguistic information is represented and processed. While traditional hybrid
quantum-classical models rely heavily on classical neural networks, recent
advancements propose a novel framework, DisCoCirc, capable of directly encoding
entire documents as parameterised quantum circuits (PQCs), besides enjoying
some additional interpretability and compositionality benefits. Following these
ideas, this paper introduces an efficient methodology for converting
large-scale texts into quantum circuits using tree-like representations of
pregroup diagrams. Exploiting the compositional parallels between language and
quantum mechanics, grounded in symmetric monoidal categories, our approach
enables faithful and efficient encoding of syntactic and discourse
relationships in long and complex texts (up to 6410 words in our experiments)
to quantum circuits. The developed system is provided to the community as part
of the augmented open-source quantum NLP package lambeq Gen II.

</details>
