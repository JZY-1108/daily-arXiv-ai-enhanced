<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.CV](#cs.CV) [Total: 87]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 7]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/abs/2505.10643)
*Shuchen Guo,Yun Wang,Jichao Yu,Xuansheng Wu,Bilgehan Ayik,Field M. Watts,Ehsan Latif,Ninghao Liu,Lei Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: The study examines scoring biases in automatic scoring systems for ELLs in science assessments, finding no significant bias with large datasets but potential concerns with smaller samples.


<details>
  <summary>Details</summary>
Motivation: To investigate how unbalanced training data affects scoring biases and disparities for ELLs in automated systems.

Method: Fine-tuned BERT with four datasets (ELLs, non-ELLs, unbalanced mixed, balanced mixed) and analyzed scoring accuracy and disparities using Friedman tests and Mean Score Gaps.

Result: No significant AI bias or disparities with large datasets (30,000 or 1,000 ELL responses), but potential issues with smaller samples (200 ELL responses).

Conclusion: Large training datasets mitigate scoring biases for ELLs, but smaller datasets may introduce concerns.

Abstract: This study investigated potential scoring biases and disparities toward
English Language Learners (ELLs) when using automatic scoring systems for
middle school students' written responses to science assessments. We
specifically focus on examining how unbalanced training data with ELLs
contributes to scoring bias and disparities. We fine-tuned BERT with four
datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting
the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced
mixed dataset with equal representation of both groups. The study analyzed 21
assessment items: 10 items with about 30,000 ELL responses, five items with
about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring
accuracy (Acc) was calculated and compared to identify bias using Friedman
tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and
then calculated the differences in MSGs generated through both the human and AI
models to identify the scoring disparities. We found that no AI bias and
distorted disparities between ELLs and non-ELLs were found when the training
dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could
exist if the sample size is limited (ELL = 200).

</details>


### [2] [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
*Bowen Jiang,Yangxinyu Xie,Xiaomeng Wang,Jiashu He,Joshua Bergerson,John K Hutchison,Jordan Branham,Camillo J Taylor,Tanwi Mallick*

Main category: cs.CL

TL;DR: GeoGrid-Bench is a benchmark for evaluating foundation models' understanding of geo-spatial grid data, featuring 3,200 QA pairs from real-world climate data. Vision-language models perform best.


<details>
  <summary>Details</summary>
Motivation: To assess foundation models' ability to handle geo-spatial data's unique challenges (dense values, spatiotemporal dependencies, multimodal representations) and support scientific research.

Method: The benchmark uses large-scale real-world data (16 climate variables, 150 locations) and 3,200 QA pairs generated from expert-curated templates, covering basic to complex tasks.

Result: Vision-language models outperform others, with detailed analysis of strengths and limitations in geo-spatial tasks.

Conclusion: GeoGrid-Bench clarifies how foundation models can effectively analyze geo-spatial data and aid scientific research.

Abstract: We present GeoGrid-Bench, a benchmark designed to evaluate the ability of
foundation models to understand geo-spatial data in the grid structure.
Geo-spatial datasets pose distinct challenges due to their dense numerical
values, strong spatial and temporal dependencies, and unique multimodal
representations including tabular data, heatmaps, and geographic
visualizations. To assess how foundation models can support scientific research
in this domain, GeoGrid-Bench features large-scale, real-world data covering 16
climate variables across 150 locations and extended time frames. The benchmark
includes approximately 3,200 question-answer pairs, systematically generated
from 8 domain expert-curated templates to reflect practical tasks encountered
by human scientists. These range from basic queries at a single location and
time to complex spatiotemporal comparisons across regions and periods. Our
evaluation reveals that vision-language models perform best overall, and we
provide a fine-grained analysis of the strengths and limitations of different
foundation models in different geo-spatial tasks. This benchmark offers clearer
insights into how foundation models can be effectively applied to geo-spatial
data analysis and used to support scientific research.

</details>


### [3] [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/abs/2505.10717)
*Jean-Philippe Corbeil,Amin Dada,Jean-Michel Attendu,Asma Ben Abacha,Alessandro Sordoni,Lucas Caccia,François Beaulieu,Thomas Lin,Jens Kleesiek,Paul Vozila*

Main category: cs.CL

TL;DR: A framework for adapting small language models (SLMs) into high-performing clinical models, outperforming GPT-4 in some tasks, using pre-instruction tuning, model merging, and clinical-task alignment.


<details>
  <summary>Details</summary>
Motivation: High computation costs and latency of large language models like GPT-4 limit clinical deployment, while SLMs need biomedical domain adaptation and face data sensitivity issues.

Method: Proposed framework includes pre-instruction tuning on medical corpora, model merging, and clinical-task alignment. Extended CLUE benchmark to CLUE+ and built synthetic dataset MediFlow.

Result: Expert models improved performance: 64.3% on medical entities, 49.5% on radiology reports, 44% on ICD-10 coding (outperforming GPT-4 by 14%). Merged model MediPhi preserved gains.

Conclusion: The framework successfully adapts SLMs for clinical use, achieving significant performance improvements and addressing data sensitivity challenges.

Abstract: High computation costs and latency of large language models such as GPT-4
have limited their deployment in clinical settings. Small language models
(SLMs) offer a cost-effective alternative, but their limited capacity requires
biomedical domain adaptation, which remains challenging. An additional
bottleneck is the unavailability and high sensitivity of clinical data. To
address these challenges, we propose a novel framework for adapting SLMs into
high-performing clinical models. We introduce the MediPhi collection of
3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning
of experts on relevant medical and clinical corpora (PMC, Medical Guideline,
MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most
clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our
expert models deliver relative improvements on this benchmark over the base
model without any task-specific fine-tuning: 64.3% on medical entities, 49.5%
on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by
14%). We unify the expert models into MediPhi via model merging, preserving
gains across benchmarks. Furthermore, we built the MediFlow collection, a
synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP
tasks, 98 fine-grained document types, and JSON format support. Alignment of
MediPhi using supervised fine-tuning and direct preference optimization
achieves further gains of 18.9% on average.

</details>


### [4] [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)
*Siddharth Suresh,Kushin Mukherjee,Tyler Giallanza,Xizheng Yu,Mia Patil,Jonathan D. Cohen,Timothy T. Rogers*

Main category: cs.CL

TL;DR: The paper introduces NOVA, an AI-enhanced feature norm dataset combining human-generated norms with LLM responses, improving feature density and outperforming human-only datasets in predicting semantic similarity.


<details>
  <summary>Details</summary>
Motivation: Traditional semantic feature norm methods are labor-intensive and face trade-offs between coverage and quality. The study aims to enhance norm datasets using LLMs while ensuring quality.

Method: The approach combines human-generated feature norms with LLM responses, validating the quality against human judgments to create the NOVA dataset.

Result: NOVA shows higher feature density and concept overlap, outperforming human-only datasets and word-embedding models in predicting semantic similarity.

Conclusion: Human conceptual knowledge is richer than previously captured, and LLMs, with proper validation, can be powerful tools for cognitive science research.

Abstract: Semantic feature norms have been foundational in the study of human
conceptual knowledge, yet traditional methods face trade-offs between
concept/feature coverage and verifiability of quality due to the
labor-intensive nature of norming studies. Here, we introduce a novel approach
that augments a dataset of human-generated feature norms with responses from
large language models (LLMs) while verifying the quality of norms against
reliable human judgments. We find that our AI-enhanced feature norm dataset,
NOVA: Norms Optimized Via AI, shows much higher feature density and overlap
among concepts while outperforming a comparable human-only norm dataset and
word-embedding models in predicting people's semantic similarity judgments.
Taken together, we demonstrate that human conceptual knowledge is richer than
captured in previous norm datasets and show that, with proper validation, LLMs
can serve as powerful tools for cognitive science research.

</details>


### [5] [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
*Tomás Vergara-Browne,Álvaro Soto*

Main category: cs.CL

TL;DR: The paper introduces tracr-injection, a method to distill RASP algorithms into pre-trained language models, improving interpretability and out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the theoretical symbolic capabilities of transformers (via RASP) and their practical learnability from unsupervised data.

Method: Proposes tracr-injection to compile RASP algorithms into transformer weights, creating interpretable subspaces in the model.

Result: Successfully injected 3 algorithms, showing interpretable subspaces and improved out-of-distribution performance.

Conclusion: tracr-injection enhances symbolic mechanisms in transformers, with potential for better model interpretability and performance.

Abstract: Motivated by the surge of large language models, there has been a push to
formally characterize the symbolic abilities intrinsic to the transformer
architecture. A programming language, called RASP, has been proposed, which can
be directly compiled into transformer weights to implement these algorithms.
However, the tasks that can be implemented in RASP are often uncommon to learn
from natural unsupervised data, showing a mismatch between theoretical
capabilities of the transformer architecture, and the practical learnability of
these capabilities from unsupervised data. We propose tracr-injection, a method
that allows us to distill algorithms written in RASP directly into a
pre-trained language model. We showcase our method by injecting 3 different
algorithms into a language model. We show how our method creates an
interpretable subspace within the model's residual stream, which can be decoded
into the variables present in the code of the RASP algorithm. Additionally, we
found that the proposed method can improve out of distribution performance
compared to our baseline, indicating that indeed a more symbolic mechanism is
taking place in the inner workings of the model. We release the code used to
run our experiments.

</details>


### [6] [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
*Ximing Dong,Shaowei Wang,Dayi Lin,Ahmed E. Hassan*

Main category: cs.CL

TL;DR: IPOMP is a two-stage method for automated prompt optimization in LLMs, using semantic clustering and iterative refinement with real-time performance data, outperforming SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is inefficient, and existing automated methods rely on unreliable evaluation subsets, leading to suboptimal prompts.

Method: IPOMP uses semantic clustering and boundary analysis for sample selection, followed by iterative refinement with real-time model performance data.

Result: IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57%, with minimal computational overhead (<1%).

Conclusion: IPOMP's real-time performance-guided refinement is universally applicable and enhances existing coreset selection methods.

Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted
prompts, but manual prompt engineering is labor-intensive and often
ineffective. Automated prompt optimization techniques address this challenge
but the majority of them rely on randomly selected evaluation subsets, which
fail to represent the full dataset, leading to unreliable evaluations and
suboptimal prompts. Existing coreset selection methods, designed for LLM
benchmarking, are unsuitable for prompt optimization due to challenges in
clustering similar samples, high data collection costs, and the unavailability
of performance data for new or private datasets. To overcome these issues, we
propose IPOMP, an Iterative evaluation data selection for effective Prompt
Optimization using real-time Model Performance. IPOMP is a two-stage approach
that selects representative and diverse samples using semantic clustering and
boundary analysis, followed by iterative refinement with real-time model
performance data to replace redundant samples. Evaluations on the BIG-bench
dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by
at least 57% compared with SOTA baselines, with minimal computational overhead
below 1%. Furthermore, the results demonstrate that our real-time
performance-guided refinement approach can be universally applied to enhance
existing coreset selection methods.

</details>


### [7] [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.10740)
*Qiwei Peng,Robert Moro,Michal Gregor,Ivan Srba,Simon Ostermann,Marian Simko,Juraj Podroužek,Matúš Mesarčík,Jaroslav Kopčan,Anders Søgaard*

Main category: cs.CL

TL;DR: A shared task on multilingual claim retrieval at SemEval 2025 addressed the gap in handling disinformation in multilingual and low-resource settings, with 179 participants and 52 submissions. The paper highlights top-performing systems and effective approaches.


<details>
  <summary>Details</summary>
Motivation: To tackle the neglect of multilingual and low-resource languages in disinformation detection, the study organized a shared task for multilingual claim retrieval.

Method: The task included monolingual and crosslingual subtracks, with participants submitting systems to match social media posts to fact-checked claims.

Result: 179 participants registered, 52 submissions were tested, and 23 teams submitted system papers, revealing effective approaches for multilingual claim retrieval.

Conclusion: The shared task and its outcomes provide insights for future research in multilingual automated fact-checking.

Abstract: The rapid spread of online disinformation presents a global challenge, and
machine learning has been widely explored as a potential solution. However,
multilingual settings and low-resource languages are often neglected in this
field. To address this gap, we conducted a shared task on multilingual claim
retrieval at SemEval 2025, aimed at identifying fact-checked claims that match
newly encountered claims expressed in social media posts across different
languages. The task includes two subtracks: (1) a monolingual track, where
social posts and claims are in the same language, and (2) a crosslingual track,
where social posts and claims might be in different languages. A total of 179
participants registered for the task contributing to 52 test submissions. 23
out of 31 teams have submitted their system papers. In this paper, we report
the best-performing systems as well as the most common and the most effective
approaches across both subtracks. This shared task, along with its dataset and
participating systems, provides valuable insights into multilingual claim
retrieval and automated fact-checking, supporting future research in this
field.

</details>


### [8] [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/abs/2505.10772)
*Weiqin Wang,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: The paper introduces ranked voting methods (Instant-runoff, Borda count, and mean reciprocal rank) to improve chain-of-thought reasoning by considering multiple ranked answers, outperforming traditional single-answer approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional chain-of-thought reasoning methods generate only one answer per trial, missing potential alternatives. This limits the reliability of majority voting.

Method: Proposes generating ranked answers in each reasoning process and using ranked voting (Instant-runoff, Borda count, mean reciprocal rank) to enhance self-consistency.

Result: Validated on six datasets, the method outperforms baselines, demonstrating improved reasoning performance.

Conclusion: Ranked voting leverages multiple answers to enhance reasoning reliability, showing promise for future applications.

Abstract: Majority voting is considered an effective method to enhance chain-of-thought
reasoning, as it selects the answer with the highest "self-consistency" among
different reasoning paths (Wang et al., 2023). However, previous
chain-of-thought reasoning methods typically generate only a single answer in
each trial, thereby ignoring the possibility of other potential answers. As a
result, these alternative answers are often overlooked in subsequent voting
processes. In this work, we propose to generate ranked answers in each
reasoning process and conduct ranked voting among multiple ranked answers from
different responses, thereby making the overall self-consistency more reliable.
Specifically, we use three ranked voting methods: Instant-runoff voting, Borda
count voting, and mean reciprocal rank voting. We validate our methods on six
datasets, including three multiple-choice and three open-ended
question-answering tasks, using both advanced open-source and closed-source
large language models. Extensive experimental results indicate that our
proposed method outperforms the baselines, showcasing the potential of
leveraging the information of ranked answers and using ranked voting to improve
reasoning performance. The code is available at
https://github.com/szu-tera/RankedVotingSC.

</details>


### [9] [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/abs/2505.10775)
*Kian Ahrabian,Pegah Jandaghi,Negar Mokhberian,Sai Praneeth Karimireddy,Jay Pujara*

Main category: cs.CL

TL;DR: The paper analyzes the impact of base model selection on reward modeling performance in RLHF, showing up to 14% improvement over default choices and highlighting the role of benchmarks and post-training steps.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked factor of base model selection in training high-quality reward models (RMs) for LLMs, given the growing pool of available models.

Method: A systematic analysis of base model selection's effect on reward modeling performance, including benchmarking and combining results to improve selection.

Result: Performance improved by up to 14% over default choices, with benchmarks strongly correlating to downstream performance. Combining benchmarks boosted top 5-10 selection by 18%.

Conclusion: Base model selection significantly impacts reward modeling, and leveraging benchmarks and post-training steps can enhance performance and reduce prediction errors.

Abstract: Reinforcement learning from human feedback (RLHF) and, at its core, reward
modeling have become a crucial part of training powerful large language models
(LLMs). One commonly overlooked factor in training high-quality reward models
(RMs) is the effect of the base model, which is becoming more challenging to
choose given the rapidly growing pool of LLMs. In this work, we present a
systematic analysis of the effect of base model selection on reward modeling
performance. Our results show that the performance can be improved by up to 14%
compared to the most common (i.e., default) choice. Moreover, we showcase the
strong statistical relation between some existing benchmarks and downstream
performances. We also demonstrate that the results from a small set of
benchmarks could be combined to boost the model selection ($+$18% on average in
the top 5-10). Lastly, we illustrate the impact of different post-training
steps on the final performance and explore using estimated data distributions
to reduce performance prediction error.

</details>


### [10] [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10792)
*Zhan Peng Lee,Andre Lin,Calvin Tan*

Main category: cs.CL

TL;DR: Finetune-RAG improves factual accuracy in LLMs by fine-tuning with a dataset mimicking real-world retrieval imperfections, achieving a 21.2% boost. Bench-RAG evaluates models under imperfect retrieval scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of irrelevant retrieved content causing hallucinations in LLMs by improving retrieval-augmented generation.

Method: Proposes Finetune-RAG, a fine-tuning approach using a dataset simulating real-world retrieval imperfections, and Bench-RAG for evaluation.

Result: Finetune-RAG improves factual accuracy by 21.2% over the base model.

Conclusion: The approach effectively enhances factuality in LLMs, with open-sourced resources for community use.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
improve factuality in large language models (LLMs) by grounding their outputs
in retrieved documents. However, ensuring perfect retrieval of relevant
information remains challenging, and when irrelevant content is passed
downstream to an LLM, it can lead to hallucinations. In this work, we propose
Finetune-RAG, a simple and effective fine-tuning approach that features the
first-of-its-kind RAG training dataset constructed to mimic real-world
imperfections. Experimental results show that Finetune-RAG improves factual
accuracy by 21.2% over the base model. We also propose a Bench-RAG, an
LLM-as-a-judge evaluation pipeline that stress tests models under realistic
imperfect retrieval scenarios. Our codebase and dataset are fully open sourced
for community use.

</details>


### [11] [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/abs/2505.10798)
*Erica Cai,Sean McQuade,Kevin Young,Brendan O'Connor*

Main category: cs.CL

TL;DR: AffilKG introduces six datasets pairing complete book scans with labeled knowledge graphs (KGs) to evaluate KG extraction accuracy for downstream analysis, addressing gaps in current datasets.


<details>
  <summary>Details</summary>
Motivation: Current annotated datasets for evaluating KG extraction accuracy are disconnected, small, or overly complex, limiting their utility for downstream analysis.

Method: AffilKG provides six datasets with affiliation graphs (simple KGs) and expanded KGs, pairing book scans with labeled KGs.

Result: Preliminary experiments show significant variability in model performance, highlighting AffilKG's potential for benchmarking extraction errors and validating KG methods.

Conclusion: AffilKG enables benchmarking KG extraction errors and validating methods for real-world social science research.

Abstract: When knowledge graphs (KGs) are automatically extracted from text, are they
accurate enough for downstream analysis? Unfortunately, current annotated
datasets can not be used to evaluate this question, since their KGs are highly
disconnected, too small, or overly complex. To address this gap, we introduce
AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six
datasets that are the first to pair complete book scans with large, labeled
knowledge graphs. Each dataset features affiliation graphs, which are simple
KGs that capture Member relationships between Person and Organization entities
-- useful in studies of migration, community interactions, and other social
phenomena. In addition, three datasets include expanded KGs with a wider
variety of relation types. Our preliminary experiments demonstrate significant
variability in model performance across datasets, underscoring AffilKG's
ability to enable two critical advances: (1) benchmarking how extraction errors
propagate to graph-level analyses (e.g., community structure), and (2)
validating KG extraction methods for real-world social science research.

</details>


### [12] [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/abs/2505.10829)
*Chen-Chi Chang,Chong-Fu Li,Chu-Hsuan Lee,Hung-Shin Lee*

Main category: cs.CL

TL;DR: The study explores using LLMs and RAG for low-resource language translation, testing configurations on Hakka. Best results (BLEU 31%) came from combining retrieval and advanced modeling, while dictionary-only scored 12%. Iterative correction and domain knowledge were key, highlighting ethical collaboration needs.


<details>
  <summary>Details</summary>
Motivation: To address challenges in translating low-resource languages by leveraging LLMs and RAG, focusing on improving accuracy, fluency, and cultural preservation.

Method: Tested various model configurations, including dictionary-only, RAG with Gemini 2.0, and a two-stage method combining dictionary outputs with Gemini refinement. Evaluated using BLEU scores.

Result: Best-performing model (BLEU 31%) combined retrieval and advanced modeling. Dictionary-only scored 12%. Iterative correction (Model 3) achieved 26%, showing value in refinement.

Conclusion: Curated resources, domain knowledge, and ethical collaboration are crucial for accurate, fluent translations, especially for culturally nuanced terms.

Abstract: This study investigates the challenges of translating low-resource languages
by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG). Various model configurations were tested on Hakka translations, with
BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).
The best-performing model (Model 4) combined retrieval and advanced language
modeling, improving lexical coverage, particularly for specialized or
culturally nuanced terms, and enhancing grammatical coherence. A two-stage
method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU
score of 26%, highlighting iterative correction's value and the challenges of
domain-specific expressions. Static dictionary-based approaches struggled with
context-sensitive content, demonstrating the limitations of relying solely on
predefined resources. These results emphasize the need for curated resources,
domain knowledge, and ethical collaboration with local communities, offering a
framework that improves translation accuracy and fluency while supporting
cultural preservation.

</details>


### [13] [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
*Songjun Tu,Jiahao Lin,Qichao Zhang,Xiangyu Tian,Linjing Li,Xiangyuan Lan,Dongbin Zhao*

Main category: cs.CL

TL;DR: AutoThink is a reinforcement learning framework that enables large reasoning models (LRMs) to dynamically decide when to engage in explicit reasoning, improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs often over-think simple problems, causing unnecessary computational overhead. The goal is to make reasoning adaptive based on problem complexity.

Method: Uses a multi-stage RL framework (AutoThink) to optimize reasoning policies, triggered by inserting an ellipsis in prompts.

Result: Achieves 6.4% higher accuracy and 52% lower token usage on benchmarks, showing better efficiency-accuracy trade-offs.

Conclusion: AutoThink provides a scalable, adaptive solution for LRMs, balancing reasoning depth with computational efficiency.

Abstract: Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis ("...") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.

</details>


### [14] [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/abs/2505.10836)
*Abhishek Dey,Aabha Bothera,Samhita Sarikonda,Rishav Aryan,Sanjay Kumar Podishetty,Akshay Havalgi,Gaurav Singh,Saurabh Srivastava*

Main category: cs.CL

TL;DR: The paper explores event detection on social media, comparing unimodal, multimodal, and generative models. Multimodal methods outperform unimodal ones, but generative models lag in precision despite handling social media noise better.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal systems struggle with the rapid, multimodal nature of social media data, prompting the need for advanced models.

Method: Uses unimodal (ModernBERT, ConvNeXt-V2), multimodal fusion, and generative models (GPT-4o, LLaVA), testing generative models with single modalities.

Result: Multimodal approaches outperform unimodal ones; generative models lag in precision but handle social media noise (leet speak, text elongation) better.

Conclusion: Generative models, despite limitations in precision, excel in handling social media noise, highlighting a trade-off between precision and robustness.

Abstract: In this paper, we study the challenges of detecting events on social media,
where traditional unimodal systems struggle due to the rapid and multimodal
nature of data dissemination. We employ a range of models, including unimodal
ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced
generative models like GPT-4o, and LLaVA. Additionally, we also study the
effect of providing multimodal generative models (such as GPT-4o) with a single
modality to assess their efficacy. Our results indicate that while multimodal
approaches notably outperform unimodal counterparts, generative approaches
despite having a large number of parameters, lag behind supervised methods in
precision. Furthermore, we also found that they lag behind instruction-tuned
models because of their inability to generate event classes correctly. During
our error analysis, we discovered that common social media issues such as leet
speak, text elongation, etc. are effectively handled by generative approaches
but are hard to tackle using supervised approaches.

</details>


### [15] [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/abs/2505.10862)
*Tairan Fu,Miguel González,Javier Conde,Elena Merino-Gómez,Pedro Reviriego*

Main category: cs.CL

TL;DR: The paper investigates why Multimodal Large Language Models (MLLMs) like GPT-4.1 struggle to tell time on analog clocks, attributing it to limited training data. It tests whether fine-tuning improves performance and questions if models truly learn or just memorize patterns.


<details>
  <summary>Details</summary>
Motivation: To understand why MLLMs fail at reading analog clocks and whether fine-tuning can address this limitation, highlighting broader challenges in abstraction and generalization.

Method: Testing GPT-4.1 with various analog clocks to evaluate its ability to tell time, assessing fine-tuning's impact, and analyzing whether models generalize or rely on training data patterns.

Result: Models show progress in reading analog clocks but may not truly learn the task, instead memorizing patterns from limited training data.

Conclusion: MLLMs still struggle with abstract tasks like reading analog clocks, revealing limitations in generalization beyond training data patterns.

Abstract: Multimodal Large Language Models which can answer complex questions on an
image struggle to tell the time on analog clocks. This is probably due to the
lack of images with clocks at different times in their training set. In this
work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand
why MLLMs fail to tell the time and whether fine-tuning can solve the problem.
The results show how models are making progress in reading the time on analog
clocks. But have they really learned to do it, or have they only learned
patterns in their training datasets? In this work we put the models to the test
with different clocks to illustrate the limitations of MLLMs to abstract and
generalize.

</details>


### [16] [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/abs/2505.10870)
*Ziyang Huang,Wangtao Sun,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: The paper introduces SIAR and R³ to improve rule retrieval by addressing semantic gaps and enhancing relevance estimation.


<details>
  <summary>Details</summary>
Motivation: Existing rule retrieval methods suffer from low accuracy due to semantic gaps between queries and abstract rule representations, impacting reasoning performance.

Method: Proposes SIAR for rule induction using LLMs and R³ for relevance re-estimation to align retrieved rules with query facts.

Result: Experiments show the methods improve retrieval effectiveness and reasoning performance across various settings.

Conclusion: SIAR and R³ offer a robust solution to enhance rule retrieval and downstream reasoning tasks.

Abstract: This paper systematically addresses the challenges of rule retrieval, a
crucial yet underexplored area. Vanilla retrieval methods using sparse or dense
retrievers to directly search for relevant rules to support downstream
reasoning, often suffer from low accuracy. This is primarily due to a
significant semantic gap between the instantiated facts in the queries and the
abstract representations of the rules. Such misalignment results in suboptimal
retrieval quality, which in turn negatively impacts reasoning performance. To
overcome these challenges, we propose Self-Induction Augmented Retrieval
(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce
potential inferential rules that might offer benefits for reasoning by
abstracting the underlying knowledge and logical structure in queries. These
induced rules are then used for query augmentation to improve retrieval
effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a
method that re-estimates the relevance of retrieved rules by assessing whether
the abstract knowledge they contain can be instantiated to align with the facts
in the queries and the helpfulness for reasoning. Extensive experiments across
various settings demonstrate the effectiveness and versatility of our proposed
methods.

</details>


### [17] [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
*Ada Chen,Yongjiang Wu,Junyuan Zhang,Shu Yang,Jen-tse Huang,Kun Wang,Wenxuan Wang,Shuai Wang*

Main category: cs.CL

TL;DR: The paper systematizes knowledge on safety and security threats of Computer-Using Agents (CUAs), categorizing threats, proposing defensive strategies, and summarizing benchmarks for future research and practical deployment.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-driven CUAs introduces novel safety and security risks, necessitating a structured analysis to address vulnerabilities and guide secure design.

Method: The authors conduct a literature review and distill findings into four objectives: defining CUAs for safety analysis, categorizing threats, proposing defensive strategies, and summarizing benchmarks.

Result: A comprehensive taxonomy of threats and defenses is provided, along with benchmarks for evaluating CUA safety and performance.

Conclusion: The work offers a foundation for future research on unexplored vulnerabilities and practical guidance for secure CUA deployment.

Abstract: Recently, AI-driven interactions with computing devices have advanced from
basic prototype tools to sophisticated, LLM-based systems that emulate
human-like operations in graphical user interfaces. We are now witnessing the
emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously
performing tasks such as navigating desktop applications, web pages, and mobile
apps. However, as these agents grow in capability, they also introduce novel
safety and security risks. Vulnerabilities in LLM-driven reasoning, with the
added complexity of integrating multiple software components and multimodal
inputs, further complicate the security landscape. In this paper, we present a
systematization of knowledge on the safety and security threats of CUAs. We
conduct a comprehensive literature review and distill our findings along four
research objectives: \textit{\textbf{(i)}} define the CUA that suits safety
analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs;
\textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive
strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets,
and evaluation metrics used to assess the safety and performance of CUAs.
Building on these insights, our work provides future researchers with a
structured foundation for exploring unexplored vulnerabilities and offers
practitioners actionable guidance in designing and deploying secure
Computer-Using Agents.

</details>


### [18] [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/abs/2505.10936)
*Jiaxing Zhao,Hongbin Xie,Yuzhen Lei,Xuan Song,Zhuoran Shi,Lianxin Li,Shuangxue Liu,Haoran Zhang*

Main category: cs.CL

TL;DR: Cochain is a collaboration prompting framework combining knowledge and prompts to solve business workflow challenges, outperforming baselines and even GPT-4 in some cases.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single-agent chain-of-thought (collaboration challenges) and multi-agent systems (high token cost and problem dilution) in business workflows.

Method: Constructs an integrated knowledge graph and maintains a prompts tree for efficient retrieval of relevant prompt information.

Result: Outperforms baselines in prompt engineering and multi-agent LLMs; small model with Cochain beats GPT-4 in expert evaluations.

Conclusion: Cochain effectively enhances collaboration in business workflows by combining knowledge and prompts, offering a cost-efficient solution.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in
executing complex reasoning tasks. Chain-of-thought effectively enhances
reasoning capabilities by unlocking the potential of large models, while
multi-agent systems provide more comprehensive solutions by integrating
collective intelligence of multiple agents. However, both approaches face
significant limitations. Single-agent with chain-of-thought, due to the
inherent complexity of designing cross-domain prompts, faces collaboration
challenges. Meanwhile, multi-agent systems consume substantial tokens and
inevitably dilute the primary problem, which is particularly problematic in
business workflow tasks. To address these challenges, we propose Cochain, a
collaboration prompting framework that effectively solves business workflow
collaboration problem by combining knowledge and prompts at a reduced cost.
Specifically, we construct an integrated knowledge graph that incorporates
knowledge from multiple stages. Furthermore, by maintaining and retrieving a
prompts tree, we can obtain prompt information relevant to other stages of the
business workflow. We perform extensive evaluations of Cochain across multiple
datasets, demonstrating that Cochain outperforms all baselines in both prompt
engineering and multi-agent LLMs. Additionally, expert evaluation results
indicate that the use of a small model in combination with Cochain outperforms
GPT-4.

</details>


### [19] [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/abs/2505.10937)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: The paper introduces OmniThought, a large-scale dataset with 2 million chain-of-thought (CoT) processes annotated with Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores to improve large reasoning models (LRMs).


<details>
  <summary>Details</summary>
Motivation: Current CoT datasets lack comprehensive reasoning problems and fail to describe internal CoT properties, hindering LRM advancement.

Method: OmniThought is created using two powerful LRMs as teacher models, with a self-reliant pipeline for curation. The dataset includes RV and CD scores for CoT processes.

Result: Experiments show RV and CD scores improve LRM training. High-performing LRMs with enhanced reasoning abilities are trained and released.

Conclusion: OmniThought and the proposed scores significantly advance LRM development for complex tasks.

Abstract: The emergence of large reasoning models (LRMs) has transformed Natural
Language Processing by excelling in complex tasks such as mathematical
problem-solving and code generation. These models leverage chain-of-thought
(CoT) processes, enabling them to emulate human-like reasoning strategies.
However, the advancement of LRMs is hindered by the lack of comprehensive CoT
datasets. Current resources often fail to provide extensive reasoning problems
with coherent CoT processes distilled from multiple teacher models and do not
account for multifaceted properties describing the internal characteristics of
CoTs. To address these challenges, we introduce OmniThought, a large-scale
dataset featuring 2 million CoT processes generated and validated by two
powerful LRMs as teacher models. Each CoT process in OmniThought is annotated
with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which
describe the appropriateness of CoT verbosity and cognitive difficulty level
for models to comprehend these reasoning processes. We further establish a
self-reliant pipeline to curate this dataset. Extensive experiments using
Qwen2.5 models of various sizes demonstrate the positive impact of our proposed
scores on LRM training effectiveness. Based on the proposed OmniThought
dataset, we further train and release a series of high-performing LRMs,
specifically equipped with stronger reasoning abilities and optimal CoT output
length and difficulty level. Our contributions significantly enhance the
development and training of LRMs for solving complex tasks.

</details>


### [20] [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/abs/2505.10938)
*Yi Su,Yuechi Zhou,Quantong Qiu,Juntao Li,Qingrong Xia,Ping Li,Xinyu Duan,Zhefeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: KV Cache quantization reduces memory overhead in LLMs, but unusual tokens impact accuracy. A method to identify and exclude outliers improves accuracy under 2-bit quantization, cutting memory usage by 6.4x and boosting throughput by 2.3x.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) require significant computational resources, and KV Cache quantization balances memory and accuracy. However, unusual tokens disrupt quantization accuracy.

Method: Identify and exclude outlier tokens during decoding to improve quantization accuracy.

Result: Achieves significant accuracy improvements under 2-bit quantization, reducing memory usage by 6.4x and increasing throughput by 2.3x.

Conclusion: Excluding outlier tokens enhances KV Cache quantization, improving efficiency and accuracy in LLM deployment.

Abstract: The impressive capabilities of Large Language Models (LLMs) come at the cost
of substantial computational resources during deployment. While KV Cache can
significantly reduce recomputation during inference, it also introduces
additional memory overhead. KV Cache quantization presents a promising
solution, striking a good balance between memory usage and accuracy. Previous
research has shown that the Keys are distributed by channel, while the Values
are distributed by token. Consequently, the common practice is to apply
channel-wise quantization to the Keys and token-wise quantization to the
Values. However, our further investigation reveals that a small subset of
unusual tokens exhibit unique characteristics that deviate from this pattern,
which can substantially impact quantization accuracy. To address this, we
develop a simple yet effective method to identify these tokens accurately
during the decoding process and exclude them from quantization as outlier
tokens, significantly improving overall accuracy. Extensive experiments show
that our method achieves significant accuracy improvements under 2-bit
quantization and can deliver a 6.4 times reduction in memory usage and a 2.3
times increase in throughput.

</details>


### [21] [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)
*Mohammadtaha Bagherifard,Sahar Rajabi,Ali Edalat,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: The paper proposes a modular framework (GenKnowSub) to disentangle general knowledge and task-specific adaptations in large language models, improving zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the entanglement of general knowledge and task-specific adaptations in large language models to enhance zero-shot generalization.

Method: Uses a library of task-specific LoRA modules and a general-domain LoRA, applying general knowledge subtraction (GenKnowSub) to refine task-specific modules. Dynamically combines modules for new inputs using the Arrow routing algorithm.

Result: Shows consistent performance gains in monolingual and cross-lingual settings on benchmarks, with improvements also demonstrated on weaker LLMs like Phi-2.

Conclusion: GenKnowSub effectively disentangles knowledge components, enhancing zero-shot generalization without additional training.

Abstract: Large language models often struggle with zero-shot generalization, and
several modular approaches have been proposed to address this challenge. Yet,
we hypothesize that a key limitation remains: the entanglement of general
knowledge and task-specific adaptations. To overcome this, we propose a modular
framework that disentangles these components by constructing a library of
task-specific LoRA modules alongside a general-domain LoRA. By subtracting this
general knowledge component from each task-specific module, we obtain residual
modules that focus more exclusively on task-relevant information, a method we
call general knowledge subtraction (GenKnowSub). Leveraging the refined
task-specific modules and the Arrow routing algorithm
\citep{ostapenko2024towards}, we dynamically select and combine modules for new
inputs without additional training. Our studies on the Phi-3 model and standard
Arrow as baselines reveal that using general knowledge LoRAs derived from
diverse languages, including English, French, and German, yields consistent
performance gains in both monolingual and cross-lingual settings across a wide
set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub
generalizes to weaker LLMs. The complete code and data are available at
https://github.com/saharsamr/Modular-LLM.

</details>


### [22] [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
*Seungyoon Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: SALT is a cross-lingual transfer technique that recycles embeddings from target language PLMs to enhance LLMs, outperforming other methods in performance and convergence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for transferring multilingual LLMs to target languages may limit expressive capacity due to reliance on English-trained source models. SALT aims to leverage target language PLMs for better representation.

Method: SALT uses regression lines based on vocabulary overlap similarity to handle non-overlapping token embeddings, recycling target PLM embeddings to transfer deep representational strengths to LLMs.

Result: SALT outperforms other transfer methods, achieving lower loss and faster convergence, with notable performance in cross-lingual understanding tasks.

Conclusion: SALT effectively enhances LLMs using PLMs, demonstrating scalability and superior performance in cross-lingual transfer.

Abstract: Large Language Models (LLMs) increasingly incorporate multilingual
capabilities, fueling the demand to transfer them into target language-specific
models. However, most approaches, which blend the source model's embedding by
replacing the source vocabulary with the target language-specific vocabulary,
may constrain expressive capacity in the target language since the source model
is predominantly trained on English data. In this paper, we propose Semantic
Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that
recycles embeddings from target language Pre-trained Language Models (PLMs) to
transmit the deep representational strengths of PLM-derived embedding to LLMs.
SALT derives unique regression lines based on the similarity in the overlap of
the source and target vocabularies, to handle each non-overlapping token's
embedding space. Our extensive experiments show that SALT significantly
outperforms other transfer methods and achieves lower loss with accelerating
faster convergence during language adaptation. Notably, SALT obtains remarkable
performance in cross-lingual understanding setups compared to other methods.
Furthermore, we highlight the scalable use of PLMs to enhance the functionality
of contemporary LLMs by conducting experiments with varying architectures.

</details>


### [23] [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/abs/2505.10948)
*Makoto Sato*

Main category: cs.CL

TL;DR: The paper explores how LLMs blend meaning using Conceptual Blending Theory, revealing parallels between artificial and biological cognition through prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind LLMs' behaviors that evoke personality and intelligence, bridging linguistics, neuroscience, and AI research.

Method: Operationalizes Conceptual Blending Theory (CBT) with prompt-based methods, studying Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH).

Result: Uncovers structural parallels and divergences between artificial and biological cognition, highlighting human-AI collaboration as a prototype for cognitive science.

Conclusion: Proposes prompt engineering as a scientific method to probe the deep structure of meaning, advancing cognitive science.

Abstract: Large language models (LLMs), inspired by neuroscience, exhibit behaviors
that often evoke a sense of personality and intelligence-yet the mechanisms
behind these effects remain elusive. Here, we operationalize Conceptual
Blending Theory (CBT) as an experimental framework, using prompt-based methods
to reveal how LLMs blend and compress meaning. By systematically investigating
Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we
uncover structural parallels and divergences between artificial and biological
cognition. Our approach bridges linguistics, neuroscience, and empirical AI
research, demonstrating that human-AI collaboration can serve as a living
prototype for the future of cognitive science. This work proposes prompt
engineering not just as a technical tool, but as a scientific method for
probing the deep structure of meaning itself.

</details>


### [24] [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/abs/2505.10975)
*Xinlu He,Jacob Whitehill*

Main category: cs.CL

TL;DR: A survey of end-to-end (E2E) neural approaches for monaural multi-speaker ASR, covering architectural paradigms, recent improvements, extensions to long-form speech, and benchmark comparisons, with a focus on challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of monaural multi-speaker ASR, such as data scarcity and overlapping speech, by reviewing E2E architectures that reduce error propagation and leverage speech-speaker synergy.

Method: Systematic taxonomy of E2E neural approaches, analyzing SIMO vs. SISO paradigms, architectural/algorithmic improvements, long-form speech extensions, and benchmark evaluations.

Result: Comparative analysis of methods across benchmarks, highlighting trade-offs and advancements in E2E multi-speaker ASR.

Conclusion: Identifies open challenges and future research directions for robust and scalable multi-speaker ASR systems.

Abstract: Monaural multi-speaker automatic speech recognition (ASR) remains challenging
due to data scarcity and the intrinsic difficulty of recognizing and
attributing words to individual speakers, particularly in overlapping speech.
Recent advances have driven the shift from cascade systems to end-to-end (E2E)
architectures, which reduce error propagation and better exploit the synergy
between speech content and speaker identity. Despite rapid progress in E2E
multi-speaker ASR, the field lacks a comprehensive review of recent
developments. This survey provides a systematic taxonomy of E2E neural
approaches for multi-speaker ASR, highlighting recent advances and comparative
analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)
for pre-segmented audio, analyzing their distinct characteristics and
trade-offs; (2) recent architectural and algorithmic improvements based on
these two paradigms; (3) extensions to long-form speech, including segmentation
strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate
and compare methods across standard benchmarks. We conclude with a discussion
of open challenges and future research directions towards building robust and
scalable multi-speaker ASR.

</details>


### [25] [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
*Jingcheng Niu,Subhabrata Dutta,Ahmed Elshabrawy,Harish Tayyar Madabushi,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper investigates in-context learning (ICL) in large-scale Transformer LMs, showing it's not just memorization but also not a fully symbolic algorithm, while clarifying training dynamics and model capabilities.


<details>
  <summary>Details</summary>
Motivation: To resolve the controversy around ICL's mechanism—whether it's memorization or symbolic algorithmic development—and provide insights for model improvement and AI security.

Method: Uses the Pythia scaling suite with interim checkpoints to systematically analyze ICL performance and mechanistic interpretability of the residual stream.

Result: ICL goes beyond memorization but isn't a standalone symbolic algorithm; insights into training dynamics and model capabilities are provided.

Conclusion: Advances understanding of ICL, offering practical insights for model developers and AI security guidelines.

Abstract: Large-scale Transformer language models (LMs) trained solely on next-token
prediction with web-scale data can solve a wide range of tasks after seeing
just a few examples. The mechanism behind this capability, known as in-context
learning (ICL), remains both controversial and poorly understood. Some studies
argue that it is merely the result of memorizing vast amounts of data, while
others contend that it reflects a fundamental, symbolic algorithmic development
in LMs. In this work, we introduce a suite of investigative tasks and a novel
method to systematically investigate ICL by leveraging the full Pythia scaling
suite, including interim checkpoints that capture progressively larger amount
of training data. By carefully exploring ICL performance on downstream tasks
and simultaneously conducting a mechanistic analysis of the residual stream's
subspace, we demonstrate that ICL extends beyond mere "memorization" of the
training corpus, yet does not amount to the implementation of an independent
symbolic algorithm. Our results also clarify several aspects of ICL, including
the influence of training dynamics, model capabilities, and elements of
mechanistic interpretability. Overall, our work advances the understanding of
ICL and its implications, offering model developers insights into potential
improvements and providing AI security practitioners with a basis for more
informed guidelines.

</details>


### [26] [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/abs/2505.11008)
*Ye Kyaw Thu,Thazin Myint Oo*

Main category: cs.CL

TL;DR: Transformer models predict syllable sequences in Abugida languages, with consonant sequences being key for accuracy, while vowel sequences are more challenging.


<details>
  <summary>Details</summary>
Motivation: To improve syllable sequence prediction in Abugida languages for applications like text prediction and spelling correction.

Method: Uses Transformer models to reconstruct syllable sequences from incomplete inputs (consonant/vowel sequences, partial/masked syllables) in six languages.

Result: Consonant sequences yield high BLEU scores; vowel sequences are harder. Model excels in partial/masked syllable tasks.

Conclusion: Advances Abugida language sequence prediction, offering practical insights for text-related applications.

Abstract: This paper explores syllable sequence prediction in Abugida languages using
Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer,
Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We
investigate the reconstruction of complete syllable sequences from various
incomplete input types, including consonant sequences, vowel sequences, partial
syllables (with random character deletions), and masked syllables (with fixed
syllable deletions). Our experiments reveal that consonant sequences play a
critical role in accurate syllable prediction, achieving high BLEU scores,
while vowel sequences present a significantly greater challenge. The model
demonstrates robust performance across tasks, particularly in handling partial
and masked syllable reconstruction, with strong results for tasks involving
consonant information and syllable masking. This study advances the
understanding of sequence prediction for Abugida languages and provides
practical insights for applications such as text prediction, spelling
correction, and data augmentation in these scripts.

</details>


### [27] [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)
*Jiangxu Wu,Cong Wang,TianHuang Su,Jun Yang,Haozhi Lin,Chao Zhang,Ming Peng,Kai Shi,SongPan Yang,BinQing Pan,ZiXian Li,Ni Yang,ZhenYu Yang*

Main category: cs.CL

TL;DR: Proposes Review-Instruct, a framework for generating high-quality multi-turn dialogues using an iterative "Ask-Respond-Review" process with multiple agent roles, improving LLM performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitation of single-turn SFT data in LLMs by enhancing contextual coherence in multi-turn dialogues through diverse and high-quality instruction generation.

Method: Introduces Review-Instruct, involving Candidate, Reviewers, and Chairman roles to iteratively refine instructions. Uses Alpaca dataset and fine-tunes LLaMA2-13B.

Result: Achieves significant improvements: 2.9% gain on MMLU-Pro and 2% on MT-Bench over prior LLaMA2-13B models.

Conclusion: Demonstrates the effectiveness of review-driven, multi-agent frameworks for scalable, high-quality conversational data generation.

Abstract: The effectiveness of large language models (LLMs) in conversational AI is
hindered by their reliance on single-turn supervised fine-tuning (SFT) data,
which limits contextual coherence in multi-turn dialogues. Existing methods for
generating multi-turn dialogue data struggle to ensure both diversity and
quality in instructions. To address this, we propose Review-Instruct, a novel
framework that synthesizes multi-turn conversations through an iterative
"Ask-Respond-Review" process involving three agent roles: a Candidate, multiple
Reviewers, and a Chairman. The framework iteratively refines instructions by
incorporating Reviewer feedback, enhancing dialogue diversity and difficulty.
We construct a multi-turn dataset using the Alpaca dataset and fine-tune the
LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate
significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\%
on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.
Ablation studies confirm the critical role of the Review stage and the use of
multiple Reviewers in boosting instruction diversity and difficulty. Our work
highlights the potential of review-driven, multi-agent frameworks for
generating high-quality conversational data at scale.

</details>


### [28] [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/abs/2505.11026)
*Maria Dziuba,Valentin Malykh*

Main category: cs.CL

TL;DR: StRuCom is a new dataset for Russian code documentation, improving model performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing models for generating structured code comments perform poorly for Russian compared to English.

Method: StRuCom combines human-written Russian GitHub comments with synthetic ones, validated for compliance with multiple programming languages. Qwen2.5-Coder models are fine-tuned on this dataset.

Result: Significant improvements in chrf++ and BERTScore metrics over baseline models.

Conclusion: StRuCom effectively bridges the gap in Russian code documentation quality.

Abstract: Structured code comments in docstring format are essential for code
comprehension and maintenance, but existing machine learning models for their
generation perform poorly for Russian compared to English. To bridge this gap,
we present StRuCom - the first large-scale dataset (153K examples) specifically
designed for Russian code documentation. Unlike machine-translated English
datasets that distort terminology (e.g., technical loanwords vs. literal
translations) and docstring structures, StRuCom combines human-written comments
from Russian GitHub repositories with synthetically generated ones, ensuring
compliance with Python, Java, JavaScript, C#, and Go standards through
automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom
shows statistically significant improvements of chrf++ and BERTScore over
baseline models.

</details>


### [29] [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)
*Xiao Zhang,Huiyuan Lai,Qianru Meng,Johan Bos*

Main category: cs.CL

TL;DR: The paper introduces OntoURL, a benchmark to evaluate LLMs' ability to handle ontologies, revealing strengths in understanding but weaknesses in reasoning and learning.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' underexplored capability in processing structured symbolic knowledge, specifically ontologies.

Method: Proposes a taxonomy of LLMs' ontological capabilities and develops OntoURL, a benchmark with 15 tasks across 40 ontologies to assess understanding, reasoning, and learning.

Result: Experiments with 20 LLMs show proficiency in understanding but significant weaknesses in reasoning and learning tasks.

Conclusion: OntoURL highlights LLMs' limitations in symbolic knowledge processing and serves as a benchmark for future integration of LLMs with formal knowledge.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing tasks, yet their ability to process
structured symbolic knowledge remains underexplored. To address this gap, we
propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the
first comprehensive benchmark designed to systematically evaluate LLMs'
proficiency in handling ontologies -- formal, symbolic representations of
domain knowledge through concepts, relationships, and instances. Based on the
proposed taxonomy, OntoURL systematically assesses three dimensions:
understanding, reasoning, and learning through 15 distinct tasks comprising
58,981 questions derived from 40 ontologies across 8 domains. Experiments with
20 open-source LLMs reveal significant performance differences across models,
tasks, and domains, with current LLMs showing proficiency in understanding
ontological knowledge but substantial weaknesses in reasoning and learning
tasks. These findings highlight fundamental limitations in LLMs' capability to
process symbolic knowledge and establish OntoURL as a critical benchmark for
advancing the integration of LLMs with formal knowledge representations.

</details>


### [30] [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)
*Iwona Christop,Maciej Czajka*

Main category: cs.CL

TL;DR: CAMEO is a multilingual emotional speech dataset collection aimed at enhancing research in emotion recognition and speech-related tasks, ensuring accessibility, reproducibility, and standardized benchmarking.


<details>
  <summary>Details</summary>
Motivation: To facilitate research in speech emotion recognition (SER) by providing a curated, standardized, and easily accessible multilingual dataset.

Method: Describes dataset selection criteria, curation, normalization processes, and evaluates performance of several models.

Result: The dataset is publicly available on Hugging Face, including metadata and a leaderboard for benchmarking.

Conclusion: CAMEO serves as a valuable resource for SER research, promoting reproducibility and standardized evaluation across languages and emotional states.

Abstract: This paper presents CAMEO -- a curated collection of multilingual emotional
speech datasets designed to facilitate research in emotion recognition and
other speech-related tasks. The main objectives were to ensure easy access to
the data, to allow reproducibility of the results, and to provide a
standardized benchmark for evaluating speech emotion recognition (SER) systems
across different emotional states and languages. The paper describes the
dataset selection criteria, the curation and normalization process, and
provides performance results for several models. The collection, along with
metadata, and a leaderboard, is publicly available via the Hugging Face
platform.

</details>


### [31] [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
*Yapei Chang,Yekyung Kim,Michael Krumdick,Amir Zadeh,Chuan Li,Chris Tanner,Mohit Iyyer*

Main category: cs.CL

TL;DR: BLEUBERI, a method using BLEU as a reward function, matches reward models in aligning LLMs with human preferences, offering a cheaper alternative.


<details>
  <summary>Details</summary>
Motivation: High costs of training reward models and the availability of synthetic datasets prompt exploration of simpler, reference-based metrics like BLEU.

Method: BLEUBERI identifies challenging instructions and applies Group Relative Policy Optimization (GRPO) using BLEU as the reward function.

Result: BLEUBERI-trained models perform competitively with reward model-guided RL across benchmarks and generate more factually grounded outputs.

Conclusion: String-matching metrics like BLEU are effective, low-cost alternatives to reward models for LLM alignment when high-quality reference outputs are available.

Abstract: Reward models are central to aligning LLMs with human preferences, but they
are costly to train, requiring large-scale human-labeled preference data and
powerful pretrained LLM backbones. Meanwhile, the increasing availability of
high-quality synthetic instruction-following datasets raises the question: can
simpler, reference-based metrics serve as viable alternatives to reward models
during RL-based alignment? In this paper, we show first that BLEU, a basic
string-matching metric, surprisingly matches strong reward models in agreement
with human preferences on general instruction-following datasets. Based on this
insight, we develop BLEUBERI, a method that first identifies challenging
instructions and then applies Group Relative Policy Optimization (GRPO) using
BLEU directly as the reward function. We demonstrate that BLEUBERI-trained
models are competitive with models trained via reward model-guided RL across
four challenging instruction-following benchmarks and three different base
language models. A human evaluation further supports that the quality of
BLEUBERI model outputs is on par with those from reward model-aligned models.
Moreover, BLEUBERI models generate outputs that are more factually grounded
than competing methods. Overall, we show that given access to high-quality
reference outputs (easily obtained via existing instruction-following datasets
or synthetic data generation), string matching-based metrics are cheap yet
effective proxies for reward models during alignment. We release our code and
data at https://github.com/lilakk/BLEUBERI.

</details>


### [32] [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/abs/2505.11095)
*Lekang Jiang,Pascal A Scherz,Stephan Goetz*

Main category: cs.CL

TL;DR: The paper introduces Patent-CE, a benchmark for evaluating patent claims, and PatClaimEval, a novel evaluation method, to address inconsistencies between automated metrics and human assessments in patent claim generation.


<details>
  <summary>Details</summary>
Motivation: The complexity and cost of drafting patent claims create barriers for small enterprises, prompting research into automating the process using LLMs. Existing evaluation methods lack alignment with human expert assessments.

Method: The authors developed Patent-CE, a benchmark with expert-annotated evaluations, and PatClaimEval, a multi-dimensional evaluation method for patent claims.

Result: PatClaimEval showed the highest correlation with human expert evaluations across all criteria compared to other metrics.

Conclusion: This work lays the foundation for more accurate evaluation of automated patent claim generation systems.

Abstract: Patent claims define the scope of protection and establish the legal
boundaries of an invention. Drafting these claims is a complex and
time-consuming process that usually requires the expertise of skilled patent
attorneys, which can form a large access barrier for many small enterprises. To
solve these challenges, researchers have investigated the use of large language
models (LLMs) for automating patent claim generation. However, existing studies
highlight inconsistencies between automated evaluation metrics and human expert
assessments. To bridge this gap, we introduce Patent-CE, the first
comprehensive benchmark for evaluating patent claims. Patent-CE includes
comparative claim evaluations annotated by patent experts, focusing on five key
criteria: feature completeness, conceptual clarity, terminology consistency,
logical linkage, and overall quality. Additionally, we propose PatClaimEval, a
novel multi-dimensional evaluation method specifically designed for patent
claims. Our experiments demonstrate that PatClaimEval achieves the highest
correlation with human expert evaluations across all assessment criteria among
all tested metrics. This research provides the groundwork for more accurate
evaluations of automated patent claim generation systems.

</details>


### [33] [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140)
*Mike Zhang,Johannes Bjerva,Russa Biswas*

Main category: cs.CL

TL;DR: Longer reasoning chains in LLMs improve factual accuracy in open-domain QA tasks, especially with added test-time compute and knowledge graph paths.


<details>
  <summary>Details</summary>
Motivation: To determine if longer reasoning chains enhance factual accuracy beyond mathematical contexts in LLMs.

Method: Distilled reasoning traces from advanced models, fine-tuned various models, and enriched traces with knowledge graph paths. Evaluated on six datasets with 168 experimental runs.

Result: Smaller reasoning models improved factual accuracy, and test-time compute boosted accuracy by 2-8%.

Conclusion: Test-time scaling and enriched reasoning traces enhance LLM performance in open-domain QA.

Abstract: Recent studies on large language model (LLM) reasoning capabilities have
demonstrated promising improvements in model performance by leveraging a
lengthy thinking process and additional computational resources during
inference, primarily in tasks involving mathematical reasoning (Muennighoff et
al., 2025). However, it remains uncertain if longer reasoning chains inherently
enhance factual accuracy, particularly beyond mathematical contexts. In this
work, we thoroughly examine LLM reasoning within complex open-domain
question-answering (QA) scenarios. We initially distill reasoning traces from
advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then
fine-tune a variety of models ranging from smaller, instruction-tuned variants
to larger architectures based on Qwen2.5. To enrich reasoning traces, we
introduce factual information from knowledge graphs in the form of paths into
our reasoning traces. Our experimental setup includes four baseline approaches
and six different instruction-tuned models evaluated across a benchmark of six
datasets, encompassing over 22.6K questions. Overall, we carry out 168
experimental runs and analyze approximately 1.7 million reasoning traces. Our
findings indicate that, within a single run, smaller reasoning models achieve
noticeable improvements in factual accuracy compared to their original
instruction-tuned counterparts. Moreover, our analysis demonstrates that adding
test-time compute and token budgets factual accuracy consistently improves by
2-8%, further confirming the effectiveness of test-time scaling for enhancing
performance and consequently improving reasoning accuracy in open-domain QA
tasks. We release all the experimental artifacts for further research.

</details>


### [34] [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/abs/2505.11166)
*Huashan Sun,Shengyi Liao,Yansen Han,Yu Bai,Yang Gao,Cheng Fu,Weizhou Shen,Fanqi Wan,Ming Yan,Ji Zhang,Fei Huang*

Main category: cs.CL

TL;DR: SoLoPO is a framework for improving long-context alignment in LLMs by decoupling preference optimization into short-context PO and short-to-long reward alignment, enhancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in LLMs' long-context utilization due to data quality, training inefficiencies, and lack of optimization objectives.

Method: Proposes SoLoPO, combining short-context PO and SoLo-RA to align rewards for short and long contexts with identical task-relevant information.

Result: Improves length and domain generalization, computational and memory efficiency across benchmarks.

Conclusion: SoLoPO effectively enhances LLMs' long-context handling and training efficiency.

Abstract: Despite advances in pretraining with extended context lengths, large language
models (LLMs) still face challenges in effectively utilizing real-world
long-context information, primarily due to insufficient long-context alignment
caused by data quality issues, training inefficiencies, and the lack of
well-designed optimization objectives. To address these limitations, we propose
a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng
$\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling
long-context preference optimization (PO) into two components: short-context PO
and short-to-long reward alignment (SoLo-RA), supported by both theoretical and
empirical evidence. Specifically, short-context PO leverages preference pairs
sampled from short contexts to enhance the model's contextual knowledge
utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score
consistency utilization for the responses when conditioned on both short and
long contexts that contain identical task-relevant information. This
facilitates transferring the model's ability to handle short contexts into
long-context scenarios. SoLoPO is compatible with mainstream preference
optimization algorithms, while substantially improving the efficiency of data
construction and training processes. Experimental results show that SoLoPO
enhances all these algorithms with respect to stronger length and domain
generalization abilities across various long-context benchmarks, while
achieving notable improvements in both computational and memory efficiency.

</details>


### [35] [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/abs/2505.11177)
*Hrishit Madhavi,Jacob Cherian,Yuvraj Khamkar,Dhananjay Bhagat*

Main category: cs.CL

TL;DR: An end-to-end multilingual system for extracting and processing text from image-based documents using OCR, translation, summarization, and additional NLP tasks, accessible via a Gradio interface.


<details>
  <summary>Details</summary>
Motivation: To bridge language gaps and improve access to information in image-based documents across diverse linguistic contexts.

Method: Uses Tesseract for OCR, Gemini for translation/summarization, and TensorFlow/Transformers/Regex for sentiment analysis, topic classification, and date extraction.

Result: Demonstrates a practical application integrating libraries, models, and APIs for multilingual document processing.

Conclusion: The system effectively enhances information accessibility in multilingual image-based documents.

Abstract: This paper presents an end-to-end suite for multilingual information
extraction and processing from image-based documents. The system uses Optical
Character Recognition (Tesseract) to extract text in languages such as English,
Hindi, and Tamil, and then a pipeline involving large language model APIs
(Gemini) for cross-lingual translation, abstractive summarization, and
re-translation into a target language. Additional modules add sentiment
analysis (TensorFlow), topic classification (Transformers), and date extraction
(Regex) for better document comprehension. Made available in an accessible
Gradio interface, the current research shows a real-world application of
libraries, models, and APIs to close the language gap and enhance access to
information in image media across different linguistic environments

</details>


### [36] [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/abs/2505.11199)
*Chris Köcher,Alexander Kozachinskiy,Anthony Widjaja Lin,Marco Sälzer,Georg Zetzsche*

Main category: cs.CL

TL;DR: NoPE-transformers with average hard attention can express complex counting languages (semi-algebraic sets), surpassing models like counter machines but failing at simple tasks like PARITY. Their analysis is undecidable.


<details>
  <summary>Details</summary>
Motivation: To explore the expressiveness of NoPE-transformers with average hard attention, challenging the assumption that PEs are essential for transformers.

Method: Characterizes languages expressible by NoPE-AHATs as semi-algebraic sets, comparing them to other models and proving undecidability of analysis.

Result: NoPE-AHATs can solve Diophantine equations (undecidable problems) but not PARITY. Their analysis is undecidable.

Conclusion: NoPE-transformers with average hard attention are surprisingly expressive but limited in certain tasks, with undecidable analysis.

Abstract: Positional Encodings (PEs) seem to be indispensable for ensuring
expressiveness of transformers; without them attention transformers reduce to a
bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard
attention mechanisms were very recently shown to only be able to express
regular languages, i.e., with limited counting ability. This paper shows that,
with average hard attention mechanisms, NoPE-transformers are still
surprisingly expressive: they can express counting languages corresponding to
nonnegative integer solutions to multivariate polynomial equations (i.e.
Diophantine equations), reasoning about which is well-known to be undecidable.
In fact, we provide a precise characterization of languages expressible by
Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond
precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of
sets of nonnegative integer solutions to systems of multivariate polynomial
inequations. We obtain several interesting consequences of our
characterization. Firstly, NoPE-transformers can express counting properties
that are far more complex than established models like simplified counter
machines and Petri nets, but cannot express a very simple counting property of
PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,
e.g., whether a given NoPE transformer classifies all input strings in one
class. To complement our results, we exhibit a counting language that is not
expressible by average hard attention transformers even with arbitrary PEs but
is expressible in the circuit complexity class TC$^0$, answering an open
problem.

</details>


### [37] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO (History-Aware Policy Optimization) improves LLM efficiency by tracking historical response lengths and incentivizing concise, correct solutions, reducing output length by 33-59% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Prior methods for efficient test-time scaling in LLMs lack historical context, limiting their ability to progressively optimize response conciseness.

Method: HAPO tracks historical response lengths and uses a novel length reward function to encourage concise, correct solutions while avoiding over-penalizing shorter incorrect responses.

Result: HAPO reduces response lengths by 33-59% with only 2-5% accuracy drops across math benchmarks.

Conclusion: HAPO effectively balances correctness and efficiency, enhancing LLMs' concise reasoning abilities.

Abstract: While scaling the length of responses at test-time has been shown to markedly
improve the reasoning abilities and performance of large language models
(LLMs), it often results in verbose outputs and increases inference cost. Prior
approaches for efficient test-time scaling, typically using universal budget
constraints or query-level length optimization, do not leverage historical
information from previous encounters with the same problem during training. We
hypothesize that this limits their ability to progressively make solutions more
concise over time. To address this, we present History-Aware Policy
Optimization (HAPO), which keeps track of a history state (e.g., the minimum
length over previously generated correct responses) for each problem. HAPO
employs a novel length reward function based on this history state to
incentivize the discovery of correct solutions that are more concise than those
previously found. Crucially, this reward structure avoids overly penalizing
shorter incorrect responses with the goal of facilitating exploration towards
more efficient solutions. By combining this length reward with a correctness
reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to
train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and
Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span
various difficulty levels. Experiment results demonstrate that HAPO effectively
induces LLMs' concise reasoning abilities, producing length reductions of
33-59% with accuracy drops of only 2-5%.

</details>


### [38] [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/abs/2505.11271)
*Camille Couturier,Spyros Mastorakis,Haiying Shen,Saravan Rajmohan,Victor Rühle*

Main category: cs.CL

TL;DR: A semantic caching method for LLMs reduces redundant computations by 50-60% while maintaining accuracy, improving efficiency in QA workflows.


<details>
  <summary>Details</summary>
Motivation: Addressing high computational overhead, memory usage, and network bandwidth in distributed systems when processing lengthy contexts for LLM-based QA.

Method: Introduces a semantic caching approach to store and reuse intermediate contextual summaries for similar queries.

Result: Demonstrates a 50-60% reduction in redundant computations with comparable answer accuracy on datasets like NaturalQuestions and TriviaQA.

Conclusion: The method balances computational cost and response quality, making it suitable for real-time AI assistants.

Abstract: Large Language Models (LLMs) are increasingly deployed across edge and cloud
platforms for real-time question-answering and retrieval-augmented generation.
However, processing lengthy contexts in distributed systems incurs high
computational overhead, memory usage, and network bandwidth. This paper
introduces a novel semantic caching approach for storing and reusing
intermediate contextual summaries, enabling efficient information reuse across
similar queries in LLM-based QA workflows. Our method reduces redundant
computations by up to 50-60% while maintaining answer accuracy comparable to
full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a
synthetic ArXiv dataset. This approach balances computational cost and response
quality, critical for real-time AI assistants.

</details>


### [39] [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
*Yaorui Shi,Shihan Li,Chang Wu,Zhiyuan Liu,Junfeng Fang,Hengxing Cai,An Zhang,Xiang Wang*

Main category: cs.CL

TL;DR: AutoRefine is a reinforcement learning framework that enhances LLMs by refining retrieved knowledge iteratively, improving reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented reasoning methods often retrieve noisy or irrelevant information, limiting accurate reasoning.

Method: AutoRefine uses a "search-and-refine-during-think" paradigm with iterative knowledge refinement and tailored retrieval-specific rewards.

Result: AutoRefine outperforms existing methods, especially in multi-hop reasoning, by issuing higher-quality searches and synthesizing evidence effectively.

Conclusion: AutoRefine significantly improves reasoning accuracy in LLMs by refining retrieved knowledge iteratively.

Abstract: Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.

</details>


### [40] [Temporal fine-tuning for early risk detection](https://arxiv.org/abs/2505.11280)
*Horacio Thompson,Esaú Villatoro-Tello,Manuel Montes-y-Gómez,Marcelo Errecalde*

Main category: cs.CL

TL;DR: The paper proposes temporal fine-tuning for Early Risk Detection (ERD) on the web, optimizing transformer-based models by incorporating time into learning, achieving competitive results in depression and eating disorder tasks.


<details>
  <summary>Details</summary>
Motivation: ERD aims to quickly and accurately detect users facing social and health issues, requiring precision and minimal delay. Standard metrics may not suffice, necessitating new approaches.

Method: Temporal fine-tuning integrates time into transformer-based models, allowing analysis of complete user post histories and evaluation with temporal metrics.

Result: Competitive results in depression and eating disorder tasks for Spanish, outperforming some MentalRiskES 2023 models. Temporal fine-tuning optimized decisions by considering context and time.

Conclusion: Temporal fine-tuning effectively addresses ERD by combining precision and speed as a single objective, leveraging transformers' power.

Abstract: Early Risk Detection (ERD) on the Web aims to identify promptly users facing
social and health issues. Users are analyzed post-by-post, and it is necessary
to guarantee correct and quick answers, which is particularly challenging in
critical scenarios. ERD involves optimizing classification precision and
minimizing detection delay. Standard classification metrics may not suffice,
resorting to specific metrics such as ERDE(theta) that explicitly consider
precision and delay. The current research focuses on applying a multi-objective
approach, prioritizing classification performance and establishing a separate
criterion for decision time. In this work, we propose a completely different
strategy, temporal fine-tuning, which allows tuning transformer-based models by
explicitly incorporating time within the learning process. Our method allows us
to analyze complete user post histories, tune models considering different
contexts, and evaluate training performance using temporal metrics. We
evaluated our proposal in the depression and eating disorders tasks for the
Spanish language, achieving competitive results compared to the best models of
MentalRiskES 2023. We found that temporal fine-tuning optimized decisions
considering context and time progress. In this way, by properly taking
advantage of the power of transformers, it is possible to address ERD by
combining precision and speed as a single objective.

</details>


### [41] [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
*Gal Astrach,Yuval Pinter*

Main category: cs.CL

TL;DR: The paper investigates how well transformers capture phonological features in morphological tasks, finding local features are better represented in phoneme embeddings, while long-distance dependencies are better in the encoder.


<details>
  <summary>Details</summary>
Motivation: Transformers excel in morphological inflection but struggle with generalization across languages and rules, possibly due to limited capture of phonological and subphonemic phenomena.

Method: A language-agnostic probing method is introduced to study phonological feature encoding in transformers trained on phonemes, tested across seven diverse languages.

Result: Local phonological features (e.g., Turkish devoicing) are well-captured in phoneme embeddings, while long-distance dependencies (e.g., vowel harmony) are better represented in the encoder.

Conclusion: The findings suggest empirical strategies for training morphological models, emphasizing subphonemic feature acquisition.

Abstract: Transformers have achieved state-of-the-art performance in morphological
inflection tasks, yet their ability to generalize across languages and
morphological rules remains limited. One possible explanation for this behavior
can be the degree to which these models are able to capture implicit phenomena
at the phonological and subphonemic levels. We introduce a language-agnostic
probing method to investigate phonological feature encoding in transformers
trained directly on phonemes, and perform it across seven morphologically
diverse languages. We show that phonological features which are local, such as
final-obstruent devoicing in Turkish, are captured well in phoneme embeddings,
whereas long-distance dependencies like vowel harmony are better represented in
the transformer's encoder. Finally, we discuss how these findings inform
empirical strategies for training morphological models, particularly regarding
the role of subphonemic feature acquisition.

</details>


### [42] [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/abs/2505.11336)
*Nuo Chen,Andre Lin HuiKai,Jiaying Wu,Junyi Hou,Zining Zhang,Qian Wang,Xidong Wang,Bingsheng He*

Main category: cs.CL

TL;DR: The paper proposes XtraGPT, a human-AI collaboration framework for academic paper revision, outperforming existing systems in scientific writing support.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs fall short in supporting high-quality scientific writing, lacking conceptual coherence and iterative revision capabilities.

Method: Introduces a dataset of 7,040 annotated research papers and develops XtraGPT, a suite of open-source LLMs for context-aware writing assistance.

Result: XtraGPT outperforms same-scale baselines and approaches proprietary system quality, validated by automated and human evaluations.

Conclusion: The framework effectively improves scientific drafts, addressing gaps in current LLM capabilities for academic writing.

Abstract: Despite the growing adoption of large language models (LLMs) in academic
workflows, their capabilities remain limited when it comes to supporting
high-quality scientific writing. Most existing systems are designed for
general-purpose scientific text generation and fail to meet the sophisticated
demands of research communication beyond surface-level polishing, such as
conceptual coherence across sections. Furthermore, academic writing is
inherently iterative and revision-driven, a process not well supported by
direct prompting-based paradigms. To address these scenarios, we propose a
human-AI collaboration framework for academic paper revision. We first
introduce a comprehensive dataset of 7,040 research papers from top-tier venues
annotated with over 140,000 instruction-response pairs that reflect realistic,
section-level scientific revisions. Building on the dataset, we develop
XtraGPT, the first suite of open-source LLMs, designed to provide
context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B
parameters. Extensive experiments validate that XtraGPT significantly
outperforms same-scale baselines and approaches the quality of proprietary
systems. Both automated preference assessments and human evaluations confirm
the effectiveness of our models in improving scientific drafts.

</details>


### [43] [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)
*Banca Calvo Figueras,Rodrigo Agerri*

Main category: cs.CL

TL;DR: The paper introduces a large-scale dataset and evaluation methods for Critical Questions Generation (CQs-Gen), benchmarking 11 LLMs and highlighting the task's difficulty.


<details>
  <summary>Details</summary>
Motivation: To advance CQs-Gen by addressing the lack of datasets and evaluation standards, fostering critical thinking in both AI and humans.

Method: Constructed a manually-annotated dataset and evaluated automatic methods, identifying LLM-based reference techniques as most aligned with human judgment.

Result: Zero-shot evaluation of 11 LLMs set a baseline, revealing the task's complexity. Data, code, and a leaderboard were released.

Conclusion: The work supports future research in CQs-Gen, emphasizing its potential for automated reasoning and human critical thinking.

Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical
thinking by enabling systems to generate questions that expose assumptions and
challenge the reasoning in arguments. Despite growing interest in this area,
progress has been hindered by the lack of suitable datasets and automatic
evaluation standards. This work presents a comprehensive approach to support
the development and benchmarking of systems for this task. We construct the
first large-scale manually-annotated dataset. We also investigate automatic
evaluation methods and identify a reference-based technique using large
language models (LLMs) as the strategy that best correlates with human
judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline
while showcasing the difficulty of the task. Data, code, and a public
leaderboard are provided to encourage further research not only in terms of
model performance, but also to explore the practical benefits of CQs-Gen for
both automated reasoning and human critical thinking.

</details>


### [44] [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/abs/2505.11352)
*Rao Ma,Tongzhou Chen,Kartik Audhkhasi,Bhuvana Ramabhadran*

Main category: cs.CL

TL;DR: LegoSLM bridges speech encoders and LLMs using ASR posterior matrices, improving ASR and speech translation performance with modularity and domain adaptation control.


<details>
  <summary>Details</summary>
Motivation: Existing methods for combining speech encoders and LLMs are suboptimal or inflexible, prompting the need for a better integration approach.

Method: LegoSLM uses CTC posteriors over the LLM vocabulary to reconstruct pseudo-audio embeddings, concatenated with text embeddings in the LLM input space.

Result: Achieves 49% WERR improvement over baseline on ASR tasks and shows modularity and zero-shot adaptability.

Conclusion: LegoSLM effectively combines speech encoders and LLMs, offering performance gains, flexibility, and domain adaptation control.

Abstract: Recently, large-scale pre-trained speech encoders and Large Language Models
(LLMs) have been released, which show state-of-the-art performance on a range
of spoken language processing tasks including Automatic Speech Recognition
(ASR). To effectively combine both models for better performance, continuous
speech prompts, and ASR error correction have been adopted. However, these
methods are prone to suboptimal performance or are inflexible. In this paper,
we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using
the ASR posterior matrices. The speech encoder is trained to generate
Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,
which are used to reconstruct pseudo-audio embeddings by computing a weighted
sum of the LLM input embeddings. These embeddings are concatenated with text
embeddings in the LLM input space. Using the well-performing USM and Gemma
models as an example, we demonstrate that our proposed LegoSLM method yields
good performance on both ASR and speech translation tasks. By connecting USM
with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline
on 8 MLS testsets. The trained model also exhibits modularity in a range of
settings -- after fine-tuning the Gemma model weights, the speech encoder can
be switched and combined with the LLM in a zero-shot fashion. Additionally, we
propose to control the decode-time influence of the USM and LLM using a softmax
temperature, which shows effectiveness in domain adaptation.

</details>


### [45] [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
*Lingxiao Diao,Xinyue Xu,Wanxuan Sun,Cheng Yang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: GuideBench is introduced to evaluate LLMs' ability to follow domain-oriented guidelines, focusing on rule adherence, robustness to updates, and human alignment. Results show significant room for improvement.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as domain-oriented agents, but existing benchmarks lack evaluation of their ability to follow domain-specific guidelines, which often conflict with commonsense knowledge.

Method: GuideBench is developed to assess LLMs on three aspects: adherence to diverse rules, robustness to rule updates, and alignment with human preferences.

Result: Experiments reveal substantial opportunities for enhancing LLMs' guideline-following capabilities.

Conclusion: GuideBench fills a critical gap in evaluating LLMs for domain-oriented tasks, highlighting areas for future improvement.

Abstract: Large language models (LLMs) have been widely deployed as autonomous agents
capable of following user instructions and making decisions in real-world
applications. Previous studies have made notable progress in benchmarking the
instruction following capabilities of LLMs in general domains, with a primary
focus on their inherent commonsense knowledge. Recently, LLMs have been
increasingly deployed as domain-oriented agents, which rely on domain-oriented
guidelines that may conflict with their commonsense knowledge. These guidelines
exhibit two key characteristics: they consist of a wide range of
domain-oriented rules and are subject to frequent updates. Despite these
challenges, the absence of comprehensive benchmarks for evaluating the
domain-oriented guideline following capabilities of LLMs presents a significant
obstacle to their effective assessment and further development. In this paper,
we introduce GuideBench, a comprehensive benchmark designed to evaluate
guideline following performance of LLMs. GuideBench evaluates LLMs on three
critical aspects: (i) adherence to diverse rules, (ii) robustness to rule
updates, and (iii) alignment with human preferences. Experimental results on a
range of LLMs indicate substantial opportunities for improving their ability to
follow domain-oriented guidelines.

</details>


### [46] [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/abs/2505.11379)
*Alicia González Martínez*

Main category: cs.CL

TL;DR: The paper explores the systematicity of tajwid rules in the Cairo Quran using a digital edition and a Python module to manipulate orthographic layers, aiming to create a framework for comparing Quranic manuscripts.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the phonetic and prosodic processes of the Quran by analyzing tajwid rules in the Cairo Quran, leveraging its completeness for computational alignment of manuscripts.

Method: A Python module was developed to add or remove the orthographic layer of tajwid in Contemporary Quranic Orthography (CQO), using a fully encoded digital edition of the Quranic text.

Result: The rules of tajwid in the Cairo Quran provide a precise witness for phonetic and prosodic studies, enabling alignment and comparison of Quranic manuscripts.

Conclusion: The framework can serve as a powerful tool for studying diacritic notation systems across connected manuscripts, enhancing understanding of Quranic orthography.

Abstract: Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic
notation that can be traced back to the early stages of Islam, when the Quran
was mainly oral in nature and the first written renderings of it served as
memory aids for this oral tradition. The early systems of diacritical marks
created on top of the Quranic Consonantal Text (QCT) motivated the creation and
further development of a fine-grained system of phonetic notation that
represented tajwid-the rules of recitation. We explored the systematicity of
the rules of tajwid, as they are encountered in the Cairo Quran, using a fully
and accurately encoded digital edition of the Quranic text. For this purpose,
we developed a python module that can remove or add the orthographic layer of
tajwid from a Quranic text in CQO. The interesting characteristic of these two
sets of rules is that they address the complete Quranic text of the Cairo
Quran, so they can be used as precise witnesses to study its phonetic and
prosodic processes. From a computational point of view, the text of the Cairo
Quran can be used as a linchpin to align and compare Quranic manuscripts, due
to its richness and completeness. This will let us create a very powerful
framework to work with the Arabic script, not just within an isolated text, but
automatically exploring a specific textual phenomenon in other connected
manuscripts. Having all the texts mapped among each other can serve as a
powerful tool to study the nature of the notation systems of diacritics added
to the consonantal skeleton.

</details>


### [47] [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/abs/2505.11413)
*Sijia Chen,Xiaomin Li,Mengxue Zhang,Eric Hanchen Jiang,Qingcheng Zeng,Chen-Hsiang Yu*

Main category: cs.CL

TL;DR: CARES is a benchmark for evaluating LLM safety in healthcare, addressing gaps in clinical specificity, harm levels, and adversarial attacks. It includes diverse prompts and a refined evaluation protocol, revealing vulnerabilities in current LLMs and proposing a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clinical specificity and graded harmfulness in existing benchmarks for LLM safety in medical contexts, and to cover jailbreak-style attacks.

Method: Introduces CARES, a benchmark with 18,000+ prompts across eight medical safety principles, four harm levels, and four prompting styles. Uses a three-way response evaluation and Safety Score metric.

Result: Many state-of-the-art LLMs are vulnerable to subtle adversarial attacks and over-refuse safe queries. A lightweight classifier is proposed for mitigation.

Conclusion: CARES offers a rigorous framework to test and improve medical LLM safety under adversarial conditions, with a proposed strategy to enhance model robustness.

Abstract: Large language models (LLMs) are increasingly deployed in medical contexts,
raising critical concerns about safety, alignment, and susceptibility to
adversarial manipulation. While prior benchmarks assess model refusal
capabilities for harmful prompts, they often lack clinical specificity, graded
harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES
(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for
evaluating LLM safety in healthcare. CARES includes over 18,000 prompts
spanning eight medical safety principles, four harm levels, and four prompting
styles: direct, indirect, obfuscated, and role-play, to simulate both malicious
and benign use cases. We propose a three-way response evaluation protocol
(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess
model behavior. Our analysis reveals that many state-of-the-art LLMs remain
vulnerable to jailbreaks that subtly rephrase harmful prompts, while also
over-refusing safe but atypically phrased queries. Finally, we propose a
mitigation strategy using a lightweight classifier to detect jailbreak attempts
and steer models toward safer behavior via reminder-based conditioning. CARES
provides a rigorous framework for testing and improving medical LLM safety
under adversarial and ambiguous conditions.

</details>


### [48] [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/abs/2505.11421)
*Phan Tran Minh Dat,Vo Hoang Nhat Khang,Quan Thanh Tho*

Main category: cs.CL

TL;DR: The paper proposes a transfer learning approach using a sequence-to-sequence model to address Bahnaric-Vietnamese translation challenges, leveraging limited bilingual resources and data augmentation.


<details>
  <summary>Details</summary>
Motivation: To bridge cultural gaps between Bahnaric and Vietnamese ethnic groups in Vietnam by enabling effective translation, despite scarce Bahnaric resources.

Method: Uses a pre-trained Vietnamese sequence-to-sequence model, fine-tuned with limited bilingual data, and employs data augmentation and heuristic methods for precision.

Result: The approach proves highly effective for Bahnaric-Vietnamese translation, aiding language preservation and mutual understanding.

Conclusion: The method successfully addresses resource imbalance and optimizes training, contributing to cultural and linguistic bridging.

Abstract: This work explores the journey towards achieving Bahnaric-Vietnamese
translation for the sake of culturally bridging the two ethnic groups in
Vietnam. However, translating from Bahnaric to Vietnamese also encounters some
difficulties. The most prominent challenge is the lack of available original
Bahnaric resources source language, including vocabulary, grammar, dialogue
patterns and bilingual corpus, which hinders the data collection process for
training. To address this, we leverage a transfer learning approach using
sequence-to-sequence pre-training language model. First of all, we leverage a
pre-trained Vietnamese language model to capture the characteristics of this
language. Especially, to further serve the purpose of machine translation, we
aim for a sequence-to-sequence model, not encoder-only like BERT or
decoder-only like GPT. Taking advantage of significant similarity between the
two languages, we continue training the model with the currently limited
bilingual resources of Vietnamese-Bahnaric text to perform the transfer
learning from language model to machine translation. Thus, this approach can
help to handle the problem of imbalanced resources between two languages, while
also optimizing the training and computational processes. Additionally, we also
enhanced the datasets using data augmentation to generate additional resources
and defined some heuristic methods to help the translation more precise. Our
approach has been validated to be highly effective for the Bahnaric-Vietnamese
translation model, contributing to the expansion and preservation of languages,
and facilitating better mutual understanding between the two ethnic people.

</details>


### [49] [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)
*Xiaomin Li,Zhou Yu,Zhiwei Zhang,Xupeng Chen,Ziji Zhang,Yingying Zhuang,Narayanan Sadagopan,Anurag Beniwal*

Main category: cs.CL

TL;DR: Explicit chain-of-thought (CoT) reasoning in large language models can degrade instruction-following accuracy, but selective reasoning strategies can mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To uncover and address the overlooked phenomenon where CoT reasoning harms instruction-following performance in large language models.

Method: Evaluated 15 models on IFEval and ComplexBench, analyzed attention patterns, and proposed four mitigation strategies (in-context learning, self-reflection, self-selective reasoning, classifier-selective reasoning).

Result: CoT reasoning often diverts attention from instruction-relevant tokens, but selective reasoning, especially classifier-selective reasoning, recovers lost performance.

Conclusion: Selective reasoning strategies effectively mitigate reasoning-induced failures in instruction-following, offering practical solutions.

Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained
for reasoning or prompted via chain-of-thought (CoT), have achieved
state-of-the-art performance on many complex reasoning tasks. However, we
uncover a surprising and previously overlooked phenomenon: explicit CoT
reasoning can significantly degrade instruction-following accuracy. Evaluating
15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)
and ComplexBench (with complex, compositional constraints), we consistently
observe performance drops when CoT prompting is applied. Through large-scale
case studies and an attention-based analysis, we identify common patterns where
reasoning either helps (e.g., with formatting or lexical precision) or hurts
(e.g., by neglecting simple constraints or introducing unnecessary content). We
propose a metric, constraint attention, to quantify model focus during
generation and show that CoT reasoning often diverts attention away from
instruction-relevant tokens. To mitigate these effects, we introduce and
evaluate four strategies: in-context learning, self-reflection, self-selective
reasoning, and classifier-selective reasoning. Our results demonstrate that
selective reasoning strategies, particularly classifier-selective reasoning,
can substantially recover lost performance. To our knowledge, this is the first
work to systematically expose reasoning-induced failures in
instruction-following and offer practical mitigation strategies.

</details>


### [50] [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
*Chenkai Zhang,Yiming Lei,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: The paper introduces GODBench, a benchmark for evaluating MLLMs' ability to generate creative video comments, and proposes Ripple of Thought (RoT) to enhance creativity.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs and benchmarks struggle with creative tasks like humor and satire in video comments, limiting exploration of comprehensive creativity.

Method: The authors develop GODBench for multimodal evaluation and propose RoT, a multi-step reasoning framework inspired by wave propagation.

Result: Experiments show MLLMs and CoT methods underperform in creative tasks, while RoT improves creative composing.

Conclusion: RoT offers a promising approach to enhance MLLM creativity, with GODBench serving as a valuable benchmark for future research.

Abstract: Video Comment Art enhances user engagement by providing creative content that
conveys humor, satire, or emotional resonance, requiring a nuanced and
comprehensive grasp of cultural and contextual subtleties. Although Multimodal
Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated
strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they
still struggle to generate creative expressions such as resonant jokes and
insightful satire. Moreover, existing benchmarks are constrained by their
limited modalities and insufficient categories, hindering the exploration of
comprehensive creativity in video-based Comment Art creation. To address these
limitations, we introduce GODBench, a novel benchmark that integrates video and
text modalities to systematically evaluate MLLMs' abilities to compose Comment
Art. Furthermore, inspired by the propagation patterns of waves in physics, we
propose Ripple of Thought (RoT), a multi-step reasoning framework designed to
enhance the creativity of MLLMs. Extensive experiments reveal that existing
MLLMs and CoT methods still face significant challenges in understanding and
generating creative video comments. In contrast, RoT provides an effective
approach to improve creative composing, highlighting its potential to drive
meaningful advancements in MLLM-based creativity. GODBench is publicly
available at https://github.com/stan-lei/GODBench-ACL2025.

</details>


### [51] [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
*Xianzhen Luo,Shijie Xuyang,Tianhao Cheng,Zheng Chu,Houyi Li,ziqi wang,Siming Huang,Qingfu Zhu,Qiufeng Wang,Xiangyu Zhang,Shuigeng Zhou,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper explores the relationship between data compression and Code LLMs, revealing a logarithmic (not linear) link between compression (BPC) and code intelligence. It introduces Format Annealing for fair evaluation and uses a new GitHub-derived validation set.


<details>
  <summary>Details</summary>
Motivation: Prior work assumed a linear relationship between compression and intelligence in Code LLMs but lacked fair evaluation and overlooked code diversity. This study aims to address these gaps.

Method: The study evaluates diverse open-source Code LLMs on multi-language, multi-task benchmarks. It introduces Format Annealing for fair assessment and uses a novel GitHub-derived validation set to measure BPC.

Result: Empirical results show a logarithmic relationship between code intelligence and BPC, contradicting prior linear assumptions.

Conclusion: The work refines understanding of compression's role in code intelligence and provides a robust evaluation framework for Code LLMs.

Abstract: Understanding the relationship between data compression and the capabilities
of Large Language Models (LLMs) is crucial, especially in specialized domains
like code intelligence. Prior work posited a linear relationship between
compression and general intelligence. However, it overlooked the multifaceted
nature of code that encompasses diverse programming languages and tasks, and
struggled with fair evaluation of modern Code LLMs. We address this by
evaluating a diverse array of open-source Code LLMs on comprehensive
multi-language, multi-task code benchmarks. To address the challenge of
efficient and fair evaluation of pre-trained LLMs' code intelligence, we
introduce \textit{Format Annealing}, a lightweight, transparent training
methodology designed to assess the intrinsic capabilities of these pre-trained
models equitably. Compression efficacy, measured as bits-per-character (BPC),
is determined using a novel, large-scale, and previously unseen code validation
set derived from GitHub. Our empirical results reveal a fundamental logarithmic
relationship between measured code intelligence and BPC. This finding refines
prior hypotheses of linearity, which we suggest are likely observations of the
logarithmic curve's tail under specific, limited conditions. Our work provides
a more nuanced understanding of compression's role in developing code
intelligence and contributes a robust evaluation framework in the code domain.

</details>


### [52] [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)
*Rahul Thapa,Qingyang Wu,Kevin Wu,Harrison Zhang,Angela Zhang,Eric Wu,Haotian Ye,Suhana Bedi,Nevin Aresh,Joseph Boen,Shriya Reddy,Ben Athiwaratkun,Shuaiwen Leon Song,James Zou*

Main category: cs.CL

TL;DR: The paper separates biomedical QA benchmarks into reasoning- and knowledge-focused subsets, revealing only 32.8% require complex reasoning. It evaluates models, showing gaps in reasoning performance, and introduces BioMed-R1, a model fine-tuned for reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks mix reasoning with factual recall, making it hard to evaluate true diagnostic thinking in LLMs.

Method: Used a PubMedBERT classifier to split benchmarks, evaluated models on knowledge and reasoning, and trained BioMed-R1 with fine-tuning and reinforcement learning.

Result: Only 32.8% of questions require complex reasoning. Biomedical models struggle with adversarial tests, while BioMed-R1 outperforms similarly sized models.

Conclusion: Improving reasoning in biomedical LLMs requires targeted training, adversarial scenarios, and clinical case reports.

Abstract: Medical reasoning in large language models (LLMs) aims to emulate clinicians'
diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and
PubMedQA often mix reasoning with factual recall. We address this by separating
11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using
a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human
performance. Our analysis shows that only 32.8 percent of questions require
complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)
and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent
gaps between knowledge and reasoning performance. For example, m1 scores 60.5
on knowledge but only 47.1 on reasoning. In adversarial tests where models are
misled with incorrect initial reasoning, biomedical models degrade sharply,
while larger or RL-trained general models show more robustness. To address
this, we train BioMed-R1 using fine-tuning and reinforcement learning on
reasoning-heavy examples. It achieves the strongest performance among similarly
sized models. Further gains may come from incorporating clinical case reports
and training with adversarial and backtracking scenarios.

</details>


### [53] [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/abs/2505.11470)
*Pascal Wullschleger,Majid Zarharan,Donnacha Daly,Marc Pouly,Jennifer Foster*

Main category: cs.CL

TL;DR: Two reference-free metrics for taxonomy quality evaluation are introduced: one measures robustness via semantic-taxonomic correlation, and the other assesses logical adequacy using Natural Language Inference. Both correlate well with F1 scores against gold standards.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for taxonomy quality evaluation lack coverage for certain types of errors, such as robustness and logical adequacy.

Method: The first metric calculates correlation between semantic and taxonomic similarity for robustness. The second uses Natural Language Inference for logical adequacy. Both are tested on five taxonomies.

Result: Both metrics correlate well with F1 scores when compared to gold-standard taxonomies.

Conclusion: The proposed metrics effectively evaluate taxonomy quality, addressing gaps in existing methods.

Abstract: We introduce two reference-free metrics for quality evaluation of taxonomies.
The first metric evaluates robustness by calculating the correlation between
semantic and taxonomic similarity, covering a type of error not handled by
existing metrics. The second uses Natural Language Inference to assess logical
adequacy. Both metrics are tested on five taxonomies and are shown to correlate
well with F1 against gold-standard taxonomies.

</details>


### [54] [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Hoo-Chang Shin,Felipe Soares,Alexander Bukharin,Ellie Evans,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: HelpSteer3-Preference is a high-quality, diverse preference dataset for RLHF, improving RM performance by ~10%.


<details>
  <summary>Details</summary>
Motivation: Advancing the quality and diversity of preference data for training instruction-following LLMs.

Method: Introduces HelpSteer3-Preference, a 40,000-sample dataset, and trains Reward Models (RMs) using it.

Result: RMs achieve 82.4% on RM-Bench and 73.7% on JudgeBench, a ~10% improvement over prior results.

Conclusion: HelpSteer3-Preference enhances RLHF training and aligns policy models effectively.

Abstract: Preference datasets are essential for training general-domain,
instruction-following language models with Reinforcement Learning from Human
Feedback (RLHF). Each subsequent data release raises expectations for future
data collection, meaning there is a constant need to advance the quality and
diversity of openly available preference data. To address this need, we
introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),
high-quality, human-annotated preference dataset comprising of over 40,000
samples. These samples span diverse real-world applications of large language
models (LLMs), including tasks relating to STEM, coding and multilingual
scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that
achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This
represents a substantial improvement (~10% absolute) over the previously
best-reported results from existing RMs. We demonstrate HelpSteer3-Preference
can also be applied to train Generative RMs and how policy models can be
aligned with RLHF using our RMs. Dataset (CC-BY-4.0):
https://huggingface.co/datasets/nvidia/HelpSteer3#preference

</details>


### [55] [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.11480)
*Anjiang Wei,Tarun Suresh,Huanmi Tan,Yinglun Xu,Gagandeep Singh,Ke Wang,Alex Aiken*

Main category: cs.CL

TL;DR: LLMs trained with reinforcement learning (PPO) can optimize assembly code, achieving 96.0% test pass rates and 1.47x speedup over gcc -O3.


<details>
  <summary>Details</summary>
Motivation: Explore LLMs' potential for optimizing assembly code, where fine-grained control can yield performance improvements hard to achieve in high-level languages.

Method: A reinforcement learning framework using PPO, with rewards for functional correctness and execution performance, tested on 8,072 real-world programs.

Result: Qwen2.5-Coder-7B-PPO achieves 96.0% test pass rates and 1.47x speedup over gcc -O3, outperforming 20 other models.

Conclusion: Reinforcement learning enables LLMs to effectively optimize assembly code performance.

Abstract: Large language models (LLMs) have demonstrated strong performance across a
wide range of programming tasks, yet their potential for code optimization
remains underexplored. This work investigates whether LLMs can optimize the
performance of assembly code, where fine-grained control over execution enables
improvements that are difficult to express in high-level languages. We present
a reinforcement learning framework that trains LLMs using Proximal Policy
Optimization (PPO), guided by a reward function that considers both functional
correctness, validated through test cases, and execution performance relative
to the industry-standard compiler gcc -O3. To support this study, we introduce
a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,
achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3
baseline, outperforming all 20 other models evaluated, including
Claude-3.7-sonnet. These results indicate that reinforcement learning can
unlock the potential of LLMs to serve as effective optimizers for assembly code
performance.

</details>


### [56] [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
*Yige Xu,Xu Guo,Zhiwei Zeng,Chunyan Miao*

Main category: cs.CL

TL;DR: SoftCoT++ enhances reasoning by diversifying exploration in continuous latent space during test-time scaling, outperforming SoftCoT and other methods.


<details>
  <summary>Details</summary>
Motivation: Existing continuous-space reasoning methods lack diversity in exploration due to fixed latent representations.

Method: SoftCoT++ perturbs latent thoughts via specialized initial tokens and uses contrastive learning to diversify soft thought representations.

Result: Outperforms SoftCoT and self-consistency scaling across five benchmarks and two LLM architectures.

Conclusion: SoftCoT++ effectively improves reasoning performance and is compatible with conventional scaling techniques.

Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning
performance by allocating extra computation during inference, without altering
the model's parameters. While existing TTS methods operate in a discrete token
space by generating more intermediate steps, recent studies in Coconut and
SoftCoT have demonstrated that thinking in the continuous latent space can
further enhance the reasoning performance. Such latent thoughts encode
informative thinking without the information loss associated with
autoregressive token generation, sparking increased interest in
continuous-space reasoning. Unlike discrete decoding, where repeated sampling
enables exploring diverse reasoning paths, latent representations in continuous
space are fixed for a given input, which limits diverse exploration, as all
decoded paths originate from the same latent thought. To overcome this
limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling
paradigm by enabling diverse exploration of thinking paths. Specifically, we
perturb latent thoughts via multiple specialized initial tokens and apply
contrastive learning to promote diversity among soft thought representations.
Experiments across five reasoning benchmarks and two distinct LLM architectures
demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms
SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility
with conventional scaling techniques such as self-consistency. Source code is
available at https://github.com/xuyige/SoftCoT.

</details>


### [57] [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/abs/2505.11485)
*Bruno Bianchi,Fermín Travi,Juan E. Kamienkowski*

Main category: cs.CL

TL;DR: Transformer-based models (GPT2, LLaMA-7B, LLaMA2-7B) outperform older models in explaining Gaze Duration variance in reading but still fall short of human predictability.


<details>
  <summary>Details</summary>
Motivation: To explore how advanced NLP models (transformers) compare to earlier models in explaining eye movement behaviors (Gaze Duration) during reading, particularly in Rioplantense Spanish.

Method: Evaluated transformer-based models (GPT2, LLaMA-7B, LLaMA2-7B) against older models (N-grams, LSTM) using Gaze Duration data from readers.

Result: Transformers outperform N-grams and LSTMs in explaining Gaze Duration variance but still cannot fully match human predictability.

Conclusion: State-of-the-art language models predict language differently from humans, indicating a gap despite their advancements.

Abstract: Recent advances in Natural Language Processing (NLP) have led to the
development of highly sophisticated language models for text generation. In
parallel, neuroscience has increasingly employed these models to explore
cognitive processes involved in language comprehension. Previous research has
shown that models such as N-grams and LSTM networks can partially account for
predictability effects in explaining eye movement behaviors, specifically Gaze
Duration, during reading. In this study, we extend these findings by evaluating
transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate
this relationship. Our results indicate that these architectures outperform
earlier models in explaining the variance in Gaze Durations recorded from
Rioplantense Spanish readers. However, similar to previous studies, these
models still fail to account for the entirety of the variance captured by human
predictability. These findings suggest that, despite their advancements,
state-of-the-art language models continue to predict language in ways that
differ from human readers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning](https://arxiv.org/abs/2505.10575)
*Adnan Ahmad,Bahareh Nakisa,Mohammad Naim Rastgoo*

Main category: cs.CV

TL;DR: A novel bi-level self-supervised continual learning framework (SSOCL) is proposed to improve emotion recognition from continuous, unlabeled EEG data by addressing cross-subject variability and noisy labels.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle continuous, unlabeled physiological data streams for emotion recognition due to cross-subject variability and noisy labels.

Method: SSOCL uses a dynamic memory buffer and bi-level architecture to refine pseudo-labels and retain representative samples, incorporating fast adaptation and cluster-mapping modules.

Result: SSOCL outperforms existing approaches in adapting to continuous EEG data streams and generalizing across subjects.

Conclusion: The framework effectively addresses challenges in emotion recognition from physiological data, offering robust performance in realistic, evolving conditions.

Abstract: Emotion recognition through physiological signals such as
electroencephalogram (EEG) has become an essential aspect of affective
computing and provides an objective way to capture human emotions. However,
physiological data characterized by cross-subject variability and noisy labels
hinder the performance of emotion recognition models. Existing domain
adaptation and continual learning methods struggle to address these issues,
especially under realistic conditions where data is continuously streamed and
unlabeled. To overcome these limitations, we propose a novel bi-level
self-supervised continual learning framework, SSOCL, based on a dynamic memory
buffer. This bi-level architecture iteratively refines the dynamic buffer and
pseudo-label assignments to effectively retain representative samples, enabling
generalization from continuous, unlabeled physiological data streams for
emotion recognition. The assigned pseudo-labels are subsequently leveraged for
accurate emotion prediction. Key components of the framework, including a fast
adaptation module and a cluster-mapping module, enable robust learning and
effective handling of evolving data streams. Experimental validation on two
mainstream EEG tasks demonstrates the framework's ability to adapt to
continuous data streams while maintaining strong generalization across
subjects, outperforming existing approaches.

</details>


### [59] [Bias and Generalizability of Foundation Models across Datasets in Breast Mammography](https://arxiv.org/abs/2505.10579)
*Germani Elodie,Selin Türk Ilayda,Zeineddine Fatima,Mourad Charbel,Albarqouni Shadi*

Main category: cs.CV

TL;DR: The paper examines fairness and bias in foundation models (FMs) for breast mammography classification, highlighting challenges like data variability and biases. It shows that while pre-training improves performance, biases persist, and fairness-aware techniques outperform domain-adaptation in achieving equitable results.


<details>
  <summary>Details</summary>
Motivation: To address the limited clinical adoption of computer-aided diagnosis tools due to data variability and biases, and to explore the fairness and generalizability of FMs in breast cancer screening.

Method: Leverages diverse datasets, including underrepresented regions, to evaluate FM performance. Tests modality-specific pre-training, dataset aggregation, domain-adaptation, and fairness-aware techniques.

Result: Pre-training enhances performance, but biases remain. Dataset aggregation improves results but not fairness. Fairness-aware techniques outperform domain-adaptation in reducing disparities.

Conclusion: Rigorous fairness evaluations and mitigation strategies are essential for inclusive and generalizable AI in breast cancer diagnosis.

Abstract: Over the past decades, computer-aided diagnosis tools for breast cancer have
been developed to enhance screening procedures, yet their clinical adoption
remains challenged by data variability and inherent biases. Although foundation
models (FMs) have recently demonstrated impressive generalizability and
transfer learning capabilities by leveraging vast and diverse datasets, their
performance can be undermined by spurious correlations that arise from
variations in image quality, labeling uncertainty, and sensitive patient
attributes. In this work, we explore the fairness and bias of FMs for breast
mammography classification by leveraging a large pool of datasets from diverse
sources-including data from underrepresented regions and an in-house dataset.
Our extensive experiments show that while modality-specific pre-training of FMs
enhances performance, classifiers trained on features from individual datasets
fail to generalize across domains. Aggregating datasets improves overall
performance, yet does not fully mitigate biases, leading to significant
disparities across under-represented subgroups such as extreme breast densities
and age groups. Furthermore, while domain-adaptation strategies can reduce
these disparities, they often incur a performance trade-off. In contrast,
fairness-aware techniques yield more stable and equitable performance across
subgroups. These findings underscore the necessity of incorporating rigorous
fairness evaluations and mitigation strategies into FM-based models to foster
inclusive and generalizable AI.

</details>


### [60] [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/abs/2505.10583)
*Diogo Freitas,Brigt Håvardstun,Cèsar Ferri,Darío Garigliotti,Jan Arne Telle,José Hernández-Orallo*

Main category: cs.CV

TL;DR: The paper explores whether multimodal language models use common representations for different modalities (e.g., images and text). It uses machine teaching to evaluate teaching complexity for visual-language models, finding image-based representations are simpler but concept simplicity transcends modalities.


<details>
  <summary>Details</summary>
Motivation: To investigate if multimodal language models integrate modalities using common representations, focusing on whether concepts like a drawing of a car and its textual description map similarly in latent space.

Method: Uses machine teaching to evaluate teaching complexity for visual-language models, comparing raw images (bitmaps) and trace coordinates (TikZ format) as representations for objects from the Quick, Draw! dataset.

Result: Image-based representations require fewer segments and achieve higher accuracy than coordinate-based ones. However, teaching size ranks concepts similarly across modalities, suggesting concept simplicity is modality-independent.

Conclusion: The simplicity of concepts may be inherent and transcend modality representations, even though image-based representations are generally more efficient.

Abstract: Large language models have become multimodal, and many of them are said to
integrate their modalities using common representations. If this were true, a
drawing of a car as an image, for instance, should map to the similar area in
the latent space as a textual description of the strokes that conform the
drawing. To explore this in a black-box access regime to these models, we
propose the use of machine teaching, a theory that studies the minimal set of
examples a teacher needs to choose so that the learner captures the concept. In
this paper we evaluate the complexity of teaching visual-language models a
subset of objects in the Quick, Draw! dataset using two presentations: raw
images as bitmaps and trace coordinates in TikZ format. The results indicate
that image-based representations generally require fewer segments and achieve
higher accuracy than coordinate-based representations. But, surprisingly, the
teaching size usually ranks concepts similarly across both modalities, even
when controlling for (a human proxy of) concept priors, suggesting that the
simplicity of concepts may be an inherent property that transcends modality
representations.

</details>


### [61] [Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios](https://arxiv.org/abs/2505.10584)
*Huafeng Shi,Jianzhong Liang,Rongchang Xie,Xian Wu,Cheng Chen,Chang Liu*

Main category: cs.CV

TL;DR: Aquarius is a family of industry-level video generation models for marketing, designed for large-scale clusters and models with hundreds of billions of parameters, showcasing high-fidelity, multi-aspect-ratio, and long-duration video synthesis.


<details>
  <summary>Details</summary>
Motivation: To demystify industrial-scale video generation systems and advance the generative video community by disclosing design details and open-sourcing components.

Method: The framework includes distributed data processing, scalable model architectures (Single-DiT and Multimodal-DiT), high-performance training infrastructure, parallel inference acceleration, and marketing-scenario applications.

Result: Achieves 36% MFU in training, 2.35x inference speedup, and supports diverse video generation tasks like image-to-video and text-to-video.

Conclusion: Aquarius demonstrates exceptional performance and scalability, with plans for further downstream applications and updates.

Abstract: This report introduces Aquarius, a family of industry-level video generation
models for marketing scenarios designed for thousands-xPU clusters and models
with hundreds of billions of parameters. Leveraging efficient engineering
architecture and algorithmic innovation, Aquarius demonstrates exceptional
performance in high-fidelity, multi-aspect-ratio, and long-duration video
synthesis. By disclosing the framework's design details, we aim to demystify
industrial-scale video generation systems and catalyze advancements in the
generative video community. The Aquarius framework consists of five components:
Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands
of CPUs and thousands of xPUs via automated task distribution, enabling
efficient video data processing. Additionally, we are about to open-source the
entire data processing framework named "Aquarius-Datapipe". Model Architectures
for Different Scales: Include a Single-DiT architecture for 2B models and a
Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,
multi-resolution, and multi-duration video generation. High-Performance
infrastructure designed for video generation model training: Incorporating
hybrid parallelism and fine-grained memory optimization strategies, this
infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference
Acceleration: Utilizes diffusion cache and attention optimization to achieve a
2.35x inference speedup. Multiple marketing-scenarios applications: Including
image-to-video, text-to-video (avatar), video inpainting and video
personalization, among others. More downstream applications and
multi-dimensional evaluation metrics will be added in the upcoming version
updates.

</details>


### [62] [Efficient Malicious UAV Detection Using Autoencoder-TSMamba Integration](https://arxiv.org/abs/2505.10585)
*Azim Akhtarshenas,Ramin Toosi,David López-Pérez,Tohid Alizadeh,Alireza Hosseini*

Main category: cs.CV

TL;DR: An integrated AE-classifier system using a 4-layer TSMamba architecture detects malicious UAVs with high accuracy (99.8% recall) and lower computational complexity.


<details>
  <summary>Details</summary>
Motivation: Malicious UAVs threaten next-gen networks with risks like surveillance and data theft, necessitating robust detection methods.

Method: Combines an autoencoder (AE) with a ResNet-based classifier, leveraging residual values for efficient and accurate detection.

Result: Achieves 99.8% recall in classification, outperforming benchmarks (96.7%), and reduces computational complexity.

Conclusion: The proposed system is robust, scalable, and effective for large-scale malicious UAV detection in NGNs.

Abstract: Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to
next-generation networks (NGNs), posing risks such as unauthorized
surveillance, data theft, and the delivery of hazardous materials. This paper
proposes an integrated (AE)-classifier system to detect malicious UAVs. The
proposed AE, based on a 4-layer Tri-orientated Spatial Mamba (TSMamba)
architecture, effectively captures complex spatial relationships crucial for
identifying malicious UAV activities. The first phase involves generating
residual values through the AE, which are subsequently processed by a
ResNet-based classifier. This classifier leverages the residual values to
achieve lower complexity and higher accuracy. Our experiments demonstrate
significant improvements in both binary and multi-class classification
scenarios, achieving up to 99.8 % recall compared to 96.7 % in the benchmark.
Additionally, our method reduces computational complexity, making it more
suitable for large-scale deployment. These results highlight the robustness and
scalability of our approach, offering an effective solution for malicious UAV
detection in NGN environments.

</details>


### [63] [Super-Resolution Generative Adversarial Networks based Video Enhancement](https://arxiv.org/abs/2505.10589)
*Kağan ÇETİN*

Main category: cs.CV

TL;DR: The paper extends SRGAN for video super-resolution by adding 3D Non-Local Blocks to handle spatio-temporal data, improving temporal coherence and reducing artifacts.


<details>
  <summary>Details</summary>
Motivation: SRGAN is effective for single images but lacks temporal continuity for videos, prompting the need for a modified framework.

Method: A modified SRGAN with 3D Non-Local Blocks is proposed, using patch-wise learning and advanced data degradation for training. Two model variants (large and lightweight) are tested.

Result: Improved temporal coherence, sharper textures, and fewer artifacts compared to single-image methods.

Conclusion: The work advances practical learning-based video enhancement, useful for streaming, gaming, and digital restoration.

Abstract: This study introduces an enhanced approach to video super-resolution by
extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution
Generative Adversarial Network (SRGAN) structure to handle spatio-temporal
data. While SRGAN has proven effective for single-image enhancement, its design
does not account for the temporal continuity required in video processing. To
address this, a modified framework that incorporates 3D Non-Local Blocks is
proposed, which is enabling the model to capture relationships across both
spatial and temporal dimensions. An experimental training pipeline is
developed, based on patch-wise learning and advanced data degradation
techniques, to simulate real-world video conditions and learn from both local
and global structures and details. This helps the model generalize better and
maintain stability across varying video content while maintaining the general
structure besides the pixel-wise correctness. Two model variants-one larger and
one more lightweight-are presented to explore the trade-offs between
performance and efficiency. The results demonstrate improved temporal
coherence, sharper textures, and fewer visual artifacts compared to traditional
single-image methods. This work contributes to the development of practical,
learning-based solutions for video enhancement tasks, with potential
applications in streaming, gaming, and digital restoration.

</details>


### [64] [ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection](https://arxiv.org/abs/2505.10595)
*Xingye Cui,Junhai Luo,Jiakun Deng,Kexuan Li,Xiangyu Qiu,Zhenming Peng*

Main category: cs.CV

TL;DR: Proposes ARFC-WAHNet, a novel network for infrared small target detection, addressing feature loss and adaptability issues with adaptive convolution and wavelet-attentive modules.


<details>
  <summary>Details</summary>
Motivation: Limited texture in infrared images and shortcomings of deep learning methods (e.g., feature loss, poor adaptability) necessitate a more robust solution.

Method: Uses MRFFIConv for adaptive feature extraction, WFED for noise suppression, HLFF for feature fusion, and GMEA for enhanced feature diversity.

Result: Outperforms state-of-the-art methods on SIRST, NUDT-SIRST, and IRSTD-1k datasets, especially in complex backgrounds.

Conclusion: ARFC-WAHNet effectively improves detection accuracy and robustness for infrared small targets.

Abstract: Infrared small target detection (ISTD) is critical in both civilian and
military applications. However, the limited texture and structural information
in infrared images makes accurate detection particularly challenging. Although
recent deep learning-based methods have improved performance, their use of
conventional convolution kernels limits adaptability to complex scenes and
diverse targets. Moreover, pooling operations often cause feature loss and
insufficient exploitation of image information. To address these issues, we
propose an adaptive receptive field convolution and wavelet-attentive
hierarchical network for infrared small target detection (ARFC-WAHNet). This
network incorporates a multi-receptive field feature interaction convolution
(MRFFIConv) module to adaptively extract discriminative features by integrating
multiple convolutional branches with a gated unit. A wavelet frequency
enhancement downsampling (WFED) module leverages Haar wavelet transform and
frequency-domain reconstruction to enhance target features and suppress
background noise. Additionally, we introduce a high-low feature fusion (HLFF)
module for integrating low-level details with high-level semantics, and a
global median enhancement attention (GMEA) module to improve feature diversity
and expressiveness via global attention. Experiments on public datasets SIRST,
NUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent
state-of-the-art methods in both detection accuracy and robustness,
particularly under complex backgrounds. The code is available at
https://github.com/Leaf2001/ARFC-WAHNet.

</details>


### [65] [SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds](https://arxiv.org/abs/2505.10601)
*Chuang Chen,Wenyi Ge*

Main category: cs.CV

TL;DR: SRMamba is a novel method for LiDAR point cloud super-resolution, addressing challenges like sparsity and irregular structure, especially for novel views. It uses Hough Voting, Hole Compensation, Visual State Space, and Multi-Directional Scanning to improve 3D spatial recovery.


<details>
  <summary>Details</summary>
Motivation: The sparsity and irregularity of LiDAR point clouds make super-resolution challenging, particularly for novel views. Existing methods struggle with recovering 3D spatial structure.

Method: SRMamba employs projection techniques (Hough Voting, Hole Compensation), Visual State Space, Multi-Directional Scanning, and an asymmetric U-Net for multi-beam LiDARs.

Result: Experiments on SemanticKITTI and nuScenes show SRMamba outperforms other algorithms in qualitative and quantitative evaluations.

Conclusion: SRMamba effectively addresses LiDAR point cloud super-resolution challenges, demonstrating superior performance in recovering 3D spatial structure.

Abstract: In recent years, range-view-based LiDAR point cloud super-resolution
techniques attract significant attention as a low-cost method for generating
higher-resolution point cloud data. However, due to the sparsity and irregular
structure of LiDAR point clouds, the point cloud super-resolution problem
remains a challenging topic, especially for point cloud upsampling under novel
views. In this paper, we propose SRMamba, a novel method for super-resolution
of LiDAR point clouds in sparse scenes, addressing the key challenge of
recovering the 3D spatial structure of point clouds from novel views.
Specifically, we implement projection technique based on Hough Voting and Hole
Compensation strategy to eliminate horizontally linear holes in range image. To
improve the establishment of long-distance dependencies and to focus on
potential geometric features in vertical 3D space, we employ Visual State Space
model and Multi-Directional Scanning mechanism to mitigate the loss of 3D
spatial structural information due to the range image. Additionally, an
asymmetric U-Net network adapts to the input characteristics of LiDARs with
different beam counts, enabling super-resolution reconstruction for multi-beam
point clouds. We conduct a series of experiments on multiple challenging public
LiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates
significant superiority over other algorithms in both qualitative and
quantitative evaluations.

</details>


### [66] [MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence](https://arxiv.org/abs/2505.10604)
*Chonghan Liu,Haoran Wang,Felix Henry,Pu Miao,Yajie Zhang,Yu Zhao,Peiran Wu*

Main category: cs.CV

TL;DR: MIRAGE is a multi-modal benchmark evaluating models' abilities in object attribute recognition and spatial relational reasoning, highlighting gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks show gaps in models' abilities for object recognition and spatial reasoning, essential for dynamic reasoning.

Method: Proposes MIRAGE, a benchmark with tasks in Counting, Relation, and Counting with Relation, using diverse scenarios.

Result: MIRAGE reveals limitations in state-of-the-art models, emphasizing the need for better representations and reasoning.

Conclusion: MIRAGE provides a foundation for advancing spatiotemporal reasoning in future research.

Abstract: Spatial perception and reasoning are core components of human cognition,
encompassing object recognition, spatial relational understanding, and dynamic
reasoning. Despite progress in computer vision, existing benchmarks reveal
significant gaps in models' abilities to accurately recognize object attributes
and reason about spatial relationships, both essential for dynamic reasoning.
To address these limitations, we propose MIRAGE, a multi-modal benchmark
designed to evaluate models' capabilities in Counting (object attribute
recognition), Relation (spatial relational reasoning), and Counting with
Relation. Through diverse and complex scenarios requiring fine-grained
recognition and reasoning, MIRAGE highlights critical limitations in
state-of-the-art models, underscoring the need for improved representations and
reasoning frameworks. By targeting these foundational abilities, MIRAGE
provides a pathway toward spatiotemporal reasoning in future research.

</details>


### [67] [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
*Zhaowei Wang,Wenhao Yu,Xiyu Ren,Jipeng Zhang,Yu Zhao,Rohit Saxena,Liang Cheng,Ginny Wong,Simon See,Pasquale Minervini,Yangqiu Song,Mark Steedman*

Main category: cs.CV

TL;DR: MMLongBench is introduced as the first benchmark for evaluating long-context vision-language models (LCVLMs) across diverse tasks and image types, revealing gaps in current models' capabilities.


<details>
  <summary>Details</summary>
Motivation: The rapid development of LCVLMs necessitates a comprehensive benchmark to evaluate their performance on long-context vision-language tasks.

Method: MMLongBench includes 13,331 examples across five task categories, standardized input lengths (8K-128K tokens), and diverse image types. It evaluates 46 LCVLMs.

Result: Findings show single-task performance is a weak proxy for overall capability, both closed and open-source models struggle, and reasoning ability correlates with better performance.

Conclusion: MMLongBench provides a foundation for diagnosing and improving LCVLMs, highlighting areas for future advancement.

Abstract: The rapid extension of context windows in large vision-language models has
given rise to long-context vision-language models (LCVLMs), which are capable
of handling hundreds of images with interleaved text tokens in a single forward
pass. In this work, we introduce MMLongBench, the first benchmark covering a
diverse set of long-context vision-language tasks, to evaluate LCVLMs
effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning
five different categories of downstream tasks, such as Visual RAG and Many-Shot
ICL. It also provides broad coverage of image types, including various natural
and synthetic images. To assess the robustness of the models to different input
lengths, all examples are delivered at five standardized input lengths (8K-128K
tokens) via a cross-modal tokenization scheme that combines vision patches and
text tokens. Through a thorough benchmarking of 46 closed-source and
open-source LCVLMs, we provide a comprehensive analysis of the current models'
vision-language long-context ability. Our results show that: i) performance on
a single task is a weak proxy for overall long-context capability; ii) both
closed-source and open-source models face challenges in long-context
vision-language tasks, indicating substantial room for future improvement; iii)
models with stronger reasoning ability tend to exhibit better long-context
performance. By offering wide task coverage, various image types, and rigorous
length control, MMLongBench provides the missing foundation for diagnosing and
advancing the next generation of LCVLMs.

</details>


### [68] [Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding](https://arxiv.org/abs/2505.10634)
*Jianfei Zhao,Feng Zhang,Xin Sun,Chong Feng*

Main category: cs.CV

TL;DR: The paper introduces Cross-Image Contrastive Decoding (CICD), a training-free method to reduce language priors in Large Vision-Language Models (LVLMs), improving visual consistency without sacrificing textual fluency.


<details>
  <summary>Details</summary>
Motivation: Language priors in LVLMs cause hallucinations by favoring linguistically plausible but visually inconsistent outputs, inherited from their LLM backbone.

Method: CICD identifies and contrasts essential vs. detrimental priors, using cross-image information to mitigate visual inconsistency while preserving coherence.

Result: CICD effectively reduces language priors, especially in image captioning, as validated on four benchmarks with six LVLMs.

Conclusion: CICD is a simple, effective solution for mitigating language priors in LVLMs, enhancing visual-textual alignment without additional training.

Abstract: Language priors constitute one of the primary causes of hallucinations in
Large Vision-Language Models (LVLMs), driving the models to generate
linguistically plausible yet visually inconsistent content. The language priors
in LVLMs originate from the linguistic knowledge inherited from their
pre-trained Large Language Model (LLM) backbone. Consequently, this
characteristic is an intrinsic property of the model that remains independent
of visual inputs. Inspired by the finding that language priors are consistent
across images, we propose Cross-Image Contrastive Decoding (CICD), a simple yet
effective training-free method to alleviate language priors in LVLMs. CICD
first identifies essential and detrimental priors, and then employs contrastive
decoding to eliminate the detrimental ones. This approach simultaneously
prevents LVLMs from generating hallucinated content while maintaining textual
fluency and coherence. Furthermore, the limited information overlap between
images helps prevent visual information loss during contrastive decoding. We
validate the effectiveness of CICD on four benchmarks with six LVLMs. Our
experiments demonstrate that CICD performs remarkably well in mitigating
language priors, especially in the image captioning task, where such priors are
most pronounced. Code will be released once accepted.

</details>


### [69] [Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging](https://arxiv.org/abs/2505.10649)
*Xianrui Li,Yufei Cui,Jun Li,Antoni B. Chan*

Main category: cs.CV

TL;DR: The paper proposes Attention Knowledge Distillation (AKD) and Pseudo-Bag Memory Pool (PMP) to improve continual learning in attention-based MIL models for WSI analysis, addressing forgetting and memory issues.


<details>
  <summary>Details</summary>
Motivation: Conventional MIL models lack adaptability to evolving datasets, and continual learning (CL) in MIL often shows limited improvements due to forgetting in attention layers.

Method: The authors analyze CL in attention MIL models, identify forgetting in attention layers, and introduce AKD to retain attention knowledge and PMP to reduce memory usage by storing informative pseudo-bags.

Result: The method improves accuracy and memory efficiency on WSI datasets, outperforming state-of-the-art CL methods.

Conclusion: This work enhances CL for large-scale clinical datasets, enabling more adaptable and resilient diagnostic models.

Abstract: Advances in medical imaging and deep learning have propelled progress in
whole slide image (WSI) analysis, with multiple instance learning (MIL) showing
promise for efficient and accurate diagnostics. However, conventional MIL
models often lack adaptability to evolving datasets, as they rely on static
training that cannot incorporate new information without extensive retraining.
Applying continual learning (CL) to MIL models is a possible solution, but
often sees limited improvements. In this paper, we analyze CL in the context of
attention MIL models and find that the model forgetting is mainly concentrated
in the attention layers of the MIL model. Using the results of this analysis we
propose two components for improving CL on MIL: Attention Knowledge
Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates
catastrophic forgetting by focusing on retaining attention layer knowledge
between learning sessions, while PMP reduces the memory footprint by
selectively storing only the most informative patches, or ``pseudo-bags'' from
WSIs. Experimental evaluations demonstrate that our method significantly
improves both accuracy and memory efficiency on diverse WSI datasets,
outperforming current state-of-the-art CL methods. This work provides a
foundation for CL in large-scale, weakly annotated clinical datasets, paving
the way for more adaptable and resilient diagnostic models.

</details>


### [70] [CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier](https://arxiv.org/abs/2505.10664)
*Ziyang Ou*

Main category: cs.CV

TL;DR: The paper explores whether CLIP embeddings can classify AI-generated images, achieving 95% accuracy on CIFAKE and 85% with few-shot adaptation, while identifying challenges in certain image types.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated images on social media necessitates tools for authenticity verification, but VLMs like CLIP lack explicit training for this task.

Method: A pipeline extracts CLIP embeddings, processes them with lightweight networks, and fine-tunes a classifier, tested on CIFAKE and custom datasets.

Result: High accuracy (95%) on CIFAKE and 85% with few-shot adaptation, though some image types (e.g., wide-angle photos, oil paintings) remain challenging.

Conclusion: CLIP embeddings show promise for AI-image classification but reveal unexplored difficulties, suggesting need for further research on specific image types.

Abstract: Verifying the authenticity of AI-generated images presents a growing
challenge on social media platforms these days. While vision-language models
(VLMs) like CLIP outdo in multimodal representation, their capacity for
AI-generated image classification is underexplored due to the absence of such
labels during the pre-training process. This work investigates whether CLIP
embeddings inherently contain information indicative of AI generation. A
proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds
its embeddings to lightweight networks, and fine-tunes only the final
classifier. Experiments on the public CIFAKE benchmark show the performance
reaches 95% accuracy without language reasoning. Few-shot adaptation to curated
custom with 20% of the data results in performance to 85%. A closed-source
baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific
styles. Notably, some specific image types, such as wide-angle photographs and
oil paintings, pose significant challenges to classification. These results
indicate previously unexplored difficulties in classifying certain types of
AI-generated images, revealing new and more specific questions in this domain
that are worth further investigation.

</details>


### [71] [GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding](https://arxiv.org/abs/2505.10671)
*Yuki Kawana,Shintaro Shiba,Quan Kong,Norimasa Kobori*

Main category: cs.CV

TL;DR: A novel 3D gaze estimation method, GA3CE, leverages 3D context (subject and object positions) to improve accuracy in unconstrained settings, reducing mean angle error by 13%-37%.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with unconstrained scenarios (e.g., distant subjects, no close-up eye views) due to reliance on 2D appearance or limited spatial cues.

Method: GA3CE encodes 3D context (poses and object positions) in egocentric space and uses D$^3$ positional encoding to capture spatial relationships.

Result: Reduces mean angle error by 13%-37% compared to baselines in single-frame settings.

Conclusion: GA3CE effectively addresses challenges in 3D gaze estimation by leveraging 3D spatial relationships, outperforming existing methods.

Abstract: We propose a novel 3D gaze estimation approach that learns spatial
relationships between the subject and objects in the scene, and outputs 3D gaze
direction. Our method targets unconstrained settings, including cases where
close-up views of the subject's eyes are unavailable, such as when the subject
is distant or facing away. Previous approaches typically rely on either 2D
appearance alone or incorporate limited spatial cues using depth maps in the
non-learnable post-processing step. Estimating 3D gaze direction from 2D
observations in these scenarios is challenging; variations in subject pose,
scene layout, and gaze direction, combined with differing camera poses, yield
diverse 2D appearances and 3D gaze directions even when targeting the same 3D
scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding.
Our method represents subject and scene using 3D poses and object positions,
treating them as 3D context to learn spatial relationships in 3D space.
Inspired by human vision, we align this context in an egocentric space,
significantly reducing spatial complexity. Furthermore, we propose D$^3$
(direction-distance-decomposed) positional encoding to better capture the
spatial relationship between 3D context and gaze direction in direction and
distance space. Experiments demonstrate substantial improvements, reducing mean
angle error by 13%-37% compared to leading baselines on benchmark datasets in
single-frame settings.

</details>


### [72] [Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?](https://arxiv.org/abs/2505.10679)
*Jianyang Xie,Yitian Zhao,Yanda Meng,He Zhao,Anh Nguyen,Yalin Zheng*

Main category: cs.CV

TL;DR: The paper proposes sparse ST-GCNs for skeleton-based human action recognition, showing comparable performance to dense models with significantly fewer parameters. Multi-level sparsity further improves accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing ST-GCNs are over-parameterized for HAR, with minimal performance differences between models. This inefficiency is confirmed via the lottery ticket hypothesis.

Method: A sparse ST-GCN generator trains sparse architectures from dense networks, maintaining performance. Multi-level sparsity integrates sparse structures at varying levels.

Result: Sparse ST-GCNs achieve comparable performance to dense models with 95% fewer parameters (<1% accuracy drop). Multi-level sparsity improves accuracy by >1% with 66% parameters.

Conclusion: Sparse and multi-level sparsity ST-GCNs offer efficient, high-performance alternatives to dense models for HAR.

Abstract: Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive
performance in skeleton-based human action recognition (HAR). However, despite
the development of numerous models, their recognition performance does not
differ significantly after aligning the input settings. With this observation,
we hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture
subsequently confirmed through experiments employing the lottery ticket
hypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which
trains a sparse architecture from a randomly initialized dense network while
maintaining comparable performance levels to the dense components. Moreover, we
generate multi-level sparsity ST-GCNs by integrating sparse structures at
various sparsity levels and demonstrate that the assembled model yields a
significant enhancement in HAR performance. Thorough experiments on four
datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate
that the proposed sparse ST-GCNs can achieve comparable performance to their
dense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a
degradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity
ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs,
demonstrate an improvement of >1% in top-1 accuracy. The code is available at
https://github.com/davelailai/Sparse-ST-GCN.

</details>


### [73] [GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention](https://arxiv.org/abs/2505.10685)
*Lingjun Zhao,Sizhe Wei,James Hays,Lu Gan*

Main category: cs.CV

TL;DR: GaussianFormer3D is a multi-modal Gaussian-based semantic occupancy prediction framework using 3D deformable attention, achieving high accuracy with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: To improve 3D semantic occupancy prediction for autonomous driving by leveraging multi-modal (LiDAR-camera) fusion and compact 3D Gaussian representations.

Method: Uses voxel-to-Gaussian initialization for geometry priors and LiDAR-guided 3D deformable attention for refining Gaussians with fusion features.

Result: Achieves comparable accuracy to state-of-the-art methods while reducing memory consumption and improving efficiency.

Conclusion: GaussianFormer3D offers an efficient and accurate solution for 3D semantic occupancy prediction in autonomous driving.

Abstract: 3D semantic occupancy prediction is critical for achieving safe and reliable
autonomous driving. Compared to camera-only perception systems, multi-modal
pipelines, especially LiDAR-camera fusion methods, can produce more accurate
and detailed predictions. Although most existing works utilize a dense
grid-based representation, in which the entire 3D space is uniformly divided
into discrete voxels, the emergence of 3D Gaussians provides a compact and
continuous object-centric representation. In this work, we propose a
multi-modal Gaussian-based semantic occupancy prediction framework utilizing 3D
deformable attention, named as GaussianFormer3D. We introduce a
voxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry
priors from LiDAR data, and design a LiDAR-guided 3D deformable attention
mechanism for refining 3D Gaussians with LiDAR-camera fusion features in a
lifted 3D space. We conducted extensive experiments on both on-road and
off-road datasets, demonstrating that our GaussianFormer3D achieves high
prediction accuracy that is comparable to state-of-the-art multi-modal
fusion-based methods with reduced memory consumption and improved efficiency.

</details>


### [74] [Automated Detection of Salvin's Albatrosses: Improving Deep Learning Tools for Aerial Wildlife Surveys](https://arxiv.org/abs/2505.10737)
*Mitchell Rogers,Theo Thompson,Isla Duporge,Johannes Fischer,Klemens Pütz,Thomas Mattern,Bing Xue,Mengjie Zhang*

Main category: cs.CV

TL;DR: The paper evaluates BirdDetector, a general-purpose avian detection model, for estimating Salvin's albatross populations using drone imagery, showing improved accuracy with fine-tuning and augmentation.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of using deep learning and UAVs for large-scale wildlife monitoring, specifically for seabird colonies in remote areas.

Method: Evaluated BirdDetector in zero-shot and fine-tuned settings using drone imagery, enhanced inference, and stronger augmentation techniques.

Result: Fine-tuning with target-domain annotations and stronger augmentation significantly improved detection accuracy over zero-shot performance.

Conclusion: Pre-trained deep-learning models, when fine-tuned, are effective for species-specific monitoring in challenging environments.

Abstract: Recent advancements in deep learning and aerial imaging have transformed
wildlife monitoring, enabling researchers to survey wildlife populations at
unprecedented scales. Unmanned Aerial Vehicles (UAVs) provide a cost-effective
means of capturing high-resolution imagery, particularly for monitoring densely
populated seabird colonies. In this study, we assess the performance of a
general-purpose avian detection model, BirdDetector, in estimating the breeding
population of Salvin's albatross (Thalassarche salvini) on the Bounty Islands,
New Zealand. Using drone-derived imagery, we evaluate the model's effectiveness
in both zero-shot and fine-tuned settings, incorporating enhanced inference
techniques and stronger augmentation methods. Our findings indicate that while
applying the model in a zero-shot setting offers a strong baseline, fine-tuning
with annotations from the target domain and stronger image augmentation leads
to marked improvements in detection accuracy. These results highlight the
potential of leveraging pre-trained deep-learning models for species-specific
monitoring in remote and challenging environments.

</details>


### [75] [IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation](https://arxiv.org/abs/2505.10743)
*Amritanshu Tiwari,Cherish Puniani,Kaustubh Sharma,Ojasva Nema*

Main category: cs.CV

TL;DR: A two-stage pipeline using LoRA-based fine-tuning on SDXL's U-Net attention weights to personalize text-to-image models, avoiding common issues like catastrophic forgetting and overfitting.


<details>
  <summary>Details</summary>
Motivation: Personalizing text-to-image models for novel subjects from few reference images is challenging, often causing issues like catastrophic forgetting or computational overhead.

Method: A two-stage approach: first generates a generic scene using SDXL, then inserts the personalized subject via a segmentation-driven Img2Img pipeline with trained LoRA weights.

Result: Achieves a DINO similarity score of 0.789 on SDXL, outperforming existing personalized text-to-image methods.

Conclusion: The method effectively isolates subject encoding from composition, preserving SDXL's generative capabilities while integrating new subjects with high fidelity.

Abstract: Recent advances in text-to-image diffusion models, particularly Stable
Diffusion, have enabled the generation of highly detailed and semantically rich
images. However, personalizing these models to represent novel subjects based
on a few reference images remains challenging. This often leads to catastrophic
forgetting, overfitting, or large computational overhead.We propose a two-stage
pipeline that addresses these limitations by leveraging LoRA-based fine-tuning
on the attention weights within the U-Net of the Stable Diffusion XL (SDXL)
model. First, we use the unmodified SDXL to generate a generic scene by
replacing the subject with its class label. Then, we selectively insert the
personalized subject through a segmentation-driven image-to-image (Img2Img)
pipeline that uses the trained LoRA weights.This framework isolates the subject
encoding from the overall composition, thus preserving SDXL's broader
generative capabilities while integrating the new subject in a high-fidelity
manner. Our method achieves a DINO similarity score of 0.789 on SDXL,
outperforming existing personalized text-to-image approaches.

</details>


### [76] [Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis](https://arxiv.org/abs/2505.10751)
*Francisco Raverta Capua,Pablo De Cristoforis*

Main category: cs.CV

TL;DR: The paper introduces a pipeline for creating semantically segmented point clouds of forests using a custom simulator and modified SfM software, addressing the lack of public annotated datasets.


<details>
  <summary>Details</summary>
Motivation: Publicly available annotated point cloud datasets for forests are scarce due to high costs and technical challenges, especially with SfM algorithms.

Method: A custom forest simulator generates labeled RGB images, processed by modified SfM software to preserve semantic data during 3D reconstruction.

Result: The pipeline produces detailed geometric and semantic point clouds, useful for training deep learning models on real forest data.

Conclusion: This work provides a valuable resource for advancing forest monitoring and segmentation tasks using SfM-derived point clouds.

Abstract: Although the use of remote sensing technologies for monitoring forested
environments has gained increasing attention, publicly available point cloud
datasets remain scarce due to the high costs, sensor requirements, and
time-intensive nature of their acquisition. Moreover, as far as we are aware,
there are no public annotated datasets generated through Structure From Motion
(SfM) algorithms applied to imagery, which may be due to the lack of SfM
algorithms that can map semantic segmentation information into an accurate
point cloud, especially in a challenging environment like forests.
  In this work, we present a novel pipeline for generating semantically
segmented point clouds of forest environments. Using a custom-built forest
simulator, we generate realistic RGB images of diverse forest scenes along with
their corresponding semantic segmentation masks. These labeled images are then
processed using modified open-source SfM software capable of preserving
semantic information during 3D reconstruction. The resulting point clouds
provide both geometric and semantic detail, offering a valuable resource for
training and evaluating deep learning models aimed at segmenting real forest
point clouds obtained via SfM.

</details>


### [77] [Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities](https://arxiv.org/abs/2505.10764)
*Jiajun Cheng,Xianwu Zhao,Shan Lin*

Main category: cs.CV

TL;DR: The paper benchmarks vision-language models (VLMs) in the surgical domain, revealing gaps in their ability to link language to surgical scenes.


<details>
  <summary>Details</summary>
Motivation: To assess the adaptability of general-purpose VLMs in surgical understanding, given the challenges of minimally invasive surgery (MIS).

Method: Benchmarking several VLMs on diverse surgical datasets, including laparoscopic procedures and endoscopic submucosal dissection.

Result: VLMs show limitations in consistently linking language to correct regions in surgical scenes.

Conclusion: General-purpose VLMs have potential but require improvements for reliable surgical domain adaptation.

Abstract: Minimally invasive surgery (MIS) presents significant visual and technical
challenges, including surgical instrument classification and understanding
surgical action involving instruments, verbs, and anatomical targets. While
many machine learning-based methods have been developed for surgical
understanding, they typically rely on procedure- and task-specific models
trained on small, manually annotated datasets. In contrast, the recent success
of vision-language models (VLMs) trained on large volumes of raw image-text
pairs has demonstrated strong adaptability to diverse visual data and a range
of downstream tasks. This opens meaningful research questions: how well do
these general-purpose VLMs perform in the surgical domain? In this work, we
explore those questions by benchmarking several VLMs across diverse surgical
datasets, including general laparoscopic procedures and endoscopic submucosal
dissection, to assess their current capabilities and limitations. Our benchmark
reveals key gaps in the models' ability to consistently link language to the
correct regions in surgical scenes.

</details>


### [78] [Unifying Segment Anything in Microscopy with Multimodal Large Language Model](https://arxiv.org/abs/2505.10769)
*Manyu Li,Ruian He,Zixian Zhang,Weimin Tan,Bo Yan*

Main category: cs.CV

TL;DR: The paper proposes uLLSAM, a method using MLLMs to enhance SAM's performance in biomedical image segmentation by injecting Vision-Language Knowledge (VLK), improving generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Existing biomedical segmentation models lack generalization on unseen domain data due to missing vision-language knowledge. MLLMs' multimodal understanding capabilities inspire leveraging them to address this gap.

Method: The authors introduce the Vision-Language Semantic Alignment (VLSA) module to inject VLK into SAM and Semantic Boundary Regularization (SBR) to improve boundary perception.

Result: uLLSAM achieves significant performance improvements (7.71% Dice, 12.10% SA) on in-domain datasets and (6.79% Dice, 10.08% SA) on out-of-domain datasets, demonstrating strong generalization.

Conclusion: The proposed method successfully enhances SAM's segmentation performance and generalization by integrating VLK, validated by state-of-the-art results on microscopy datasets.

Abstract: Accurate segmentation of regions of interest in biomedical images holds
substantial value in image analysis. Although several foundation models for
biomedical segmentation have currently achieved excellent performance on
certain datasets, they typically demonstrate sub-optimal performance on unseen
domain data. We owe the deficiency to lack of vision-language knowledge before
segmentation. Multimodal Large Language Models (MLLMs) bring outstanding
understanding and reasoning capabilities to multimodal tasks, which inspires us
to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling
vision models to demonstrate superior generalization capabilities on
cross-domain datasets. In this paper, we propose using MLLMs to guide SAM in
learning microscopy crose-domain data, unifying Segment Anything in Microscopy,
named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment
(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find
that after SAM receives global VLK prompts, its performance improves
significantly, but there are deficiencies in boundary contour perception.
Therefore, we further propose Semantic Boundary Regularization (SBR) to prompt
SAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%
in SA across 9 in-domain microscopy datasets, achieving state-of-the-art
performance. Our method also demonstrates improvements of 6.79% in Dice and
10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization
capabilities. Code is available at https://github.com/ieellee/uLLSAM.

</details>


### [79] [Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation](https://arxiv.org/abs/2505.10781)
*David Minkwan Kim,Soeun Lee,Byeongkeun Kang*

Main category: cs.CV

TL;DR: A completely weakly supervised method for class-incremental semantic segmentation (CISS) is introduced, using only image-level labels to learn both base and novel classes, outperforming partially weakly supervised methods.


<details>
  <summary>Details</summary>
Motivation: Traditional CISS requires costly pixel-level annotations, and while partially weakly-supervised methods exist, this is the first completely weakly-supervised approach for CISS.

Method: Generates robust pseudo-labels by combining outputs from a localizer and foundation models based on uncertainty, and uses exemplar-guided data augmentation to mitigate catastrophic forgetting.

Result: Outperforms partially weakly supervised methods in 15-5 VOC and 10-10 VOC settings and achieves competitive accuracy in COCO-to-VOC.

Conclusion: The proposed method is effective for completely weakly supervised CISS, demonstrating superior performance in key settings.

Abstract: This work addresses the task of completely weakly supervised
class-incremental learning for semantic segmentation to learn segmentation for
both base and additional novel classes using only image-level labels. While
class-incremental semantic segmentation (CISS) is crucial for handling diverse
and newly emerging objects in the real world, traditional CISS methods require
expensive pixel-level annotations for training. To overcome this limitation,
partially weakly-supervised approaches have recently been proposed. However, to
the best of our knowledge, this is the first work to introduce a completely
weakly-supervised method for CISS. To achieve this, we propose to generate
robust pseudo-labels by combining pseudo-labels from a localizer and a sequence
of foundation models based on their uncertainty. Moreover, to mitigate
catastrophic forgetting, we introduce an exemplar-guided data augmentation
method that generates diverse images containing both previous and novel classes
with guidance. Finally, we conduct experiments in three common experimental
settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint
and overlap. The experimental results demonstrate that our completely weakly
supervised method outperforms even partially weakly supervised methods in the
15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the
COCO-to-VOC setting.

</details>


### [80] [SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios](https://arxiv.org/abs/2505.10784)
*Qiushi Guo,Jason Rambach*

Main category: cs.CV

TL;DR: The paper introduces SynRailObs, a synthetic dataset for railway obstacle detection, addressing gaps in existing datasets. It uses diffusion models for rare obstacles and demonstrates strong performance in real-world tests.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack the diversity and quality needed for effective railway obstacle detection, limiting safety research.

Method: Created SynRailObs, a synthetic dataset with diverse conditions, and employed diffusion models for rare obstacles. Tested in real-world railway environments.

Result: Models trained on SynRailObs perform consistently across distances and conditions, showing zero-shot capabilities.

Conclusion: SynRailObs advances railway safety research by providing a high-quality dataset and demonstrating practical effectiveness.

Abstract: Detecting potential obstacles in railway environments is critical for
preventing serious accidents. Identifying a broad range of obstacle categories
under complex conditions requires large-scale datasets with precisely
annotated, high-quality images. However, existing publicly available datasets
fail to meet these requirements, thereby hindering progress in railway safety
research. To address this gap, we introduce SynRailObs, a high-fidelity
synthetic dataset designed to represent a diverse range of weather conditions
and geographical features. Furthermore, diffusion models are employed to
generate rare and difficult-to-capture obstacles that are typically challenging
to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs,
we perform experiments in real-world railway environments, testing on both
ballasted and ballastless tracks across various weather conditions. The results
demonstrate that SynRailObs holds substantial potential for advancing obstacle
detection in railway safety applications. Models trained on this dataset show
consistent performance across different distances and environmental conditions.
Moreover, the model trained on SynRailObs exhibits zero-shot capabilities,
which are essential for applications in security-sensitive domains. The data is
available in https://www.kaggle.com/datasets/qiushi910/synrailobs.

</details>


### [81] [EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes](https://arxiv.org/abs/2505.10787)
*Jianlin Guo,Haihong Xiao,Wenxiong Kang*

Main category: cs.CV

TL;DR: EA-3DGS improves 3D Gaussian Splatting for outdoor scenes by introducing adaptive mesh initialization, efficient pruning, and structure-aware densification, achieving high-quality real-time rendering with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: Current NeRF-based methods are slow for large-scale scenes, and 3D Gaussian Splatting struggles with memory constraints and lacks adjustment mechanisms for outdoor scenes.

Method: EA-3DGS uses adaptive tetrahedral mesh for Gaussian initialization, efficient pruning, structure-aware densification, and vector quantization for parameter compression.

Result: The method outperforms on 13 scenes, including public datasets and self-collected UAV data, with reduced memory and disk space usage.

Conclusion: EA-3DGS addresses limitations of 3DGS for outdoor scenes, offering efficient, high-quality rendering with minimal quality loss.

Abstract: Efficient scene representations are essential for many real-world
applications, especially those involving spatial measurement. Although current
NeRF-based methods have achieved impressive results in reconstructing
building-scale scenes, they still suffer from slow training and inference
speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian
Splatting (3DGS) has demonstrated excellent performance with its high-quality
rendering and real-time speed, especially for objects and small-scale scenes.
However, in outdoor scenes, its point-based explicit representation lacks an
effective adjustment mechanism, and the millions of Gaussian points required
often lead to memory constraints during training. To address these challenges,
we propose EA-3DGS, a high-quality real-time rendering method designed for
outdoor scenes. First, we introduce a mesh structure to regulate the
initialization of Gaussian components by leveraging an adaptive tetrahedral
mesh that partitions the grid and initializes Gaussian components on each face,
effectively capturing geometric structures in low-texture regions. Second, we
propose an efficient Gaussian pruning strategy that evaluates each 3D
Gaussian's contribution to the view and prunes accordingly. To retain
geometry-critical Gaussian points, we also present a structure-aware
densification strategy that densifies Gaussian points in low-curvature regions.
Additionally, we employ vector quantization for parameter quantization of
Gaussian components, significantly reducing disk space requirements with only a
minimal impact on rendering quality. Extensive experiments on 13 scenes,
including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \&
Temples, WHU) and five self-collected scenes acquired through UAV
photogrammetry measurement from SCUT-CA and plateau regions, further
demonstrate the superiority of our method.

</details>


### [82] [MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation](https://arxiv.org/abs/2505.10810)
*Gabriel Maldonado,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: MoCLIP is a fine-tuned CLIP model with a motion encoding head, improving text-to-motion generation by enhancing motion fidelity and alignment.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based text encoders lack understanding of temporal and kinematic structures in motion, limiting their effectiveness in motion generation.

Method: MoCLIP fine-tunes CLIP with a motion encoding head, trained on motion sequences using contrastive learning and tethering loss.

Result: MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining competitive FID, enhancing text-to-motion alignment.

Conclusion: MoCLIP is a versatile and effective framework for improving motion generation, compatible with existing CLIP-based methods.

Abstract: Human motion generation is essential for fields such as animation, robotics,
and virtual reality, requiring models that effectively capture motion dynamics
from text descriptions. Existing approaches often rely on Contrastive
Language-Image Pretraining (CLIP)-based text encoders, but their training on
text-image pairs constrains their ability to understand temporal and kinematic
structures inherent in motion and motion generation. This work introduces
MoCLIP, a fine-tuned CLIP model with an additional motion encoding head,
trained on motion sequences using contrastive learning and tethering loss. By
explicitly incorporating motion-aware representations, MoCLIP enhances motion
fidelity while remaining compatible with existing CLIP-based pipelines and
seamlessly integrating into various CLIP-based methods. Experiments demonstrate
that MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining
competitive FID, leading to improved text-to-motion alignment results. These
results highlight MoCLIP's versatility and effectiveness, establishing it as a
robust framework for enhancing motion generation.

</details>


### [83] [From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification](https://arxiv.org/abs/2505.10823)
*Xue Li,Jameson Merkow,Noel C. F. Codella,Alberto Santamaria-Pang,Naiteek Sangani,Alexander Ersoy,Christopher Burt,John W. Garrett,Richard J. Bruce,Joshua D. Warner,Tyler Bradshaw,Ivan Tarapov,Matthew P. Lungren,Alan B. McMillan*

Main category: cs.CV

TL;DR: The study evaluates foundation model embeddings for multi-class radiography classification, finding MedImageInsight with an SVM adapter most effective (93.8% mAUC), while also highlighting computational efficiency and fairness.


<details>
  <summary>Details</summary>
Motivation: To assess the utility of general-purpose and medical-specific foundation model embeddings for training lightweight adapter models in radiographic classification, focusing on tube placement.

Method: Embeddings from six foundation models (e.g., DenseNet121, BiomedCLIP) were extracted from 8842 radiographs. Lightweight adapter models were trained using classical ML algorithms.

Result: MedImageInsight embeddings with an SVM adapter achieved the highest mAUC (93.8%). Most adapters were computationally efficient (training <1 min, inference in seconds). Fairness analyses showed minimal disparities.

Conclusion: Foundation model embeddings, particularly MedImageInsight, enable accurate, efficient, and equitable diagnostic classification in radiographic analysis.

Abstract: Foundation models, pretrained on extensive datasets, have significantly
advanced machine learning by providing robust and transferable embeddings
applicable to various domains, including medical imaging diagnostics. This
study evaluates the utility of embeddings derived from both general-purpose and
medical domain-specific foundation models for training lightweight adapter
models in multi-class radiography classification, focusing specifically on tube
placement assessment. A dataset comprising 8842 radiographs classified into
seven distinct categories was employed to extract embeddings using six
foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,
Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using
classical machine learning algorithms. Among these combinations,
MedImageInsight embeddings paired with an support vector machine adapter
yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely
by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and
DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,
respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.
Notably, most adapter models demonstrated computational efficiency, achieving
training within one minute and inference within seconds on CPU, underscoring
their practicality for clinical applications. Furthermore, fairness analyses on
adapters trained on MedImageInsight-derived embeddings indicated minimal
disparities, with gender differences in performance within 2% and standard
deviations across age groups not exceeding 3%. These findings confirm that
foundation model embeddings-especially those from MedImageInsight-facilitate
accurate, computationally efficient, and equitable diagnostic classification
using lightweight adapters for radiographic image analysis.

</details>


### [84] [A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation](https://arxiv.org/abs/2505.10825)
*Jinke Li,Yue Wu,Xiaoyan Yang*

Main category: cs.CV

TL;DR: CRT-YOLO is a novel thermal infrared object detection framework using centralized feature regulation and multi-scale attention to outperform traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional TIR object detection methods fail to effectively extract and fuse local-global information, limiting feature attention in the TIR domain.

Method: The CRT-YOLO framework integrates efficient multi-scale attention (EMA) modules and a Centralized Feature Pyramid (CFP) network for global-range interaction and minimal computational overhead.

Result: Experiments on benchmark datasets show CRT-YOLO significantly outperforms conventional TIR object detection methods.

Conclusion: The proposed modules are effective, demonstrating CRT-YOLO's potential to advance thermal infrared object detection.

Abstract: Thermal Infrared (TIR) technology involves the use of sensors to detect and
measure infrared radiation emitted by objects, and it is widely utilized across
a broad spectrum of applications. The advancements in object detection methods
utilizing TIR images have sparked significant research interest. However, most
traditional methods lack the capability to effectively extract and fuse
local-global information, which is crucial for TIR-domain feature attention. In
this study, we present a novel and efficient thermal infrared object detection
framework, known as CRT-YOLO, that is based on centralized feature regulation,
enabling the establishment of global-range interaction on TIR information. Our
proposed model integrates efficient multi-scale attention (EMA) modules, which
adeptly capture long-range dependencies while incurring minimal computational
overhead. Additionally, it leverages the Centralized Feature Pyramid (CFP)
network, which offers global regulation of TIR features. Extensive experiments
conducted on two benchmark datasets demonstrate that our CRT-YOLO model
significantly outperforms conventional methods for TIR image object detection.
Furthermore, the ablation study provides compelling evidence of the
effectiveness of our proposed modules, reinforcing the potential impact of our
approach on advancing the field of thermal infrared object detection.

</details>


### [85] [NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits](https://arxiv.org/abs/2505.10827)
*Nail Ibrahimli,Julian F. P. Kooij,Liangliang Nan*

Main category: cs.CV

TL;DR: NeuSEditor is a novel method for text-guided editing of neural implicit surfaces, addressing challenges in preserving identity and geometric consistency. It outperforms existing methods like PDS and InstructNeRF2NeRF.


<details>
  <summary>Details</summary>
Motivation: Existing methods for editing implicit surfaces often fail to preserve identity and geometric consistency, prompting the need for a more efficient solution.

Method: NeuSEditor uses an identity-preserving architecture to separate scenes into foreground and background, with a geometry-aware distillation loss for enhanced rendering and geometric quality.

Result: NeuSEditor outperforms state-of-the-art methods, delivering superior quantitative and qualitative results without requiring continuous dataset updates.

Conclusion: NeuSEditor provides an effective solution for text-guided editing of neural implicit surfaces, simplifying workflows and improving quality.

Abstract: Implicit surface representations are valued for their compactness and
continuity, but they pose significant challenges for editing. Despite recent
advancements, existing methods often fail to preserve identity and maintain
geometric consistency during editing. To address these challenges, we present
NeuSEditor, a novel method for text-guided editing of neural implicit surfaces
derived from multi-view images. NeuSEditor introduces an identity-preserving
architecture that efficiently separates scenes into foreground and background,
enabling precise modifications without altering the scene-specific elements.
Our geometry-aware distillation loss significantly enhances rendering and
geometric quality. Our method simplifies the editing workflow by eliminating
the need for continuous dataset updates and source prompting. NeuSEditor
outperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF,
delivering superior quantitative and qualitative results. For more visual
results, visit: neuseditor.github.io.

</details>


### [86] [RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects](https://arxiv.org/abs/2505.10841)
*Jaeguk Kim,Jaewoo Park,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: RefPose is a novel method for 6D pose estimation of unseen objects using reference images and geometric correspondence, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The challenge of estimating 6D poses of unseen objects from monocular RGB images due to lack of prior knowledge.

Method: RefPose uses reference images and geometric correspondence, predicting an initial pose and refining it iteratively with a render-and-compare approach and a correlation volume-guided attention mechanism.

Result: Achieves state-of-the-art performance on BOP benchmark datasets with competitive runtime.

Conclusion: RefPose dynamically adapts to new object shapes, offering robust pose estimation for unseen objects.

Abstract: Estimating the 6D pose of unseen objects from monocular RGB images remains a
challenging problem, especially due to the lack of prior object-specific
knowledge. To tackle this issue, we propose RefPose, an innovative approach to
object pose estimation that leverages a reference image and geometric
correspondence as guidance. RefPose first predicts an initial pose by using
object templates to render the reference image and establish the geometric
correspondence needed for the refinement stage. During the refinement stage,
RefPose estimates the geometric correspondence of the query based on the
generated references and iteratively refines the pose through a
render-and-compare approach. To enhance this estimation, we introduce a
correlation volume-guided attention mechanism that effectively captures
correlations between the query and reference images. Unlike traditional methods
that depend on pre-defined object models, RefPose dynamically adapts to new
object shapes by leveraging a reference image and geometric correspondence.
This results in robust performance across previously unseen objects. Extensive
evaluation on the BOP benchmark datasets shows that RefPose achieves
state-of-the-art results while maintaining a competitive runtime.

</details>


### [87] [A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination](https://arxiv.org/abs/2505.10869)
*Go Fukino,Kanta Tachibana*

Main category: cs.CV

TL;DR: The paper proposes a method to evaluate gait symmetry using an LTI system and a dissimilarity metric, tested on five subjects.


<details>
  <summary>Details</summary>
Motivation: Traditional gait symmetry assessments rely on EMG or acceleration differences, which this study aims to improve upon.

Method: The study models intersegmental coordination with an LTI system and introduces a dissimilarity metric for symmetry evaluation.

Result: The method was tested on five subjects, including those with symmetric and asymmetric gait.

Conclusion: The proposed method offers a novel approach to assessing gait symmetry, potentially improving upon traditional techniques.

Abstract: This study focuses on the velocity patterns of various body parts during
walking and proposes a method for evaluating gait symmetry. Traditional motion
analysis studies have assessed gait symmetry based on differences in
electromyographic (EMG) signals or acceleration between the left and right
sides. In contrast, this paper models intersegmental coordination using an LTI
system and proposes a dissimilarity metric to evaluate symmetry. The method was
tested on five subjects with both symmetric and asymmetric gait.

</details>


### [88] [A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision](https://arxiv.org/abs/2505.10875)
*Alexey Magay,Dhurba Tripathi,Yu Hao,Yi Fang*

Main category: cs.CV

TL;DR: A novel spatial-enhanced multi-modal large language model (MLLM) approach is proposed to assist visually impaired individuals by improving spatial reasoning and environmental context understanding, combined with a hardware attachment for real-time feedback.


<details>
  <summary>Details</summary>
Motivation: People with blindness and low vision (pBLV) struggle with navigation and object location due to limited visual cues, and existing MLLM models lack adequate spatial reasoning capabilities.

Method: Fine-tune MLLM to enhance spatial reasoning, integrate with a hardware component (glasses attachment), and use advanced VLMs for real-time visual data interpretation.

Result: Substantial improvements in accuracy and user experience, demonstrated via evaluation on the VizWiz dataset and a custom real-world dataset.

Conclusion: The approach bridges the gap between advanced ML models and practical assistive devices, enabling visually impaired users to navigate more effectively and independently.

Abstract: People with blindness and low vision (pBLV) face significant challenges,
struggling to navigate environments and locate objects due to limited visual
cues. Spatial reasoning is crucial for these individuals, as it enables them to
understand and interpret the spatial relationships in their surroundings,
enhancing their ability to navigate and interact more safely and independently.
Current multi-modal large language (MLLM) models for low vision people lack the
spatial reasoning capabilities needed to effectively assist in these tasks.
Moreover, there is a notable absence of lightweight, easy-to-use systems that
allow pBLV to effectively perceive and interact with their surrounding
environment. In this paper, we propose a novel spatial enhanced multi-modal
large language model based approach for visually impaired individuals. By
fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method
significantly improves the understanding of environmental context, which is
critical for navigation and object recognition. The innovation extends to a
hardware component, designed as an attachment for glasses, ensuring increased
accessibility and ease of use. This integration leverages advanced VLMs to
interpret visual data and provide real-time, spatially aware feedback to the
user. Our approach aims to bridge the gap between advanced machine learning
models and practical, user-friendly assistive devices, offering a robust
solution for visually impaired users to navigate their surroundings more
effectively and independently. The paper includes an in-depth evaluation using
the VizWiz dataset, demonstrating substantial improvements in accuracy and user
experience. Additionally, we design a comprehensive dataset to evaluate our
method's effectiveness in realworld situations, demonstrating substantial
improvements in accuracy and user experience.

</details>


### [89] [PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2505.10888)
*Saad Manzur,Bryan Vela,Brandon Vela,Aditya Agrawal,Lan-Anh Dang-Vu,David Li,Wayne Hayes*

Main category: cs.CV

TL;DR: PoseBench3D is a unified framework for cross-dataset evaluation of 3D human pose estimation methods, ensuring fair comparisons and analyzing generalization on unseen data.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks adaptability to diverse real-world conditions, necessitating a standardized testing environment for fair and consistent evaluations across datasets.

Method: PoseBench3D provides a pre-configured yet modifiable interface to evaluate 18 methods across four datasets, using MPJPE and PA-MPJPE metrics.

Result: Over 100 novel cross-dataset results were generated, with analysis of pre-processing and dataset preparation impacts on model generalization.

Conclusion: PoseBench3D enables systematic re-evaluation of models, offering insights into generalization and compatibility with diverse architectures.

Abstract: Reliable three-dimensional human pose estimation is becoming increasingly
important for real-world applications, yet much of prior work has focused
solely on the performance within a single dataset. In practice, however,
systems must adapt to diverse viewpoints, environments, and camera setups --
conditions that differ significantly from those encountered during training,
which is often the case in real-world scenarios. To address these challenges,
we present a standardized testing environment in which each method is evaluated
on a variety of datasets, ensuring consistent and fair cross-dataset
comparisons -- allowing for the analysis of methods on previously unseen data.
Therefore, we propose PoseBench3D, a unified framework designed to
systematically re-evaluate prior and future models across four of the most
widely used datasets for human pose estimation -- with the framework able to
support novel and future datasets as the field progresses. Through a unified
interface, our framework provides datasets in a pre-configured yet easily
modifiable format, ensuring compatibility with diverse model architectures. We
re-evaluated the work of 18 methods, either trained or gathered from existing
literature, and reported results using both Mean Per Joint Position Error
(MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE)
metrics, yielding more than 100 novel cross-dataset evaluation results.
Additionally, we analyze performance differences resulting from various
pre-processing techniques and dataset preparation parameters -- offering
further insight into model generalization capabilities.

</details>


### [90] [Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach](https://arxiv.org/abs/2505.10902)
*Shuo Wang,Tong Ren,Nan Cheng,Rong Wang,Li Zhang*

Main category: cs.CV

TL;DR: A dynamic cardiac model framework using 4D-CTA and digital twin technology was developed for precise coronary intervention planning and training.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accurate simulation of cardiac physiological dynamics in existing training systems.

Method: Segmented 4D-CTA data, constructed dynamic models, simulated vessel deformation, manufactured transparent vascular models, and developed virtual angiography and guidewire reconstruction systems.

Result: Achieved 80.9% morphological consistency in angiography, Dice coefficients of 0.741-0.812 for guidewire motion, and mean trajectory errors below 1.1 mm.

Conclusion: The digital-physical twin approach effectively replicates coronary anatomy and dynamics, providing valuable tools for education and clinical planning.

Abstract: Background and Objective: Precise preoperative planning and effective
physician training for coronary interventions are increasingly important.
Despite advances in medical imaging technologies, transforming static or
limited dynamic imaging data into comprehensive dynamic cardiac models remains
challenging. Existing training systems lack accurate simulation of cardiac
physiological dynamics. This study develops a comprehensive dynamic cardiac
model research framework based on 4D-CTA, integrating digital twin technology,
computer vision, and physical model manufacturing to provide precise,
personalized tools for interventional cardiology. Methods: Using 4D-CTA data
from a 60-year-old female with three-vessel coronary stenosis, we segmented
cardiac chambers and coronary arteries, constructed dynamic models, and
implemented skeletal skinning weight computation to simulate vessel deformation
across 20 cardiac phases. Transparent vascular physical models were
manufactured using medical-grade silicone. We developed cardiac output analysis
and virtual angiography systems, implemented guidewire 3D reconstruction using
binocular stereo vision, and evaluated the system through angiography
validation and CABG training applications. Results: Morphological consistency
between virtual and real angiography reached 80.9%. Dice similarity
coefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory
errors below 1.1 mm. The transparent model demonstrated advantages in CABG
training, allowing direct visualization while simulating beating heart
challenges. Conclusion: Our patient-specific digital-physical twin approach
effectively reproduces both anatomical structures and dynamic characteristics
of coronary vasculature, offering a dynamic environment with visual and tactile
feedback valuable for education and clinical planning.

</details>


### [91] [VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross-Modal Mutual Information Maximization](https://arxiv.org/abs/2505.10917)
*Mingxiao Li,Na Su,Fang Qu,Zhizhou Zhong,Ziyang Chen,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CV

TL;DR: The paper identifies modality alignment bias in MLLMs, proposes VISTA to improve cross-modal alignment, and demonstrates superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with modality alignment, favoring text over vision, limiting effective multimodal fusion.

Method: Theoretical analysis of cross-entropy loss reveals its limitations; VISTA introduces an explicit alignment objective to maximize cross-modal mutual information.

Result: VISTA outperforms baselines on multiple benchmarks (e.g., VQAv2, MMStar, MME) without extra modules or data.

Conclusion: VISTA offers an efficient, practical solution for improving MLLM alignment, advancing multimodal research.

Abstract: Current multimodal large language models (MLLMs) face a critical challenge in
modality alignment, often exhibiting a bias towards textual information at the
expense of other modalities like vision. This paper conducts a systematic
information-theoretic analysis of the widely used cross-entropy loss in MLLMs,
uncovering its implicit alignment objective. Our theoretical investigation
reveals that this implicit objective has inherent limitations, leading to a
degradation of cross-modal alignment as text sequence length increases, thereby
hindering effective multimodal information fusion. To overcome these drawbacks,
we propose Vision-Text Alignment (VISTA), a novel approach guided by our
theoretical insights. VISTA introduces an explicit alignment objective designed
to maximize cross-modal mutual information, preventing the degradation of
visual alignment. Notably, VISTA enhances the visual understanding capabilities
of existing MLLMs without requiring any additional trainable modules or extra
training data, making it both efficient and practical. Our method significantly
outperforms baseline models across more than a dozen benchmark datasets,
including VQAv2, MMStar, and MME, paving the way for new directions in MLLM
modal alignment research.

</details>


### [92] [Towards Cross-modal Retrieval in Chinese Cultural Heritage Documents: Dataset and Solution](https://arxiv.org/abs/2505.10921)
*Junyi Yuan,Jian Zhang,Fangyu Wu,Dongming Lu,Huanda Lu,Qiufeng Wang*

Main category: cs.CV

TL;DR: The paper introduces CulTi, a multimodal dataset for Chinese cultural heritage, and LACLIP, a training-free local alignment method for cross-modal retrieval, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of specialized datasets for Chinese cultural heritage, which hinders cross-modal learning model development.

Method: Proposes CulTi dataset (5,726 image-text pairs) and LACLIP, a local alignment strategy using a fine-tuned Chinese-CLIP for weighted similarity scoring.

Result: LACLIP significantly improves cross-modal retrieval, especially for fine-grained semantic associations in cultural heritage.

Conclusion: The CulTi dataset and LACLIP method advance cross-modal retrieval for Chinese cultural heritage, addressing local alignment challenges.

Abstract: China has a long and rich history, encompassing a vast cultural heritage that
includes diverse multimodal information, such as silk patterns, Dunhuang
murals, and their associated historical narratives. Cross-modal retrieval plays
a pivotal role in understanding and interpreting Chinese cultural heritage by
bridging visual and textual modalities to enable accurate text-to-image and
image-to-text retrieval. However, despite the growing interest in multimodal
research, there is a lack of specialized datasets dedicated to Chinese cultural
heritage, limiting the development and evaluation of cross-modal learning
models in this domain. To address this gap, we propose a multimodal dataset
named CulTi, which contains 5,726 image-text pairs extracted from two series of
professional documents, respectively related to ancient Chinese silk and
Dunhuang murals. Compared to existing general-domain multimodal datasets, CulTi
presents a challenge for cross-modal retrieval: the difficulty of local
alignment between intricate decorative motifs and specialized textual
descriptions. To address this challenge, we propose LACLIP, a training-free
local alignment strategy built upon a fine-tuned Chinese-CLIP. LACLIP enhances
the alignment of global textual descriptions with local visual regions by
computing weighted similarity scores during inference. Experimental results on
CulTi demonstrate that LACLIP significantly outperforms existing models in
cross-modal retrieval, particularly in handling fine-grained semantic
associations within Chinese cultural heritage.

</details>


### [93] [M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection](https://arxiv.org/abs/2505.10931)
*Chao Wang,Wei Lu,Xiang Li,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: The paper introduces M4-SAR, a comprehensive dataset for optical-SAR fusion object detection, and proposes E2E-OSDet, a novel framework to improve detection accuracy by leveraging complementary advantages of optical and SAR images.


<details>
  <summary>Details</summary>
Motivation: Optical and SAR images have complementary strengths but face challenges like low-light conditions and speckle noise. Lack of standardized datasets hinders progress in fusion-based object detection.

Method: The authors create M4-SAR, a large-scale dataset with aligned optical-SAR pairs, and propose E2E-OSDet, an end-to-end fusion framework to address cross-domain discrepancies.

Result: Fusing optical and SAR data improves mAP by 5.7%, especially in complex environments.

Conclusion: The M4-SAR dataset and E2E-OSDet framework advance optical-SAR fusion research, providing a robust baseline for future work.

Abstract: Single-source remote sensing object detection using optical or SAR images
struggles in complex environments. Optical images offer rich textural details
but are often affected by low-light, cloud-obscured, or low-resolution
conditions, reducing the detection performance. SAR images are robust to
weather, but suffer from speckle noise and limited semantic expressiveness.
Optical and SAR images provide complementary advantages, and fusing them can
significantly improve the detection accuracy. However, progress in this field
is hindered by the lack of large-scale, standardized datasets. To address these
challenges, we propose the first comprehensive dataset for optical-SAR fusion
object detection, named Multi-resolution, Multi-polarization, Multi-scene,
Multi-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image
pairs and nearly one million labeled instances with arbitrary orientations,
spanning six key categories. To enable standardized evaluation, we develop a
unified benchmarking toolkit that integrates six state-of-the-art multi-source
fusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end
multi-source fusion detection framework that mitigates cross-domain
discrepancies and establishes a robust baseline for future studies. Extensive
experiments on M4-SAR demonstrate that fusing optical and SAR data can improve
$mAP$ by 5.7\% over single-source inputs, with particularly significant gains
in complex environments. The dataset and code are publicly available at
https://github.com/wchao0601/M4-SAR.

</details>


### [94] [Visual Anomaly Detection under Complex View-Illumination Interplay: A Large-Scale Benchmark](https://arxiv.org/abs/2505.10996)
*Yunkang Cao,Yuqi Cheng,Xiaohao Xu,Yiheng Zhang,Yihan Sun,Yuxiang Tan,Yuxin Zhang,Xiaonan Huang,Weiming Shen*

Main category: cs.CV

TL;DR: M2AD is a new benchmark for Visual Anomaly Detection (VAD) addressing robustness under varying viewpoints and illumination, revealing current methods' limitations.


<details>
  <summary>Details</summary>
Motivation: Current VAD systems struggle with real-world imaging variations like viewpoint and illumination, which are overlooked in existing benchmarks.

Method: M2AD introduces 119,880 high-resolution images of 999 specimens across 10 categories, captured under 120 view-illumination configurations. Two evaluation protocols (M2AD-Synergy and M2AD-Invariant) are established.

Result: State-of-the-art VAD methods perform poorly on M2AD, highlighting the challenge of view-illumination interplay.

Conclusion: M2AD is a critical tool for advancing VAD methods to handle real-world complexities, with the dataset and test suite publicly released.

Abstract: The practical deployment of Visual Anomaly Detection (VAD) systems is
hindered by their sensitivity to real-world imaging variations, particularly
the complex interplay between viewpoint and illumination which drastically
alters defect visibility. Current benchmarks largely overlook this critical
challenge. We introduce Multi-View Multi-Illumination Anomaly Detection (M2AD),
a new large-scale benchmark comprising 119,880 high-resolution images designed
explicitly to probe VAD robustness under such interacting conditions. By
systematically capturing 999 specimens across 10 categories using 12
synchronized views and 10 illumination settings (120 configurations total),
M2AD enables rigorous evaluation. We establish two evaluation protocols:
M2AD-Synergy tests the ability to fuse information across diverse
configurations, and M2AD-Invariant measures single-image robustness against
realistic view-illumination effects. Our extensive benchmarking shows that
state-of-the-art VAD methods struggle significantly on M2AD, demonstrating the
profound challenge posed by view-illumination interplay. This benchmark serves
as an essential tool for developing and validating VAD methods capable of
overcoming real-world complexities. Our full dataset and test suite will be
released at https://hustcyq.github.io/M2AD to facilitate the field.

</details>


### [95] [DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning](https://arxiv.org/abs/2505.10999)
*Weilai Xiang,Hongyu Yang,Di Huang,Yunhong Wang*

Main category: cs.CV

TL;DR: The paper introduces self-conditioning in diffusion models to improve both generative and discriminative performance without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address whether diffusion models' representations can enhance their own training and improve feature quality without losing generative capability.

Method: Proposes self-conditioning, a mechanism using the denoising network's semantics to guide decoding layers, forming a bottleneck for better generation and representation.

Result: Boosts generation FID and recognition accuracy with minimal overhead, outperforming self-supervised models in linear evaluations.

Conclusion: Self-conditioning effectively integrates discriminative techniques into diffusion models, enhancing both generation and representation learning.

Abstract: While diffusion models have gained prominence in image synthesis, their
generative pre-training has been shown to yield discriminative representations,
paving the way towards unified visual generation and understanding. However,
two key questions remain: 1) Can these representations be leveraged to improve
the training of diffusion models themselves, rather than solely benefiting
downstream tasks? 2) Can the feature quality be enhanced to rival or even
surpass modern self-supervised learners, without compromising generative
capability? This work addresses these questions by introducing
self-conditioning, a straightforward yet effective mechanism that internally
leverages the rich semantics inherent in denoising network to guide its own
decoding layers, forming a tighter bottleneck that condenses high-level
semantics to improve generation. Results are compelling: our method boosts both
generation FID and recognition accuracy with 1% computational overhead and
generalizes across diverse diffusion architectures. Crucially,
self-conditioning facilitates an effective integration of discriminative
techniques, such as contrastive self-distillation, directly into diffusion
models without sacrificing generation quality. Extensive experiments on
pixel-space and latent-space datasets show that in linear evaluations, our
enhanced diffusion models, particularly UViT and DiT, serve as strong
representation learners, surpassing various self-supervised models.

</details>


### [96] [ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization](https://arxiv.org/abs/2505.11003)
*Bo Du,Xuekang Zhu,Xiaochen Ma,Chenfan Qu,Kaiwen Feng,Zhe Yang,Chi-Man Pun,Jian Liu,Jizhe Zhou*

Main category: cs.CV

TL;DR: ForensicHub is a unified benchmark for Fake Image Detection and Localization (FIDL), addressing fragmentation across four domains by providing modular architecture, baseline models, and new benchmarks.


<details>
  <summary>Details</summary>
Motivation: The FIDL field is fragmented with domain silos, lacking interoperability and unified benchmarks, hindering progress.

Method: ForensicHub introduces a modular, configuration-driven architecture, implements 10 baseline models, and integrates existing benchmarks.

Result: The benchmark enables cross-domain comparisons, offers 8 actionable insights, and breaks domain silos.

Conclusion: ForensicHub advances the FIDL field by unifying benchmarks and inspiring future research.

Abstract: The field of Fake Image Detection and Localization (FIDL) is highly
fragmented, encompassing four domains: deepfake detection (Deepfake), image
manipulation detection and localization (IMDL), artificial
intelligence-generated image detection (AIGC), and document image manipulation
localization (Doc). Although individual benchmarks exist in some domains, a
unified benchmark for all domains in FIDL remains blank. The absence of a
unified benchmark results in significant domain silos, where each domain
independently constructs its datasets, models, and evaluation protocols without
interoperability, preventing cross-domain comparisons and hindering the
development of the entire FIDL field. To close the domain silo barrier, we
propose ForensicHub, the first unified benchmark & codebase for all-domain fake
image detection and localization. Considering drastic variations on dataset,
model, and evaluation configurations across all domains, as well as the
scarcity of open-sourced baseline models and the lack of individual benchmarks
in some domains, ForensicHub: i) proposes a modular and configuration-driven
architecture that decomposes forensic pipelines into interchangeable components
across datasets, transforms, models, and evaluators, allowing flexible
composition across all domains; ii) fully implements 10 baseline models, 6
backbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing
benchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii)
conducts indepth analysis based on the ForensicHub, offering 8 key actionable
insights into FIDL model architecture, dataset characteristics, and evaluation
standards. ForensicHub represents a significant leap forward in breaking the
domain silos in the FIDL field and inspiring future breakthroughs.

</details>


### [97] [Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion](https://arxiv.org/abs/2505.11013)
*Zongye Zhang,Bohan Kong,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: MoMADiff combines masked modeling and diffusion processes for robust 3D human motion generation from text, offering fine-grained control and strong generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with out-of-distribution motions and lack fine-grained control, limiting real-world applicability.

Method: MoMADiff integrates masked modeling with diffusion processes, using frame-level continuous representations and flexible keyframe specifications.

Result: Outperforms state-of-the-art models in motion quality, instruction fidelity, and keyframe adherence on held-out and benchmark datasets.

Conclusion: MoMADiff addresses limitations of existing methods, providing robust and controllable motion generation for diverse real-world scenarios.

Abstract: Generating 3D human motion from text descriptions remains challenging due to
the diverse and complex nature of human motion. While existing methods excel
within the training distribution, they often struggle with out-of-distribution
motions, limiting their applicability in real-world scenarios. Existing
VQVAE-based methods often fail to represent novel motions faithfully using
discrete tokens, which hampers their ability to generalize beyond seen data.
Meanwhile, diffusion-based methods operating on continuous representations
often lack fine-grained control over individual frames. To address these
challenges, we propose a robust motion generation framework MoMADiff, which
combines masked modeling with diffusion processes to generate motion using
frame-level continuous representations. Our model supports flexible
user-provided keyframe specification, enabling precise control over both
spatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong
generalization capability on novel text-to-motion datasets with sparse
keyframes as motion prompts. Extensive experiments on two held-out datasets and
two standard benchmarks show that our method consistently outperforms
state-of-the-art models in motion quality, instruction fidelity, and keyframe
adherence.

</details>


### [98] [WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?](https://arxiv.org/abs/2505.11015)
*An-Lan Wang,Jingqun Tang,Liao Lei,Hao Feng,Qi Liu,Xiang Fei,Jinghui Lu,Han Wang,Weiwei Liu,Hao Liu,Yuliang Liu,Xiang Bai,Can Huang*

Main category: cs.CV

TL;DR: WildDoc is a new benchmark for assessing document understanding in natural environments, highlighting performance declines of MLLMs in real-world conditions compared to traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like DocVQA and ChartQA focus on scanned or digital documents, failing to address real-world challenges like variable illumination and physical distortions.

Method: WildDoc includes manually captured document images under diverse real-world conditions, with each document captured four times under different scenarios.

Result: Evaluations show significant performance drops in state-of-the-art MLLMs on WildDoc, revealing their lack of robustness in real-world settings.

Conclusion: WildDoc underscores the need for improved robustness in document understanding models for real-world applications.

Abstract: The rapid advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced capabilities in Document Understanding. However,
prevailing benchmarks like DocVQA and ChartQA predominantly comprise
\textit{scanned or digital} documents, inadequately reflecting the intricate
challenges posed by diverse real-world scenarios, such as variable illumination
and physical distortions. This paper introduces WildDoc, the inaugural
benchmark designed specifically for assessing document understanding in natural
environments. WildDoc incorporates a diverse set of manually captured document
images reflecting real-world conditions and leverages document sources from
established benchmarks to facilitate comprehensive comparisons with digital or
scanned documents. Further, to rigorously evaluate model robustness, each
document is captured four times under different conditions. Evaluations of
state-of-the-art MLLMs on WildDoc expose substantial performance declines and
underscore the models' inadequate robustness compared to traditional
benchmarks, highlighting the unique challenges posed by real-world document
understanding. Our project homepage is available at
https://bytedance.github.io/WildDoc.

</details>


### [99] [Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning](https://arxiv.org/abs/2505.11018)
*Pengchen Zhang,Alan J. X. Guo,Sipin Luo,Zhe Han,Lin Guo*

Main category: cs.CV

TL;DR: The paper proposes Dual Teacher-Student Learning (DTSL), a semi-supervised medical image segmentation method using cross-architectural models and a consensus label generator for improved performance.


<details>
  <summary>Details</summary>
Motivation: To reduce manual annotation costs in medical image segmentation by leveraging semi-supervised learning and improving the mean teacher strategy.

Method: Introduces DTSL with two teacher-student model groups of different architectures, using output agreement and a Jensen-Shannon divergence-based consensus label generator for pseudo-labels.

Result: Outperforms state-of-the-art methods on popular datasets, with ablation studies confirming module effectiveness.

Conclusion: DTSL enhances semi-supervised segmentation by integrating cross-architectural models and consensus-based pseudo-labeling, demonstrating superior performance.

Abstract: Semi-supervised medical image segmentation has attracted significant
attention due to its potential to reduce manual annotation costs. The mean
teacher (MT) strategy, commonly understood as introducing smoothed, temporally
lagged consistency regularization, has demonstrated strong performance across
various tasks in this field. In this work, we reinterpret the MT strategy on
supervised data as a form of self-paced learning, regulated by the output
agreement between the temporally lagged teacher model and the ground truth
labels. This idea is further extended to incorporate agreement between a
temporally lagged model and a cross-architectural model, which offers greater
flexibility in regulating the learning pace and enables application to
unlabeled data. Specifically, we propose dual teacher-student learning (DTSL),
a framework that introduces two groups of teacher-student models with different
architectures. The output agreement between the cross-group teacher and student
models is used as pseudo-labels, generated via a Jensen-Shannon
divergence-based consensus label generator (CLG). Extensive experiments on
popular datasets demonstrate that the proposed method consistently outperforms
existing state-of-the-art approaches. Ablation studies further validate the
effectiveness of the proposed modules.

</details>


### [100] [Classifying Shelf Life Quality of Pineapples by Combining Audio and Visual Features](https://arxiv.org/abs/2505.11020)
*Yi-Lu Jiang,Wen-Chang Chang,Ching-Lin Wang,Kung-Liang Hsu,Chih-Yi Chiu*

Main category: cs.CV

TL;DR: A multimodal and multiview classification model was developed to classify pineapple quality using audio and visual features, achieving 84% accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce waste and increase income by non-destructively determining pineapple shelf life quality.

Method: Constructed a cross-modal classification model using audio and visual data from the PQC500 dataset, employing contrastive audiovisual masked autoencoder training and efficient data sampling.

Result: The proposed model achieved 84% accuracy, outperforming unimodal audio (78%) and visual (66%) models.

Conclusion: Cross-modal training with audio-major sampling is effective for pineapple quality classification.

Abstract: Determining the shelf life quality of pineapples using non-destructive
methods is a crucial step to reduce waste and increase income. In this paper, a
multimodal and multiview classification model was constructed to classify
pineapples into four quality levels based on audio and visual characteristics.
For research purposes, we compiled and released the PQC500 dataset consisting
of 500 pineapples with two modalities: one was tapping pineapples to record
sounds by multiple microphones and the other was taking pictures by multiple
cameras at different locations, providing multimodal and multi-view audiovisual
features. We modified the contrastive audiovisual masked autoencoder to train
the cross-modal-based classification model by abundant combinations of audio
and visual pairs. In addition, we proposed to sample a compact size of training
data for efficient computation. The experiments were evaluated under various
data and model configurations, and the results demonstrated that the proposed
cross-modal model trained using audio-major sampling can yield 84% accuracy,
outperforming the unimodal models of only audio and only visual by 6% and 18%,
respectively.

</details>


### [101] [CleanPatrick: A Benchmark for Image Data Cleaning](https://arxiv.org/abs/2505.11034)
*Fabian Gröger,Simone Lionetti,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Elisabeth Victoria Goessinger,Hanna Lindemann,Marie Bargiela,Marie Hofbauer,Omar Badri,Philipp Tschandl,Arash Koochek,Matthew Groh,Alexander A. Navarini,Marc Pouly*

Main category: cs.CV

TL;DR: CleanPatrick is a large-scale benchmark for image data cleaning, using the Fitzpatrick17k dataset with crowdsourced annotations to identify off-topic samples, near-duplicates, and label errors. It evaluates various methods, showing self-supervised representations excel in near-duplicate detection, while label-error detection remains challenging.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for image data cleaning rely on synthetic noise or limited human studies, lacking real-world relevance and comparability. CleanPatrick addresses this gap by providing a large-scale, realistic benchmark.

Method: CleanPatrick uses the Fitzpatrick17k dataset, collecting 496,377 binary annotations from 933 medical crowd workers. It identifies off-topic samples, near-duplicates, and label errors, employing an aggregation model inspired by item-response theory and expert review for high-quality ground truth.

Result: Self-supervised representations perform well in near-duplicate detection, classical methods are competitive for off-topic detection under budget constraints, and label-error detection remains challenging for fine-grained medical classification.

Conclusion: CleanPatrick enables systematic comparison of image-cleaning strategies, advancing reliable data-centric AI by providing a realistic benchmark and evaluation framework.

Abstract: Robust machine learning depends on clean data, yet current image data
cleaning benchmarks rely on synthetic noise or narrow human studies, limiting
comparison and real-world relevance. We introduce CleanPatrick, the first
large-scale benchmark for data cleaning in the image domain, built upon the
publicly available Fitzpatrick17k dermatology dataset. We collect 496,377
binary annotations from 933 medical crowd workers, identify off-topic samples
(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation
model inspired by item-response theory followed by expert review to derive
high-quality ground truth. CleanPatrick formalizes issue detection as a ranking
task and adopts typical ranking metrics mirroring real audit workflows.
Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident
Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick,
self-supervised representations excel at near-duplicate detection, classical
methods achieve competitive off-topic detection under constrained review
budgets, and label-error detection remains an open challenge for fine-grained
medical classification. By releasing both the dataset and the evaluation
framework, CleanPatrick enables a systematic comparison of image-cleaning
strategies and paves the way for more reliable data-centric artificial
intelligence.

</details>


### [102] [Artifacts of Idiosyncracy in Global Street View Data](https://arxiv.org/abs/2505.11046)
*Tim Alpherts,Sennay Ghebreab,Nanne van Noord*

Main category: cs.CV

TL;DR: The paper highlights biases in street view data due to city idiosyncrasies, proposes a method to evaluate these biases, and includes a case study of Amsterdam to address representation issues.


<details>
  <summary>Details</summary>
Motivation: Street view data is assumed to represent cities systematically, but prior work shows coverage gaps. This paper investigates how city-specific traits introduce biases even in densely sampled data.

Method: Quantitative analysis of street view data coverage biases across 28 cities, proposing an evaluation method. A case study of Amsterdam with semi-structured interviews explores collection process impacts.

Result: Biases in street view data distribution are identified, linked to city layout and collection processes. The case study reveals how these biases affect representation.

Conclusion: City idiosyncrasies cause biases in street view data. The proposed evaluation method and case study provide insights to address these biases at their source.

Abstract: Street view data is increasingly being used in computer vision applications
in recent years. Machine learning datasets are collected for these applications
using simple sampling techniques. These datasets are assumed to be a systematic
representation of cities, especially when densely sampled. Prior works however,
show that there are clear gaps in coverage, with certain cities or regions
being covered poorly or not at all. Here we demonstrate that a cities'
idiosyncracies, such as city layout, may lead to biases in street view data for
28 cities across the globe, even when they are densely covered. We
quantitatively uncover biases in the distribution of coverage of street view
data and propose a method for evaluation of such distributions to get better
insight in idiosyncracies in a cities' coverage. In addition, we perform a case
study of Amsterdam with semi-structured interviews, showing how idiosyncracies
of the collection process impact representation of cities and regions and
allowing us to address biases at their source.

</details>


### [103] [CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs](https://arxiv.org/abs/2505.11060)
*David Méndez,Gianpaolo Bontempo,Elisa Ficarra,Roberto Confalonieri,Natalia Díaz-Rodríguez*

Main category: cs.CV

TL;DR: CUBIC is a novel method for unsupervised bias identification in deep vision models, using concept embeddings to discover interpretable biases without predefined candidates or failure examples.


<details>
  <summary>Details</summary>
Motivation: Deep vision models often learn spurious correlations from datasets, and identifying these biases requires human-understandable concepts, which are labor-intensive to annotate.

Method: CUBIC leverages image-text latent space and linear classifier probes to measure how concepts influence the latent representation of a superclass label, identifying biases by shifts against the classifier's decision boundary.

Result: CUBIC effectively uncovers unknown biases in Vision-Language Models (VLMs) without needing prior knowledge or failure samples.

Conclusion: CUBIC provides a scalable, unsupervised solution for bias identification in deep vision models, eliminating the need for extensive annotations or predefined bias candidates.

Abstract: Deep vision models often rely on biases learned from spurious correlations in
datasets. To identify these biases, methods that interpret high-level,
human-understandable concepts are more effective than those relying primarily
on low-level features like heatmaps. A major challenge for these concept-based
methods is the lack of image annotations indicating potentially bias-inducing
concepts, since creating such annotations requires detailed labeling for each
dataset and concept, which is highly labor-intensive. We present CUBIC (Concept
embeddings for Unsupervised Bias IdentifiCation), a novel method that
automatically discovers interpretable concepts that may bias classifier
behavior. Unlike existing approaches, CUBIC does not rely on predefined bias
candidates or examples of model failures tied to specific biases, as such
information is not always available. Instead, it leverages image-text latent
space and linear classifier probes to examine how the latent representation of
a superclass label$\unicode{x2014}$shared by all instances in the
dataset$\unicode{x2014}$is influenced by the presence of a given concept. By
measuring these shifts against the normal vector to the classifier's decision
boundary, CUBIC identifies concepts that significantly influence model
predictions. Our experiments demonstrate that CUBIC effectively uncovers
previously unknown biases using Vision-Language Models (VLMs) without requiring
the samples in the dataset where the classifier underperforms or prior
knowledge of potential biases.

</details>


### [104] [HSRMamba: Efficient Wavelet Stripe State Space Model for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2505.11062)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: HSRMamba improves single hyperspectral image super-resolution by reducing artifacts and modal conflicts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing artifacts in Visual Mamba's 1D scanning and modal conflicts between spatial and spectral features.

Method: Introduces strip-based scanning and wavelet decomposition to enhance efficiency and performance.

Result: Achieves state-of-the-art results with reduced computational load and model size.

Conclusion: HSRMamba effectively balances performance and efficiency in hyperspectral image super-resolution.

Abstract: Single hyperspectral image super-resolution (SHSR) aims to restore
high-resolution images from low-resolution hyperspectral images. Recently, the
Visual Mamba model has achieved an impressive balance between performance and
computational efficiency. However, due to its 1D scanning paradigm, the model
may suffer from potential artifacts during image generation. To address this
issue, we propose HSRMamba. While maintaining the computational efficiency of
Visual Mamba, we introduce a strip-based scanning scheme to effectively reduce
artifacts from global unidirectional scanning. Additionally, HSRMamba uses
wavelet decomposition to alleviate modal conflicts between high-frequency
spatial features and low-frequency spectral features, further improving
super-resolution performance. Extensive experiments show that HSRMamba not only
excels in reducing computational load and model size but also outperforms
existing methods, achieving state-of-the-art results.

</details>


### [105] [Towards Self-Improvement of Diffusion Models via Group Preference Optimization](https://arxiv.org/abs/2505.11070)
*Renjie Chen,Wenfeng Lin,Yichen Zhang,Jiangchuan Wei,Boyuan Liu,Chao Feng,Jiao Ran,Mingyu Guo*

Main category: cs.CV

TL;DR: The paper introduces Group Preference Optimization (GPO), an improved method for aligning text-to-image diffusion models by addressing challenges in Direct Preference Optimization (DPO), such as sensitivity to preference pairs and data annotation costs. GPO extends DPO to groupwise comparisons and incorporates reward standardization, achieving performance gains without additional data.


<details>
  <summary>Details</summary>
Motivation: DPO's limitations in handling preference pairs with marginal differences and the high cost of data annotation motivate the development of GPO.

Method: GPO extends DPO from pairwise to groupwise comparisons and uses reward standardization for reweighting. It leverages the model's own capabilities for self-improvement without external data.

Result: GPO improves Stable Diffusion 3.5 Medium's counting and text rendering by 20 percentage points when combined with models like YOLO and OCR. It introduces no extra inference overhead.

Conclusion: GPO is a plug-and-play method that effectively enhances diffusion models' performance without additional data or inference costs, demonstrating broad applicability.

Abstract: Aligning text-to-image (T2I) diffusion models with Direct Preference
Optimization (DPO) has shown notable improvements in generation quality.
However, applying DPO to T2I faces two challenges: the sensitivity of DPO to
preference pairs and the labor-intensive process of collecting and annotating
high-quality data. In this work, we demonstrate that preference pairs with
marginal differences can degrade DPO performance. Since DPO relies exclusively
on relative ranking while disregarding the absolute difference of pairs, it may
misclassify losing samples as wins, or vice versa. We empirically show that
extending the DPO from pairwise to groupwise and incorporating reward
standardization for reweighting leads to performance gains without explicit
data selection. Furthermore, we propose Group Preference Optimization (GPO), an
effective self-improvement method that enhances performance by leveraging the
model's own capabilities without requiring external data. Extensive experiments
demonstrate that GPO is effective across various diffusion models and tasks.
Specifically, combining with widely used computer vision models, such as YOLO
and OCR, the GPO improves the accurate counting and text rendering capabilities
of the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a
plug-and-play method, no extra overhead is introduced during inference.

</details>


### [106] [Pseudo-Label Quality Decoupling and Correction for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.11075)
*Jianghang Lin,Yilin Lu,Yunhang Shen,Chaoyang Zhu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: The paper introduces PL-DC, a framework for Semi-Supervised Instance Segmentation (SSIS) that decouples class and mask quality assessments, corrects instance categories dynamically, and re-weights mask loss to improve performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of unstable performance in SSIS due to noisy pseudo-labels by decoupling class and mask quality assessments and correcting errors dynamically.

Method: Proposes a dual-threshold filtering mechanism, dynamic instance category correction, and pixel-level mask uncertainty-aware mechanism.

Result: Achieves significant improvements (+11.6 mAP with 1% COCO data, +15.5 mAP with 5% Cityscapes data), setting new state-of-the-art results.

Conclusion: PL-DC effectively tackles pseudo-label noise in SSIS, demonstrating robust performance even with minimal labeled data.

Abstract: Semi-Supervised Instance Segmentation (SSIS) involves classifying and
grouping image pixels into distinct object instances using limited labeled
data. This learning paradigm usually faces a significant challenge of unstable
performance caused by noisy pseudo-labels of instance categories and pixel
masks. We find that the prevalent practice of filtering instance pseudo-labels
assessing both class and mask quality with a single score threshold, frequently
leads to compromises in the trade-off between the qualities of class and mask
labels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and
Correction (PL-DC) framework for SSIS to tackle the above challenges. Firstly,
at the instance level, a decoupled dual-threshold filtering mechanism is
designed to decouple class and mask quality estimations for instance-level
pseudo-labels, thereby independently controlling pixel classifying and grouping
qualities. Secondly, at the category level, we introduce a dynamic instance
category correction module to dynamically correct the pseudo-labels of instance
categories, effectively alleviating category confusion. Lastly, we introduce a
pixel-level mask uncertainty-aware mechanism at the pixel level to re-weight
the mask loss for different pixels, thereby reducing the impact of noise
introduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO
and Cityscapes datasets demonstrate that the proposed PL-DC achieves
significant performance improvements, setting new state-of-the-art results for
SSIS. Notably, our PL-DC shows substantial gains even with minimal labeled
data, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and
+15.5 mAP with 5% Cityscapes labeled data. The code will be public.

</details>


### [107] [Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification](https://arxiv.org/abs/2505.11099)
*Bin Liu,Chunyang Wang,Xuelian Liu,Guan Xi,Ge Zhang,Ziteng Yao,Mengxue Dong*

Main category: cs.CV

TL;DR: Hybrid-Emba3D, a bidirectional Mamba model, enhances point cloud classification by coupling geometry-feature and cross-path hybridization, achieving 95.99% accuracy on ModelNet40 with minimal added parameters.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of extracting local geometric features efficiently while managing model complexity, overcoming the limitations of unidirectional Mamba architectures for unordered point clouds.

Method: Proposes Hybrid-Emba3D with geometry-feature coupling and cross-path hybridization, enhancing local feature discriminative power and handling local mutations.

Result: Achieves state-of-the-art 95.99% classification accuracy on ModelNet40 with only 0.03M additional parameters.

Conclusion: Hybrid-Emba3D effectively balances local geometric feature extraction and global modeling, outperforming traditional SSM approaches.

Abstract: The point cloud classification tasks face the dual challenge of efficiently
extracting local geometric features while maintaining model complexity. The
Mamba architecture utilizes the linear complexity advantage of state space
models (SSMs) to overcome the computational bottleneck of Transformers while
balancing global modeling capabilities. However, the inherent contradiction
between its unidirectional dependency and the unordered nature of point clouds
impedes modeling spatial correlation in local neighborhoods, thus constraining
geometric feature extraction. This paper proposes Hybrid-Emba3D, a
bidirectional Mamba model enhanced by geometry-feature coupling and cross-path
feature hybridization. The Local geometric pooling with geometry-feature
coupling mechanism significantly enhances local feature discriminative power
via coordinated propagation and dynamic aggregation of geometric information
between local center points and their neighborhoods, without introducing
additional parameters. The designed Collaborative feature enhancer adopts
dual-path hybridization, effectively handling local mutations and sparse key
signals, breaking through the limitations of traditional SSM long-range
modeling. Experimental results demonstrate that the proposed model achieves a
new SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M
additional.

</details>


### [108] [MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark](https://arxiv.org/abs/2505.11109)
*Florinel-Alin Croitoru,Vlad Hondru,Marius Popescu,Radu Tudor Ionescu,Fahad Shahbaz Khan,Mubarak Shah*

Main category: cs.CV

TL;DR: A large-scale multilingual audio-video deepfake detection benchmark is introduced, featuring 250+ hours of real and fake videos across eight languages, with 60% generated content. The dataset includes challenging open-set evaluations, and tests show current detectors fail in these scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive open-set benchmark for multilingual audio-video deepfake detection, enabling robust evaluation of detectors.

Method: The dataset includes real and fake videos across eight languages, with fake content generated by seven distinct models. Training, validation, and test splits are designed to create open-set challenges.

Result: State-of-the-art detectors perform poorly in open-set scenarios, highlighting their limitations.

Conclusion: The benchmark reveals gaps in current deepfake detection methods, emphasizing the need for more robust solutions.

Abstract: We present the first large-scale open-set benchmark for multilingual
audio-video deepfake detection. Our dataset comprises over 250 hours of real
and fake videos across eight languages, with 60% of data being generated. For
each language, the fake videos are generated with seven distinct deepfake
generation models, selected based on the quality of the generated content. We
organize the training, validation and test splits such that only a subset of
the chosen generative models and languages are available during training, thus
creating several challenging open-set evaluation setups. We perform experiments
with various pre-trained and fine-tuned deepfake detectors proposed in recent
literature. Our results show that state-of-the-art detectors are not currently
able to maintain their performance levels when tested in our open-set
scenarios. We publicly release our data and code at:
https://huggingface.co/datasets/unibuc-cs/MAVOS-DD.

</details>


### [109] [Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation](https://arxiv.org/abs/2505.11110)
*Massimiliano Cassia,Luca Guarnera,Mirko Casu,Ignazio Zangara,Sebastiano Battiato*

Main category: cs.CV

TL;DR: A forensic framework for identifying GAN-generated images' training datasets using spectral transforms, color metrics, and local features, achieving high accuracy and addressing legal/ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Challenges in verifying authenticity and tracing origins of GAN-generated media, impacting copyright, privacy, and legal compliance.

Method: Integrates spectral transforms (Fourier/DCT), color distribution metrics, and local descriptors (SIFT) to extract features, using supervised classifiers (Random Forest, SVM, XGBoost).

Result: 98-99% accuracy in binary and multi-class classification, with frequency-domain features proving most effective.

Conclusion: The framework enhances accountability in generative modeling, aiding digital forensics, content moderation, and legal compliance.

Abstract: Synthetic media generated by Generative Adversarial Networks (GANs) pose
significant challenges in verifying authenticity and tracing dataset origins,
raising critical concerns in copyright enforcement, privacy protection, and
legal compliance. This paper introduces a novel forensic framework for
identifying the training dataset (e.g., CelebA or FFHQ) of GAN-generated images
through interpretable feature analysis. By integrating spectral transforms
(Fourier/DCT), color distribution metrics, and local feature descriptors
(SIFT), our pipeline extracts discriminative statistical signatures embedded in
synthetic outputs. Supervised classifiers (Random Forest, SVM, XGBoost) achieve
98-99% accuracy in binary classification (real vs. synthetic) and multi-class
dataset attribution across diverse GAN architectures (StyleGAN, AttGAN, GDWCT,
StarGAN, and StyleGAN2). Experimental results highlight the dominance of
frequency-domain features (DCT/FFT) in capturing dataset-specific artifacts,
such as upsampling patterns and spectral irregularities, while color histograms
reveal implicit regularization strategies in GAN training. We further examine
legal and ethical implications, showing how dataset attribution can address
copyright infringement, unauthorized use of personal data, and regulatory
compliance under frameworks like GDPR and California's AB 602. Our framework
advances accountability and governance in generative modeling, with
applications in digital forensics, content moderation, and intellectual
property litigation.

</details>


### [110] [Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing](https://arxiv.org/abs/2505.11121)
*Mathis Jürgen Adler,Leonard Hackel,Gencer Sumbul,Begüm Demir*

Main category: cs.CV

TL;DR: The paper introduces a weighted feature aggregation (WFA) strategy for vision-language model (VLM) pretraining in remote sensing to handle redundant captions, using uniqueness and attention-based techniques for efficient training and improved performance.


<details>
  <summary>Details</summary>
Motivation: Redundant captions in pretraining data increase computational costs and reduce efficiency. The goal is to extract complementary information while minimizing redundancies.

Method: Proposes WFA with two techniques: (i) non-parametric uniqueness (BLEU-based weights) and (ii) learning-based attention (adaptive weights).

Result: WFA improves efficiency and downstream performance in text-to-image retrieval, with guidelines for technique selection based on task needs.

Conclusion: The WFA strategy effectively reduces redundancy and enhances VLM pretraining, offering practical solutions for remote sensing applications.

Abstract: The development of foundation models through pretraining of vision-language
models (VLMs) has recently attracted great attention in remote sensing (RS).
VLM pretraining aims to learn image and language alignments from a large number
of image-text pairs. Each pretraining image is often associated with multiple
captions containing redundant information due to repeated or semantically
similar phrases, resulting in increased pretraining and inference time. To
overcome this, we introduce a weighted feature aggregation (WFA) strategy for
VLM pretraining in RS. Our strategy aims to extract and exploit complementary
information from multiple captions per image while reducing redundancies
through feature aggregation with importance weighting. To calculate adaptive
importance weights for different captions of each image, we propose two
techniques: (i) non-parametric uniqueness and (ii) learning-based attention. In
the first technique, importance weights are calculated based on the bilingual
evaluation understudy (BLEU) scores of the captions to emphasize unique
sentences and reduce the influence of repetitive ones. In the second technique,
importance weights are learned through an attention mechanism instead of
relying on hand-crafted features. The effectiveness of the proposed WFA
strategy with the two techniques is analyzed in terms of downstream performance
on text-to-image retrieval in RS. Experimental results show that the proposed
strategy enables efficient and effective pretraining of VLMs in RS. Based on
the experimental analysis, we derive guidelines for selecting appropriate
techniques depending on downstream task requirements and resource constraints.
The code of this work is publicly available at
https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.

</details>


### [111] [PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video](https://arxiv.org/abs/2505.11129)
*Makoto Yamada,Kian Ming A. Chai,Ayoub Rhim,Satoki Ishikawa,Mohammad Sabokrou,Yao-Hung Hubert Tsai*

Main category: cs.CV

TL;DR: PhiNet v2 is a Transformer-based SSL model for temporal visual input, outperforming state-of-the-art models without strong augmentation, inspired by human visual processing.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SSL in computer vision and biological visual processing by introducing a biologically plausible model.

Method: Uses a Transformer-based architecture with variational inference to process sequential image inputs without strong augmentation.

Result: Achieves competitive performance against state-of-the-art vision models while learning from sequential input.

Conclusion: PhiNet v2 advances biologically plausible computer vision, aligning more closely with human cognitive processes.

Abstract: Recent advances in self-supervised learning (SSL) have revolutionized
computer vision through innovative architectures and learning objectives, yet
they have not fully leveraged insights from biological visual processing
systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is
based on a ResNet backbone and operates on static image inputs with strong
augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based
architecture that processes temporal visual input (that is, sequences of
images) without relying on strong augmentation. Our model leverages variational
inference to learn robust visual representations from continuous input streams,
similar to human visual processing. Through extensive experimentation, we
demonstrate that PhiNet v2 achieves competitive performance compared to
state-of-the-art vision foundation models, while maintaining the ability to
learn from sequential input without strong data augmentation. This work
represents a significant step toward more biologically plausible computer
vision systems that process visual information in a manner more closely aligned
with human cognitive processes.

</details>


### [112] [One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework](https://arxiv.org/abs/2505.11131)
*Feiran Li,Qianqian Xu,Shilong Bao,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: The paper introduces Co-Erasing, a text-image collaborative framework for concept erasure in diffusion models, improving efficacy and usability by integrating visual supervision.


<details>
  <summary>Details</summary>
Motivation: Current concept erasure methods rely on text prompts, creating a gap between text and image modalities, limiting efficacy and usability.

Method: Co-Erasing combines text prompts and undesirable images for negative guidance, refining concepts via text-guided image refinement.

Result: Co-Erasing outperforms state-of-the-art methods, balancing efficacy (erasure) and usability (minimal impact on benign concepts).

Conclusion: The framework effectively bridges the text-image gap, enhancing concept erasure while preserving other benign features.

Abstract: Concept erasing has recently emerged as an effective paradigm to prevent
text-to-image diffusion models from generating visually undesirable or even
harmful content. However, current removal methods heavily rely on manually
crafted text prompts, making it challenging to achieve a high erasure
(efficacy) while minimizing the impact on other benign concepts (usability). In
this paper, we attribute the limitations to the inherent gap between the text
and image modalities, which makes it hard to transfer the intricately entangled
concept knowledge from text prompts to the image generation process. To address
this, we propose a novel solution by directly integrating visual supervision
into the erasure process, introducing the first text-image Collaborative
Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the
concept jointly by text prompts and the corresponding undesirable images
induced by the prompts, and then reduces the generating probability of the
target concept through negative guidance. This approach effectively bypasses
the knowledge gap between text and image, significantly enhancing erasure
efficacy. Additionally, we design a text-guided image concept refinement
strategy that directs the model to focus on visual features most relevant to
the specified text concept, minimizing disruption to other benign concepts.
Finally, comprehensive experiments suggest that Co-Erasing outperforms
state-of-the-art erasure approaches significantly with a better trade-off
between efficacy and usability. Codes are available at
https://github.com/Ferry-Li/Co-Erasing.

</details>


### [113] [Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans](https://arxiv.org/abs/2505.11141)
*Yansheng Qiu,Li Xiao,Zhaopan Xu,Pengfei Zhou,Zheng Wang,Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces Human-Aligned Bench, a benchmark to evaluate multimodal reasoning in MLLMs against human performance, revealing significant gaps.


<details>
  <summary>Details</summary>
Motivation: To assess whether current MLLMs match human reasoning capabilities in multimodal tasks, addressing a gap in existing benchmarks.

Method: Developed a benchmark with 9,794 multimodal questions (bilingual and text-based) across four reasoning types, incorporating human success rates and error-prone options.

Result: Experiments show notable differences between MLLM and human performance in multimodal reasoning.

Conclusion: The benchmark highlights gaps in MLLM reasoning, guiding future model development.

Abstract: The goal of achieving Artificial General Intelligence (AGI) is to imitate
humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have
demonstrated that large language models (LLMs) with human-like reasoning
capabilities exhibit exceptional performance and are being gradually integrated
into multimodal large language models (MLLMs). However, whether these models
possess capabilities comparable to humans in handling reasoning tasks remains
unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark
for fine-grained alignment of multimodal reasoning with human performance.
Specifically, we collected 9,794 multimodal questions that solely rely on
contextual reasoning, including bilingual (Chinese and English) multimodal
questions and pure text-based questions, encompassing four question types:
visual reasoning, definition judgment, analogical reasoning, and logical
judgment. More importantly, each question is accompanied by human success rates
and options that humans are prone to choosing incorrectly. Extensive
experiments on the Human-Aligned Bench reveal notable differences between the
performance of current MLLMs in multimodal reasoning and human performance. The
findings on our benchmark provide insights into the development of the
next-generation models.

</details>


### [114] [Learning Dense Hand Contact Estimation from Imbalanced Data](https://arxiv.org/abs/2505.11152)
*Daniel Sungho Jung,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: The paper introduces HACO, a framework for dense hand contact estimation, addressing class and spatial imbalance issues in datasets using balanced contact sampling and vertex-level class-balanced loss.


<details>
  <summary>Details</summary>
Motivation: Understanding hand interactions is crucial, but existing datasets suffer from class and spatial imbalances, making dense hand contact estimation challenging.

Method: Proposes balanced contact sampling for class imbalance and vertex-level class-balanced (VCB) loss for spatial imbalance.

Result: The framework effectively predicts dense hand contact without suffering from imbalance issues.

Conclusion: HACO successfully addresses dataset imbalances, improving dense hand contact estimation.

Abstract: Hands are essential to human interaction, and understanding contact between
hands and the world can promote comprehensive understanding of their function.
Recently, there have been growing number of hand interaction datasets that
cover interaction with object, other hand, scene, and body. Despite the
significance of the task and increasing high-quality data, how to effectively
learn dense hand contact estimation remains largely underexplored. There are
two major challenges for learning dense hand contact estimation. First, there
exists class imbalance issue from hand contact datasets where majority of
samples are not in contact. Second, hand contact datasets contain spatial
imbalance issue with most of hand contact exhibited in finger tips, resulting
in challenges for generalization towards contacts in other hand regions. To
tackle these issues, we present a framework that learns dense HAnd COntact
estimation (HACO) from imbalanced data. To resolve the class imbalance issue,
we introduce balanced contact sampling, which builds and samples from multiple
sampling groups that fairly represent diverse contact statistics for both
contact and non-contact samples. Moreover, to address the spatial imbalance
issue, we propose vertex-level class-balanced (VCB) loss, which incorporates
spatially varying contact distribution by separately reweighting loss
contribution of each vertex based on its contact frequency across dataset. As a
result, we effectively learn to predict dense hand contact estimation with
large-scale hand contact data without suffering from class and spatial
imbalance issue. The codes will be released.

</details>


### [115] [CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer](https://arxiv.org/abs/2505.11168)
*Xinran Li,Yu Liu,Xiujuan Xu,Xiaowei Zhao*

Main category: cs.CV

TL;DR: CheX-DS combines CNNs and Transformers for chest X-ray classification, addressing data imbalance and outperforming previous methods with an AUC of 83.76%.


<details>
  <summary>Details</summary>
Motivation: Current CNN-based methods for chest disease diagnosis neglect global features, while self-attention mechanisms show promise.

Method: CheX-DS integrates DenseNet (CNN) and Swin Transformer (self-attention) using ensemble deep learning, with a loss function combining weighted binary cross-entropy and asymmetric loss.

Result: Achieves an average AUC of 83.76% on the NIH ChestX-ray14 dataset, surpassing prior studies.

Conclusion: CheX-DS effectively combines CNNs and Transformers, addressing data imbalance and improving performance in chest X-ray classification.

Abstract: The automatic diagnosis of chest diseases is a popular and challenging task.
Most current methods are based on convolutional neural networks (CNNs), which
focus on local features while neglecting global features. Recently,
self-attention mechanisms have been introduced into the field of computer
vision, demonstrating superior performance. Therefore, this paper proposes an
effective model, CheX-DS, for classifying long-tail multi-label data in the
medical field of chest X-rays. The model is based on the excellent CNN model
DenseNet for medical imaging and the newly popular Swin Transformer model,
utilizing ensemble deep learning techniques to combine the two models and
leverage the advantages of both CNNs and Transformers. The loss function of
CheX-DS combines weighted binary cross-entropy loss with asymmetric loss,
effectively addressing the issue of data imbalance. The NIH ChestX-ray14
dataset is selected to evaluate the model's effectiveness. The model
outperforms previous studies with an excellent average AUC score of 83.76\%,
demonstrating its superior performance.

</details>


### [116] [CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback](https://arxiv.org/abs/2505.11178)
*Yixin Wan,Kai-Wei Chang*

Main category: cs.CV

TL;DR: CompAlign is a benchmark for evaluating T2I models on compositional image generation, focusing on 3D-spatial relationships. CompQuest provides fine-grained feedback, and an alignment framework improves model performance.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with accurately depicting complex compositional scenes involving multiple objects, attributes, and spatial relations.

Method: Introduces CompAlign (900 prompts for evaluation) and CompQuest (evaluation framework using MLLM for fine-grained feedback). Proposes an alignment framework using CompQuest's feedback.

Result: Evaluation shows models struggle with complex 3D-spatial configurations, and open-source models lag behind commercial ones. Alignment improves compositional accuracy.

Conclusion: CompAlign and CompQuest effectively assess and improve T2I models' compositional generation, with alignment yielding significant improvements.

Abstract: State-of-the-art T2I models are capable of generating high-resolution images
given textual prompts. However, they still struggle with accurately depicting
compositional scenes that specify multiple objects, attributes, and spatial
relations. We present CompAlign, a challenging benchmark with an emphasis on
assessing the depiction of 3D-spatial relationships, for evaluating and
improving models on compositional image generation. CompAlign consists of 900
complex multi-subject image generation prompts that combine numerical and
3D-spatial relationships with varied attribute bindings. Our benchmark is
remarkably challenging, incorporating generation tasks with 3+ generation
subjects with complex 3D-spatial relationships. Additionally, we propose
CompQuest, an interpretable and accurate evaluation framework that decomposes
complex prompts into atomic sub-questions, then utilizes a MLLM to provide
fine-grained binary feedback on the correctness of each aspect of generation
elements in model-generated images. This enables precise quantification of
alignment between generated images and compositional prompts. Furthermore, we
propose an alignment framework that uses CompQuest's feedback as preference
signals to improve diffusion models' compositional image generation abilities.
Using adjustable per-image preferences, our method is easily scalable and
flexible for different tasks. Evaluation of 9 T2I models reveals that: (1)
models remarkable struggle more with compositional tasks with more complex
3D-spatial configurations, and (2) a noticeable performance gap exists between
open-source accessible models and closed-source commercial models. Further
empirical study on using CompAlign for model alignment yield promising results:
post-alignment diffusion models achieve remarkable improvements in
compositional accuracy, especially on complex generation tasks, outperforming
previous approaches.

</details>


### [117] [Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning](https://arxiv.org/abs/2505.11182)
*Yuzhuo Dai,Jiaqi Jin,Zhibin Dong,Siwei Wang,Xinwang Liu,En Zhu,Xihong Yang,Xinbiao Gan,Yu Feng*

Main category: cs.CV

TL;DR: FreeCSL is a new IMVC framework that learns consensus semantics without imputation or alignment, addressing limitations in existing methods by leveraging consensus prototypes and heuristic graph clustering.


<details>
  <summary>Details</summary>
Motivation: Existing IMVC methods fail to construct a shared semantic space and rely excessively on consistency, leading to unreliable imputation and alignment.

Method: FreeCSL learns consensus prototypes for a shared semantic space and uses heuristic graph clustering to enhance cluster semantics within views.

Result: FreeCSL outperforms state-of-the-art methods, providing more confident and robust clustering assignments.

Conclusion: FreeCSL effectively bridges semantic gaps and improves clustering performance in IMVC tasks.

Abstract: In incomplete multi-view clustering (IMVC), missing data induce prototype
shifts within views and semantic inconsistencies across views. A feasible
solution is to explore cross-view consistency in paired complete observations,
further imputing and aligning the similarity relationships inherently shared
across views. Nevertheless, existing methods are constrained by two-tiered
limitations: (1) Neither instance- nor cluster-level consistency learning
construct a semantic space shared across views to learn consensus semantics.
The former enforces cross-view instances alignment, and wrongly regards
unpaired observations with semantic consistency as negative pairs; the latter
focuses on cross-view cluster counterparts while coarsely handling fine-grained
intra-cluster relationships within views. (2) Excessive reliance on consistency
results in unreliable imputation and alignment without incorporating
view-specific cluster information. Thus, we propose an IMVC framework,
imputation- and alignment-free for consensus semantics learning (FreeCSL). To
bridge semantic gaps across all observations, we learn consensus prototypes
from available data to discover a shared space, where semantically similar
observations are pulled closer for consensus semantics learning. To capture
semantic relationships within specific views, we design a heuristic graph
clustering based on modularity to recover cluster structure with intra-cluster
compactness and inter-cluster separation for cluster semantics enhancement.
Extensive experiments demonstrate, compared to state-of-the-art competitors,
FreeCSL achieves more confident and robust assignments on IMVC task.

</details>


### [118] [FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining](https://arxiv.org/abs/2505.11192)
*Myunsoo Kim,Seong-Woong Shim,Byung-Jun Lee*

Main category: cs.CV

TL;DR: FALCON is a learning-based mini-batch strategy for VLP that dynamically balances hard and false negatives, improving embedding quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: False negatives in VLP degrade embedding quality and hard negative sampling effectiveness, necessitating a solution to balance these negatives adaptively.

Method: FALCON uses a negative mining scheduler to dynamically select appropriate negative samples during mini-batch construction, guided by cross-modal alignment improvement.

Result: FALCON enhances performance across VLP frameworks (ALBEF, BLIP-2) and various downstream tasks, proving its robustness against false negatives.

Conclusion: FALCON effectively mitigates false negative impact in VLP, offering a dynamic and adaptive solution for improved contrastive learning.

Abstract: False negatives pose a critical challenge in vision-language pretraining
(VLP) due to the many-to-many correspondence between images and texts in
large-scale datasets. These false negatives introduce conflicting supervision
signals that degrade the learned embedding space and diminish the effectiveness
of hard negative sampling. In this paper, we propose FALCON (False-negative
Aware Learning of COntrastive Negatives), a learning-based mini-batch
construction strategy that adaptively balances the trade-off between hard and
false negatives during VLP. Rather than relying on fixed heuristics, FALCON
employs a negative mining scheduler that dynamically selects negative samples
of appropriate hardness for each anchor instance during mini-batch
construction, guided by a proxy for cross-modal alignment improvement.
Experimental results demonstrate that FALCON significantly improves performance
across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of
downstream tasks and evaluation settings, underscoring its effectiveness and
robustness in mitigating the impact of false negatives.

</details>


### [119] [DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling](https://arxiv.org/abs/2505.11196)
*Yuang Ai,Qihang Fan,Xuefeng Hu,Zhenheng Yang,Ran He,Huaibo Huang*

Main category: cs.CV

TL;DR: The paper introduces Diffusion ConvNet (DiCo), a more efficient alternative to Diffusion Transformer (DiT) for visual generation, using convolution and a compact channel attention mechanism to improve performance and speed.


<details>
  <summary>Details</summary>
Motivation: The high computational overhead and redundancy in global self-attention of DiT models motivate the exploration of convolution-based alternatives for efficient diffusion models.

Method: The authors replace self-attention with convolution, address channel redundancy via a compact channel attention mechanism, and develop DiCo, a family of ConvNet-based diffusion models.

Result: DiCo outperforms DiT in image quality and speed, achieving FID scores of 2.05 (256x256) and 2.53 (512x512) with significant speedups. DiCo-H (1B parameters) reaches an FID of 1.90.

Conclusion: DiCo demonstrates that convolution-based diffusion models can achieve superior performance and efficiency compared to transformer-based approaches, offering a promising direction for future research.

Abstract: Diffusion Transformer (DiT), a promising diffusion model for visual
generation, demonstrates impressive performance but incurs significant
computational overhead. Intriguingly, analysis of pre-trained DiT models
reveals that global self-attention is often redundant, predominantly capturing
local patterns-highlighting the potential for more efficient alternatives. In
this paper, we revisit convolution as an alternative building block for
constructing efficient and expressive diffusion models. However, naively
replacing self-attention with convolution typically results in degraded
performance. Our investigations attribute this performance gap to the higher
channel redundancy in ConvNets compared to Transformers. To resolve this, we
introduce a compact channel attention mechanism that promotes the activation of
more diverse channels, thereby enhancing feature diversity. This leads to
Diffusion ConvNet (DiCo), a family of diffusion models built entirely from
standard ConvNet modules, offering strong generative performance with
significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo
outperforms previous diffusion models in both image quality and generation
speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53
at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively.
Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID
of 1.90 on ImageNet 256x256-without any additional supervision during training.
Code: https://github.com/shallowdream204/DiCo.

</details>


### [120] [GeoMM: On Geodesic Perspective for Multi-modal Learning](https://arxiv.org/abs/2505.11216)
*Shibin Mei,Hang Wang,Bingbing Ni*

Main category: cs.CV

TL;DR: The paper introduces geodesic distance as a novel metric in multimodal learning to address limitations of traditional distance metrics, using graph structures and efficient computation strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional distance metrics fail to distinguish semantically different but similar samples in nonlinear manifolds common in multimodal learning.

Method: Constructs a graph structure for adjacency relationships, applies shortest-path algorithms for geodesic distance, and uses hierarchical clustering and incremental updates for efficiency.

Result: Extensive experiments show the method captures complex sample relationships and improves multimodal learning performance.

Conclusion: Geodesic distance is effective for multimodal learning, addressing traditional metric limitations and enhancing model performance.

Abstract: Geodesic distance serves as a reliable means of measuring distance in
nonlinear spaces, and such nonlinear manifolds are prevalent in the current
multimodal learning. In these scenarios, some samples may exhibit high
similarity, yet they convey different semantics, making traditional distance
metrics inadequate for distinguishing between positive and negative samples.
This paper introduces geodesic distance as a novel distance metric in
multi-modal learning for the first time, to mine correlations between samples,
aiming to address the limitations of common distance metric. Our approach
incorporates a comprehensive series of strategies to adapt geodesic distance
for the current multimodal learning. Specifically, we construct a graph
structure to represent the adjacency relationships among samples by
thresholding distances between them and then apply the shortest-path algorithm
to obtain geodesic distance within this graph. To facilitate efficient
computation, we further propose a hierarchical graph structure through
clustering and combined with incremental update strategies for dynamic status
updates. Extensive experiments across various downstream tasks validate the
effectiveness of our proposed method, demonstrating its capability to capture
complex relationships between samples and improve the performance of multimodal
learning models.

</details>


### [121] [AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition](https://arxiv.org/abs/2505.11232)
*Haiyu Li,Charith Abhayaratne*

Main category: cs.CV

TL;DR: Proposes an Adaptive Graph-based Noisy Data Removal framework for event-based object recognition, improving accuracy and noise reduction.


<details>
  <summary>Details</summary>
Motivation: Event cameras produce redundant and noisy data, challenging object recognition without losing critical spatiotemporal information.

Method: Integrates adaptive event segmentation, multifactorial edge-weighting, and adaptive graph-based denoising.

Result: Achieves superior recognition accuracies (83.77% to 99.30%) and noise reduction (up to 19.57%) on four datasets.

Conclusion: The framework effectively filters noise while preserving structural features, outperforming existing methods.

Abstract: Event cameras, which capture brightness changes with high temporal
resolution, inherently generate a significant amount of redundant and noisy
data beyond essential object structures. The primary challenge in event-based
object recognition lies in effectively removing this noise without losing
critical spatial-temporal information. To address this, we propose an Adaptive
Graph-based Noisy Data Removal framework for Event-based Object Recognition.
Specifically, our approach integrates adaptive event segmentation based on
normalized density analysis, a multifactorial edge-weighting mechanism, and
adaptive graph-based denoising strategies. These innovations significantly
enhance the integration of spatiotemporal information, effectively filtering
noise while preserving critical structural features for robust recognition.
Experimental evaluations on four challenging datasets demonstrate that our
method achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and
96.89%, surpassing existing graph-based methods by up to 8.79%, and improving
noise reduction performance by up to 19.57%, with an additional accuracy gain
of 6.26% compared to traditional Euclidean-based techniques.

</details>


### [122] [Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models](https://arxiv.org/abs/2505.11245)
*Fu-Yun Wang,Yunhao Shui,Jingtan Piao,Keqiang Sun,Hongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses the issue of diffusion models generating outputs misaligned with human preferences by proposing a method to handle unconditional/negative-conditional outputs, improving alignment without new training strategies.


<details>
  <summary>Details</summary>
Motivation: Existing preference alignment methods for diffusion models neglect unconditional/negative-conditional outputs, limiting the effectiveness of classifier-free guidance (CFG) and reducing output quality.

Method: The authors propose training a model specifically attuned to negative preferences, requiring minor modifications to existing techniques and no new training strategies or datasets.

Result: The method enhances alignment with human preferences for models like SD1.5, SDXL, video diffusion models, and those with preference optimization.

Conclusion: The proposed approach effectively improves the alignment of diffusion model outputs with human preferences by addressing the oversight of unconditional/negative-conditional outputs.

Abstract: Diffusion models have made substantial advances in image generation, yet
models trained on large, unfiltered datasets often yield outputs misaligned
with human preferences. Numerous methods have been proposed to fine-tune
pre-trained diffusion models, achieving notable improvements in aligning
generated outputs with human preferences. However, we argue that existing
preference alignment methods neglect the critical role of handling
unconditional/negative-conditional outputs, leading to a diminished capacity to
avoid generating undesirable outcomes. This oversight limits the efficacy of
classifier-free guidance~(CFG), which relies on the contrast between
conditional generation and unconditional/negative-conditional generation to
optimize output quality. In response, we propose a straightforward but
versatile effective approach that involves training a model specifically
attuned to negative preferences. This method does not require new training
strategies or datasets but rather involves minor modifications to existing
techniques. Our approach integrates seamlessly with models such as SD1.5, SDXL,
video diffusion models and models that have undergone preference optimization,
consistently enhancing their alignment with human preferences.

</details>


### [123] [Entropy-Driven Genetic Optimization for Deep-Feature-Guided Low-Light Image Enhancement](https://arxiv.org/abs/2505.11246)
*Nirjhor Datta,Afroza Akther,M. Sohel Rahman*

Main category: cs.CV

TL;DR: A novel unsupervised image enhancement framework using NSGA-II optimizes brightness, contrast, and gamma, balancing visual quality and semantic fidelity without paired data.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on pixel-level details, neglecting semantic features, which this work addresses.

Method: Uses a pre-trained deep neural network for feature extraction and GPU-accelerated NSGA-II to optimize multiple objectives (entropy, perceptual similarity, brightness), followed by local search.

Result: Achieves strong quantitative scores (BRISQUE: 19.82, NIQE: 3.652) and qualitative improvements in visibility, contrast, and detail preservation.

Conclusion: Introduces a promising direction for unsupervised image enhancement with semantic consistency.

Abstract: Image enhancement methods often prioritize pixel level information,
overlooking the semantic features. We propose a novel, unsupervised,
fuzzy-inspired image enhancement framework guided by NSGA-II algorithm that
optimizes image brightness, contrast, and gamma parameters to achieve a balance
between visual quality and semantic fidelity. Central to our proposed method is
the use of a pre trained deep neural network as a feature extractor. To find
the best enhancement settings, we use a GPU-accelerated NSGA-II algorithm that
balances multiple objectives, namely, increasing image entropy, improving
perceptual similarity, and maintaining appropriate brightness. We further
improve the results by applying a local search phase to fine-tune the top
candidates from the genetic algorithm. Our approach operates entirely without
paired training data making it broadly applicable across domains with limited
or noisy labels. Quantitatively, our model achieves excellent performance with
average BRISQUE and NIQE scores of 19.82 and 3.652, respectively, in all
unpaired datasets. Qualitatively, enhanced images by our model exhibit
significantly improved visibility in shadowed regions, natural balance of
contrast and also preserve the richer fine detail without introducing noticable
artifacts. This work opens new directions for unsupervised image enhancement
where semantic consistency is critical.

</details>


### [124] [DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models](https://arxiv.org/abs/2505.11257)
*Giulia Bertazzini,Daniele Baracchi,Dasara Shullani,Isao Echizen,Alessandro Piva*

Main category: cs.CV

TL;DR: DRAGON is a comprehensive dataset of images from 25 diffusion models, designed to aid in detecting synthetic content. It includes diverse images and a test set for benchmarking detection methods.


<details>
  <summary>Details</summary>
Motivation: The rise of synthetic images for misinformation necessitates robust detection tools. Existing datasets are limited and outdated, prompting the creation of DRAGON.

Method: DRAGON collects images from 25 diffusion models, uses a pipeline with a large language model to enhance prompt diversity and image quality, and offers the dataset in multiple sizes.

Result: The dataset improves image realism and diversity, supported by quality metrics, and provides a benchmark for evaluating detection methods.

Conclusion: DRAGON addresses the need for up-to-date, diverse datasets to develop and test synthetic content detection tools.

Abstract: The remarkable ease of use of diffusion models for image generation has led
to a proliferation of synthetic content online. While these models are often
employed for legitimate purposes, they are also used to generate fake images
that support misinformation and hate speech. Consequently, it is crucial to
develop robust tools capable of detecting whether an image has been generated
by such models. Many current detection methods, however, require large volumes
of sample images for training. Unfortunately, due to the rapid evolution of the
field, existing datasets often cover only a limited range of models and quickly
become outdated. In this work, we introduce DRAGON, a comprehensive dataset
comprising images from 25 diffusion models, spanning both recent advancements
and older, well-established architectures. The dataset contains a broad variety
of images representing diverse subjects. To enhance image realism, we propose a
simple yet effective pipeline that leverages a large language model to expand
input prompts, thereby generating more diverse and higher-quality outputs, as
evidenced by improvements in standard quality metrics. The dataset is provided
in multiple sizes (ranging from extra-small to extra-large) to accomodate
different research scenarios. DRAGON is designed to support the forensic
community in developing and evaluating detection and attribution techniques for
synthetic content. Additionally, the dataset is accompanied by a dedicated test
set, intended to serve as a benchmark for assessing the performance of newly
developed methods.

</details>


### [125] [Multi-view dense image matching with similarity learning and geometry priors](https://arxiv.org/abs/2505.11264)
*Mohamed Ali Chebbi,Ewelina Rupnik,Paul Lopes,Marc Pierrot-Deseilligny*

Main category: cs.CV

TL;DR: MV-DeepSimNets is a deep neural network suite for multi-view similarity learning, using epipolar geometry and online geometry priors to enhance multi-view reconstruction without extensive dataset creation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve multi-view reconstruction by leveraging geometry-aware features and avoiding labor-intensive dataset preparation.

Method: The approach uses epipolar geometry and homography rectification to generate geometry-aware features, aggregates learned similarities, and regularizes the cost volume for reconstruction.

Result: MV-DeepSimNets outperforms leading similarity learning and regression models, especially in generalizing across aerial and satellite imagery.

Conclusion: The method integrates into MicMac software, offering a practical solution for multi-view reconstruction with superior performance and generalization.

Abstract: We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks
designed for multi-view similarity learning, leveraging epipolar geometry for
training. Our approach incorporates an online geometry prior to characterize
pixel relationships, either along the epipolar line or through homography
rectification. This enables the generation of geometry-aware features from
native images, which are then projected across candidate depth hypotheses using
plane sweeping. Our method geometric preconditioning effectively adapts
epipolar-based features for enhanced multi-view reconstruction, without
requiring the laborious multi-view training dataset creation. By aggregating
learned similarities, we construct and regularize the cost volume, leading to
improved multi-view surface reconstruction over traditional dense matching
approaches. MV-DeepSimNets demonstrates superior performance against leading
similarity learning networks and end-to-end regression models, especially in
terms of generalization capabilities across both aerial and satellite imagery
with varied ground sampling distances. Our pipeline is integrated into MicMac
software and can be readily adopted in standard multi-resolution image matching
pipelines.

</details>


### [126] [Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity](https://arxiv.org/abs/2505.11267)
*Wuzhou Quan,Mingqiang Wei,Jinhui Tang*

Main category: cs.CV

TL;DR: FairHyp is a fairness-directed framework for hyperspectral image representation, addressing non-uniformity through specialized modules, outperforming state-of-the-art methods in tasks like classification and denoising.


<details>
  <summary>Details</summary>
Motivation: Existing HSI models assume homogeneity, leading to suboptimal performance. FairHyp addresses non-uniformity by disentangling spectral, spatial, and feature dependencies.

Method: FairHyp uses a Runge-Kutta-inspired spatial adapter, multi-receptive field convolution, and a spectral-context state space model with bidirectional Mamba scanning.

Result: FairHyp consistently outperforms state-of-the-art methods in tasks like classification, denoising, super-resolution, and inpainting.

Conclusion: FairHyp redefines fairness as a structural necessity in HSI modeling, balancing adaptability, efficiency, and fidelity.

Abstract: Hyperspectral image (HSI) representation is fundamentally challenged by
pervasive non-uniformity, where spectral dependencies, spatial continuity, and
feature efficiency exhibit complex and often conflicting behaviors. Most
existing models rely on a unified processing paradigm that assumes homogeneity
across dimensions, leading to suboptimal performance and biased
representations. To address this, we propose FairHyp, a fairness-directed
framework that explicitly disentangles and resolves the threefold
non-uniformity through cooperative yet specialized modules. We introduce a
Runge-Kutta-inspired spatial variability adapter to restore spatial coherence
under resolution discrepancies, a multi-receptive field convolution module with
sparse-aware refinement to enhance discriminative features while respecting
inherent sparsity, and a spectral-context state space model that captures
stable and long-range spectral dependencies via bidirectional Mamba scanning
and statistical aggregation. Unlike one-size-fits-all solutions, FairHyp
achieves dimension-specific adaptation while preserving global consistency and
mutual reinforcement. This design is grounded in the view that non-uniformity
arises from the intrinsic structure of HSI representations, rather than any
particular task setting. To validate this, we apply FairHyp across four
representative tasks including classification, denoising, super-resolution, and
inpaintin, demonstrating its effectiveness in modeling a shared structural
flaw. Extensive experiments show that FairHyp consistently outperforms
state-of-the-art methods under varied imaging conditions. Our findings redefine
fairness as a structural necessity in HSI modeling and offer a new paradigm for
balancing adaptability, efficiency, and fidelity in high-dimensional vision
tasks.

</details>


### [127] [MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection](https://arxiv.org/abs/2505.11282)
*Shrutarv Awasthi,Anas Gouda,Sven Franke,Jérôme Rutinowski,Frank Hoffmann,Moritz Roidl*

Main category: cs.CV

TL;DR: MTevent is a dataset for 6D pose estimation and moving object detection in dynamic environments, combining stereo-event and RGB cameras to address limitations of RGB-based approaches in high-speed robotics.


<details>
  <summary>Details</summary>
Motivation: RGB cameras struggle with motion blur and latency in high-speed robotics, while event cameras offer low-latency sensing. MTevent aims to bridge this gap by providing a dataset for dynamic environments.

Method: The dataset includes 75 scenes captured with stereo-event and RGB cameras, featuring 16 unique objects under challenging conditions like extreme angles, lighting variations, and occlusions.

Result: Baseline evaluation using NVIDIA's FoundationPose on RGB images achieved an Average Recall of 0.22, showing RGB limitations. MTevent provides a resource for improving event-based vision models.

Conclusion: MTevent advances event-based vision in robotics by addressing high-speed motion and long-range perception challenges, offering a novel dataset for research.

Abstract: Mobile robots are reaching unprecedented speeds, with platforms like Unitree
B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s.
However, effectively utilizing such speeds remains a challenge due to the
limitations of RGB cameras, which suffer from motion blur and fail to provide
real-time responsiveness. Event cameras, with their asynchronous operation, and
low-latency sensing, offer a promising alternative for high-speed robotic
perception. In this work, we introduce MTevent, a dataset designed for 6D pose
estimation and moving object detection in highly dynamic environments with
large detection distances. Our setup consists of a stereo-event camera and an
RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16
unique objects under challenging conditions such as extreme viewing angles,
varying lighting, and occlusions. MTevent is the first dataset to combine
high-speed motion, long-range perception, and real-world object interactions,
making it a valuable resource for advancing event-based vision in robotics. To
establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's
FoundationPose on RGB images, achieving an Average Recall of 0.22 with
ground-truth masks, highlighting the limitations of RGB-based approaches in
such dynamic settings. With MTevent, we provide a novel resource to improve
perception models and foster further research in high-speed robotic vision. The
dataset is available for download
https://huggingface.co/datasets/anas-gouda/MTevent

</details>


### [128] [Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining](https://arxiv.org/abs/2505.11293)
*Raghuveer Thirukovalluru,Rui Meng,Ye Liu,Karthikeyan K,Mingyi Su,Ping Nie,Semih Yavuz,Yingbo Zhou,Wenhu Chen,Bhuwan Dhingra*

Main category: cs.CV

TL;DR: The paper introduces 'Breaking the Batch Barrier' (B3), a batch construction strategy for contrastive learning, improving model performance with smaller batch sizes.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning relies on in-batch negatives, whose quality and size impact model effectiveness. B3 aims to enhance batch quality for better performance.

Method: B3 uses a pretrained teacher model to rank examples, constructs a similarity graph, applies community detection for clusters, and forms batches rich in negatives.

Result: B3 achieves state-of-the-art results on the MMEB benchmark, outperforming prior methods by +1.3 and +2.9 points at 7B and 2B scales, even with batch sizes as small as 64.

Conclusion: B3 demonstrates that high-quality batch construction can significantly improve contrastive learning performance, even with reduced batch sizes.

Abstract: Contrastive learning (CL) is a prevalent technique for training embedding
models, which pulls semantically similar examples (positives) closer in the
representation space while pushing dissimilar ones (negatives) further apart. A
key source of negatives are 'in-batch' examples, i.e., positives from other
examples in the batch. Effectiveness of such models is hence strongly
influenced by the size and quality of training batches. In this work, we
propose 'Breaking the Batch Barrier' (B3), a novel batch construction strategy
designed to curate high-quality batches for CL. Our approach begins by using a
pretrained teacher embedding model to rank all examples in the dataset, from
which a sparse similarity graph is constructed. A community detection algorithm
is then applied to this graph to identify clusters of examples that serve as
strong negatives for one another. The clusters are then used to construct
batches that are rich in in-batch negatives. Empirical results on the MMEB
multimodal embedding benchmark (36 tasks) demonstrate that our method sets a
new state of the art, outperforming previous best methods by +1.3 and +2.9
points at the 7B and 2B model scales, respectively. Notably, models trained
with B3 surpass existing state-of-the-art results even with a batch size as
small as 64, which is 4-16x smaller than that required by other methods.

</details>


### [129] [CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/abs/2505.11314)
*Christoph Leiter,Yuki M. Asano,Margret Keuper,Steffen Eger*

Main category: cs.CV

TL;DR: CROC is a scalable framework for automated meta-evaluation of text-to-image metrics, using contrastive robustness checks and synthetic datasets to identify and address robustness issues in existing metrics.


<details>
  <summary>Details</summary>
Motivation: Human-based meta-evaluation is costly and time-intensive, and automated alternatives are lacking, necessitating a scalable solution.

Method: CROC synthesizes contrastive test cases across image properties, creating a pseudo-labeled dataset (CROC$^{syn}$) and training a new metric (CROCScore). A human-supervised benchmark (CROC$^{hum}$) is also introduced.

Result: CROCScore achieves state-of-the-art performance, and existing metrics show robustness issues, failing on prompts with negation or body part identification.

Conclusion: CROC provides a scalable, automated solution for meta-evaluation, highlighting and addressing robustness gaps in text-to-image metrics.

Abstract: The assessment of evaluation metrics (meta-evaluation) is crucial for
determining the suitability of existing metrics in text-to-image (T2I)
generation tasks. Human-based meta-evaluation is costly and time-intensive, and
automated alternatives are scarce. We address this gap and propose CROC: a
scalable framework for automated Contrastive Robustness Checks that
systematically probes and quantifies metric robustness by synthesizing
contrastive test cases across a comprehensive taxonomy of image properties.
With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one
million contrastive prompt-image pairs to enable a fine-grained comparison of
evaluation metrics. We also use the dataset to train CROCScore, a new metric
that achieves state-of-the-art performance among open-source methods,
demonstrating an additional key application of our framework. To complement
this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)
targeting especially challenging categories. Our results highlight robustness
issues in existing metrics: for example, many fail on prompts involving
negation, and all tested open-source metrics fail on at least 25% of cases
involving correct identification of body parts.

</details>


### [130] [Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models](https://arxiv.org/abs/2505.11326)
*Keunwoo Peter Yu,Joyce Chai*

Main category: cs.CV

TL;DR: The paper introduces a new benchmark task, TGLG, to evaluate real-time vision-language models (VLMs) by focusing on perceptual updating and contingency awareness. It proposes VLM-TSI, a model for time-synchronized language generation, and a new metric, TRACE, for evaluation. Results show VLM-TSI outperforms baselines but highlight the challenge of TGLG.


<details>
  <summary>Details</summary>
Motivation: Real-time interactive environments require VLMs to generate semantically accurate and precisely timed utterances, which existing offline VLMs struggle with.

Method: The authors propose TGLG as a benchmark task, curate datasets from sports and egocentric interactions, and introduce TRACE for evaluation. They develop VLM-TSI, a model that interleaves visual and linguistic tokens in real-time.

Result: VLM-TSI outperforms baselines but overall performance remains modest, indicating the difficulty of TGLG.

Conclusion: The work highlights the challenges of real-time VLMs and motivates further research, with code and data made available.

Abstract: Vision-language models (VLMs) have shown remarkable progress in offline tasks
such as image captioning and video question answering. However, real-time
interactive environments impose new demands on VLMs, requiring them to generate
utterances that are not only semantically accurate but also precisely timed. We
identify two core capabilities necessary for such settings --
$\textit{perceptual updating}$ and $\textit{contingency awareness}$ -- and
propose a new benchmark task, $\textbf{Temporally-Grounded Language Generation
(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in
response to streaming video such that both content and timing align with
dynamic visual input. To support this benchmark, we curate evaluation datasets
from sports broadcasting and egocentric human interaction domains, and
introduce a new metric, $\textbf{TRACE}$, to evaluate TGLG by jointly measuring
semantic similarity and temporal alignment. Finally, we present
$\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,
a model that interleaves visual and linguistic tokens in a time-synchronized
manner, enabling real-time language generation without relying on turn-based
assumptions. Experimental results show that VLM-TSI significantly outperforms a
strong baseline, yet overall performance remains modest -- highlighting the
difficulty of TGLG and motivating further research in real-time VLMs. Code and
data available $\href{https://github.com/yukw777/tglg}{here}$.

</details>


### [131] [MARRS: Masked Autoregressive Unit-based Reaction Synthesis](https://arxiv.org/abs/2505.11334)
*Y. B. Wang,S Wang,J. N. Zhang,J. F. Wu,Q. D. He,C. C. Fu,C. J. Wang,Y. Liu*

Main category: cs.CV

TL;DR: The paper introduces MARRS, a framework for human action-reaction synthesis, addressing challenges like fine-grained hand movements and autoregressive modeling limitations with novel techniques like UD-VAE, ACF, and AUM.


<details>
  <summary>Details</summary>
Motivation: The task of generating human reactions based on actions is challenging, especially due to the need for fine-grained hand movements and the limitations of current autoregressive and VQ methods.

Method: Proposes MARRS with UD-VAE for unit-distinguished encoding, ACF for action-conditioned fusion, and AUM for adaptive unit modulation. Uses a diffusion model with MLP noise predictors.

Result: MARRS achieves superior performance in generating coordinated and fine-grained reaction motions, as shown by quantitative and qualitative results.

Conclusion: The framework effectively addresses the challenges of human action-reaction synthesis, offering improved performance and fine-grained motion generation.

Abstract: This work aims at a challenging task: human action-reaction synthesis, i.e.,
generating human reactions based on the action sequence of the other as
conditions. Currently, autoregressive modeling approaches have achieved
remarkable performance in motion generation tasks, e.g. text-to-motion.
However, vector quantization (VQ) accompanying autoregressive generation has
inherent disadvantages, including loss of quantization information, low
codebook utilization, etc. Moreover, unlike text-to-motion, which focuses
solely on the movement of body joints, human action-reaction synthesis also
encompasses fine-grained hand movements. In this work, we propose MARRS, a
novel framework designed to generate coordinated and fine-grained reaction
motions in continuous representations. Initially, we present the
Unit-distinguished Motion Variational AutoEncoder (UD-VAE), which segments the
entire body into distinct body and hand units, encoding them independently.
Subsequently, we propose Action-Conditioned Fusion (ACF), which involves
randomly masking a subset of reactive tokens and extracting specific
information about the body and hands from the active tokens. Furthermore, we
introduce Adaptive Unit Modulation (AUM) to facilitate interaction between body
and hand units by using the information from one unit to adaptively modulate
the other. Finally, for the diffusion model, we employ a compact MLP as a noise
predictor for each distinct body unit and incorporate the diffusion loss to
model the probability distribution of each token. Quantitative and qualitative
results demonstrate that our method achieves superior performance. The code
will be released upon acceptance.

</details>


### [132] [Dynamic Base model Shift for Delta Compression](https://arxiv.org/abs/2505.11344)
*Chenyu Huang,Peng Ye,Shenghe Zheng,Xiaohui Wang,Lei Bai,Tao Chen,Wanli Ouyang*

Main category: cs.CV

TL;DR: The paper proposes Dynamic Base Model Shift (DBMS) to improve delta compression performance by dynamically adapting the base model for each task, outperforming existing methods under high compression ratios.


<details>
  <summary>Details</summary>
Motivation: Existing delta compression methods degrade performance under high compression rates due to reliance on the pretrained base model, which may not be optimal.

Method: DBMS dynamically adjusts the base model shift magnitude and delta compression scale for each task through low-cost learning.

Result: DBMS maintains performance under extreme compression, surpassing other methods, and works across language, vision, and multi-modal models.

Conclusion: DBMS is an effective, adaptable solution for delta compression, compatible with various models and methods.

Abstract: Transformer-based models with the pretrain-finetune paradigm bring about
significant progress, along with the heavy storage and deployment costs of
finetuned models on multiple tasks. Delta compression attempts to lower the
costs by reducing the redundancy of delta parameters (i.e., the difference
between the finetuned and pre-trained model weights) through pruning or
quantization. However, existing methods by default employ the pretrained model
as the base model and compress the delta parameters for every task, which may
causes significant performance degradation, especially when the compression
rate is extremely high. To tackle this issue, we investigate the impact of
different base models on the performance of delta compression and find that the
pre-trained base model can hardly be optimal. To this end, we propose Dynamic
Base Model Shift (DBMS), which dynamically adapts the base model to the target
task before performing delta compression. Specifically, we adjust two
parameters, which respectively determine the magnitude of the base model shift
and the overall scale of delta compression, to boost the compression
performance on each task. Through low-cost learning of these two parameters,
our DBMS can maintain most of the finetuned model's performance even under an
extremely high compression ratio setting, significantly surpassing existing
methods. Moreover, our DBMS is orthogonal and can be integrated with a variety
of other methods, and it has been evaluated across different types of models
including language, vision transformer, and multi-modal models.

</details>


### [133] [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2505.11405)
*Bohao Xing,Xin Liu,Guoying Zhao,Chengyu Liu,Xiaolan Fu,Heikki Kälviäinen*

Main category: cs.CV

TL;DR: The paper introduces EmotionHallucer, the first benchmark for evaluating emotion-related hallucinations in Multimodal Large Language Models (MLLMs), addressing a critical gap in the field.


<details>
  <summary>Details</summary>
Motivation: Despite MLLMs' advancements in emotion understanding, they often generate irrelevant or nonsensical content (hallucinations). No prior work has specifically evaluated emotion-related hallucinations.

Method: The benchmark assesses hallucinations using an adversarial binary QA framework, evaluating models on emotion psychology knowledge and real-world multimodal perception.

Result: Evaluation of 38 models revealed widespread emotion hallucinations, with closed-source models outperforming open-source ones. Models performed better in emotion psychology than multimodal perception.

Conclusion: The findings led to the PEP-MEK framework, improving hallucination detection by 9.90%. Resources are shared for further research.

Abstract: Emotion understanding is a critical yet challenging task. Recent advances in
Multimodal Large Language Models (MLLMs) have significantly enhanced their
capabilities in this area. However, MLLMs often suffer from hallucinations,
generating irrelevant or nonsensical content. To the best of our knowledge,
despite the importance of this issue, there has been no dedicated effort to
evaluate emotion-related hallucinations in MLLMs. In this work, we introduce
EmotionHallucer, the first benchmark for detecting and analyzing emotion
hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from
the interplay of biology and social learning, MLLMs rely solely on data-driven
learning and lack innate emotional instincts. Fortunately, emotion psychology
provides a solid foundation of knowledge about human emotions. Building on
this, we assess emotion hallucinations from two dimensions: emotion psychology
knowledge and real-world multimodal perception. To support robust evaluation,
we utilize an adversarial binary question-answer (QA) framework, which employs
carefully crafted basic and hallucinated pairs to assess the emotion
hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on
EmotionHallucer, we reveal that: i) most current models exhibit substantial
issues with emotion hallucinations; ii) closed-source models outperform
open-source ones in detecting emotion hallucinations, and reasoning capability
provides additional advantages; iii) existing models perform better in emotion
psychology knowledge than in multimodal emotion perception. As a byproduct,
these findings inspire us to propose the PEP-MEK framework, which yields an
average improvement of 9.90% in emotion hallucination detection across selected
models. Resources will be available at
https://github.com/xxtars/EmotionHallucer.

</details>


### [134] [Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation](https://arxiv.org/abs/2505.11383)
*Zihan Wang,Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: Dynam3D is a dynamic layered 3D representation model addressing VLN challenges like 3D understanding, exploration, and adaptability, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video-VLMs struggle with 3D geometry, large-scale exploration, and dynamic environments in VLN tasks.

Method: Dynam3D projects 2D CLIP features into 3D space, creating hierarchical representations with dynamic updates for 3D understanding and navigation.

Result: Dynam3D achieves top performance on VLN benchmarks (R2R-CE, REVERIE-CE, NavRAG-CE) and validates practical deployment.

Conclusion: Dynam3D effectively addresses VLN challenges, demonstrating strong performance and adaptability for real-world navigation.

Abstract: Vision-and-Language Navigation (VLN) is a core task where embodied agents
leverage their spatial mobility to navigate in 3D environments toward
designated destinations based on natural language instructions. Recently,
video-language large models (Video-VLMs) with strong generalization
capabilities and rich commonsense knowledge have shown remarkable performance
when applied to VLN tasks. However, these models still encounter the following
challenges when applied to real-world 3D navigation: 1) Insufficient
understanding of 3D geometry and spatial semantics; 2) Limited capacity for
large-scale exploration and long-term environmental memory; 3) Poor
adaptability to dynamic and changing environments.To address these limitations,
we propose Dynam3D, a dynamic layered 3D representation model that leverages
language-aligned, generalizable, and hierarchical 3D representations as visual
input to train 3D-VLM in navigation action prediction. Given posed RGB-D
images, our Dynam3D projects 2D CLIP features into 3D space and constructs
multi-level 3D patch-instance-zone representations for 3D geometric and
semantic understanding with a dynamic and layer-wise update strategy. Our
Dynam3D is capable of online encoding and localization of 3D instances, and
dynamically updates them in changing environments to provide large-scale
exploration and long-term memory capabilities for navigation. By leveraging
large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D
sets new state-of-the-art performance on VLN benchmarks including R2R-CE,
REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for
pre-exploration, lifelong memory, and real-world robot validate the
effectiveness of practical deployment.

</details>


### [135] [MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory](https://arxiv.org/abs/2505.11386)
*Zifan Wang,Jingwei Li,Yitang Li,Yunze Liu*

Main category: cs.CV

TL;DR: MutualNeRF enhances NeRF performance under limited samples using Mutual Information Theory, improving sparse view sampling and few-shot view synthesis.


<details>
  <summary>Details</summary>
Motivation: NeRF struggles with limited data and lacks theoretical support for prior knowledge integration. Mutual Information provides a unified metric for image correlation.

Method: Uses Mutual Information to select viewpoints (minimizing MI) and improve synthesis (maximizing MI). Implements a greedy algorithm and plug-and-play regularization.

Result: Consistent improvement over state-of-the-art baselines in limited-sample settings.

Conclusion: MutualNeRF is effective for enhancing NeRF performance with limited data, supported by theoretical and experimental results.

Abstract: This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field
(NeRF) performance under limited samples using Mutual Information Theory. While
NeRF excels in 3D scene synthesis, challenges arise with limited data and
existing methods that aim to introduce prior knowledge lack theoretical support
in a unified framework. We introduce a simple but theoretically robust concept,
Mutual Information, as a metric to uniformly measure the correlation between
images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints
containing more non-overlapping scene information by minimizing mutual
information without knowing ground truth images beforehand. Our framework
employs a greedy algorithm, offering a near-optimal solution.
  For few-shot view synthesis, we maximize the mutual information between
inferred images and ground truth, expecting inferred images to gain more
relevant information from known images. This is achieved by incorporating
efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over
state-of-the-art baselines in different settings, affirming the efficacy of our
framework.

</details>


### [136] [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/abs/2505.11404)
*Wenchuan Zhang,Penghao Zhang,Jingru Guo,Tao Cheng,Jie Chen,Shuwan Zhang,Zhang Zhang,Yuhao Yi,Hong Bu*

Main category: cs.CV

TL;DR: The paper introduces Patho-R1, a multimodal RL-based pathology Reasoner, and PathoCLIP, addressing limitations in current pathology VLMs by leveraging high-quality datasets and a three-stage training pipeline.


<details>
  <summary>Details</summary>
Motivation: Current pathology VLMs lack diagnostic accuracy and reasoning plausibility due to shallow datasets. The study aims to improve this by using expert-curated datasets and advanced training methods.

Method: A three-stage pipeline: (1) pretraining on image-text pairs, (2) fine-tuning on Chain-of-Thought samples, (3) reinforcement learning for reasoning refinement. PathoCLIP is also introduced for dataset alignment assessment.

Result: Patho-R1 and PathoCLIP show robust performance in tasks like zero-shot classification, cross-modal retrieval, VQA, and MCQs.

Conclusion: The study successfully enhances pathology VLMs through high-quality datasets and advanced training, achieving strong performance across multiple tasks.

Abstract: Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.

</details>


### [137] [Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study](https://arxiv.org/abs/2505.11424)
*Rana Poureskandar,Shiva Razzagzadeh*

Main category: cs.CV

TL;DR: Evaluation of YOLOv8-based segmentation for wrinkle detection in facial images.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of YOLOv8 in detecting and segmenting facial wrinkles.

Method: Utilized a YOLOv8-based segmentation model for wrinkle detection in facial images.

Result: Performance of the model was evaluated, though specific metrics are not mentioned.

Conclusion: The study demonstrates the potential of YOLOv8 for wrinkle segmentation in facial images.

Abstract: This study evaluated the performance of a YOLOv8-based segmentation model for
detecting and segmenting wrinkles in facial images.

</details>


### [138] [Face Consistency Benchmark for GenAI Video](https://arxiv.org/abs/2505.11425)
*Michal Podstawski,Malgorzata Kudelska,Haohong Wang*

Main category: cs.CV

TL;DR: The paper introduces the Face Consistency Benchmark (FCB) to evaluate character consistency in AI-generated videos, addressing a key challenge in the field.


<details>
  <summary>Details</summary>
Motivation: Current AI video generation models struggle with maintaining character consistency, necessitating a standardized evaluation framework.

Method: The paper proposes the FCB, a framework with standardized metrics to assess and compare character consistency in AI-generated videos.

Result: The benchmark identifies gaps in existing solutions and encourages the development of more reliable approaches.

Conclusion: This work is a significant advancement toward improving character consistency in AI video generation.

Abstract: Video generation driven by artificial intelligence has advanced
significantly, enabling the creation of dynamic and realistic content. However,
maintaining character consistency across video sequences remains a major
challenge, with current models struggling to ensure coherence in appearance and
attributes. This paper introduces the Face Consistency Benchmark (FCB), a
framework for evaluating and comparing the consistency of characters in
AI-generated videos. By providing standardized metrics, the benchmark
highlights gaps in existing solutions and promotes the development of more
reliable approaches. This work represents a crucial step toward improving
character consistency in AI video generation technologies.

</details>


### [139] [SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision](https://arxiv.org/abs/2505.11439)
*Utsav Rai,Haozheng Xu,Stamatia Giannarou*

Main category: cs.CV

TL;DR: A novel 6-DoF pose estimation pipeline for surgical instruments in RMIS, using zero-shot RGB-D models (FoundationPose and SAM-6D) with enhanced depth estimation and segmentation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of marker-based and supervised learning methods in RMIS by exploring zero-shot pose estimation for unseen surgical tools.

Method: Leveraged zero-shot RGB-D models (FoundationPose, SAM-6D) with RAFT-Stereo for depth estimation and replaced SAM with fine-tuned Mask R-CNN for better segmentation.

Result: Enhanced SAM-6D outperforms FoundationPose in zero-shot pose estimation, setting a new benchmark for RMIS.

Conclusion: This work improves generalizability for unseen tools and pioneers zero-shot RGB-D methods in RMIS.

Abstract: Accurate pose estimation of surgical tools in Robot-assisted Minimally
Invasive Surgery (RMIS) is essential for surgical navigation and robot control.
While traditional marker-based methods offer accuracy, they face challenges
with occlusions, reflections, and tool-specific designs. Similarly, supervised
learning methods require extensive training on annotated datasets, limiting
their adaptability to new tools. Despite their success in other domains,
zero-shot pose estimation models remain unexplored in RMIS for pose estimation
of surgical instruments, creating a gap in generalising to unseen surgical
tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation
pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D
models like the FoundationPose and SAM-6D. We advanced these models by
incorporating vision-based depth estimation using the RAFT-Stereo method, for
robust depth estimation in reflective and textureless environments.
Additionally, we enhanced SAM-6D by replacing its instance segmentation module,
Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly
boosting segmentation accuracy in occluded and complex conditions. Extensive
validation reveals that our enhanced SAM-6D surpasses FoundationPose in
zero-shot pose estimation of unseen surgical instruments, setting a new
benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the
generalisability of pose estimation for unseen objects and pioneers the
application of RGB-D zero-shot methods in RMIS.

</details>


### [140] [HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation](https://arxiv.org/abs/2505.11454)
*Shaina Raza,Aravind Narayanan,Vahid Reza Khazaie,Ashmal Vayani,Mukund S. Chettiar,Amandeep Singh,Mubarak Shah,Deval Pandya*

Main category: cs.CV

TL;DR: HumaniBench is a new benchmark for evaluating Large Multimodal Models (LMMs) on human-centered AI principles like fairness, ethics, and empathy, using 32K real-world image-question pairs.


<details>
  <summary>Details</summary>
Motivation: LMMs perform well on standard benchmarks but lack alignment with human values like fairness and inclusivity. HumaniBench addresses this gap.

Method: A dataset of 32K image-question pairs was created using a GPT4-assisted pipeline and expert verification. It evaluates seven HCAI principles across seven tasks.

Result: Proprietary models generally outperform open-source ones, but robustness and visual grounding are weak points. Some models struggle to balance accuracy with human-aligned principles.

Conclusion: HumaniBench is the first benchmark focused on HCAI principles, providing a testbed for improving LMM alignment with human values.

Abstract: Large multimodal models (LMMs) now excel on many vision language benchmarks,
however, they still struggle with human centered criteria such as fairness,
ethics, empathy, and inclusivity, key to aligning with human values. We
introduce HumaniBench, a holistic benchmark of 32K real-world image question
pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively
verified by domain experts. HumaniBench evaluates seven Human Centered AI
(HCAI) principles: fairness, ethics, understanding, reasoning, language
inclusivity, empathy, and robustness, across seven diverse tasks, including
open and closed ended visual question answering (VQA), multilingual QA, visual
grounding, empathetic captioning, and robustness tests. Benchmarking 15 state
of the art LMMs (open and closed source) reveals that proprietary models
generally lead, though robustness and visual grounding remain weak points. Some
open-source models also struggle to balance accuracy with adherence to
human-aligned principles. HumaniBench is the first benchmark purpose built
around HCAI principles. It provides a rigorous testbed for diagnosing alignment
gaps and guiding LMMs toward behavior that is both accurate and socially
responsible. Dataset, annotation prompts, and evaluation code are available at:
https://vectorinstitute.github.io/HumaniBench

</details>


### [141] [PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment](https://arxiv.org/abs/2505.11468)
*Dingbang Huang,Wenbo Li,Yifei Zhao,Xinyu Pan,Yanhong Zeng,Bo Dai*

Main category: cs.CV

TL;DR: PSDiffusion is a unified diffusion framework for simultaneous multi-layer text-to-image generation, addressing interactions among layers while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-layer generation methods fail to handle layer interactions like global layout, physics-plausible contacts, and visual effects (e.g., shadows, reflections) while preserving alpha quality.

Method: PSDiffusion introduces a global-layer interactive mechanism to generate layered images (RGB background + RGBA foregrounds) concurrently and collaboratively in a single feed-forward process.

Result: The model ensures high quality and completeness for each layer while enabling spatial and visual interactions for global coherence.

Conclusion: PSDiffusion outperforms existing methods by generating multi-layer images with coherent interactions in a unified process.

Abstract: Diffusion models have made remarkable advancements in generating high-quality
images from textual descriptions. Recent works like LayerDiffuse have extended
the previous single-layer, unified image generation paradigm to transparent
image layer generation. However, existing multi-layer generation methods fail
to handle the interactions among multiple layers such as rational global
layout, physics-plausible contacts and visual effects like shadows and
reflections while maintaining high alpha quality. To solve this problem, we
propose PSDiffusion, a unified diffusion framework for simultaneous multi-layer
text-to-image generation. Our model can automatically generate multi-layer
images with one RGB background and multiple RGBA foregrounds through a single
feed-forward process. Unlike existing methods that combine multiple tools for
post-decomposition or generate layers sequentially and separately, our method
introduces a global-layer interactive mechanism that generates layered-images
concurrently and collaboratively, ensuring not only high quality and
completeness for each layer, but also spatial and visual interactions among
layers for global coherence.

</details>


### [142] [Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models](https://arxiv.org/abs/2505.11482)
*Shirin Shoushtari,Edward P. Chandler,Yuanhao Wang,M. Salman Asif,Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: A fully unsupervised metric for estimating distribution shifts in diffusion models using corrupted measurements and score functions, improving reconstruction quality in inverse problems.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of diffusion models under distribution shifts between training and test images, without requiring clean test images.

Method: Proposes a score-based metric using corrupted measurements and score functions to estimate KL divergence between training and test distributions.

Result: The metric approximates KL divergence from clean images and aligning scores improves reconstruction quality.

Conclusion: Unsupervised score-based alignment reduces KL divergence and enhances performance in inverse problems.

Abstract: Diffusion models are widely used as priors in imaging inverse problems.
However, their performance often degrades under distribution shifts between the
training and test-time images. Existing methods for identifying and quantifying
distribution shifts typically require access to clean test images, which are
almost never available while solving inverse problems (at test time). We
propose a fully unsupervised metric for estimating distribution shifts using
only indirect (corrupted) measurements and score functions from diffusion
models trained on different datasets. We theoretically show that this metric
estimates the KL divergence between the training and test image distributions.
Empirically, we show that our score-based metric, using only corrupted
measurements, closely approximates the KL divergence computed from clean
images. Motivated by this result, we show that aligning the out-of-distribution
score with the in-distribution score -- using only corrupted measurements --
reduces the KL divergence and leads to improved reconstruction quality across
multiple inverse problems.

</details>


### [143] [GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing](https://arxiv.org/abs/2505.11493)
*Yusu Qian,Jiasen Lu,Tsu-Jui Fu,Xinze Wang,Chen Chen,Yinfei Yang,Wenze Hu,Zhe Gan*

Main category: cs.CV

TL;DR: A new benchmark (GIE-Bench) evaluates text-guided image editing models on functional correctness and content preservation, revealing GPT-Image-1's strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics (e.g., CLIP) lack precision for text-guided image editing, necessitating a more grounded benchmark.

Method: The benchmark uses automatically generated multiple-choice questions for functional correctness and object-aware masking for content preservation, with 1000+ examples across 20 categories.

Result: GPT-Image-1 excels in instruction-following but over-modifies irrelevant regions; the benchmark validates automatic metrics against human ratings.

Conclusion: GIE-Bench offers a scalable, reproducible framework for improving evaluation in text-guided image editing.

Abstract: Editing images using natural language instructions has become a natural and
expressive way to modify visual content; yet, evaluating the performance of
such models remains challenging. Existing evaluation approaches often rely on
image-text similarity metrics like CLIP, which lack precision. In this work, we
introduce a new benchmark designed to evaluate text-guided image editing models
in a more grounded manner, along two critical dimensions: (i) functional
correctness, assessed via automatically generated multiple-choice questions
that verify whether the intended change was successfully applied; and (ii)
image content preservation, which ensures that non-targeted regions of the
image remain visually consistent using an object-aware masking technique and
preservation scoring. The benchmark includes over 1000 high-quality editing
examples across 20 diverse content categories, each annotated with detailed
editing instructions, evaluation questions, and spatial object masks. We
conduct a large-scale study comparing GPT-Image-1, the latest flagship in the
text-guided image editing space, against several state-of-the-art editing
models, and validate our automatic metrics against human ratings. Results show
that GPT-Image-1 leads in instruction-following accuracy, but often
over-modifies irrelevant image regions, highlighting a key trade-off in the
current model behavior. GIE-Bench provides a scalable, reproducible framework
for advancing more accurate evaluation of text-guided image editing.

</details>


### [144] [QVGen: Pushing the Limit of Quantized Video Generative Models](https://arxiv.org/abs/2505.11497)
*Yushi Huang,Ruihao Gong,Jing Liu,Yifu Ding,Chengtao Lv,Haotong Qin,Jun Zhang*

Main category: cs.CV

TL;DR: QVGen is a quantization-aware training framework for video diffusion models, enabling high performance under low-bit quantization (e.g., 4-bit) by reducing gradient norms and eliminating inference overhead.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models face high computational and memory demands, and existing quantization methods are ineffective for them. QVGen addresses this gap.

Method: QVGen uses auxiliary modules to reduce quantization errors and a rank-decay strategy (SVD + rank-based regularization) to eliminate inference overhead while retaining performance.

Result: QVGen achieves full-precision quality under 4-bit settings and outperforms existing methods, e.g., improving Dynamic Degree by +25.28 and Scene Consistency by +8.43 on VBench.

Conclusion: QVGen is a breakthrough for efficient video diffusion models, offering high performance under extreme low-bit quantization.

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,
their substantial computational and memory demands pose serious challenges to
real-world deployment, even on high-end GPUs. As a commonly adopted solution,
quantization has proven notable success in reducing cost for image DMs, while
its direct application to video DMs remains ineffective. In this paper, we
present QVGen, a novel quantization-aware training (QAT) framework tailored for
high-performance and inference-efficient video DMs under extremely low-bit
quantization (e.g., 4-bit or below). We begin with a theoretical analysis
demonstrating that reducing the gradient norm is essential to facilitate
convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to
mitigate large quantization errors, leading to significantly enhanced
convergence. To eliminate the inference overhead of $\Phi$, we propose a
rank-decay strategy that progressively eliminates $\Phi$. Specifically, we
repeatedly employ singular value decomposition (SVD) and a proposed rank-based
regularization $\mathbf{\gamma}$ to identify and decay low-contributing
components. This strategy retains performance while zeroing out inference
overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,
with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the
first to reach full-precision comparable quality under 4-bit settings.
Moreover, it significantly outperforms existing methods. For instance, our
3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and
$+8.43$ in Scene Consistency on VBench.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [145] [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
*Jacob Trauger,Ambuj Tewari*

Main category: stat.ML

TL;DR: The paper studies the consistency of next-token prediction algorithms (greedy, lookahead, random sampling, temperature-scaled sampling) with respect to various loss functions in LLMs, revealing trade-offs between goals like information retrieval and creative generation.


<details>
  <summary>Details</summary>
Motivation: To analyze the theoretical grounding of decoding algorithms in LLMs and their alignment with different goals (e.g., mimicking true distributions or minimizing sequence-level loss).

Method: Examines consistency of next-token prediction algorithms (greedy, lookahead, random sampling, temperature-scaled sampling) with respect to target loss functions.

Result: Random sampling is consistent with mimicking true distributions if next-token prediction converges, but no polynomial-time algorithm is universally optimal for all goals. Decoding algorithms are only optimal for specific probability distributions.

Conclusion: Choosing the right decoding algorithm is crucial, as current methods lack theoretical grounding for many scenarios, highlighting a dichotomy between information retrieval and creative generation goals.

Abstract: Probabilistic next-token prediction trained using cross-entropy loss is the
basis of most large language models. Given a sequence of previous values,
next-token prediction assigns a probability to each possible next value in the
vocabulary. There are many ways to use next-token prediction to output token
sequences. This paper examines a few of these algorithms (greedy, lookahead,
random sampling, and temperature-scaled random sampling) and studies their
consistency with respect to various goals encoded as loss functions. Although
consistency of surrogate losses with respect to a target loss function is a
well researched topic, we are the first to study it in the context of LLMs (to
the best of our knowledge). We find that, so long as next-token prediction
converges to its true probability distribution, random sampling is consistent
with outputting sequences that mimic sampling from the true probability
distribution. For the other goals, such as minimizing the 0-1 loss on the
entire sequence, we show no polynomial-time algorithm is optimal for all
probability distributions and all decoding algorithms studied are only optimal
for a subset of probability distributions. When analyzing these results, we see
that there is a dichotomy created between the goals of information retrieval
and creative generation for the decoding algorithms. This shows that choosing
the correct decoding algorithm based on the desired goal is extremely important
and many of the ones used are lacking theoretical grounding in numerous
scenarios.

</details>


### [146] [A Fourier Space Perspective on Diffusion Models](https://arxiv.org/abs/2505.11278)
*Fabian Falck,Teodora Pandeva,Kiarash Zahirnia,Rachel Lawrence,Richard Turner,Edward Meeds,Javier Zazo,Sushrut Karmalkar*

Main category: stat.ML

TL;DR: The paper analyzes the inductive bias of diffusion models in Fourier space, showing that standard DDPM corrupts high-frequency components faster, degrading generation quality. An alternative forward process equalizing corruption rates improves performance for high-frequency datasets.


<details>
  <summary>Details</summary>
Motivation: To understand how the forward process in diffusion models affects frequency components and generation quality, particularly for high-frequency data.

Method: Theoretical analysis and empirical demonstration of DDPM's frequency-based corruption, followed by proposing and testing an alternative forward process in Fourier space.

Result: Standard DDPM degrades high-frequency generation due to faster corruption. The alternative process improves performance for high-frequency datasets while matching DDPM on standard benchmarks.

Conclusion: The study highlights the impact of forward process design on generation quality and proposes a solution for high-frequency data.

Abstract: Diffusion models are state-of-the-art generative models on data modalities
such as images, audio, proteins and materials. These modalities share the
property of exponentially decaying variance and magnitude in the Fourier
domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)
forward process of additive white noise, this property results in
high-frequency components being corrupted faster and earlier in terms of their
Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then
generates low-frequency information before high-frequency details. In this
work, we study the inductive bias of the forward process of diffusion models in
Fourier space. We theoretically analyse and empirically demonstrate that the
faster noising of high-frequency components in DDPM results in violations of
the normality assumption in the reverse process. Our experiments show that this
leads to degraded generation quality of high-frequency components. We then
study an alternate forward process in Fourier space which corrupts all
frequencies at the same rate, removing the typical frequency hierarchy during
generation, and demonstrate marked performance improvements on datasets where
high frequencies are primary, while performing on par with DDPM on standard
imaging benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [147] [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
*Jiazheng Zhang,Wenqing Jing,Zizhuo Zhang,Zhiheng Xi,Shihan Dou,Rongxiang Weng,Jiahuan Li,Jingang Wang,MingXu Cai,Shibo Hong,Tao Gui,Qi Zhang*

Main category: cs.LG

TL;DR: CRM improves reward model robustness by combining peer review and curriculum learning to filter noisy preferences, enhancing LLM alignment with human values.


<details>
  <summary>Details</summary>
Motivation: Noisy human feedback leads to reward misgeneralization, causing RMs to overfit and provide misleading signals during policy optimization.

Method: Proposes Collaborative Reward Modeling (CRM), an online framework using peer review and curriculum learning to filter noise and structure training data.

Result: CRM achieves up to 9.94 points accuracy gain on RewardBench under 40% label noise and works with implicit-reward methods.

Conclusion: CRM offers a practical, versatile strategy for robust alignment of LLMs with human values.

Abstract: Reward models (RMs) are essential for aligning large language models (LLMs)
with human values. However, noisy preferences in human feedback often lead to
reward misgeneralization, where RMs overfit to spurious patterns and provide
misleading signals during policy optimization. We systematically analyze the
training dynamics of preference pairs and identify that noisy examples are
harder to fit and introduce instability. Empirical evidence shows that LLMs
optimized using reward models trained on full noisy datasets perform worse than
those trained on filtered, high-quality preferences. To address this, we
propose Collaborative Reward Modeling (CRM), an online framework that enhances
robustness by combining peer review and curriculum learning. Two reward models
are trained in parallel and assess each other's data selections to filter out
potential noise. Curriculum learning structures the preference data from easy
to hard, ensuring synchronized training and stable feedback. Extensive
experiments demonstrate that CRM improves generalization, with up to 9.94
points of accuracy gain on RewardBench under 40 percent label noise. CRM is
also compatible with implicit-reward alignment methods, offering a practical
and versatile strategy for robust alignment.

</details>


### [148] [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
*Jiaxuan Liu,Zhenhua Ling*

Main category: cs.LG

TL;DR: UDDETTS is a neural codec model for controllable emotional TTS, combining discrete and dimensional emotions using the ADV space for better emotion control and synthesis.


<details>
  <summary>Details</summary>
Motivation: Traditional TTS methods struggle with capturing the complexity of human emotions due to reliance on discrete labels and lack of fine-grained datasets.

Method: Proposes UDDETTS, integrating the ADV space for dimensional emotion control and a semi-supervised training strategy to utilize diverse datasets.

Result: UDDETTS achieves linear emotion control in ADV space and superior emotional speech synthesis.

Conclusion: UDDETTS addresses limitations in emotional TTS by unifying discrete and dimensional emotion control, improving synthesis quality.

Abstract: Recent neural codec language models have made great progress in the field of
text-to-speech (TTS), but controllable emotional TTS still faces many
challenges. Traditional methods rely on predefined discrete emotion labels to
control emotion categories and intensities, which can't capture the complexity
and continuity of human emotional perception and expression. The lack of
large-scale emotional speech datasets with balanced emotion distributions and
fine-grained emotion annotations often causes overfitting in synthesis models
and impedes effective emotion control. To address these issues, we propose
UDDETTS, a neural codec language model unifying discrete and dimensional
emotions for controllable emotional TTS. This model introduces the
interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion
description and supports emotion control driven by either discrete emotion
labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised
training strategy is designed to comprehensively utilize diverse speech
datasets with different types of emotion annotations to train the UDDETTS.
Experiments show that UDDETTS achieves linear emotion control along the three
dimensions of ADV space, and exhibits superior end-to-end emotional speech
synthesis capabilities.

</details>


### [149] [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
*Ran Li,Hao Wang,Chengzhi Mao*

Main category: cs.LG

TL;DR: LARGO introduces a gradient-based latent space attack for LLMs, outperforming existing methods by 44% in success rate.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of gradient-based methods in discrete language spaces for uncovering LLM vulnerabilities.

Method: LARGO optimizes adversarial latent vectors in the LLM's continuous latent space and decodes them into natural language.

Result: LARGO achieves a 44-point higher attack success rate than leading methods like AutoDAN.

Conclusion: LARGO demonstrates the effectiveness of gradient optimization for attacking LLM internals, offering a potent alternative to agentic prompting.

Abstract: Efficient red-teaming method to uncover vulnerabilities in Large Language
Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers,
the discrete language space make gradient-based methods struggle. We introduce
LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel
latent self-reflection attack that reasserts the power of gradient-based
optimization for generating fluent jailbreaking prompts. By operating within
the LLM's continuous latent space, LARGO first optimizes an adversarial latent
vector and then recursively call the same LLM to decode the latent into natural
language. This methodology yields a fast, effective, and transferable attack
that produces fluent and stealthy prompts. On standard benchmarks like AdvBench
and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including
AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent
alternative to agentic LLM prompting, highlighting the efficacy of interpreting
and attacking LLM internals through gradient optimization.

</details>


### [150] [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
*Haiqing Hao,Nikola Zubić,Weihua He,Zhipeng Sui,Davide Scaramuzza,Wenhui Wang*

Main category: cs.LG

TL;DR: EVA introduces a novel asynchronous-to-synchronous (A2S) framework for event cameras, leveraging language modeling techniques to create expressive, generalizable event representations, outperforming prior methods in recognition and detection tasks.


<details>
  <summary>Details</summary>
Motivation: Event cameras' sparse, asynchronous data challenges standard ML methods, and existing A2S approaches lack expressivity and generalizability compared to dense methods.

Method: EVA adapts linear attention and self-supervised learning from language modeling to asynchronously encode events into learned representations.

Result: EVA outperforms prior A2S methods on recognition tasks (DVS128-Gesture, N-Cars) and achieves 47.7 mAP on Gen1 detection tasks.

Conclusion: EVA demonstrates transformative potential for real-time event-based vision applications by bridging the gap between asynchronous event data and ML pipelines.

Abstract: Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.

</details>


### [151] [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
*Yi Xu,Chengzu Li,Han Zhou,Xingchen Wan,Caiqi Zhang,Anna Korhonen,Ivan Vulić*

Main category: cs.LG

TL;DR: The paper introduces Visual Planning, a paradigm for reasoning via visual representations instead of text, outperforming text-based methods in spatial tasks.


<details>
  <summary>Details</summary>
Motivation: Language may not be the most effective modality for reasoning in tasks involving spatial or geometrical information.

Method: Proposes Visual Planning via Reinforcement Learning (VPRL) using GRPO for post-training large vision models, tested in visual navigation tasks.

Result: VPRL outperforms text-only reasoning methods in tasks like FrozenLake, Maze, and MiniBehavior.

Conclusion: Visual Planning is a promising alternative to language-based reasoning, especially for intuitive, image-based tasks.

Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.

</details>


### [152] [A probabilistic framework for dynamic quantization](https://arxiv.org/abs/2505.10689)
*Gabriele Santini,Francesco Paissan,Elisabetta Farella*

Main category: cs.LG

TL;DR: A probabilistic framework for dynamic quantization of neural networks, enabling input-adaptive rescaling of quantization parameters with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of neural network quantization by dynamically adjusting parameters per input, reducing memory overhead while maintaining performance.

Method: Uses a probabilistic model on pre-activations via a lightweight surrogate to adaptively adjust quantization parameters.

Result: Validated on computer vision tasks with negligible performance loss, outperforming standard quantization in performance-overhead tradeoff.

Conclusion: The proposed framework offers an efficient and adaptive solution for neural network quantization, balancing performance and computational cost.

Abstract: We propose a probabilistic framework for dynamic quantization of neural
networks that allows for a computationally efficient input-adaptive rescaling
of the quantization parameters. Our framework applies a probabilistic model to
the network's pre-activations through a lightweight surrogate, enabling the
adaptive adjustment of the quantization parameters on a per-input basis without
significant memory overhead. We validate our approach on a set of popular
computer vision tasks and models, observing only a negligible loss in
performance. Our method strikes the best performance and computational overhead
tradeoff compared to standard quantization strategies.

</details>


### [153] [Hashing for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10873)
*Filippo Leveni,Luca Magri,Cesare Alippi,Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper introduces an isolation-based anomaly detection method using Locality Sensitive Hashing in a high-dimensional Preference Space, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To identify anomalies in datasets by leveraging structured patterns and high-dimensional embeddings without explicit distance computations.

Method: Uses Locality Sensitive Hashing (LSH) in a Preference Space to detect anomalies as isolated points, avoiding high computational costs.

Result: Achieves state-of-the-art anomaly detection performance with reduced computational expense.

Conclusion: The proposed method effectively identifies anomalies efficiently, with publicly available code for implementation.

Abstract: We focus on the problem of identifying samples in a set that do not conform
to structured patterns represented by low-dimensional manifolds. An effective
way to solve this problem is to embed data in a high dimensional space, called
Preference Space, where anomalies can be identified as the most isolated
points. In this work, we employ Locality Sensitive Hashing to avoid explicit
computation of distances in high dimensions and thus improve Anomaly Detection
efficiency. Specifically, we present an isolation-based anomaly detection
technique designed to work in the Preference Space which achieves
state-of-the-art performance at a lower computational cost. Code is publicly
available at
https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.

</details>


### [154] [MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection](https://arxiv.org/abs/2505.10874)
*Luca Magri,Filippo Leveni,Giacomo Boracchi*

Main category: cs.LG

TL;DR: MultiLink is a new algorithm for robustly fitting multiple geometric structures in noisy datasets, outperforming state-of-the-art methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of recovering multiple geometric structures (e.g., planes, cylinders) in noisy datasets with outliers, where existing methods struggle with sensitivity and efficiency.

Method: MultiLink combines preference analysis and clustering, integrating on-the-fly model fitting and selection via a novel linkage scheme to merge clusters.

Result: MultiLink is faster, less sensitive to inlier thresholds, and compensates for sampling limitations, outperforming alternatives in experiments.

Conclusion: MultiLink is a robust and efficient solution for multi-class and single-class geometric structure recovery, with publicly available code.

Abstract: We address the problem of recovering multiple structures of different classes
in a dataset contaminated by noise and outliers. In particular, we consider
geometric structures defined by a mixture of underlying parametric models (e.g.
planes and cylinders, homographies and fundamental matrices), and we tackle the
robust fitting problem by preference analysis and clustering. We present a new
algorithm, termed MultiLink, that simultaneously deals with multiple classes of
models. MultiLink combines on-the-fly model fitting and model selection in a
novel linkage scheme that determines whether two clusters are to be merged. The
resulting method features many practical advantages with respect to methods
based on preference analysis, being faster, less sensitive to the inlier
threshold, and able to compensate limitations deriving from hypotheses
sampling. Experiments on several public datasets demonstrate that Multi-Link
favourably compares with state of the art alternatives, both in multi-class and
single-class problems. Code is publicly made available for download.

</details>


### [155] [Preference Isolation Forest for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10876)
*Filippo Leveni,Luca Magri,Cesare Alippi,Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper introduces Preference Isolation Forest (PIF), a framework for anomaly detection by embedding data into a preference space and identifying anomalies as isolated points. Three isolation methods are proposed: Voronoi-iForest, RuzHash-iForest, and Sliding-PIF.


<details>
  <summary>Details</summary>
Motivation: To detect anomalies as samples deviating from low-dimensional manifolds, combining adaptive isolation with preference embedding.

Method: PIF embeds data into a high-dimensional preference space and uses three isolation approaches: Voronoi-iForest, RuzHash-iForest, and Sliding-PIF.

Result: The framework effectively identifies anomalies by leveraging isolation techniques in a preference space.

Conclusion: PIF provides a flexible and efficient solution for anomaly detection by combining isolation-based methods with preference embedding.

Abstract: We address the problem of detecting anomalies as samples that do not conform
to structured patterns represented by low-dimensional manifolds. To this end,
we conceive a general anomaly detection framework called Preference Isolation
Forest (PIF), that combines the benefits of adaptive isolation-based methods
with the flexibility of preference embedding. The key intuition is to embed the
data into a high-dimensional preference space by fitting low-dimensional
manifolds, and to identify anomalies as isolated points. We propose three
isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most
general solution, $ii$) RuzHash-iForest, that avoids explicit computation of
distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a
locality prior to improve efficiency and effectiveness.

</details>


### [156] [CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting](https://arxiv.org/abs/2505.10894)
*Yishuo Wang,Feng Zhou,Muping Zhou,Qicheng Meng,Zhijun Hu,Yi Wang*

Main category: cs.LG

TL;DR: CTP is a deep learning framework combining CNN, Transformer, and PINN for ocean front prediction, outperforming existing methods in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Existing methods like LSTM and ConvLSTM struggle with spatial continuity and physical consistency in multi-step forecasts for ocean fronts.

Method: CTP integrates CNN for spatial encoding, Transformer for temporal attention, and PINN for physical constraints.

Result: CTP achieves SOTA performance in single-step and multi-step predictions, excelling in accuracy, F1 score, and temporal stability.

Conclusion: CTP effectively addresses the limitations of prior methods, offering a robust solution for ocean front prediction.

Abstract: This paper proposes CTP, a novel deep learning framework that integrates
convolutional neural network(CNN), Transformer architectures, and
physics-informed neural network(PINN) for ocean front prediction. Ocean fronts,
as dynamic interfaces between distinct water masses, play critical roles in
marine biogeochemical and physical processes. Existing methods such as LSTM,
ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and
physical consistency over multi-step forecasts. CTP addresses these challenges
by combining localized spatial encoding, long-range temporal attention, and
physical constraint enforcement. Experimental results across south China
sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP
achieves state-of-the-art(SOTA) performance in both single-step and multi-step
predictions, significantly outperforming baseline models in accuracy, $F_1$
score, and temporal stability.

</details>


### [157] [Assessing the Performance of Analog Training for Transfer Learning](https://arxiv.org/abs/2505.11067)
*Omobayode Fagbohungbe,Corey Lammie,Malte J. Rasch,Takashi Ando,Tayfun Gokmen,Vijay Narayanan*

Main category: cs.LG

TL;DR: The paper introduces the c-TTv2 algorithm to address challenges in analog in-memory computing for deep learning training, demonstrating its performance and robustness in analog transfer learning.


<details>
  <summary>Details</summary>
Motivation: Current training algorithms fail to handle the asymmetric, non-linear, and variable behavior of analog memory devices, limiting the potential of analog in-memory computing for efficient deep learning.

Method: The c-TTv2 algorithm, leveraging a chopped technique, is evaluated for analog transfer learning using a Swin-ViT model on a CIFAR100 subset, testing robustness against device variations.

Result: The c-TTv2 algorithm shows promise in overcoming device-specific challenges, achieving good training outcomes despite weight transfer noise and symmetry issues.

Conclusion: The c-TTv2 algorithm is a viable solution for analog in-memory computing, addressing key limitations of existing methods and enabling efficient deep learning training.

Abstract: Analog in-memory computing is a next-generation computing paradigm that
promises fast, parallel, and energy-efficient deep learning training and
transfer learning (TL). However, achieving this promise has remained elusive
due to a lack of suitable training algorithms. Analog memory devices exhibit
asymmetric and non-linear switching behavior in addition to device-to-device
variation, meaning that most, if not all, of the current off-the-shelf training
algorithms cannot achieve good training outcomes. Also, recently introduced
algorithms have enjoyed limited attention, as they require bi-directionally
switching devices of unrealistically high symmetry and precision and are highly
sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which
leverages the chopped technique to address many of the challenges mentioned
above. In this paper, we assess the performance of the c-TTv2 algorithm for
analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also
investigate the robustness of our algorithm to changes in some device
specifications, including weight transfer noise, symmetry point skew, and
symmetry point variability

</details>


### [158] [What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold](https://arxiv.org/abs/2505.11128)
*Simone Azeglio,Arianna Di Bernardo*

Main category: cs.LG

TL;DR: The paper introduces a score-based Riemannian metric to analyze the geometric properties of data manifolds learned by diffusion models, improving interpolation and extrapolation of image data.


<details>
  <summary>Details</summary>
Motivation: To understand the poorly explored geometric properties of data manifolds in diffusion models and leverage them for better image transformations.

Method: Develops a score-based Riemannian metric using the Stein score function to define a metric tensor, enabling geodesic computation for interpolation and extrapolation.

Result: Outperforms baselines on perceptual (LPIPS) and distribution-level (FID, KID) metrics, producing smoother, more realistic image transitions.

Conclusion: The method reveals the implicit geometric structure in diffusion models and offers a principled way to navigate natural image manifolds using Riemannian geometry.

Abstract: Recent advances in diffusion models have demonstrated their remarkable
ability to capture complex image distributions, but the geometric properties of
the learned data manifold remain poorly understood. We address this gap by
introducing a score-based Riemannian metric that leverages the Stein score
function from diffusion models to characterize the intrinsic geometry of the
data manifold without requiring explicit parameterization. Our approach defines
a metric tensor in the ambient space that stretches distances perpendicular to
the manifold while preserving them along tangential directions, effectively
creating a geometry where geodesics naturally follow the manifold's contours.
We develop efficient algorithms for computing these geodesics and demonstrate
their utility for both interpolation between data points and extrapolation
beyond the observed data distribution. Through experiments on synthetic data
with known geometry, Rotated MNIST, and complex natural images via Stable
Diffusion, we show that our score-based geodesics capture meaningful
transformations that respect the underlying data distribution. Our method
consistently outperforms baseline approaches on perceptual metrics (LPIPS) and
distribution-level metrics (FID, KID), producing smoother, more realistic image
transitions. These results reveal the implicit geometric structure learned by
diffusion models and provide a principled way to navigate the manifold of
natural images through the lens of Riemannian geometry.

</details>


### [159] [Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection](https://arxiv.org/abs/2505.11134)
*Desong Zhang,Jia Hu,Geyong Min*

Main category: cs.LG

TL;DR: SNNs trained with direct encoding and BPTT are vulnerable to catastrophic collapse from slight data distribution shifts. DEP, a hyperparameter-free method, mitigates this by reducing the Hessian spectral radius, enhancing robustness.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the vulnerability of SNNs trained with direct encoding and BPTT to minor data distribution shifts, which can cause catastrophic network collapse.

Method: The authors propose Dominant Eigencomponent Projection (DEP), which orthogonally projects gradients to remove dominant components, reducing the Hessian spectral radius.

Result: DEP effectively prevents SNNs from settling into sharp minima, mitigating vulnerability to heterogeneous data poisoning and improving overall robustness.

Conclusion: DEP provides a safer and more reliable method for SNN deployment, addressing the identified vulnerability.

Abstract: Spiking Neural Networks (SNNs) process information via discrete spikes,
enabling them to operate at remarkably low energy levels. However, our
experimental observations reveal a striking vulnerability when SNNs are trained
using the mainstream method--direct encoding combined with backpropagation
through time (BPTT): even a single backward pass on data drawn from a slightly
different distribution can lead to catastrophic network collapse. Our
theoretical analysis attributes this vulnerability to the repeated inputs
inherent in direct encoding and the gradient accumulation characteristic of
BPTT, which together produce an exceptional large Hessian spectral radius. To
address this challenge, we develop a hyperparameter-free method called Dominant
Eigencomponent Projection (DEP). By orthogonally projecting gradients to
precisely remove their dominant components, DEP effectively reduces the Hessian
spectral radius, thereby preventing SNNs from settling into sharp minima.
Extensive experiments demonstrate that DEP not only mitigates the vulnerability
of SNNs to heterogeneous data poisoning, but also significantly enhances
overall robustness compared to key baselines, providing strong support for
safer and more reliable SNN deployment.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [160] [$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/abs/2505.11079)
*Hao Gu,Jiangyan Yi,Chenglong Wang,Jianhua Tao,Zheng Lian,Jiayi He,Yong Ren,Yujie Chen,Zhengqi Wen*

Main category: cs.SD

TL;DR: The paper explores using Audio Large Language Models (ALLMs) for Audio Deepfake Detection (ADD), proposing a framework called ALLM4ADD that reformulates ADD as an audio question-answering task.


<details>
  <summary>Details</summary>
Motivation: The rise of high-fidelity audio generative models and their misuse potential necessitates effective ADD methods. ALLMs' success in audio tasks prompts investigation into their utility for ADD.

Method: The paper evaluates ALLMs' zero-shot performance on ADD, then introduces ALLM4ADD, a framework reformulating ADD as an audio question-answering task with supervised fine-tuning.

Result: ALLM4ADD outperforms in fake audio detection, especially in data-scarce scenarios, demonstrating ALLMs' potential for ADD.

Conclusion: This pioneering work highlights ALLMs' promise for ADD and encourages further research in leveraging them for more effective detection systems.

Abstract: Audio deepfake detection (ADD) has grown increasingly important due to the
rise of high-fidelity audio generative models and their potential for misuse.
Given that audio large language models (ALLMs) have made significant progress
in various audio processing tasks, a heuristic question arises: Can ALLMs be
leveraged to solve ADD?. In this paper, we first conduct a comprehensive
zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in
detecting fake audio. To enhance their performance, we propose
$\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we
reformulate ADD task as an audio question answering problem, prompting the
model with the question: "Is this audio fake or real?". We then perform
supervised fine-tuning to enable the ALLM to assess the authenticity of query
audio. Extensive experiments are conducted to demonstrate that our ALLM-based
method can achieve superior performance in fake audio detection, particularly
in data-scarce scenarios. As a pioneering study, we anticipate that this work
will inspire the research community to leverage ALLMs to develop more effective
ADD systems.

</details>


### [161] [Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese](https://arxiv.org/abs/2505.11200)
*Xihuai Wang,Ziyi Zhao,Siyu Ren,Shao Zhang,Song Li,Xiaoyu Li,Ziwen Wang,Lin Qiu,Guanglu Wan,Xuezhi Cao,Xunliang Cai,Weinan Zhang*

Main category: cs.SD

TL;DR: The paper introduces the Audio Turing Test (ATT) and ATT-Corpus, a multi-dimensional Chinese dataset, to improve TTS evaluation by simplifying human judgment and reducing bias. It also presents Auto-ATT, a finetuned model for automatic evaluation, showing strong alignment with human judgments.


<details>
  <summary>Details</summary>
Motivation: Current TTS evaluation methods like MOS are subjective and lack multi-dimensional design, especially for Chinese TTS. The paper aims to address these limitations.

Method: Proposes ATT-Corpus and a Turing-Test-inspired protocol where evaluators judge if a voice sounds human. Also finetunes Qwen2-Audio-Instruct as Auto-ATT for automatic evaluation.

Result: ATT effectively differentiates TTS models across dimensions, and Auto-ATT aligns well with human evaluations, proving its reliability.

Conclusion: ATT and Auto-ATT offer robust, simplified evaluation tools for TTS systems, addressing current limitations and supporting rapid model development.

Abstract: Recent advances in large language models (LLMs) have significantly improved
text-to-speech (TTS) systems, enhancing control over speech style, naturalness,
and emotional expression, which brings TTS Systems closer to human-level
performance. Although the Mean Opinion Score (MOS) remains the standard for TTS
System evaluation, it suffers from subjectivity, environmental inconsistencies,
and limited interpretability. Existing evaluation datasets also lack a
multi-dimensional design, often neglecting factors such as speaking styles,
context diversity, and trap utterances, which is particularly evident in
Chinese TTS evaluation. To address these challenges, we introduce the Audio
Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired
with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on
complex MOS scales or direct model comparisons, ATT asks evaluators to judge
whether a voice sounds human. This simplification reduces rating bias and
improves evaluation robustness. To further support rapid model development, we
also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for
automatic evaluation. Experimental results show that ATT effectively
differentiates models across specific capability dimensions using its
multi-dimensional design. Auto-ATT also demonstrates strong alignment with
human evaluations, confirming its value as a fast and reliable assessment tool.
The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face
Collection
(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).

</details>


### [162] [Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization](https://arxiv.org/abs/2505.11217)
*Yanhao Jia,Ji Xie,S Jivaganesh,Hao Li,Xu Wu,Mengmi Zhang*

Main category: cs.SD

TL;DR: The paper explores how AI and humans resolve sensory conflicts in sound localization, finding humans outperform AI by prioritizing sound. The study benchmarks AI models against humans, revealing AI's bias toward visuals. A refined AI model, trained with stereo audio, improves performance and mimics human-like localization bias.


<details>
  <summary>Details</summary>
Motivation: To understand how AI systems handle cross-modal conflicts in sound localization and compare their performance to humans, who prioritize auditory cues over misleading visuals.

Method: Benchmarking leading multimodal AI models against human performance in psychophysics experiments across six audiovisual conditions, including congruent, conflicting, and absent cues. A refined AI model is trained using a stereo audio-image dataset generated via 3D simulations.

Result: Humans outperform AI in resolving conflicts, relying more on auditory cues. AI models default to visuals, degrading performance. The refined AI model surpasses benchmarks and mimics human-like horizontal localization bias.

Conclusion: Sensory input quality and system architecture significantly impact multimodal representation accuracy. The refined AI model shows promise in bridging the gap between AI and human performance in sound localization.

Abstract: Imagine hearing a dog bark and turning toward the sound only to see a parked
car, while the real, silent dog sits elsewhere. Such sensory conflicts test
perception, yet humans reliably resolve them by prioritizing sound over
misleading visuals. Despite advances in multimodal AI integrating vision and
audio, little is known about how these systems handle cross-modal conflicts or
whether they favor one modality. In this study, we systematically examine
modality bias and conflict resolution in AI sound localization. We assess
leading multimodal models and benchmark them against human performance in
psychophysics experiments across six audiovisual conditions, including
congruent, conflicting, and absent cues. Humans consistently outperform AI,
demonstrating superior resilience to conflicting or missing visuals by relying
on auditory information. In contrast, AI models often default to visual input,
degrading performance to near chance levels. To address this, we finetune a
state-of-the-art model using a stereo audio-image dataset generated via 3D
simulations. Even with limited training data, the refined model surpasses
existing benchmarks. Notably, it also mirrors human-like horizontal
localization bias favoring left-right precision-likely due to the stereo audio
structure reflecting human ear placement. These findings underscore how sensory
input quality and system architecture shape multimodal representation accuracy.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [163] [Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports](https://arxiv.org/abs/2505.10586)
*Poli A. Nemkova,Suleyman O. Polat,Rafid I. Jahan,Sagnik Ray Choudhury,Sun-joo Lee,Shouryadipta Sarkar,Mark V. Albert*

Main category: cs.CY

TL;DR: A dynamic RAG system automates situation awareness reports from real-time data, evaluated via a three-level framework for quality assurance, improving decision-making efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of vast, heterogeneous data causes delays in humanitarian response and conflict monitoring, necessitating automated solutions.

Method: The system integrates real-time data from diverse sources, constructs query-specific knowledge bases, and uses a three-level evaluation framework (NLP metrics, expert feedback, LLM-as-a-Judge) for quality assurance.

Result: The system effectively produces coherent, insightful, and actionable reports, reducing human analyst burden and accelerating decision-making.

Conclusion: The approach enhances situation awareness by automating report generation, with open-sourced tools for reproducibility and further research.

Abstract: Timely and accurate situation awareness is vital for decision-making in
humanitarian response, conflict monitoring, and early warning and early action.
However, the manual analysis of vast and heterogeneous data sources often
results in delays, limiting the effectiveness of interventions. This paper
introduces a dynamic Retrieval-Augmented Generation (RAG) system that
autonomously generates situation awareness reports by integrating real-time
data from diverse sources, including news articles, conflict event databases,
and economic indicators. Our system constructs query-specific knowledge bases
on demand, ensuring timely, relevant, and accurate insights.
  To ensure the quality of generated reports, we propose a three-level
evaluation framework that combines semantic similarity metrics, factual
consistency checks, and expert feedback. The first level employs automated NLP
metrics to assess coherence and factual accuracy. The second level involves
human expert evaluation to verify the relevance and completeness of the
reports. The third level utilizes LLM-as-a-Judge, where large language models
provide an additional layer of assessment to ensure robustness. The system is
tested across multiple real-world scenarios, demonstrating its effectiveness in
producing coherent, insightful, and actionable reports. By automating report
generation, our approach reduces the burden on human analysts and accelerates
decision-making processes. To promote reproducibility and further research, we
openly share our code and evaluation tools with the community via GitHub.

</details>


### [164] [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation](https://arxiv.org/abs/2505.10588)
*Manisha Mehta,Fausto Giunchiglia*

Main category: cs.CY

TL;DR: The paper evaluates AI systems' ability to detect hidden harassment in Gen Alpha's digital language, revealing gaps in comprehension and proposing improvements for youth safety.


<details>
  <summary>Details</summary>
Motivation: Gen Alpha's unique digital communication, shaped by gaming and memes, poses new online risks that current AI and human moderation tools fail to address.

Method: Four AI models (GPT-4, Claude, Gemini, Llama 3) were tested on a dataset of 100 Gen Alpha expressions from gaming, social media, and videos to assess detection of masked harassment.

Result: The study found critical failures in AI comprehension, highlighting the need for redesigned safety systems attuned to youth communication.

Conclusion: The research underscores the urgency of adapting AI moderation tools to Gen Alpha's linguistic divergence and involving youth perspectives in digital safety solutions.

Abstract: This research offers a unique evaluation of how AI systems interpret the
digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first
cohort raised alongside AI, Gen Alpha faces new forms of online risk due to
immersive digital engagement and a growing mismatch between their evolving
communication and existing safety tools. Their distinct language, shaped by
gaming, memes, and AI-driven trends, often conceals harmful interactions from
both human moderators and automated systems. We assess four leading AI models
(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked
harassment and manipulation within Gen Alpha discourse. Using a dataset of 100
recent expressions from gaming platforms, social media, and video content, the
study reveals critical comprehension failures with direct implications for
online safety. This work contributes: (1) a first-of-its-kind dataset capturing
Gen Alpha expressions; (2) a framework to improve AI moderation systems for
youth protection; (3) a multi-perspective evaluation including AI systems,
human moderators, and parents, with direct input from Gen Alpha co-researchers;
and (4) an analysis of how linguistic divergence increases youth vulnerability.
Findings highlight the urgent need to redesign safety systems attuned to youth
communication, especially given Gen Alpha reluctance to seek help when adults
fail to understand their digital world. This study combines the insight of a
Gen Alpha researcher with systematic academic analysis to address critical
digital safety challenges.

</details>


### [165] [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
*Pierre Le Jeune,Benoît Malésieux,Weixuan Xiao,Matteo Dora*

Main category: cs.CY

TL;DR: Phare is a multilingual framework evaluating LLM safety across hallucination, biases, and harmful content, revealing systematic vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluations focus on performance, neglecting failure modes, necessitating a tool like Phare for comprehensive safety assessment.

Method: Phare probes LLMs across three dimensions: hallucination/reliability, social biases, and harmful content, evaluating 17 state-of-the-art models.

Result: Identified systematic vulnerabilities like sycophancy, prompt sensitivity, and stereotype reproduction in LLMs.

Conclusion: Phare offers actionable insights to improve LLM robustness, alignment, and trustworthiness by pinpointing specific failure modes.

Abstract: Ensuring the safety of large language models (LLMs) is critical for
responsible deployment, yet existing evaluations often prioritize performance
over identifying failure modes. We introduce Phare, a multilingual diagnostic
framework to probe and evaluate LLM behavior across three critical dimensions:
hallucination and reliability, social biases, and harmful content generation.
Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic
vulnerabilities across all safety dimensions, including sycophancy, prompt
sensitivity, and stereotype reproduction. By highlighting these specific
failure modes rather than simply ranking models, Phare provides researchers and
practitioners with actionable insights to build more robust, aligned, and
trustworthy language systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [166] [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
*Simeng Han,Stephen Xia,Grant Zhang,Howard Dai,Chen Liu,Lichang Chen,Hoang Huy Nguyen,Hongyuan Mei,Jiayuan Mao,R. Thomas McCoy*

Main category: cs.AI

TL;DR: The paper introduces a benchmark using brainteasers to evaluate LLMs' reasoning strategies, focusing on correctness, quality, and creativity of solutions.


<details>
  <summary>Details</summary>
Motivation: To move beyond accuracy and understand the reasoning strategies LLMs use, especially creative and insightful problem-solving.

Method: A benchmark with brainteasers analyzed across multiple reasoning layers: semantic parsing, solution generation, self-correction, step-by-step sketches, and hint usage.

Result: LLMs can produce creative solutions but sometimes default to brute force, indicating room for improvement in reasoning.

Conclusion: LLMs show potential for creative problem-solving but need refinement to avoid brute-force approaches.

Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.

</details>


### [167] [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
*Yexiang Liu,Zekun Li,Zhi Fang,Nan Xu,Ran He,Tieniu Tan*

Main category: cs.AI

TL;DR: The paper investigates how reasoning prompting strategies scale with test-time compute in LLMs, finding that simpler strategies like Chain-of-Thought outperform complex ones as compute increases. It also proposes a method to predict scaling performance and suggests improvements.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on how prompting strategies scale with compute, despite its importance for practical LLM applications.

Method: Systematic experiments on 6 LLMs, 8 prompting strategies, and 6 benchmarks, followed by theoretical analysis and a proposed prediction method.

Result: Complex prompting strategies initially perform better but are outperformed by simpler ones like Chain-of-Thought as compute scales. A method to predict scaling performance is introduced.

Conclusion: The study encourages re-evaluating complex prompting, highlights the potential of simple strategies, and offers insights for improving test-time scaling.

Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a method according to probability theory to quickly
and accurately predict the scaling performance and select the best strategy
under large sampling times without extra resource-intensive inference in
practice. It can serve as the test-time scaling law for majority voting.
Furthermore, we introduce two ways derived from our theoretical analysis to
significantly improve the scaling performance. We hope that our research can
promote to re-examine the role of complicated prompting, unleash the potential
of simple prompting strategies, and provide new insights for enhancing
test-time scaling performance.

</details>


### [168] [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
*Zheng Li,Qingxiu Dong,Jingyuan Ma,Di Zhang,Zhifang Sui*

Main category: cs.AI

TL;DR: SelfBudgeter is a self-adaptive reasoning strategy that optimizes resource use by pre-estimating query difficulty and controlling output length, achieving significant compression without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models inefficiently process all queries uniformly, wasting resources and increasing latency.

Method: Dual-phase training: pre-estimating reasoning cost and using budget-guided GPRO for reinforcement learning to control output length.

Result: Up to 74.47% response length compression on MATH benchmark with maintained accuracy.

Conclusion: SelfBudgeter efficiently allocates budgets based on query complexity, improving resource use and user control.

Abstract: Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [169] [REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?](https://arxiv.org/abs/2505.10872)
*Chenxi Jiang,Chuhao Zhou,Jianfei Yang*

Main category: cs.RO

TL;DR: The paper addresses vagueness in human instructions for robot task planning, introduces REI-Bench to benchmark performance drops, and proposes a solution improving clarity.


<details>
  <summary>Details</summary>
Motivation: Real-world users, especially non-experts like the elderly and children, often give vague instructions, degrading robot task planning performance.

Method: Proposes REI-Bench to evaluate vagueness impact and introduces task-oriented context cognition to clarify instructions.

Result: Vagueness in referring expressions causes up to 77.9% performance drop; the proposed method outperforms existing techniques.

Conclusion: The work enhances practical robot task planning for non-expert users, advancing human-robot interaction research.

Abstract: Robot task planning decomposes human instructions into executable action
sequences that enable robots to complete a series of complex tasks. Although
recent large language model (LLM)-based task planners achieve amazing
performance, they assume that human instructions are clear and straightforward.
However, real-world users are not experts, and their instructions to robots
often contain significant vagueness. Linguists suggest that such vagueness
frequently arises from referring expressions (REs), whose meanings depend
heavily on dialogue context and environment. This vagueness is even more
prevalent among the elderly and children, who robots should serve more. This
paper studies how such vagueness in REs within human instructions affects
LLM-based robot task planning and how to overcome this issue. To this end, we
propose the first robot task planning benchmark with vague REs (REI-Bench),
where we discover that the vagueness of REs can severely degrade robot planning
performance, leading to success rate drops of up to 77.9%. We also observe that
most failure cases stem from missing objects in planners. To mitigate the REs
issue, we propose a simple yet effective approach: task-oriented context
cognition, which generates clear instructions for robots, achieving
state-of-the-art performance compared to aware prompt and chains of thought.
This work contributes to the research community of human-robot interaction
(HRI) by making robot task planning more practical, particularly for non-expert
users, e.g., the elderly and children.

</details>


### [170] [TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation](https://arxiv.org/abs/2505.10696)
*Manthan Patel,Fan Yang,Yuheng Qiu,Cesar Cadena,Sebastian Scherer,Marco Hutter,Wenshan Wang*

Main category: cs.RO

TL;DR: TartanGround is a large-scale, multi-modal dataset for ground robots, featuring diverse data types like RGB stereo, depth, LiDAR, and semantic labels, collected via an automated pipeline. It aims to improve robotic perception and autonomy across varied environments.


<details>
  <summary>Details</summary>
Motivation: Existing datasets struggle to generalize across diverse scenes, limiting advancements in robotic perception and autonomy. TartanGround addresses this gap by providing a comprehensive, multi-modal dataset.

Method: Data is collected using an automated pipeline in photorealistic simulations, mimicking motion patterns of various ground robots. The dataset includes 910 trajectories across 70 environments, totaling 1.5 million samples.

Result: Evaluations show state-of-the-art methods trained on existing datasets fail to generalize. TartanGround serves as a robust testbed for tasks like occupancy prediction, SLAM, and neural scene representation.

Conclusion: TartanGround enables advancements in robotic perception and autonomy by providing a diverse, scalable dataset, fostering generalizable models for real-world applications.

Abstract: We present TartanGround, a large-scale, multi-modal dataset to advance the
perception and autonomy of ground robots operating in diverse environments.
This dataset, collected in various photorealistic simulation environments
includes multiple RGB stereo cameras for 360-degree coverage, along with depth,
optical flow, stereo disparity, LiDAR point clouds, ground truth poses,
semantic segmented images, and occupancy maps with semantic labels. Data is
collected using an integrated automatic pipeline, which generates trajectories
mimicking the motion patterns of various ground robot platforms, including
wheeled and legged robots. We collect 910 trajectories across 70 environments,
resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM
tasks reveal that state-of-the-art methods trained on existing datasets
struggle to generalize across diverse scenes. TartanGround can serve as a
testbed for training and evaluation of a broad range of learning-based tasks,
including occupancy prediction, SLAM, neural scene representation,
perception-based navigation, and more, enabling advancements in robotic
perception and autonomy towards achieving robust models generalizable to more
diverse scenarios. The dataset and codebase for data collection will be made
publicly available upon acceptance. Webpage: https://tartanair.org/tartanground

</details>


### [171] [GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats](https://arxiv.org/abs/2505.10923)
*Simeon Adebola,Shuangyu Xie,Chung Min Kim,Justin Kerr,Bart M. van Marrewijk,Mieke van Vlaardingen,Tim van Daalen,Robert van Loo,Jose Luis Susa Rincon,Eugen Solowjow,Rick van de Zedde,Ken Goldberg*

Main category: cs.RO

TL;DR: A novel framework for temporal plant growth reconstruction using 3D Gaussian Splatting and a two-stage alignment pipeline.


<details>
  <summary>Details</summary>
Motivation: Accurate temporal reconstructions of plant growth are challenging due to complex geometries, occlusions, and deformations.

Method: Combines 3D Gaussian Splatting with a two-stage registration (coarse feature-based matching and fine ICP alignment).

Result: Produces consistent 4D models of plant development, validated on Sequoia and Quinoa species.

Conclusion: The framework enables detailed temporal reconstructions for plant phenotyping and breeding.

Abstract: Accurate temporal reconstructions of plant growth are essential for plant
phenotyping and breeding, yet remain challenging due to complex geometries,
occlusions, and non-rigid deformations of plants. We present a novel framework
for building temporal digital twins of plants by combining 3D Gaussian
Splatting with a robust sample alignment pipeline. Our method begins by
reconstructing Gaussian Splats from multi-view camera data, then leverages a
two-stage registration approach: coarse alignment through feature-based
matching and Fast Global Registration, followed by fine alignment with
Iterative Closest Point. This pipeline yields a consistent 4D model of plant
development in discrete time steps. We evaluate the approach on data from the
Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal
reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at
https://berkeleyautomation.github.io/GrowSplat/

</details>


### [172] [DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy](https://arxiv.org/abs/2505.11032)
*Yuran Wang,Ruihai Wu,Yue Chen,Jiarui Wang,Jiaqi Liang,Ziyu Zhu,Haoran Geng,Jitendra Malik,Pieter Abbeel,Hao Dong*

Main category: cs.RO

TL;DR: DexGarmentLab introduces a simulation environment for dexterous garment manipulation, leveraging garment structural correspondence for dataset generation and proposing HALO, a hierarchical policy for improved generalization.


<details>
  <summary>Details</summary>
Motivation: Existing research struggles with realistic simulations of dexterous garment manipulation, hindered by labor-intensive data collection methods and lack of generalization.

Method: Proposes DexGarmentLab with high-quality 3D assets and refined simulation techniques, and HALO, a hierarchical policy for transferable affordance points and generalizable trajectories.

Result: HALO outperforms existing methods, generalizing to unseen garment shapes and deformations.

Conclusion: DexGarmentLab and HALO address key challenges in garment manipulation, offering a scalable and efficient solution.

Abstract: Garment manipulation is a critical challenge due to the diversity in garment
categories, geometries, and deformations. Despite this, humans can effortlessly
handle garments, thanks to the dexterity of our hands. However, existing
research in the field has struggled to replicate this level of dexterity,
primarily hindered by the lack of realistic simulations of dexterous garment
manipulation. Therefore, we propose DexGarmentLab, the first environment
specifically designed for dexterous (especially bimanual) garment manipulation,
which features large-scale high-quality 3D assets for 15 task scenarios, and
refines simulation techniques tailored for garment modeling to reduce the
sim-to-real gap. Previous data collection typically relies on teleoperation or
training expert reinforcement learning (RL) policies, which are labor-intensive
and inefficient. In this paper, we leverage garment structural correspondence
to automatically generate a dataset with diverse trajectories using only a
single expert demonstration, significantly reducing manual intervention.
However, even extensive demonstrations cannot cover the infinite states of
garments, which necessitates the exploration of new algorithms. To improve
generalization across diverse garment shapes and deformations, we propose a
Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies
transferable affordance points to accurately locate the manipulation area, then
generates generalizable trajectories to complete the task. Through extensive
experiments and detailed analysis of our method and baseline, we demonstrate
that HALO consistently outperforms existing methods, successfully generalizing
to previously unseen instances even with significant variations in shape and
deformation where others fail. Our project page is available at:
https://wayrise.github.io/DexGarmentLab/.

</details>


### [173] [Planar Velocity Estimation for Fast-Moving Mobile Robots Using Event-Based Optical Flow](https://arxiv.org/abs/2505.11116)
*Liam Boyle,Jonas Kühne,Nicolas Baumann,Niklas Bastuck,Michele Magno*

Main category: cs.RO

TL;DR: A novel velocity estimation method for mobile robotics uses event cameras and planar kinematics, improving accuracy by 38.3% in lateral error compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current velocity estimation methods rely on wheel odometry and IMU data, which require strong assumptions or complex models that fail under varying conditions like slippery surfaces.

Method: The approach leverages planar kinematics and optical flow from event cameras pointed at the ground, avoiding traction assumptions.

Result: Experiments show performance on par with Event-VIO and a 38.3% improvement in lateral error, with effectiveness confirmed at highway speeds.

Conclusion: The method demonstrates significant potential for real-world deployment in autonomous driving and robotics.

Abstract: Accurate velocity estimation is critical in mobile robotics, particularly for
driver assistance systems and autonomous driving. Wheel odometry fused with
Inertial Measurement Unit (IMU) data is a widely used method for velocity
estimation; however, it typically requires strong assumptions, such as non-slip
steering, or complex vehicle dynamics models that do not hold under varying
environmental conditions like slippery surfaces. We introduce an approach to
velocity estimation that is decoupled from wheel-to-surface traction
assumptions by leveraging planar kinematics in combination with optical flow
from event cameras pointed perpendicularly at the ground. The asynchronous
micro-second latency and high dynamic range of event cameras make them highly
robust to motion blur, a common challenge in vision-based perception techniques
for autonomous driving. The proposed method is evaluated through in-field
experiments on a 1:10 scale autonomous racing platform and compared to precise
motion capture data, demonstrating not only performance on par with the
state-of-the-art Event-VIO method but also a 38.3 % improvement in lateral
error. Qualitative experiments at highway speeds of up to 32 m/s further
confirm the effectiveness of our approach, indicating significant potential for
real-world deployment.

</details>


### [174] [Open-Source Multi-Viewpoint Surgical Telerobotics](https://arxiv.org/abs/2505.11142)
*Guido Caccianiga,Yarden Sharon,Bernard Javot,Senya Polikovsky,Gökce Ergün,Ivan Capobianco,André L. Mihaljevic,Anton Deguet,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: The paper proposes expanding visualization and control in robotic minimally invasive surgery (MIS) by introducing adjustable viewpoints and multi-view 3D measurements to enhance collaboration, autonomy, and perception.


<details>
  <summary>Details</summary>
Motivation: To rethink and improve surgical teleoperation by leveraging additional viewpoints and advanced perception for better visualization, collaboration, and shared autonomy.

Method: Building a synchronized multi-viewpoint, multi-sensor robotic surgery system by integrating high-performance vision components and upgrading the da Vinci Research Kit control logic.

Result: A functional setup is reported, with potential impacts on research and future clinical practice, including open-sourcing the system for community improvement.

Conclusion: The system aims to advance robotic MIS by enabling novel visualization, collaboration, and autonomy, with open-source availability to accelerate research and clinical translation.

Abstract: As robots for minimally invasive surgery (MIS) gradually become more
accessible and modular, we believe there is a great opportunity to rethink and
expand the visualization and control paradigms that have characterized surgical
teleoperation since its inception. We conjecture that introducing one or more
additional adjustable viewpoints in the abdominal cavity would not only unlock
novel visualization and collaboration strategies for surgeons but also
substantially boost the robustness of machine perception toward shared
autonomy. Immediate advantages include controlling a second viewpoint and
teleoperating surgical tools from a different perspective, which would allow
collaborating surgeons to adjust their views independently and still maneuver
their robotic instruments intuitively. Furthermore, we believe that capturing
synchronized multi-view 3D measurements of the patient's anatomy would unlock
advanced scene representations. Accurate real-time intraoperative 3D perception
will allow algorithmic assistants to directly control one or more robotic
instruments and/or robotic cameras. Toward these goals, we are building a
synchronized multi-viewpoint, multi-sensor robotic surgery system by
integrating high-performance vision components and upgrading the da Vinci
Research Kit control logic. This short paper reports a functional summary of
our setup and elaborates on its potential impacts in research and future
clinical practice. By fully open-sourcing our system, we will enable the
research community to reproduce our setup, improve it, and develop powerful
algorithms, effectively boosting clinical translation of cutting-edge research.

</details>


### [175] [Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views](https://arxiv.org/abs/2505.11467)
*Abhishek Kashyap,Henrik Andreasson,Todor Stoyanov*

Main category: cs.RO

TL;DR: Using novel view synthesis (e.g., Gaussian Splatting) improves grasp pose accuracy and coverage by generating additional virtual views, reducing the need for multiple real camera movements.


<details>
  <summary>Details</summary>
Motivation: Multiple camera views improve grasp accuracy but are time-consuming and constrained by reachability. Novel view synthesis offers a solution by generating additional virtual viewpoints.

Method: Leverages scene representations like Gaussian Splatting to synthesize novel views for grasp pose generation, tested on the Graspnet-1billion dataset.

Result: Novel views added force-closure grasps and improved grasp coverage compared to sparse real views.

Conclusion: Novel view synthesis enhances grasp extraction, with potential for future extensions like single-image radiance fields or diffusion models.

Abstract: Vision based robot manipulation uses cameras to capture one or more images of
a scene containing the objects to be manipulated. Taking multiple images can
help if any object is occluded from one viewpoint but more visible from another
viewpoint. However, the camera has to be moved to a sequence of suitable
positions for capturing multiple images, which requires time and may not always
be possible, due to reachability constraints. So while additional images can
produce more accurate grasp poses due to the extra information available, the
time-cost goes up with the number of additional views sampled. Scene
representations like Gaussian Splatting are capable of rendering accurate
photorealistic virtual images from user-specified novel viewpoints. In this
work, we show initial results which indicate that novel view synthesis can
provide additional context in generating grasp poses. Our experiments on the
Graspnet-1billion dataset show that novel views contributed force-closure
grasps in addition to the force-closure grasps obtained from sparsely sampled
real views while also improving grasp coverage. In the future we hope this work
can be extended to improve grasp extraction from radiance fields constructed
with a single input image, using for example diffusion models or generalizable
radiance fields.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [176] [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852)
*Siyu Liu,Jiamin Xu,Beilin Ye,Bo Hu,David J. Srolovitz,Tongqi Wen*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces MatTools, a benchmark to evaluate LLMs in materials science by testing their ability to generate and execute code for physics-based computational tasks. It includes a QA benchmark (69,225 pairs) and a real-world tool-usage benchmark (49 tasks). Key findings highlight generalist LLMs outperform specialists, AI models understand AI tools better, and simplicity is advantageous.


<details>
  <summary>Details</summary>
Motivation: To assess and improve LLM capabilities in materials science by creating a standardized framework for evaluating their proficiency in generating and executing code for materials property calculations.

Method: Developed MatTools with two components: a QA benchmark derived from pymatgen (69,225 QA pairs) and a real-world tool-usage benchmark (49 tasks). Evaluated diverse LLMs using these benchmarks.

Result: Generalist LLMs outperformed specialists, AI models showed better understanding of AI tools, and simpler approaches were more effective.

Conclusion: MatTools provides a standardized framework for assessing and enhancing LLM capabilities in materials science, aiding the development of more effective AI systems for scientific research.

Abstract: Large language models (LLMs) are increasingly applied to materials science
questions, including literature comprehension, property prediction, materials
discovery and alloy design. At the same time, a wide range of physics-based
computational approaches have been developed in which materials properties can
be calculated. Here, we propose a benchmark application to evaluate the
proficiency of LLMs to answer materials science questions through the
generation and safe execution of codes based on such physics-based
computational materials science packages. MatTools is built on two
complementary components: a materials simulation tool question-answer (QA)
benchmark and a real-world tool-usage benchmark. We designed an automated
methodology to efficiently collect real-world materials science tool-use
examples. The QA benchmark, derived from the pymatgen (Python Materials
Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the
ability of an LLM to understand materials science tools. The real-world
benchmark contains 49 tasks (138 subtasks) requiring the generation of
functional Python code for materials property calculations. Our evaluation of
diverse LLMs yields three key insights: (1)Generalists outshine
specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a
standardized framework for assessing and improving LLM capabilities for
materials science tool applications, facilitating the development of more
effective AI systems for materials science and general scientific research.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [177] [Textured mesh Quality Assessment using Geometry and Color Field Similarity](https://arxiv.org/abs/2505.10824)
*Kaifa Yang,Qi Yang,Zhu Li,Yiling Xu*

Main category: cs.GR

TL;DR: A novel point-based TMQA method, FMQM, uses signed distance and color fields to assess textured mesh quality, outperforming SOTA methods with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing TMQA methods lack accuracy and robustness, prompting the need for a more effective solution leveraging fields for geometry and color representation.

Method: FMQM employs signed distance fields and a new color field (nearest surface point color field) to extract four visual perception-related features for mesh quality assessment.

Result: FMQM surpasses SOTA TMQA metrics on three benchmark datasets and offers low computational complexity.

Conclusion: FMQM is a practical, efficient solution for real-world 3D graphics applications, with publicly available code.

Abstract: Textured mesh quality assessment (TMQA) is critical for various 3D mesh
applications. However, existing TMQA methods often struggle to provide accurate
and robust evaluations. Motivated by the effectiveness of fields in
representing both 3D geometry and color information, we propose a novel
point-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes
signed distance fields and a newly proposed color field named nearest surface
point color field to realize effective mesh feature description. Four features
related to visual perception are extracted from the geometry and color fields:
geometry similarity, geometry gradient similarity, space color distribution
similarity, and space color gradient similarity. Experimental results on three
benchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA)
TMQA metrics. Furthermore, FMQM exhibits low computational complexity, making
it a practical and efficient solution for real-world applications in 3D
graphics and visualization. Our code is publicly available at:
https://github.com/yyyykf/FMQM.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [178] [MPMA: Preference Manipulation Attack Against Model Context Protocol](https://arxiv.org/abs/2505.11154)
*Zihan Wang,Hongwei Li,Rui Zhang,Yu Liu,Wenbo Jiang,Wenshu Fan,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: The paper introduces MCP Preference Manipulation Attack (MPMA), a security threat where attackers manipulate LLMs via customized MCP servers for economic gain. It proposes two methods (DPMA and GAPMA) to achieve this, with GAPMA balancing effectiveness and stealthiness.


<details>
  <summary>Details</summary>
Motivation: To expose and address security vulnerabilities in the MCP ecosystem, particularly the risk of third-party servers manipulating LLM preferences for economic benefits.

Method: Introduces DPMA (direct manipulation) and GAPMA (genetic algorithm-based stealthy manipulation) to demonstrate the attack.

Result: GAPMA effectively balances attack success and stealthiness, revealing a critical MCP vulnerability.

Conclusion: Urges robust defense mechanisms to ensure MCP ecosystem fairness.

Abstract: Model Context Protocol (MCP) standardizes interface mapping for large
language models (LLMs) to access external data and tools, which revolutionizes
the paradigm of tool selection and facilitates the rapid expansion of the LLM
agent tool ecosystem. However, as the MCP is increasingly adopted, third-party
customized versions of the MCP server expose potential security
vulnerabilities. In this paper, we first introduce a novel security threat,
which we term the MCP Preference Manipulation Attack (MPMA). An attacker
deploys a customized MCP server to manipulate LLMs, causing them to prioritize
it over other competing MCP servers. This can result in economic benefits for
attackers, such as revenue from paid MCP services or advertising income
generated from free servers. To achieve MPMA, we first design a Direct
Preference Manipulation Attack ($\mathtt{DPMA}$) that achieves significant
effectiveness by inserting the manipulative word and phrases into the tool name
and description. However, such a direct modification is obvious to users and
lacks stealthiness. To address these limitations, we further propose
Genetic-based Advertising Preference Manipulation Attack ($\mathtt{GAPMA}$).
$\mathtt{GAPMA}$ employs four commonly used strategies to initialize
descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.
The experiment results demonstrate that $\mathtt{GAPMA}$ balances high
effectiveness and stealthiness. Our study reveals a critical vulnerability of
the MCP in open ecosystems, highlighting an urgent need for robust defense
mechanisms to ensure the fairness of the MCP ecosystem.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [179] [GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution](https://arxiv.org/abs/2505.10577)
*Yutong Guo*

Main category: eess.IV

TL;DR: The paper proposes using 'Ghost features' to reduce redundancy in VSR models and combines them with RNN to address gradient disappearance, improving PSNR and SSIM while preserving texture details.


<details>
  <summary>Details</summary>
Motivation: Feature redundancy in VSR models and gradient disappearance in RNNs are identified as key issues needing improvement.

Method: Introduces 'Ghost features' to reduce redundancy and combines them with RNN for temporal modeling, using current and next frames, previous output, and hidden state as inputs.

Result: Experiments show improved PSNR and SSIM, with better preservation of texture details in videos.

Conclusion: The proposed method effectively addresses redundancy and gradient issues, enhancing VSR performance.

Abstract: Modern video super-resolution (VSR) systems based on convolutional neural
networks (CNNs) require huge computational costs. The problem of feature
redundancy is present in most models in many domains, but is rarely discussed
in VSR. We experimentally observe that many features in VSR models are also
similar to each other, so we propose to use "Ghost features" to reduce this
redundancy. We also analyze the so-called "gradient disappearance" phenomenon
generated by the conventional recurrent convolutional network (RNN) model, and
combine the Ghost module with RNN to complete the modeling on time series. The
current frame is used as input to the model together with the next frame, the
output of the previous frame and the hidden state. Extensive experiments on
several benchmark models and datasets show that the PSNR and SSIM of our
proposed modality are improved to some extent. Some texture details in the
video are also better preserved.

</details>


### [180] [ExploreGS: a vision-based low overhead framework for 3D scene reconstruction](https://arxiv.org/abs/2505.10578)
*Yunji Feng,Chengpu Yu,Fengrui Ran,Zhi Yang,Yinni Liu*

Main category: eess.IV

TL;DR: ExploreGS is a vision-based 3D scene reconstruction framework for drones, replacing lidar with RGB images for cost-effective, high-quality reconstructions. It integrates scene exploration and real-time processing using a BoW model, enabling on-board 3DGS training.


<details>
  <summary>Details</summary>
Motivation: To provide a low-cost, efficient alternative to lidar-based 3D reconstruction for drones, leveraging vision models for high-quality results.

Method: Uses RGB images and a BoW model for real-time processing, integrating scene exploration and reconstruction. On-board 3DGS training is enabled.

Result: Demonstrates efficient, high-quality reconstructions in simulations and real-world tests, comparable to state-of-the-art methods.

Conclusion: ExploreGS is a viable, cost-effective solution for 3D scene reconstruction on resource-constrained drones.

Abstract: This paper proposes a low-overhead, vision-based 3D scene reconstruction
framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces
traditional lidar-based point cloud acquisition process with a vision model,
achieving a high-quality reconstruction at a lower cost. The framework
integrates scene exploration and model reconstruction, and leverags a
Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore,
the 3D Gaussian Splatting (3DGS) training can be executed on-board.
Comprehensive experiments in both simulation and real-world environments
demonstrate the efficiency and applicability of the ExploreGS framework on
resource-constrained devices, while maintaining reconstruction quality
comparable to state-of-the-art methods.

</details>


### [181] [MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation](https://arxiv.org/abs/2505.10672)
*Hania Ghouse,Muzammil Behzad*

Main category: eess.IV

TL;DR: A novel anatomically-aware slice selector pipeline using a vision-language model (VLM) improves multi-organ segmentation by filtering irrelevant slices, introducing a new metric (SLC) for localization fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing 3D segmentation methods are resource-intensive, while 2D methods lack contextual awareness and suffer from class imbalance.

Method: Proposes a VLM-based pipeline for cross-view organ presence detection using tri-slice (2.5D) representations and introduces the SLC metric for evaluation.

Result: Substantial improvements over baselines in organ-focused slice filtering, reducing segmentation costs while maintaining accuracy.

Conclusion: The method enables efficient, spatially consistent organ filtering, enhancing segmentation efficiency without compromising anatomical fidelity.

Abstract: Efficient and accurate multi-organ segmentation from abdominal CT volumes is
a fundamental challenge in medical image analysis. Existing 3D segmentation
approaches are computationally and memory intensive, often processing entire
volumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods
suffer from class imbalance and lack cross-view contextual awareness. To
address these limitations, we propose a novel, anatomically-aware slice
selector pipeline that reduces input volume prior to segmentation. Our unified
framework introduces a vision-language model (VLM) for cross-view organ
presence detection using fused tri-slice (2.5D) representations from axial,
sagittal, and coronal planes. Our proposed model acts as an "expert" in
anatomical localization, reasoning over multi-view representations to
selectively retain slices with high structural relevance. This enables
spatially consistent filtering across orientations while preserving contextual
cues. More importantly, since standard segmentation metrics such as Dice or IoU
fail to measure the spatial precision of such slice selection, we introduce a
novel metric, Slice Localization Concordance (SLC), which jointly captures
anatomical coverage and spatial alignment with organ-centric reference slices.
Unlike segmentation-specific metrics, SLC provides a model-agnostic evaluation
of localization fidelity. Our model offers substantial improvement gains
against several baselines across all organs, demonstrating both accurate and
reliable organ-focused slice filtering. These results show that our method
enables efficient and spatially consistent organ filtering, thereby
significantly reducing downstream segmentation cost while maintaining high
anatomical fidelity.

</details>


### [182] [ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation](https://arxiv.org/abs/2505.10687)
*Sayed Mehedi Azim,Brian Corbett,Iman Dehzangi*

Main category: eess.IV

TL;DR: The paper introduces ROIsGAN, a novel GAN-based method for automated segmentation of hippocampal subregions from IHC images, outperforming existing models by 1-10% in Dice score and up to 11% in IoU.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of hippocampal subregions (DG, CA1, CA3) from IHC images is lacking, hindering research on disease mechanisms and therapies.

Method: Proposes ROIsGAN, a region-guided U-Net-based GAN, using adversarial learning and a novel discriminator loss combining Dice and binary cross-entropy.

Result: ROIsGAN outperforms conventional models, achieving 1-10% higher Dice scores and up to 11% better IoU, especially in challenging staining conditions.

Conclusion: The work provides foundational datasets and tools for automated hippocampal segmentation, advancing neuroscience research.

Abstract: The hippocampus, a critical brain structure involved in memory processing and
various neurodegenerative and psychiatric disorders, comprises three key
subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3
(CA3). Accurate segmentation of these subregions from histological tissue
images is essential for advancing our understanding of disease mechanisms,
developmental dynamics, and therapeutic interventions. However, no existing
methods address the automated segmentation of hippocampal subregions from
tissue images, particularly from immunohistochemistry (IHC) images. To bridge
this gap, we introduce a novel set of four comprehensive murine hippocampal IHC
datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed
stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing
structural, neuronal activity, and plasticity associated information.
Additionally, we propose ROIsGAN, a region-guided U-Net-based generative
adversarial network tailored for hippocampal subregion segmentation. By
leveraging adversarial learning, ROIsGAN enhances boundary delineation and
structural detail refinement through a novel region-guided discriminator loss
combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3
subregions, ROIsGAN consistently outperforms conventional segmentation models,
achieving performance gains ranging from 1-10% in Dice score and up to 11% in
Intersection over Union (IoU), particularly under challenging staining
conditions. Our work establishes foundational datasets and methods for
automated hippocampal segmentation, enabling scalable, high-precision analysis
of tissue images in neuroscience research. Our generated datasets, proposed
model as a standalone tool, and its corresponding source code are publicly
available at: https://github.com/MehediAzim/ROIsGAN

</details>


### [183] [Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients](https://arxiv.org/abs/2505.10691)
*Wanying Dou,Gorkem Durak,Koushik Biswas,Ziliang Hong,Andrea Mia Bejar,Elif Keles,Kaan Akin,Sukru Mehmet Erturk,Alpay Medetalibeyoglu,Marc Sala,Alexander Misharin,Hatice Savas,Mary Salvatore,Sachin Jambawalikar,Drew Torigian,Jayaram K. Udupa,Ulas Bagci*

Main category: eess.IV

TL;DR: The paper presents a deep learning and radiomics framework for predicting lung fibrosis in Long COVID patients using chest CT scans, achieving high accuracy and AUC scores.


<details>
  <summary>Details</summary>
Motivation: Address the uncertainty and challenges in diagnosing and managing Post-Acute Sequelae of COVID-19 (PASC), particularly lung fibrosis, due to its heterogeneous symptoms.

Method: A multi-center chest CT analysis framework combining convolutional neural networks (CNNs) and radiomics for fibrosis prediction, with Grad-CAM visualization for interpretability.

Result: Achieved 82.2% accuracy and 85.5% AUC in classification tasks, demonstrating the framework's effectiveness for early detection and risk assessment.

Conclusion: The study highlights the potential of deep learning-driven methods for improving clinical assessment and treatment planning for PASC-related lung fibrosis.

Abstract: While the acute phase of the COVID-19 pandemic has subsided, its long-term
effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known
as Long COVID. There remains substantial uncertainty regarding both its
duration and optimal management strategies. PASC manifests as a diverse array
of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and
neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and
musculoskeletal abnormalities--that extend beyond the acute infection phase.
This heterogeneous presentation poses substantial challenges for clinical
assessment, diagnosis, and treatment planning. In this paper, we focus on
imaging findings that may suggest fibrotic damage in the lungs, a critical
manifestation characterized by scarring of lung tissue, which can potentially
affect long-term respiratory function in patients with PASC. This study
introduces a novel multi-center chest CT analysis framework that combines deep
learning and radiomics for fibrosis prediction. Our approach leverages
convolutional neural networks (CNNs) and interpretable feature extraction,
achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate
the effectiveness of Grad-CAM visualization and radiomics-based feature
analysis in providing clinically relevant insights for PASC-related lung
fibrosis prediction. Our findings highlight the potential of deep
learning-driven computational methods for early detection and risk assessment
of PASC-related lung fibrosis--presented for the first time in the literature.

</details>


### [184] [Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling](https://arxiv.org/abs/2505.10729)
*NingFeng Que,Xiaofei Wang,Jingjing Chen,Yixuan Jiang,Chao Li*

Main category: eess.IV

TL;DR: C2-STi is a novel method for interpolating missing spatial transcriptomics (ST) slices, addressing challenges like tissue heterogeneity and gene correlations, and outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: The high cost and missing intermediate sections in spatial transcriptomics (ST) limit 3D insights. C2-STi aims to interpolate missing slices for comprehensive analysis.

Method: C2-STi uses distance-aware local structural modulation, pyramid gene co-expression correlation, and cross-modal alignment with H&E images to enhance ST interpolation.

Result: Experiments show C2-STi outperforms state-of-the-art methods in single-slice and multi-slice ST interpolation.

Conclusion: C2-STi effectively interpolates missing ST slices, improving spatial transcriptomics analysis feasibility.

Abstract: Spatial transcriptomics (ST) is a promising technique that characterizes the
spatial gene profiling patterns within the tissue context. Comprehensive ST
analysis depends on consecutive slices for 3D spatial insights, whereas the
missing intermediate tissue sections and high costs limit the practical
feasibility of generating multi-slice ST. In this paper, we propose C2-STi, the
first attempt for interpolating missing ST slices at arbitrary intermediate
positions between adjacent ST slices. Despite intuitive, effective ST
interpolation presents significant challenges, including 1) limited continuity
across heterogeneous tissue sections, 2) complex intrinsic correlation across
genes, and 3) intricate cellular structures and biological semantics within
each tissue section. To mitigate these challenges, in C2-STi, we design 1) a
distance-aware local structural modulation module to adaptively capture
cross-slice deformations and enhance positional correlations between ST slices,
2) a pyramid gene co-expression correlation module to capture multi-scale
biological associations among genes, and 3) a cross-modal alignment module that
integrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter
and align the essential cellular features across ST and H\&E images. Extensive
experiments on the public dataset demonstrate our superiority over
state-of-the-art approaches on both single-slice and multi-slice ST
interpolation. Codes are available at
https://github.com/XiaofeiWang2018/C2-STi.

</details>


### [185] [Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers](https://arxiv.org/abs/2505.10855)
*Aneesh Rangnekar,Nikhil Mankuzhy,Jonas Willmann,Chloe Choi,Abraham Wu,Maria Thor,Andreas Rimner,Harini Veeraraghavan*

Main category: eess.IV

TL;DR: A hybrid transformer convolutional network (HTN) was refined for robust cardiac substructure segmentation in lung and breast cancer patients, outperforming benchmarks and showing resilience to imaging and patient variations.


<details>
  <summary>Details</summary>
Motivation: AI segmentations for radiation treatment planning can fail in clinical cases differing from training data. This study aims to improve robustness for varying imaging contrasts and patient positions.

Method: A pretrained transformer was refined into an HTN, trained on balanced datasets (CECT and NCCT scans), and evaluated on validation sets with diverse patient characteristics.

Result: The balanced HTN model matched oracle performance with fewer training cases, outperformed TotalSegmentator, and showed robustness to contrast and scan position variations.

Conclusion: The HTN provides accurate and robust cardiac substructure segmentation, suitable for clinical use, even with limited labeled data.

Abstract: AI automated segmentations for radiation treatment planning (RTP) can
deteriorate when applied in clinical cases with different characteristics than
training dataset. Hence, we refined a pretrained transformer into a hybrid
transformer convolutional network (HTN) to segment cardiac substructures lung
and breast cancer patients acquired with varying imaging contrasts and patient
scan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124
non-contrast CT (NCCT) scans from patients with non-small cell lung cancers
acquired in supine position, was used to create oracle with all 180 training
cases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models were
evaluated on a held-out validation set of 60 cohort I patients and 66 patients
with breast cancer from cohort II acquired in supine (n=45) and prone (n=21)
positions. Accuracy was measured using DSC, HD95, and dose metrics. Publicly
available TotalSegmentator served as the benchmark. The oracle and balanced
models were similarly accurate (DSC Cohort I: 0.80 \pm 0.10 versus 0.81 \pm
0.10; Cohort II: 0.77 \pm 0.13 versus 0.80 \pm 0.12), outperforming
TotalSegmentator. The balanced model, using half the training cases as oracle,
produced similar dose metrics as manual delineations for all cardiac
substructures. This model was robust to CT contrast in 6 out of 8 substructures
and patient scan position variations in 5 out of 8 substructures and showed low
correlations of accuracy to patient size and age. A HTN demonstrated robustly
accurate (geometric and dose metrics) cardiac substructures segmentation from
CTs with varying imaging and patient characteristics, one key requirement for
clinical use. Moreover, the model combining pretraining with balanced
distribution of NCCT and CECT scans was able to provide reliably accurate
segmentations under varied conditions with far fewer labeled datasets compared
to an oracle model.

</details>


### [186] [Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges](https://arxiv.org/abs/2505.10993)
*Yuan Zhang,Xinfeng Zhang,Xiaoming Qi Xinyu Wu,Feng Chen,Guanyu Yang,Huazhu Fu*

Main category: eess.IV

TL;DR: A review of generative modeling in computational pathology, covering image/text generation, multimodal applications, and challenges like fidelity and ethics.


<details>
  <summary>Details</summary>
Motivation: To synthesize recent progress and highlight the potential of generative models in computational pathology for tasks like data augmentation and multimodal representation.

Method: Analysis of over 150 studies, tracing the evolution of generative architectures (GANs, diffusion models, foundation models) and examining datasets/evaluation protocols.

Result: Identifies key domains (image/text generation, multimodal applications) and ongoing limitations (fidelity, interpretability, ethical concerns).

Conclusion: Calls for unified, multimodal, clinically deployable systems and serves as a foundational reference for researchers in the field.

Abstract: Generative modeling has emerged as a promising direction in computational
pathology, offering capabilities such as data-efficient learning, synthetic
data augmentation, and multimodal representation across diverse diagnostic
tasks. This review provides a comprehensive synthesis of recent progress in the
field, organized into four key domains: image generation, text generation,
multimodal image-text generation, and other generative applications, including
spatial simulation and molecular inference. By analyzing over 150
representative studies, we trace the evolution of generative architectures from
early generative adversarial networks to recent advances in diffusion models
and foundation models with generative capabilities. We further examine the
datasets and evaluation protocols commonly used in this domain and highlight
ongoing limitations, including challenges in generating high-fidelity whole
slide images, clinical interpretability, and concerns related to the ethical
and legal implications of synthetic data. The review concludes with a
discussion of open challenges and prospective research directions, with an
emphasis on developing unified, multimodal, and clinically deployable
generative systems. This work aims to provide a foundational reference for
researchers and practitioners developing and applying generative models in
computational pathology.

</details>


### [187] [Diffusion Model in Hyperspectral Image Processing and Analysis: A Review](https://arxiv.org/abs/2505.11158)
*Xing Hu,Xiangcheng Liu,Qianqian Duan,Danfeng Hong,Dawei Zhang*

Main category: eess.IV

TL;DR: The paper reviews Diffusion Model's advantages in hyperspectral image processing, highlighting its effectiveness in high-dimensional data handling, denoising, and data enhancement.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with hyperspectral image analysis due to high dimensionality and noise. Diffusion Models offer a promising alternative.

Method: The paper reviews Diffusion Model applications in hyperspectral tasks like noise removal, classification, and anomaly detection.

Result: Diffusion Models improve accuracy and efficiency in hyperspectral image analysis.

Conclusion: Diffusion Models provide a new research direction for hyperspectral image processing, overcoming traditional limitations.

Abstract: Hyperspectral image processing and analysis has important application value
in remote sensing, agriculture and environmental monitoring, but its high
dimensionality, data redundancy and noise interference etc. bring great
challenges to the analysis. Traditional models have limitations in dealing with
these complex data, and it is difficult to meet the increasing demand for
analysis. In recent years, Diffusion Model, as an emerging generative model,
has shown unique advantages in hyperspectral image processing. By simulating
the diffusion process of data in time, the Diffusion Model can effectively
process high-dimensional data, generate high-quality samples, and perform well
in denoising and data enhancement. In this paper, we review the recent research
advances in diffusion modeling for hyperspectral image processing and analysis,
and discuss its applications in tasks such as high-dimensional data processing,
noise removal, classification, and anomaly detection. The performance of
diffusion-based models on image processing is compared and the challenges are
summarized. It is shown that the diffusion model can significantly improve the
accuracy and efficiency of hyperspectral image analysis, providing a new
direction for future research.

</details>


### [188] [From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging](https://arxiv.org/abs/2505.11394)
*Alexander Oberstrass,Esteban Vaca,Eric Upschulte,Meiqi Niu,Nicola Palomero-Gallagher,David Graessel,Christian Schiffer,Markus Axer,Katrin Amunts,Timo Dickscheid*

Main category: eess.IV

TL;DR: The paper proposes a deep learning method to virtually stain 3D-PLI images for cytoarchitectonic analysis, addressing misalignment issues and enabling detailed study of brain microstructure.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of post-staining histological sections and enable detailed study of the relationship between fiber- and cytoarchitecture in the same brain section.

Method: Uses deep learning for image-to-image translation, leveraging Fourier-based registration to align training data and predict Cresyl violet staining from 3D-PLI images.

Result: The method successfully predicts Cresyl violet staining from 3D-PLI, matching individual cell instances.

Conclusion: Deep learning enables virtual staining of 3D-PLI, facilitating high-resolution analysis of brain microstructure without physical staining limitations.

Abstract: Comprehensive assessment of the various aspects of the brain's microstructure
requires the use of complementary imaging techniques. This includes measuring
the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers
(myeloarchitecture). The gold standard for cytoarchitectonic analysis is light
microscopic imaging of cell-body stained tissue sections. To reveal the 3D
orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been
introduced as a reliable technique providing a resolution in the micrometer
range while allowing processing of series of complete brain sections. 3D-PLI
acquisition is label-free and allows subsequent staining of sections after
measurement. By post-staining for cell bodies, a direct link between fiber- and
cytoarchitecture can potentially be established within the same section.
However, inevitable distortions introduced during the staining process make a
nonlinear and cross-modal registration necessary in order to study the detailed
relationships between cells and fibers in the images. In addition, the
complexity of processing histological sections for post-staining only allows
for a limited number of samples. In this work, we take advantage of deep
learning methods for image-to-image translation to generate a virtual staining
of 3D-PLI that is spatially aligned at the cellular level. In a supervised
setting, we build on a unique dataset of brain sections, to which Cresyl violet
staining has been applied after 3D-PLI measurement. To ensure high
correspondence between both modalities, we address the misalignment of training
data using Fourier-based registration methods. In this way, registration can be
efficiently calculated during training for local image patches of target and
predicted staining. We demonstrate that the proposed method enables prediction
of a Cresyl violet staining from 3D-PLI, matching individual cell instances.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [189] [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)
*Omar Shaikh,Shardul Sapkota,Shan Rizvi,Eric Horvitz,Joon Sung Park,Diyi Yang,Michael S. Bernstein*

Main category: cs.HC

TL;DR: The paper introduces a General User Model (GUM) that learns from unstructured user interactions to infer preferences and knowledge, enabling proactive and context-aware applications like chat assistants and OS notifications.


<details>
  <summary>Details</summary>
Motivation: Current user models are fragmented and lack flexibility. The paper aims to create a unified, general-purpose model that understands users deeply from multimodal observations.

Method: GUM processes unstructured observations (e.g., screenshots) to construct confidence-weighted propositions about users. It infers new knowledge, retrieves context, and revises propositions continuously.

Result: GUMs make accurate, calibrated inferences and enable proactive assistants (GUMBOs) that perform useful actions without explicit user requests.

Conclusion: GUMs advance HCI by leveraging multimodal models to anticipate user needs, fulfilling long-standing visions of adaptive technology.

Abstract: Human-computer interaction has long imagined technology that understands
us-from our preferences and habits, to the timing and purpose of our everyday
actions. Yet current user models remain fragmented, narrowly tailored to
specific apps, and incapable of the flexible reasoning required to fulfill
these visions. This paper presents an architecture for a general user model
(GUM) that learns about you by observing any interaction you have with your
computer. The GUM takes as input any unstructured observation of a user (e.g.,
device screenshots) and constructs confidence-weighted propositions that
capture that user knowledge and preferences. GUMs can infer that a user is
preparing for a wedding they're attending from messages with a friend. Or
recognize that a user is struggling with a collaborator's feedback on a draft
by observing multiple stalled edits and a switch to reading related work. GUMs
introduce an architecture that infers new propositions about a user from
multimodal observations, retrieves related propositions for context, and
continuously revises existing propositions. To illustrate the breadth of
applications that GUMs enable, we demonstrate how they augment chat-based
assistants with context, manage OS notifications to selectively surface
important information, and enable interactive agents that adapt to preferences
across apps. We also instantiate proactive assistants (GUMBOs) that discover
and execute useful suggestions on a user's behalf using their GUM. In our
evaluations, we find that GUMs make calibrated and accurate inferences about
users, and that assistants built on GUMs proactively identify and perform
actions that users wouldn't think to request explicitly. Altogether, GUMs
introduce methods that leverage multimodal models to understand unstructured
context, enabling long-standing visions of HCI and entirely new interactive
systems that anticipate user needs.

</details>


### [190] [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)
*Jenny Xiyu Fu,Brennan Antone,Kowe Kadoma,Malte Jung*

Main category: cs.HC

TL;DR: The paper examines how co-writing with AI affects people's locus of control, finding that employment status influences reliance on AI and perceived agency.


<details>
  <summary>Details</summary>
Motivation: To understand the psychological impact of AI co-writing tools on users' sense of control and self-perception.

Method: An empirical study with 462 participants, analyzing quantitative and qualitative data.

Result: Employed participants relied more on AI and shifted toward internal control, while unemployed users felt reduced personal agency.

Conclusion: The study highlights AI's role in shaping personal agency and identity, calling for further discussion.

Abstract: As AI tools increasingly shape how we write, they may also quietly reshape
how we perceive ourselves. This paper explores the psychological impact of
co-writing with AI on people's locus of control. Through an empirical study
with 462 participants, we found that employment status plays a critical role in
shaping users' reliance on AI and their locus of control. Current results
demonstrated that employed participants displayed higher reliance on AI and a
shift toward internal control, while unemployed users tended to experience a
reduction in personal agency. Through quantitative results and qualitative
observations, this study opens a broader conversation about AI's role in
shaping personal agency and identity.

</details>
