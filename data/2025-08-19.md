<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.CV](#cs.CV) [Total: 156]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LG](#cs.LG) [Total: 16]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.AI](#cs.AI) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: 提出基于LLM内部权重激活构建语言度量空间的新框架，通过剪枝算法自动生成高维向量表示，无需手工特征工程


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工设计的语言学特征，需要探索利用LLM内部权重自动捕捉语言内在特性的新方法

Method: 使用改进的剪枝算法计算权重重要性分数，从多语言LLM中自动推导高维向量表示，覆盖106种语言

Result: 结果与传统语言家族分类高度一致，同时揭示了可能反映历史接触或语言演化的意外语言间联系

Conclusion: 该方法能有效捕捉语言内在特征，为语言关系研究提供了新的数据驱动视角，相关代码和工具已开源

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [2] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: 研究探索使用LLM生成的合成QA数据能否替代人工标注基准，发现在检索器配置评估中表现可靠，但在生成器架构比较中不一致


<details>
  <summary>Details</summary>
Motivation: 当缺乏人工标注数据时，需要验证合成数据能否作为可靠的评估基准

Method: 通过两个实验：固定生成器变化检索器参数，以及固定检索器变化生成器架构，在四个数据集上进行测试

Result: 合成基准在检索器配置评估中与人工基准一致，但在生成器架构比较中产生不一致的排名

Conclusion: 合成数据可作为检索器评估的代理，但不适用于生成器架构比较，存在任务不匹配和风格偏见问题

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [3] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: 使用模仿学习从专家对话数据中训练对话策略和判别器，策略能基于提示生成对话，判别器可区分专家与合成对话，该方法能识别对话模型的局限性


<details>
  <summary>Details</summary>
Motivation: 在没有奖励函数的情况下，通过专家演示来创建对话策略，解决对话系统中的策略学习问题

Method: 应用模仿学习技术，从专家对话数据中训练生成策略和判别模型

Result: 成功获得了能够基于输入状态与用户对话的策略，以及能够区分专家对话和合成对话的判别器，但判别器结果揭示了对话模型的局限性

Conclusion: 该技术可用于识别面向对话任务的任意数据模型的不良行为，为对话模型评估提供了新方法

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [4] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: 研究发现Faetar ASR基准中的转录不一致性问题存在但不是主要挑战，有限词典约束解码有益，但任务仍极困难


<details>
  <summary>Details</summary>
Motivation: 检验低资源ASR基准Faetar中转录不一致性的影响程度

Method: 使用手动构建的小型词典分析转录不一致性，测试bigram词级语言模型和有限词典约束解码的效果

Result: 转录不一致确实存在但不是主要问题，bigram语言模型无额外收益，有限词典约束解码有积极作用

Conclusion: Faetar ASR任务极其困难，转录不一致不是主要障碍，词典约束是有效的改进方向

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [5] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（如Google Gemini）在学术文本处理方面的能力，通过四个任务测试其在学术同行评审中的实用性，发现其表现有限，不推荐在同行评审中未经检查地使用LLM。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在辅助科学发现和学术同行评审方面的实际应用潜力，评估其处理学术文本的能力。

Method: 采用四个任务评估LLM：内容复现/比较/评分/反思，每个任务要求LLM扮演不同角色（预言家/判断仲裁者/知识仲裁者/合作者），使用一流信息系统期刊文章作为输入文本，结合多种文本指标进行严格性能评估。

Result: Gemini在学术文本摘要和转述方面表现可接受，但在文本排序方面扩展性差，评分能力区分度低，反思内容虽然自洽但缺乏洞察力。基于指标的内部评估、外部评估和人工评估结果一致，且对提示词变化具有鲁棒性。

Conclusion: LLM的文本处理能力有限，不建议在构建同行评审中未经检查地使用LLM。

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [6] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 提出一个两阶段LLM框架，分别处理句子级和文档级的科学文本简化任务


<details>
  <summary>Details</summary>
Motivation: 解决科学文本简化中保持连贯性和上下文忠实性的挑战

Method: 使用大语言模型生成结构化计划进行句子级简化，以及生成摘要指导文档级简化

Result: 实现了更连贯和上下文忠实的科学文本简化

Conclusion: 两阶段LLM方法在科学文本简化任务中表现出色

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [7] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 本文提出了一个集成多种方法的框架，用于检测科学文本简化中的创造性生成和信息失真，结合了BERT分类器、语义相似度、自然语言推理和LLM推理，并使用元分类器增强检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决CLEF 2025 SimpleText Task 2任务，专注于检测和评估科学文本简化中的创造性生成和信息失真问题，提高简化文本的质量和可靠性。

Method: 构建集成框架，整合BERT分类器、语义相似度测量、自然语言推理模型和大型语言模型推理；使用元分类器组合多种信号；采用基于LLM的后编辑系统根据原始输入文本修订简化内容。

Result: 开发了一个能够有效检测科学文本简化中创造性生成和信息失真的多策略集成系统，提高了检测的准确性和鲁棒性。

Conclusion: 通过集成多种AI技术和元分类器方法，成功构建了一个强大的科学文本简化检测系统，为评估简化文本质量提供了有效的解决方案。

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [8] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: 这篇论文调查了53个关于习语研究的心理语言学和计算语言学数据集，分析了它们的注释实践、覆盖范围和任务框架，发现两个领域的研究目前缺乏关联。


<details>
  <summary>Details</summary>
Motivation: 习语作为无法从单个词汇推断含义的比喻表达，在计算处理和人类实验研究中都面临挑战，需要系统梳理现有研究资源来促进该领域发展。

Method: 通过系统综述方法，分析53个心理语言学和计算语言学数据集的内容、形式和用途，比较它们在注释实践、语言覆盖和任务框架方面的趋势。

Result: 心理语言学资源主要包含熟悉度、透明度和组合性等标准化评分，而计算数据集支持习语性检测、释义和跨语言建模等任务。虽然近期努力扩展了语言覆盖和任务多样性，但两个领域的研究尚未建立有效联系。

Conclusion: 当前习语研究在心理语言学和计算语言学领域存在脱节，需要加强跨学科合作以推动习语处理的综合研究发展。

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [9] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: 该论文提出通过模拟生物节律（月经和昼夜节律）来增强AI系统的上下文相关性处理能力，发现激素周期会影响语言模型的情感表达和性能表现


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在处理指数级信息空间时面临的框架问题——如何确定上下文相关信息，受生物节律作为自然相关性过滤器的启发

Method: 开发了一个框架，通过周期性函数模拟关键激素（雌激素、睾酮、皮质醇）生成系统提示，将模拟的生物节律嵌入大型语言模型中

Result: 语言分析显示情感和风格变化与生物周期同步：经期悲伤情绪达到峰值，排卵期快乐情绪占主导；昼夜节律显示早晨乐观转向夜间内省。在多个基准测试中观察到与生物预期一致的性能变化

Conclusion: 该方法为上下文AI提供了新途径，同时揭示了语言模型中嵌入的关于性别和生物学的社会偏见

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [10] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本文研究通过顺序微调进行跨语言迁移对委婉语检测的影响，比较了顺序微调与单语和同步微调方法，发现在高资源语言L1基础上顺序微调能显著提升低资源语言L2的委婉语检测性能。


<details>
  <summary>Details</summary>
Motivation: 委婉语具有文化差异性和歧义性，对语言模型特别是低资源语言环境下的处理构成挑战，需要探索有效的跨语言迁移方法来提升多语言委婉语检测能力。

Method: 使用XLM-R和mBERT模型，在英语、西班牙语、中文、土耳其语和约鲁巴语五种语言上，比较顺序微调与单语微调、同步微调的效果，分析语言配对、类型学特征和预训练覆盖度对性能的影响。

Result: 顺序微调使用高资源L1语言能显著提升L2语言性能，特别是对约鲁巴语和土耳其语等低资源语言。XLM-R获得更大提升但对预训练差距和灾难性遗忘更敏感，mBERT结果更稳定但性能较低。

Conclusion: 顺序微调是一种简单有效的策略，可显著提升多语言模型在委婉语检测方面的性能，特别是在涉及低资源语言时效果尤为明显。

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [11] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok是一种新颖的分词架构，通过跨边界模式学习、熵驱动数据筛选和多阶段课程学习，实现了比主流分词器更高的效率（31%提升）和竞争性的多语言性能，在GPT-2规模模型上带来8.4-9.5%的基准测试提升。


<details>
  <summary>Details</summary>
Motivation: 当前分词方法在NLP中仍是未充分探索的瓶颈，尽管模型架构取得了显著进展，但分词策略基本保持静态，需要重新思考子词分割方法。

Method: SupraTok扩展了BPE算法，通过三个创新：跨边界模式学习发现多词语义单元、熵驱动数据优化训练语料质量、多阶段课程学习确保稳定收敛，学习保持语义完整性的"超词"标记。

Result: 英语分词效率提升31%（5.91 vs 4.51字符/标记），优于OpenAI o200k和Google Gemma 3分词器，在38种语言保持竞争力。集成GPT-2模型后在HellaSWAG和MMLU基准分别提升8.4%和9.5%。

Conclusion: 高效分词可以作为架构创新的补充路径来提升语言模型性能，虽然当前结果有希望，但需要在大规模模型上进一步验证。

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [12] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: 提出InitERC，一个简单有效的一阶段上下文指令调优框架，用于对话情感识别，通过上下文学习实现说话人-上下文-情感的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多阶段指令调优方法无法联合捕捉说话人特征和对话上下文之间的动态交互，导致在统一框架内说话人身份、上下文线索和情感状态的弱对齐。

Method: InitERC框架包含四个组件：演示池构建、上下文示例选择、提示模板设计和上下文指令调优，通过上下文示例让LLMs学习说话人-上下文-情感对齐。

Result: 在三个广泛使用的数据集上进行的广泛实验表明，InitERC相比最先进的基线方法取得了显著改进。

Conclusion: InitERC通过一阶段上下文指令调优有效解决了说话人-上下文-情感对齐问题，为对话情感识别提供了简单而强大的解决方案。

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [13] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出了CORE指标来量化多智能体系统中语言使用的有效性，通过聚类熵、词汇重复和语义相似度来评估对话质量，发现在合作设置中语言重复更多但词汇扩展更大，而竞争环境中词汇更受限


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在多智能体博弈交互中展现出许多涌现能力，但这些交互的语言多样性尚未得到充分量化，需要开发一个指标来评估多智能体系统中的语言使用效果

Method: 提出CORE指标，整合聚类熵、词汇重复和语义相似度三个维度，在竞争、合作和中立三种博弈设置中对成对LLM对话进行分析，并基于Zipf定律和Heaps定律来表征词频分布和词汇增长

Result: 合作设置表现出更陡峭的Zipf分布和更高的Heap指数，表明更多重复同时词汇扩展更大；竞争交互显示较低的Zipf和Heaps指数，反映较少重复和更受限的词汇

Conclusion: 研究结果提供了社会激励如何影响语言适应性的新见解，并证明CORE作为测量多智能体LLM系统中语言鲁棒性的强大诊断工具

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [14] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 该研究构建了中文和日语中完成体(perfect aspect)的NLI数据集，发现即使先进的大语言模型在时态推理方面也存在困难，特别是在检测细微的时态和参照时间变化方面。


<details>
  <summary>Details</summary>
Motivation: 中文和日语缺乏英语中完成体特有的时态标记形式(如had, has, will have)，这给自然语言推理(NLI)带来了挑战。研究者希望探索这些语言中完成体的时态推理问题。

Method: 构建基于语言学动机的模板化NLI数据集(每种语言1,350对样本)，专注于中文和日语中的完成体，并利用该数据集进行实验评估。

Result: 实验显示，即使是先进的大语言模型在时态推理方面也表现不佳，特别是在检测细微的时态和参照时间变化方面存在困难。

Conclusion: 研究结果揭示了模型在时态语义理解方面的局限性，并强调了跨语言评估在时态语义研究中的必要性。数据集已公开提供。

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [15] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: CAMF是一个新颖的多智能体框架，通过三个阶段的协作对抗过程来检测机器生成文本，在零样本检测方面显著优于现有技术


<details>
  <summary>Details</summary>
Motivation: 现有零样本检测方法存在分析浅层、缺乏跨维度一致性研究的问题，需要解决机器生成文本检测的挑战

Method: 使用基于LLM的多智能体协作对抗框架，包含多维语言特征提取、对抗一致性探测和综合判断聚合三个阶段

Result: 实证评估显示CAMF在零样本MGT检测技术上具有显著优势

Conclusion: CAMF通过深度分析跨维度文本不一致性，有效检测非人类生成的文本，为机器文本检测提供了新思路

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [16] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于指令的持续对比调优方法，通过专门利用错误案例来改善大语言模型在持续关系提取任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的持续关系提取方法没有充分重视错误案例，而错误案例能更有效地揭示模型的认知偏差，因此需要一种专门利用错误案例来改善模型性能的方法。

Method: 提出了指令基础的持续对比调优方法：1）将训练和记忆数据按初始响应正确性分成两部分；2）通过双任务精调区别处理；3）利用LLM的指令跟随能力进行对比调优，持续缩小旧新关系间的间隔。

Result: 在TACRED和FewRel数据集上进行实验评估，结果显示该模型实现了新的最高性能水平，取得了显著收益，证明了专门利用错误案例的重要性。

Conclusion: 通过专门分析和利用错误案例，该方法能够更有效地减轻大语言模型在持续关系提取任务中的恐怖忘却问题，提高模型的学习效果和表现。

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [17] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE是一种新颖的细粒度置信度估计方法，通过监督学习和后向置信度集成策略，为LLM生成过程提供连续的置信度评分，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏自我意识且经常表现出过度自信，现有置信度估计方法只能提供粗粒度的评分，无法在生成过程中提供细粒度的连续置信度估计。

Method: 开发了构建训练数据的完整流程来捕捉LLM响应的概率分布，然后以监督方式训练模型预测任意文本序列的置信度分数。提出了后向置信度集成(BCI)策略，利用后续文本信息增强当前序列的置信度估计，并引入了三种策略来识别生成过程中的最佳置信度估计位置。

Result: 在多个基准数据集上的广泛实验表明，FineCE始终优于现有的经典置信度估计方法。

Conclusion: FineCE通过细粒度的置信度估计显著提升了LLM生成输出的可信度和可靠性，为解决LLM过度自信问题提供了有效解决方案。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [18] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: J6方法通过雅可比矩阵分解为六个可解释组件，实现多目标优化的结构化梯度交互分析，平衡事实性和置信度等冲突目标


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化方法依赖标量梯度聚合，忽略了目标与参数间的几何结构，难以平衡LLM适配中的冲突目标（如事实性与置信度）

Method: 提出J6结构化雅可比方法，将梯度交互矩阵分解为六个可解释组件，支持硬决策（argmax选择主导方向）和软策略（softmax注意力加权）的动态更新框架

Result: 该方法能够适应局部冲突和协同，提供参数归因、任务干扰和几何对齐适配的可解释性洞察

Conclusion: J6为冲突感知的提示优化提供了原则性可扩展机制，开辟了将结构化雅可比推理融入多目标神经调优的新途径

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [19] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: STEM是一个轻量级可解释的评估框架，通过分析同架构不同参数规模LLM的性能转变样本来高效估计模型相对能力


<details>
  <summary>Details</summary>
Motivation: 传统基准测试难以有效区分模型真实能力差异，存在过拟合问题且计算成本高昂，需要更高效的评估方法

Method: 识别显著转变样本(STS)，分析同架构不同规模模型间的性能一致性转变，构建STS池来估计未知模型能力位置

Result: STEM能可靠捕捉性能趋势，与真实模型能力排名一致，在六个多样化基准测试上验证了有效性

Conclusion: STEM是实用且可扩展的细粒度评估方法，适用于不同架构的LLM能力评估

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [20] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: 本研究首次系统评估了医学推理任务中的思维预算机制，揭示了计算资源与推理质量之间的基本缩放规律。研究发现思维预算和模型大小之间存在对数缩放关系，并确定了三个效率区间，为医疗AI系统的优化提供了关键机制。


<details>
  <summary>Details</summary>
Motivation: 随着医疗AI系统在临床决策支持中的应用日益广泛，如何有效分配计算资源以平衡推理质量和效率成为关键问题。思维预算机制作为一种控制计算资源分配的方法，在医疗领域尚未得到系统评估，需要研究其在医学推理任务中的效果和适用性。

Method: 系统评估了两个主要模型家族（Qwen3和DeepSeek-R1）在15个医学数据集上的表现，涵盖不同专业和难度级别。通过控制思维预算从零到无限制的token数量，建立了思维预算与模型大小之间的对数缩放关系，并识别了三个不同的效率区间。

Result: 研究发现思维预算和模型大小之间存在可预测的对数缩放关系。确定了三个效率区间：高效率区间（0-256 token）适合实时应用，平衡区间（256-512 token）提供最优性价比，高精度区间（512+ token）仅适用于关键诊断任务。小模型从扩展思维中获益更大，改进幅度达15-20%，而大模型仅为5-10%。不同医学专业对推理深度的需求存在明显差异。

Conclusion: 思维预算控制是优化医疗AI系统的关键机制，能够实现与临床需求相匹配的动态资源分配，同时保持医疗部署所需的透明度。研究结果验证了思维预算概念在不同架构间的通用性，为医疗AI系统的实际部署提供了重要指导。

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [21] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: 本文研究使用LLM作为隐私评估器来评估文本数据的隐私敏感性，通过大规模实验验证LLM能够准确模拟人类隐私视角，为解决隐私保护NLP中的评估难题提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 隐私保护自然语言处理领域面临隐私评估的挑战，传统方法难以准确衡量隐私敏感性。受LLM在其他NLP子领域评估任务中成功的启发，研究者希望探索LLM作为隐私评估器的可行性。

Method: 采用LLM-as-a-Judge范式，通过涉及10个数据集、13个LLM模型和677名人类调查参与者的大规模研究，比较LLM评估与人类隐私感知的一致性，并分析推理模式。

Result: 研究发现隐私确实难以量化测量（人类间一致性较低），但LLM能够准确建模全局人类隐私视角，在隐私评估任务中表现出与人类高度一致的结果。

Conclusion: LLM作为隐私评估器具有可行性，能够有效解决文本数据隐私评估的核心挑战，为隐私保护技术提供了创新的解决方案路径。

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [22] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: 本文对阿拉伯语多模态机器学习进行了全面综述，提出了包含数据集、应用、方法和挑战的新分类法，旨在推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语多模态机器学习已达到一定成熟度，需要进行系统性总结和分类，以识别研究空白并指导未来研究方向。

Method: 通过构建包含数据集、应用、方法和挑战四个维度的新分类法，对现有阿拉伯语MML研究进行系统性分析和归类。

Result: 提供了阿拉伯语MML领域的结构化概览，识别了未探索的研究领域和关键研究缺口。

Conclusion: 该综述为研究人员提供了基础，有助于利用已识别的研究机会并应对挑战，从而推动阿拉伯语多模态机器学习领域的进步。

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [23] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: SEA-BED是首个针对东南亚地区的大规模句子嵌入基准测试，包含169个数据集、9个任务和10种语言，其中71%由人工构建，解决了该地区缺乏本地化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 东南亚地区拥有近7亿人口，但现有的多语言基准测试缺乏针对该地区的专门评估，数据集多为机器翻译，无法捕捉本地语言特性。

Method: 构建包含169个数据集的SEA-BED基准，涵盖9个任务和10种语言，其中71%为人工构建。评估17个嵌入模型，分析任务挑战性、语言性能差距以及人工vs机器翻译的影响。

Result: 结果显示模型排名发生显著变化，东南亚语言间性能表现不一致，人工构建数据集对低资源语言（如缅甸语）评估至关重要。

Conclusion: 东南亚地区需要专门的语言基准测试，人工构建的数据集能更准确评估低资源语言性能，机器翻译数据集可能产生误导性结果。

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [24] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: 该论文提出了一个轻量级分析框架来研究语音基础模型(SFMs)中编码的声学和语言知识，并创建了新的口语理解评测任务(NER和NEL)，发现端到端模型优于传统级联方法。


<details>
  <summary>Details</summary>
Motivation: 尽管语音基础模型在各类任务中表现出色，但对其获取的知识理解滞后，且缺乏对需要深度理解的口语理解任务的充分探索。

Method: 使用统计工具和无训练任务的分析框架，比较多个SFMs；创建口语命名实体识别和定位任务，开发基于SFM的端到端方法。

Result: 分析洞察对下游任务性能有具体影响；端到端模型超越传统级联方法；评估了不同SFMs和适应策略对任务性能的影响。

Conclusion: 该研究解决了关于SFMs的未解问题，提供了工具和数据集来促进理解，并为未来模型开发和采用提供明智的设计选择。

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [25] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文对文本到结构转换技术进行了系统性综述，涵盖了方法、数据集和评估指标，并提出了通用评估框架，为下一代AI系统奠定基础


<details>
  <summary>Details</summary>
Motivation: AI系统向代理操作和上下文感知检索发展，需要将非结构化文本转换为表格、知识图谱和图表等结构化格式，但目前缺乏对方法、数据集和指标的综合研究

Method: 采用系统性文献综述方法，分析文本到结构转换技术、现有数据集和评估标准，并建立通用评估框架

Result: 全面梳理了文本到结构转换的技术现状、面临的挑战，评估了当前数据集和评估标准，提出了未来研究方向

Conclusion: 文本到结构转换是下一代AI系统的基础设施，需要建立统一的评估框架和标准化的数据集来推动该领域发展

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [26] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: 该论文提出了一个基于认知心理学的新颖LLM推理策略分类法，包含快速/慢速和内部/外部两个知识边界维度，系统综述了LLM自适应推理方法并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要LLMs能够根据问题需求自适应地选择推理策略，从快速直觉响应到深思熟虑的分步推理，但目前缺乏系统性的分类框架。

Method: 提出基于认知心理学的二维分类法：快速/慢速边界（区分直觉与审慎过程）和内部/外部边界（区分参数内推理与外部工具增强推理），并系统综述现有自适应推理方法。

Result: 建立了LLM推理策略的完整分类框架，将现有工作按关键决策因素进行分类，为理解和设计自适应推理系统提供了理论基础。

Conclusion: 需要进一步研究以实现更自适应、高效和可靠的LLMs，论文指出了开放挑战和未来发展方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [27] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: LLMs在预测自身响应特性方面表现不佳，模型规模和能力提升并不能改善这一表现，揭示了LLMs在自我行为表征和推理方面的根本局限性


<details>
  <summary>Details</summary>
Motivation: 探索不同于传统知识和推理能力评估的新评估方式，测试LLM是否能预测自身响应的各个方面

Method: 引入自我执行基准测试，测量模型预测输出特性的能力，包括问题难度预测、拒绝回答可能性预测、可能产生的关联类型预测

Result: 模型在该基准测试上普遍表现不佳，模型规模或能力的增加并不能持续带来性能提升

Conclusion: LLMs在表征和推理自身行为方面存在根本性限制，自我认知能力有限

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [28] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: LegalΔ是一个强化学习框架，通过思维链引导的信息增益来增强法律推理能力，在准确性和可解释性方面优于现有基线模型


<details>
  <summary>Details</summary>
Motivation: 现有法律大语言模型在生成可靠和可解释的推理过程方面存在困难，往往直接给出答案而缺乏多步推理，限制了在复杂法律场景中的应用效果

Method: 采用双模式输入（直接答案模式和推理增强模式），最大化两者间的信息增益；两阶段方法：1)从DeepSeek-R1蒸馏潜在推理能力 2)通过差异比较和多维奖励机制（评估结构连贯性和法律领域特异性）精炼推理质量

Result: 在多个法律推理任务上的实验结果表明，LegalΔ在准确性和可解释性方面均优于强基线模型，能够产生更稳健和可信的法律判断，且不依赖标注的偏好数据

Conclusion: LegalΔ框架有效解决了法律AI中推理过程不可靠的问题，通过强化学习和信息增益机制显著提升了法律推理的质量和可解释性

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [29] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA是一个大规模中文问答基准数据集，专门用于评估检索增强生成(RAG)系统中的时间推理能力，包含5,176个高质量问题，涵盖多种时间类型和表达方式。


<details>
  <summary>Details</summary>
Motivation: 现有的问答系统在时间推理方面存在不足，特别是在处理时间对齐和逻辑一致性方面。需要构建一个专门针对中文时间敏感问答的可靠基准数据集来推动相关研究发展。

Method: 从2019-2024年的30多万篇新闻文章中构建数据集，包含绝对、聚合和相对时间类型的问题，支持单文档和多文档场景。采用基于规则、基于LLM和人工评估的多阶段验证确保数据质量。

Result: 创建了一个包含5,176个高质量问题的数据集，具有全面的结构化标注，支持多种时间推理任务的评估，为时间敏感的检索增强问答系统提供了可靠的基准。

Conclusion: ChronoQA作为一个动态、可靠且可扩展的资源，能够支持广泛的时间相关任务的结构化评估，有助于推动时间敏感检索增强问答系统的进步。

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [30] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: 提出了一种将法律逻辑融入深度学习模型的缓刑预测新方法，通过构建专业数据集和基于双轨惩罚理论的多任务模型，显著提升了预测性能


<details>
  <summary>Details</summary>
Motivation: 当前智能司法辅助系统缺乏专门的缓刑预测方法，现有研究主要依赖数据驱动方法而忽视了司法决策的法律逻辑基础，需要同时分析犯罪情节和悔罪表现

Method: 三阶段方法：1)构建包含事实描述和缓刑法律要素的专业数据集；2)基于缓刑法律逻辑和双轨惩罚理论设计多任务双理论缓刑预测模型(MT-DT)；3)在数据集上进行实验验证

Result: MT-DT模型在缓刑数据集上表现优于基线模型，法律逻辑分析进一步验证了方法的有效性

Conclusion: 将法律逻辑整合到深度学习模型中能够有效提升缓刑预测的准确性和可解释性，为智能司法辅助系统提供了更符合法律原理的解决方案

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [31] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: 将Transformer编码器-解码器模型转换为低延迟流式ASR模型的方法，通过LoRA微调和因果编码器改造实现实时转录


<details>
  <summary>Details</summary>
Motivation: 现有SOTA ASR模型（如Whisper、Canary）专为离线转录设计，无法满足流式实时转录的低延迟需求

Method: 使用LoRA微调将非因果编码器转换为因果编码器，配合弱对齐数据集训练，提出新的推理机制支持贪心和束搜索解码

Result: 在低延迟块大小（<300ms）下优于现有非微调流式方法，复杂度更低，同时获得更好的对齐效果和词级时间戳提取能力

Conclusion: 成功实现了Transformer编码器-解码器模型向低延迟流式ASR的转换，为流式语音识别研究提供了有效解决方案

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [32] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了NATCONFQA基准测试，用于评估大语言模型在多答案问答任务中处理冲突答案的能力，发现现有模型在冲突处理方面存在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 多答案问答任务中可能存在冲突答案，现有基准测试要么成本高昂，要么依赖合成数据或自动化标注，缺乏真实冲突场景的评估。

Method: 利用事实核查数据集构建NATCONFQA基准，要求模型不仅识别所有有效答案，还要检测特定的冲突答案对。

Result: 评估了8个高端大语言模型，发现它们在处理各种类型冲突时表现脆弱，并采用了有缺陷的解决策略。

Conclusion: 需要开发更好的方法来处理多答案问答中的冲突情况，当前大语言模型在这方面存在明显不足。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [33] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: ReaLM是一个强化学习框架，通过多路径过程验证、渐进式自主诱导和引导式思维链蒸馏，提升小语言模型在垂直领域的推理能力、自主性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 小语言模型(SLMs)成本低但推理能力有限，现有方法往往牺牲推理能力、自主性或泛化性中的一个或多个方面。需要一种能同时提升这三方面的方法。

Method: 1) MRPV: 对比正负推理路径提取关键模式；2) EAAI: 渐进减少外部信号依赖；3) 引导式思维链蒸馏：将领域知识编码到模型参数中。

Result: 在垂直领域和通用推理任务上的大量实验表明，ReaLM显著提升了SLMs在推理能力、自主性和泛化性三个方面的性能。

Conclusion: ReaLM框架成功解决了小语言模型在复杂推理中的核心挑战，为垂直领域应用提供了有效的解决方案。

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [34] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: MedKGent是一个基于LLM代理的框架，用于构建时间演化的医学知识图谱，通过提取器和构造器代理从PubMed摘要中增量构建高质量KG，准确率达90%，在医学问答任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱构建方法要么依赖监督流水线（泛化性有限），要么简单聚合LLM输出，忽略了生物医学知识的时序动态性和上下文不确定性。

Method: 使用Qwen2.5-32B-Instruct模型驱动的两个专业代理：提取器代理识别知识三元组并分配置信度分数，构造器代理基于置信度和时间戳增量整合三元组到时间演化图中。

Result: 构建了包含156,275个实体和2,971,384个关系三元组的KG，准确率接近90%，在7个医学问答基准测试中使用5个领先LLM进行RAG，相比无增强基线持续获得显著改进。

Conclusion: MedKGent框架成功解决了医学知识图谱构建中的时序动态性问题，证明了基于置信度的时间演化KG在医学知识管理和应用中的实用价值。

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [35] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: 开发了一个混合NLP流水线，结合基于规则的命名实体识别和BERT断言检测模块，用于从临床笔记中提取PASC症状和检测断言，在内部和外部验证中表现良好。


<details>
  <summary>Details</summary>
Motivation: 准确高效诊断COVID-19后遗症(PASC)具有挑战性，因为其症状多样且随时间变化。

Method: 开发了混合自然语言处理流水线，整合基于规则的命名实体识别和基于BERT的断言检测模块，构建了全面的PASC词典。

Result: 内部验证F1得分0.82，外部验证0.76；处理每个笔记平均2.448秒；Spearman相关系数正提及>0.83，负提及>0.72。

Conclusion: 模型有效且高效，有潜力改善PASC诊断。

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [36] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: ZigzagAttention通过将检索头和流式头分离到不同层来优化KV缓存内存占用，减少延迟同时保持性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长上下文时KV缓存消耗巨大，现有方法将注意力头分为检索头和流式头但会带来额外延迟

Method: 设计新标准强制将检索头和流式头分别聚集在不同层，避免混合计算带来的张量访问和索引延迟

Result: 显著降低延迟，性能损失可忽略，在基准测试中表现竞争力

Conclusion: 层间头类型分离是优化KV缓存的有效策略，能在减少内存占用的同时避免额外延迟开销

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [37] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: 该研究提出'文化基因'概念，通过200个提示词的文化探测数据集，发现GPT-4展现西方个人主义倾向，ERNIE Bot展现东方集体主义倾向，验证了LLMs是其训练语料文化的统计镜像。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在全球部署，但其底层文化伦理假设未被充分探索。研究者希望揭示LLMs如何继承训练语料中的系统性价值取向，避免算法文化霸权。

Method: 创建包含200个提示词的文化探测数据集(CPD)，针对个人主义-集体主义(IDV)和权力距离(PDI)两个经典跨文化维度。使用标准化零样本提示比较GPT-4(西方中心)和ERNIE Bot(东方中心)模型。

Result: GPT-4展现个人主义(得分约1.21)和低权力距离倾向(得分约-1.05)，ERNIE Bot展现集体主义(得分约-0.89)和高权力距离倾向(得分约0.76)，差异统计显著(p<0.001)。文化对齐指数显示GPT-4更接近美国文化，ERNIE Bot更接近中国文化。

Conclusion: LLMs是其文化语料的统计镜像，支持文化基因概念。需要文化意识评估和部署，避免算法文化霸权，促进文化多样性。

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [38] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 这篇论文通过物理学任务探索大语言模型的上下文学习机制，发现模型能在上下文中编码关键物理变量如能量


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在上下文学习方面表现突出，但其内部机制仍不明确，需要找到适合的测试平台来探索

Method: 使用物理动力学预测任务作为代理，通过稀疏自编码器分析模型激活状态，研究模型如何在上下文中学习物理概念

Result: 发现模型性能随着输入上下文长度增加而提升，稀疏自编码器抓取的特征与关键物理变量如能量呈现显著相关性

Conclusion: 这为理解大语言模型如何在上下文中学习提供了新的案例研究，证明有意义的物理概念确实被编码在模型中

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [39] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: M3PO是一种新颖的多模态偏好优化方法，通过智能选择LVLM生成的高价值偏好样本对，结合多模态对齐分数和模型自一致性，显著提升视觉指令跟随能力


<details>
  <summary>Details</summary>
Motivation: 传统监督微调和偏好优化方法（如RLHF、DPO）难以有效利用模型自身生成空间来识别高信息量的困难负样本，且人工标注成本高、不一致

Method: 提出M3P-Score机制，整合多模态对齐分数（外部质量评估）和模型自一致性/置信度（内部信念），从LVLM生成的候选样本中选择最有学习价值的偏好对，用于高效的DPO微调

Result: 在多个多模态指令跟随基准测试（MME-Bench、POPE、IFT、Human Pref. Score）上一致优于SFT、模拟RLHF、普通DPO和RM-DPO等强基线方法

Conclusion: M3PO提供了一种数据高效的解决方案，能够显著提升大型视觉语言模型在复杂多模态指令跟随任务中的性能，解决了人工标注成本高和传统方法效率低的问题

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [40] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: LoraxBench是一个针对印度尼西亚低资源语言的NLP基准测试，涵盖6个任务和20种语言，发现现有模型在低资源语言上表现不佳，且语言形式变化对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 印度尼西亚作为世界人口大国，拥有700种语言，但在NLP发展方面相对落后。需要专门针对该国低资源语言的基准测试来推动相关研究。

Method: 构建LoraxBench基准测试，涵盖阅读理解、开放域问答、语言推理、因果推理、翻译和文化问答6个任务，覆盖20种语言，其中3种语言还包含两种形式变体。

Result: 基准测试具有挑战性，印尼语与其他语言（尤其是低资源语言）之间存在明显性能差距。区域特定模型与通用多语言模型相比没有明显优势。语言形式变化（如高级礼貌形式）显著影响模型性能。

Conclusion: 需要更多针对印度尼西亚低资源语言的NLP研究，特别是在处理不同语言形式和变体方面，当前模型在这些语言上的表现仍有很大提升空间。

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [41] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI发布GPT-OSS开源模型（20B和120B参数），评估显示20B版本在多个基准测试中优于120B版本，特别是在代码生成方面表现突出，但在多语言任务上较弱。研究表明稀疏架构的扩展不一定带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 评估OpenAI首个开源大语言模型GPT-OSS的性能表现，比较不同参数规模稀疏架构模型的效率，为开源模型选择和优化提供实证依据。

Method: 在10个基准测试上对比6个开源大语言模型（14.7B-235B参数），使用标准化推理设置，采用McNemar检验和效应量分析进行统计验证。

Result: GPT-OSS-20B在HumanEval和MMLU等多个基准上表现优于GPT-OSS-120B，且内存和能耗更低；两个模型在开源模型中处于中游水平，代码生成能力强但多语言能力弱。

Conclusion: 稀疏架构的扩展不一定带来性能的成比例提升，需要进一步研究优化策略，为未来开源部署提供更高效的模型选择指导。

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [42] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在动词学习中也表现出类似儿童的句法引导现象，当去除句法信息时动词表征比去除共现信息时退化更严重，特别是心理动词受影响更大，而名词表征则相反。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型是否表现出与儿童语言习得类似的句法引导现象，即通过句法环境学习动词含义。

Method: 通过在去除句法信息的扰动数据集上训练RoBERTa和GPT-2模型，比较句法信息去除和共现信息去除对模型表征的影响。

Result: 模型动词表征在句法线索去除时比共现信息去除时退化更严重；心理动词表征比物理动词受影响更大；名词表征则在共现信息扭曲时受影响更大。

Conclusion: 研究证实了句法引导在动词学习中的重要作用，并证明了通过操纵大型语言模型学习环境来测试发展假设的可行性。

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [43] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: 提出了CDCR-SFT框架，通过显式构建因果DAG来增强LLMs的因果推理能力，在多个任务上实现SOTA性能并显著减少幻觉


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的推理方法（如CoT）在语言token层面操作，缺乏对底层因果关系的建模能力，无法表示条件独立性或满足因果识别假设，导致逻辑不一致的幻觉

Method: CDCR-SFT监督微调框架，训练LLMs显式构建变量级有向无环图（DAG）并在图上进行推理，同时构建了包含25,368个样本的CausalDR数据集

Result: 在8个任务的4个LLMs上，CDCR-SFT在CLADDER上达到95.33%的SOTA准确率（首次超越人类94.8%），在HaluEval上幻觉减少10%

Conclusion: LLMs中显式因果结构建模能有效缓解输出中的逻辑不一致性，证明了因果推理能力与幻觉之间的负相关关系

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [44] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer是一种基于相关性选择稀疏自编码器特征的新方法，仅使用推理时激活就能自动提取相关特征进行模型引导，在多个任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器在引导任务中需要对比数据集或大量激活存储，限制了其有效性。需要一种更高效、自动化的特征选择方法

Method: 通过将样本正确性与推理时生成的token的SAE激活进行相关性分析来选择特征，使用平均激活自动获取引导系数

Result: 在Gemma 2 2B和LLaMA 3.1 8B上，QA、偏见缓解、越狱预防和推理基准任务表现显著提升，MMLU性能提高4.1%，HarmBench提升22.9%（仅需4000样本）

Conclusion: 基于相关性选择的方法为SAE引导提供了一种有效且可扩展的自动化解决方案，揭示了驱动性能的底层能力

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [45] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: 本文首次系统性研究多模态大语言模型在自动说话评测中的应用，提出Speech-First多模态训练方法，在内容和语言使用方面显著提升性能，并在表达方面获得4%精度提升。


<details>
  <summary>Details</summary>
Motivation: 传统自动说话评测系统存在模态限制：文本方法缺乏音响信息，音频方法缺少语义上下文。多模态大语言模型为全面评测提供了新机遇。

Method: 提出Speech-First多模态训练（SFMT）方法，利用课程学习原理，先建立健壮的语音建模基础，再进行跨模态协同融合。

Result: 在标准数据集上将整体评测性能从PCC 0.783提升到0.846。SFMT在表达方面的评估中显著优势，超过传统训练方法4%的绝对精度提升。

Conclusion: 多模态大语言模型能够显著提升自动说话评测的整体性能，而SFMT训练策略为解决表达方面的特殊挑战开启了新途径。

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [46] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: 该论文提出了一种名为"语义锚定"的混合记忆架构，通过结合依赖解析、话语关系标注和共指消解等显性语言线索来增强基于向量的记忆存储，在长期对话中相比传统RAG方法提高了18%的事实回忆和话语连贯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多会话和长期交互中的有效性受到有限记忆持久性的限制。传统RAG系统将对话历史存储为密集向量，虽然能捕捉语义相似性，但忽略了更精细的语言结构如句法依赖、话语关系和共指链接。

Method: 提出语义锚定混合代理记忆架构，结合依赖解析、话语关系标注和共指消解技术来创建结构化记忆条目，丰富基于向量的存储方式。

Result: 在适应的长期对话数据集上的实验表明，语义锚定相比强大的RAG基线在事实回忆和话语连贯性方面提高了18%。还进行了消融研究、人工评估和错误分析来评估鲁棒性和可解释性。

Conclusion: 语义锚定方法通过整合显性语言线索显著提升了长期对话中的记忆性能，为解决LLMs在持续交互中的记忆限制问题提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [47] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro是一个测试时路由框架，通过动态分配查询给不同容量和效率的LLM，在性能与效率之间实现最优平衡，在6个基准测试中超越最强单模型7%准确率，同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型性能与效率平衡的核心挑战，传统单一模型无法同时满足高性能和低成本需求。

Method: 使用测试时路由框架，嵌入和聚类输入查询，基于性能-效率分数将每个查询路由到最合适的模型（高效或高容量模型）。

Result: 在6个基准测试和8个领先模型上实现SOTA：超越最强单模型GPT-5-medium 7%准确率；以27%更低成本达到相同准确率；以63%更低成本达到90%性能；实现帕累托最优。

Conclusion: Avengers-Pro提供了一个统一的解决方案，能够在任何给定成本下获得最高准确率，或在任何给定准确率下实现最低成本，有效解决了LLM性能-效率权衡问题。

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [48] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: 提出LIFE方法，通过分析LLM生成真假新闻的概率分布差异来检测AI生成的假新闻，在LLM和人工假新闻检测上都达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，假新闻生成变得容易且难以检测，现有方法主要关注文本内容本身，但AI生成的假新闻往往看起来连贯一致，伪造痕迹难以发现

Method: 通过分布差异分析发现提示诱导的语言指纹，提出LIFE方法重构词级概率分布来发现判别模式，并利用关键片段技术放大语言差异

Result: LIFE在LLM生成的假新闻检测上达到最先进性能，在人工撰写的假新闻上也保持高检测性能

Conclusion: 基于概率分布的语言指纹分析是检测LLM生成假新闻的有效方法，LIFE方法具有优异的检测能力

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [49] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 本文提出通过合成代码转换文本微调LLM的方法，来弥合低资源语言与高资源语言在常识推理任务上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在低资源语言（如印地语、斯瓦希里语）的常识推理任务中表现远不如高资源语言（如英语），这种性能不平等需要解决以确保语言公平性。

Method: 使用受控语言混合方法生成合成代码转换文本，并基于此对LLM进行微调。创建了基于CommonSenseQA数据集的合成代码转换数据集，包含三种不同的语言比例配置。

Result: 实验证明，在合成代码转换数据集上微调LLM能显著提升低资源语言的模型性能，同时保持甚至增强高资源语言的性能。

Conclusion: 通过合成代码转换文本进行微调是解决LLM在不同语言间性能不平等问题的有效方法，有助于促进语言公平性。

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [50] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: 本研究探索使用大语言模型从自然语言描述预测人类感知的痛苦分数，通过多种提示策略和创新的游戏化评估框架来测试模型的情感推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大语言模型在情感计算领域的应用，特别是如何从文本描述准确预测人类的情感痛苦程度，这有助于理解模型的情感推理能力。

Method: 采用回归问题框架，评估零样本、固定上下文少样本和基于BERT句子嵌入的检索式提示策略。创新性地设计了"痛苦游戏秀"游戏化评估框架，包含排序比较、二元分类、标量估计和反馈驱动推理等多个环节。

Result: 少样本方法持续优于零样本基线，表明上下文示例在情感预测中的重要性。游戏化评估显示大语言模型在动态情感推理任务中具有超越标准回归的潜力。

Conclusion: 研究表明大语言模型能够有效预测人类感知的痛苦分数，游戏化评估框架为测试模型的情感推理和适应能力提供了新方法，展现了在动态情感计算任务中的广阔应用前景。

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [51] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: ToolACE-MT是一个非自回归迭代生成框架，用于高效构建高质量多轮代理对话数据，通过三阶段流程替代昂贵的自回归交互方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模拟的数据生成方法依赖多个LLM代理之间的昂贵自回归交互，限制了代理任务在现实世界中的性能表现。

Method: 三阶段框架：粗粒度初始化构建对话骨架，迭代细化通过掩码填充操作引入现实复杂性，离线验证通过规则和模型检查确保正确性和连贯性。

Result: 实验证明ToolACE-MT能够实现高效、有效且可泛化的代理数据生成。

Conclusion: 该框架为工具增强LLM场景中的高质量数据构建提供了新范式，解决了传统自回归方法的成本和效率问题。

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [52] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: DESIGNER是一个创新的数据合成框架，通过设计逻辑概念从多学科原始文档生成大规模、高难度的推理问题数据集，显著提升了LLM的跨学科推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理数据集在学科广度和结构深度方面存在不足，无法充分激发LLM的复杂多步推理能力，需要创建更具挑战性和多样性的多学科推理数据集。

Method: 提出设计逻辑概念，模仿人类教育者的出题过程，从12万+现有问题中逆向工程抽象出设计逻辑，然后与学科源材料匹配生成推理问题，构建了两个大规模数据集DLR-Book（304万问题）和DLR-Web（166万问题）。

Result: 合成的问题在难度和多样性上远超基线数据集，SFT实验显示使用该数据集训练的模型显著优于同规模多学科数据集，甚至超越了官方Qwen3模型的跨学科推理性能。

Conclusion: DESIGNER框架有效解决了多学科复杂推理数据稀缺问题，为提升LLM的推理能力提供了高质量数据支持，证明了设计逻辑引导的数据合成方法的有效性。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [53] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: LinguaSafe是一个包含12种语言、45k条目的多语言安全基准测试，通过翻译、转创和原生数据构建，提供多维度的安全评估框架，填补了LLM在多语言安全评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多语言安全评估缺乏全面性和多样性数据，限制了多语言安全对齐的发展，需要构建更全面的多语言安全基准。

Method: 结合翻译、转创和原生数据源，构建包含12种语言45k条目的数据集，采用直接和间接安全评估的多维度框架，包括过度敏感性评估。

Result: 安全和有用性评估结果在不同领域和语言间存在显著差异，即使是资源水平相似的语言也有明显区别。

Conclusion: 多语言安全评估对实现平衡的LLM安全对齐至关重要，LinguaSafe为深入研究提供了全面的评估工具和数据集。

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [54] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL是一个针对大规模数据库的Text-to-SQL框架，通过集群检索和执行描述语言来解决语义不匹配问题，在两个大型跨域基准测试中达到了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据库中自然语言问题与SQL查询之间的语义不匹配问题，特别是在语义相似属性阻碍模式链接和SQL生成过程中出现语义漂移的情况下。

Method: 提出CRED-SQL框架，包含集群检索和执行描述语言(EDL)。首先进行基于集群的大规模模式检索来定位相关表和列，然后使用EDL作为中间自然语言表示，将任务分解为Text-to-EDL和EDL-to-SQL两个阶段。

Result: 在两个大规模跨域基准测试SpiderUnion和BirdUnion上实现了新的最先进(SOTA)性能，验证了其有效性和可扩展性。

Conclusion: CRED-SQL通过结合集群检索和EDL中间表示，有效解决了大规模数据库中的语义不匹配问题，显著提升了Text-to-SQL系统的准确性。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [55] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: SALAMANDRATA模型家族是SALAMANDRA LLMs的改进版本，专门针对38种欧洲语言的翻译任务进行优化，提供2B和7B两种参数规模，采用持续预训练和监督微调的两阶段训练方法，并在WMT25机器翻译任务中应用了质量感知的解码策略。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对多语言翻译任务的高性能模型，特别是针对38种欧洲语言的翻译需求，提升机器翻译的质量和效率。

Method: 采用两阶段训练方法：首先在平行数据上进行持续预训练，然后在高质量指令上进行监督微调。针对WMT25任务，还进行了词汇表适配和额外的训练阶段。解码时使用Minimum Bayes Risk Decoding和Tuned Re-ranking策略。

Result: 开发了SALAMANDRATA 2B和7B两个版本的模型，并公开发布了SALAMANDRATA-V2模型。模型针对38种欧洲语言进行了优化，并适配了WMT25任务中的非欧洲语言。

Conclusion: SALAMANDRATA模型家族为多语言机器翻译任务提供了有效的解决方案，通过专门的训练方法和质量感知解码策略，在翻译性能上取得了显著提升，模型已公开发布供社区使用。

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [56] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: Med-LVLMs存在事实不准确和不可靠输出的问题，HeteroRAG框架通过多模态检索增强生成技术，整合异质知识源，显著提升了医学视觉语言模型的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学大视觉语言模型在临床应用中存在事实不准确和输出不可靠的问题，当前的多模态RAG系统无法在异质数据源中进行有效检索，影响临床决策的可信度。

Method: 构建MedAtlas多模态报告库，开发HeteroRAG框架，包括模态特定CLIPs进行报告检索、多语料查询生成器动态构建查询，并通过异质知识偏好调优实现跨模态多源知识对齐。

Result: 在12个数据集和3种模态上的广泛实验表明，HeteroRAG在大多数医学视觉语言基准测试中达到最先进性能，显著提高了Med-LVLMs的事实准确性和可靠性。

Conclusion: HeteroRAG框架通过整合异质知识源有效解决了医学视觉语言模型的事实准确性问题，为临床诊断提供了更可靠的决策支持。

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [57] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: 提出了Atomic Thought思维范式和Atom-Searcher强化学习框架，通过细粒度推理单元和过程奖励解决传统RAG和RL方法在多跳推理和奖励稀疏性方面的问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂任务时受限于静态内部知识，RAG方法在多跳推理和策略搜索方面存在局限，现有基于结果奖励的强化学习方法面临梯度冲突和奖励稀疏性问题

Method: 1. Atomic Thought：将推理分解为细粒度功能单元；2. Reasoning Reward Models提供细粒度奖励指导；3. Atom-Searcher框架：整合Atomic Thought和ATR，采用课程式奖励调度策略

Result: 在7个基准测试中均优于现有最先进方法，实现了计算扩展、更好的可解释性和更接近人类的推理模式

Conclusion: Atomic Thought和Atom-Searcher框架有效解决了agentic深度研究中的关键挑战，为LLM的复杂推理任务提供了新的解决方案

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [58] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: 本文挑战了高资源标准语言对齐有助于低资源方言建模的假设，通过因果研究发现过度表征纠缠反而会阻碍生成建模，并提出在线变分探测框架进行子空间解耦，在25种阿拉伯方言上平均提升2.0 chrF++生成质量


<details>
  <summary>Details</summary>
Motivation: 挑战高资源标准语言对齐必然有益于相关低资源变体建模的传统假设，研究过度表征纠缠对生成建模的负面影响

Method: 提出在线变分探测框架，在微调过程中持续估计标准语言的子空间，通过投影解耦实现表征分离；使用阿拉伯语作为案例研究，因其在25种方言上具有丰富的平行资源

Result: 在25种阿拉伯方言上，干预方法相比标准微调平均提升2.0 chrF++，最高提升4.9 chrF++，但标准语言性能有所下降

Conclusion: 提供了因果证据表明高资源变体的子空间主导会限制相关变体的生成能力，统一了几何和信息论探测与子空间级因果干预，为多语言和多领域LLMs的表征分配控制提供了实用工具

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [59] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: 构建法语语义语料库，通过AMR标注法语对话，扩展框架以更好处理自发语音和法语特有结构，并训练AMR解析器作为辅助标注工具


<details>
  <summary>Details</summary>
Motivation: 为法语对话开发语义资源，解决AMR在自发语音动态性和法语特有句子结构方面的覆盖不足问题

Method: 标注DinG语料库（Catan棋盘游戏对话转录），扩展AMR框架，制定详细标注指南，训练和评估AMR解析器

Result: 发布了免费许可的标注语料库，开发了能够作为辅助标注工具的AMR解析模型

Conclusion: 这项工作促进了法语对话语义资源的发展，为后续研究提供了有价值的语料库和工具

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [60] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本研究探讨利用父推文上下文特征来检测回复推文中的辱骂性语言，发现结合上下文内容特征能显著提升检测性能，优于仅使用回复推文特征的方法。


<details>
  <summary>Details</summary>
Motivation: 现有辱骂性语言检测研究主要关注单个社交媒体帖子，忽略了从周围帖子中获得的额外上下文信息。本研究旨在探索利用父推文上下文是否有助于检测回复推文中的辱骂内容。

Method: 使用四种不同分类模型，在包含父推文-回复推文对话对的数据集上进行实验，比较仅使用回复推文特征与结合上下文特征（内容基和账户基特征）的效果。

Result: 实验表明，结合上下文特征相比仅使用回复推文特征有显著改进。内容基特征（发布内容）比账户基特征（发布者身份）对分类性能贡献更大，且组合多种内容特征效果最佳。

Conclusion: 研究证实了利用上下文信息对辱骂性语言检测的重要性，为在真实对话场景中开发情境化检测模型提供了见解，强调应综合使用多种内容特征而非选择性使用少量特征。

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [61] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: 本文应用文体计量学技术分析中世纪经院哲学文献报告集，验证编辑工作层次假设，比较手动与自动数据提取性能，测试transformer OCR在拉丁语语料库中的有效性


<details>
  <summary>Details</summary>
Motivation: 早期经院哲学时期基于口头教学记录的文献生产很常见，但缺乏相关实践评论。需要验证关于Stephen Langton神学问题集形成的假设，并探索中世纪大学协作文学生产的分析方法

Method: 采用HTR（手写文本识别）流水线和文体计量分析，基于高频词、词性标签和伪词缀进行作者归属分析，比较手动撰写和自动提取数据的性能，测试基于transformer的OCR和自动转录对齐技术

Result: 研究尚在进行中，但预期将提供可重复使用的分析模板，直接比较不同数据源的性能，验证transformer OCR在经院拉丁语语料库工作流程中的有效性

Conclusion: 如果成功，本研究将为探索性分析中世纪大学协作文学生产提供一个易于重复使用的模板，推动经院传统计算研究的方法学发展

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [62] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: 研究发现Transformer语言模型的词嵌入空间编码了丰富的语义信息，通过聚类分析验证了模型具有类似词汇存储的语义表征能力


<details>
  <summary>Details</summary>
Motivation: 探究Transformer语言模型是否采用类似词汇存储的机制来表示词语含义，验证模型是否真正编码语义信息

Method: 提取RoBERTa-base模型的词嵌入空间，使用k-means聚类为200个簇，手动检查簇的语义敏感性，并测试对五个心理语言学指标的敏感性

Result: 词嵌入空间编码了广泛的语义信息，簇对情感效价、具体性、象似性、禁忌性和习得年龄等心理语言学指标表现出敏感性

Conclusion: 研究结果支持Transformer语言模型具有语义处理能力，排除了某些"意义消除主义"假说，表明模型确实编码了丰富的语义信息

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [63] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: 本文提出基于LLM代理的方法解决语义表格标注(STA)任务，通过ReAct框架设计五个外部工具，动态选择标注策略，在Tough Tables和BiodivTab数据集上表现优异，并通过Levenshtein距离减少冗余标注，显著降低时间和计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决复杂表格标注中的挑战：列名/单元格值语义丢失、严格的本体层次要求、同义词、拼写错误和缩写等问题，这些因素影响了标注准确性。

Method: 基于ReAct框架设计五个外部工具，使用定制提示词，使STA代理能够根据表格特征动态选择合适的标注策略。利用Levenshtein距离减少冗余标注。

Result: 在SemTab挑战的Tough Tables和BiodivTab数据集上，方法在各项指标上优于现有方法。时间成本减少70%，LLM token使用减少60%。

Conclusion: 该方法为STA任务提供了高效且成本效益高的解决方案，通过LLM代理和动态策略选择有效解决了复杂表格标注的挑战。

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [64] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: PASR是一种主动式自优化方法，让LLM在生成过程中动态决定是否、何时以及如何优化输出，相比固定迭代次数的传统方法显著提升了效率（减少41.6% token消耗）和准确性（提升8.2%）


<details>
  <summary>Details</summary>
Motivation: 现有自优化方法采用固定迭代次数的被动优化过程，难以根据生成上下文动态确定最佳优化时机和内容，而人类在思考过程中会动态优化自己的想法

Method: 提出ProActive Self-Refinement (PASR)方法，基于模型内部状态和演化上下文主动决定是否、何时以及如何优化，而非重新生成整个响应

Result: 在10个多样化任务上的实验显示，PASR显著提升问题解决性能，在Qwen3-8B上相比标准生成平均减少41.6% token消耗，同时准确率提升8.2%

Conclusion: PASR通过主动动态优化机制有效解决了传统自优化方法的局限性，在提升模型性能的同时大幅降低了计算成本

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [65] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: 该研究构建了一个基于LLM的多智能体系统用于旅行规划任务，通过引入笔记本机制和协调器智能体，显著提升了复杂约束条件下的规划性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在长时域、多约束规划任务中面临信息细节处理和复杂约束满足的挑战，需要更好的信息共享和协调机制。

Method: 构建LLM-based多智能体系统，评估笔记本机制促进信息共享，以及协调器智能体改善自由对话中的协调效果。

Result: 笔记本减少幻觉错误18%，协调器进一步减少错误13.5%，两者结合在TravelPlanner基准上达到25%通过率，相比单智能体基线7.5%提升17.5%。

Conclusion: 结构化信息共享和反思性协调是多智能体系统在LLM长时域规划中的关键组件，具有重要潜力。

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [66] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: WebMall是一个多商店在线购物基准测试，用于评估网页代理在比较购物中的效果和效率，包含91个跨商店任务和真实产品数据，相比现有基准更复杂和真实。


<details>
  <summary>Details</summary>
Motivation: 现有的电子商务基准测试如WebShop或ShoppingBench主要关注单一商店内的任务，缺乏跨商店比较购物的评估能力，无法充分测试网页代理在真实多商店环境中的表现。

Method: 构建了四个模拟在线商店，使用Common Crawl抓取的真实产品数据填充，设计了91个跨商店任务，包括基础任务（价格比较、购物车操作）和高级任务（模糊需求搜索、替代品识别）。

Result: 评估了8个基线代理，最佳配置在基础任务集上达到75%完成率和87% F1分数，在高级任务集上达到53%完成率和63% F1分数。

Conclusion: WebMall为网页代理研究提供了更真实、复杂的多商店比较购物评估基准，有助于推动电子商务场景中的导航、推理和效率方面的进步。

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [67] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文提出了一种结合双模态讽刺检测模型反馈损失和两阶段微调策略的讽刺语音合成方法，有效提升了合成语音的讽刺表达质量


<details>
  <summary>Details</summary>
Motivation: 讽刺语音合成对于增强娱乐和人机交互应用的自然性至关重要，但由于讽刺语音的微妙韵律特征和标注数据稀缺，现有方法面临挑战

Method: 1) 将双模态讽刺检测模型的反馈损失整合到TTS训练过程中；2) 采用两阶段微调：先在多样化语音风格数据集上微调，再在讽刺语音专用数据集上精调

Result: 主客观评估表明，所提方法显著提高了合成语音的质量、自然度和讽刺感知能力

Conclusion: 该方法通过整合多模态反馈和渐进式微调，有效解决了讽刺语音合成的关键挑战，为情感语音合成提供了新思路

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [68] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: LoRID方法通过多LoRA交互蒸馏，结合系统1和系统2思维模式，显著提升小语言模型的数学推理能力，在GSM8K数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型数学推理能力差的问题，现有方法主要依赖大语言模型生成数据进行填鸭式训练，类似于心理学的系统1思维，但缺乏系统2的知识获取和强化过程。

Method: 提出基于多LoRA交互的数学推理蒸馏方法：1）创建知识增强数据集；2）训练直觉推理器直接生成思维链；3）训练知识生成器和深度推理器模仿系统2思维；4）通过输出一致性检查和迭代推理实现相互反馈。

Result: 在GSM8K数据集上，LoRID在五个基础模型上分别比第二佳方法提升了2.3%、16.1%、2.4%、12.3%和1.8%的准确率，达到最先进性能。

Conclusion: LoRID通过结合系统1和系统2思维模式，有效提升了小语言模型的数学推理能力，证明了多LoRA交互蒸馏方法的有效性。

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [69] [Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Banu Diri,Savaş Yıldırım,Öner Aytaş*

Main category: cs.CL

TL;DR: 提出了土耳其语MMLU基准(TR-MMLU)，包含6200道选择题，用于评估大语言模型在土耳其语上的语言和概念能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估主要针对英语等资源丰富语言，土耳其语等资源有限语言缺乏全面的评估基准，需要专门的评估框架来推动土耳其NLP研究发展。

Method: 基于土耳其教育体系精心策划了62个领域的6200道选择题数据集，构建了TR-MMLU基准，并用最先进的大语言模型进行测试评估。

Result: 建立了首个全面的土耳其语评估基准，为土耳其NLP研究提供了标准化框架，通过测试发现了模型设计中需要改进的方面。

Conclusion: TR-MMLU为土耳其NLP研究设立了新标准，将推动土耳其语语言模型的发展并激发未来创新。

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [70] [Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım*

Main category: cs.CL

TL;DR: 该研究为土耳其语等形态丰富语言提出了新的分词评估框架，发现语言特定token比例比token纯度与下游任务性能相关性更强，模型参数增加不一定提升语言性能


<details>
  <summary>Details</summary>
Motivation: 解决形态丰富和低资源语言（如土耳其语）在NLP分词中的特殊挑战，传统分词方法可能无法有效捕捉这些语言的复杂语言结构

Method: 使用土耳其MMLU数据集（6200个选择题），评估分词器在词汇量、token数量、处理时间、语言特定token百分比(%TR)和token纯度(%Pure)等新指标上的表现

Result: 语言特定token百分比(%TR)比token纯度(%Pure)与下游任务性能（如MMLU分数）有更强的相关性；仅增加模型参数不一定能提升语言性能

Conclusion: 需要针对特定语言定制分词方法，提出的框架为形态复杂语言建立了稳健实用的分词标准

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [71] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: SFUSED语音错误数据库可用于评估语音识别模型性能，通过分析WhisperX在5300个语音错误上的转录准确率，验证了该数据库作为ASR系统诊断工具的有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个公开的语音错误数据库，用于语言学和心理语言学研究，并探索其在语音识别模型评估中的应用价值。

Method: 使用SFUSED数据库中的系统标注语音错误数据，评估WhisperX模型在5300个单词和音韵错误上的转录准确性，分析不同分类维度对模型性能的影响。

Result: 分析表明SFUSED数据库能够有效诊断ASR系统的性能表现，不同分类维度（如语言层级、上下文敏感性等）对模型评估具有重要价值。

Conclusion: SFUSED数据库是一个有效的语音识别模型诊断工具，其多维标注体系为ASR系统性能评估提供了有价值的分析维度。

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [72] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: 提出ReCOR强化学习框架，通过自适应token生成顺序提升语言模型在复杂推理任务上的性能，无需标注数据即可学习最优生成顺序


<details>
  <summary>Details</summary>
Motivation: 现有因果语言模型和扩散模型采用固定或随机token生成顺序，与原始逻辑顺序存在偏差，导致在需要自适应生成顺序的复杂问题上表现不佳

Method: 基于强化学习的ReCOR框架，通过token预测统计进行自监督，估计未填充token的预测难度，在训练和推理时自适应选择下一个生成token

Result: 在具有挑战性的推理和规划数据集上，ReCOR表现出优越性能，有时甚至优于使用真实顺序监督的oracle模型

Conclusion: 自适应token生成顺序对复杂推理任务至关重要，ReCOR框架有效解决了现有模型在自适应生成顺序方面的局限性

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [73] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*Dayyán O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,Jörg Tiedemann*

Main category: cs.CL

TL;DR: DocHPLT是迄今为止最大的公开文档级翻译数据集，包含50种语言与英语的1.24亿对齐文档对，共42.6亿句子，为多语言文档级翻译提供重要基础设施


<details>
  <summary>Details</summary>
Motivation: 现有文档级机器翻译资源仅覆盖少数高资源语言，需要为全球社区提供长上下文建模的训练和评估资源

Method: 修改现有的网页提取流程以保持源文档的完整性，保留所有内容包括未对齐部分，并通过初步实验确定最佳训练上下文策略

Result: 在DocHPLT上微调的LLM显著优于现成的指令调优基线，对低资源语言的改进尤为显著

Conclusion: 该数据集在宽松许可下开源，为推进多语言文档级翻译提供了必要的基础设施

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [74] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: 本文提出了一个改进的法律领域检索增强生成(RAG)管道，通过上下文感知查询翻译、开源检索策略和综合评估框架，显著提升了检索性能，在保持成本效益的同时达到了与专有方法相当或更好的效果。


<details>
  <summary>Details</summary>
Motivation: 在法律领域中，检索增强生成(RAG)通过引用来源来减少大语言模型的幻觉问题至关重要。现有的LegalBenchRAG基线需要改进，特别是在查询处理、检索效率和答案忠实度方面。

Method: 1) 上下文感知查询翻译器，分离文档引用和自然语言问题，根据专业性和特异性调整检索深度和响应风格；2) 使用SBERT和GTE嵌入的开源检索策略；3) 结合RAGAS、BERTScore-F1和ROUGE-Recall的综合评估框架

Result: 检索性能显著提升：Recall@K提高30-95%，Precision@K提升约2.5倍(K>4)；开源管道在检索质量上可与专有方法相媲美或更优；定制法律基础提示词能产生更忠实和上下文相关的答案

Conclusion: 通过任务感知的组件级调优，可以开发出法律基础扎实、可复现且成本效益高的RAG系统，为法律研究辅助提供了有效解决方案

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [75] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: AutoBnB-RAG是一个扩展框架，通过检索增强生成(RAG)技术为多智能体事件响应模拟提供外部知识支持，显著提升了网络安全决策质量。


<details>
  <summary>Details</summary>
Motivation: 事件响应需要快速、协调和明智的决策，但现有LLM在模拟事件响应中因缺乏外部知识而受限，需要增强其推理能力。

Method: 基于Backdoors & Breaches桌面游戏环境，扩展AutoBnB框架集成RAG技术，引入两种检索设置：技术文档检索(RAG-Wiki)和事件报告检索(RAG-News)，评估八种团队结构。

Result: 检索增强显著提高了决策质量和成功率，能够重建复杂的多阶段网络攻击，在不同组织模型中均表现出色。

Conclusion: 将检索机制集成到基于LLM的多智能体系统中对网络安全决策具有重要价值，提升了事件响应的有效性和实用性。

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [76] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 论文提出了BlindSpot框架，用于检测和量化LLM在客服中心摘要生成中的操作偏见，发现所有评估模型都存在系统性偏见


<details>
  <summary>Details</summary>
Motivation: LLM在客服中心生成大量通话摘要，但尚不清楚是否存在对特定方面的系统性偏见，特别是操作偏见尚未被研究

Method: 开发BlindSpot框架，包含15个操作偏见维度的分类法，使用LLM作为零样本分类器计算分布差异，采用Fidelity Gap和Coverage两个指标量化偏见

Result: 对2500个真实通话记录和20个不同规模LLM生成的摘要进行分析，发现所有模型都存在系统性偏见，与模型大小或家族无关

Conclusion: 操作偏见在LLM摘要生成中普遍存在，需要开发偏见缓解技术来确保摘要的公平性和准确性

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [77] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 提出了MuDRiC数据集和基于GCN的新方法，用于阿拉伯语多方言常识验证，填补了阿拉伯语方言常识推理的资源空白


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语常识验证任务研究不足，现有资源主要关注现代标准阿拉伯语(MSA)，忽略了方言的丰富语言多样性，尽管方言在口语环境中普遍使用

Method: 引入MuDRiC多方言阿拉伯语常识数据集，并提出基于图卷积网络(GCN)的新方法，增强语义关系建模以改进常识验证

Result: 实验结果表明该方法在阿拉伯语常识验证任务上取得了优越性能

Conclusion: 本研究通过提供基础数据集和新颖方法，增强了阿拉伯语自然语言理解能力，处理其复杂的语言变体

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [78] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: 本文研究了将水印检测器与非水印检测器结合的混合方案，在语言模型生成检测中取得了比单一检测器更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 水印检测在语言模型生成检测中效果显著，但其强度严重依赖语言模型的熵和输入提示。在实际应用中，特别是经过指令微调或RLHF后训练的模型，熵往往有限，使得仅依赖水印检测变得困难。

Method: 探索了多种混合方案，将水印检测器与非水印检测器相结合，在广泛的实验条件下进行测试。

Result: 观察到混合方案在各种实验条件下都获得了性能提升，表现优于任何单一类型的检测器。

Conclusion: 通过结合水印和非水印检测器，可以有效提高语言模型生成检测的准确性和鲁棒性，特别是在熵受限的实际应用场景中。

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [79] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: OptimalThinkingBench是一个统一基准，用于评估LLMs的过度思考（overthinking）和思考不足（underthinking）问题，旨在推动开发在性能和效率间平衡的最优思考模型。


<details>
  <summary>Details</summary>
Motivation: 现有思考型LLMs在处理复杂任务时计算成本高且在简单问题上过度思考，而非思考型LLMs虽然快速廉价但在困难推理问题上思考不足。这导致需要用户为每个查询选择合适模型，缺乏统一评估标准。

Method: 构建包含两个子基准的统一评测框架：OverthinkingBench（72个领域的简单查询）和UnderthinkingBench（11个挑战性推理任务），使用新颖的思考调整准确率指标评估33个不同模型。

Result: 评估显示没有模型能在该基准上实现最优思考。思考型模型常在简单查询上过度思考数百个token但性能无提升，而大型非思考型模型思考不足，表现甚至不如更小的思考型模型。尝试的优化方法往往在一个子基准上改进却牺牲另一个。

Conclusion: 当前LLMs普遍存在过度思考或思考不足的问题，需要开发更好的统一最优思考模型来平衡性能与效率。

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [80] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: 该论文分析了评估基准的质量指标（信号和噪声），提出了三个改进干预措施，并推荐创建高信号低噪声的基准。


<details>
  <summary>Details</summary>
Motivation: 开发大型语言模型成本高昂，需要依赖小规模实验通过多任务评估套件做出决策，因此需要更可靠的评估基准。

Method: 引入信号和噪声两个关键指标，分析30个基准和375个语言模型，提出三种干预措施：改进度量指标、过滤噪声子任务、平均中间检查点输出。

Result: 信号噪声比更好的基准在小规模决策中更可靠，噪声较低的基准具有更低的缩放定律预测误差。三种干预措施均能提高基准可靠性。

Conclusion: 建议创建新基准或选择现有基准时追求高信号和低噪声，使用改进的度量指标和噪声过滤技术可以提高评估质量。

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [81] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: RepreGuard是一种基于LLM内部表示统计特征的检测方法，在分布内和分布外场景下平均AUROC达到94.92%，优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成文本检测方法在分布外场景下的鲁棒性不足，作者假设LLM内部表示包含更全面和原始的特征，能更有效区分AI生成文本和人类写作文本

Method: 使用代理模型收集LLM生成文本和人类文本的表示，提取能够更好识别AI生成文本的独特激活特征，通过计算文本表示在该特征方向上的投影分数并与预计算阈值比较来进行分类

Result: 在分布内和分布外场景下平均AUROC达到94.92%，优于所有基线方法，同时对不同文本大小和主流攻击具有鲁棒性

Conclusion: LLM内部表示确实包含区分AI生成文本和人类文本的有效统计特征，RepreGuard方法在检测性能和鲁棒性方面都表现出色

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: 提出基于深度学习的实时吸烟检测系统，用于消防出口区域的CCTV监控，在多种环境下实现高性能检测，特别适合边缘设备部署


<details>
  <summary>Details</summary>
Motivation: 由于消防安全要求，需要实时监控消防出口区域的吸烟行为，防止火灾风险并确保自动合规监管

Method: 评估YOLOv8、YOLOv11、YOLOv12三种先进目标检测模型，基于YOLOv8开发定制模型，添加特殊结构处理监控场景挑战，并在多边缘设备上进行多线程性能测试

Result: 提出的定制模型表现最佳，召回率达到78.90%，mAP@50达到83.70%，在Jetson Xavier NX上推理时间为52-97毫秒/次

Conclusion: 该系统为公共安全监控提供了强大且适应性强的平台，能够满足实时性要求并实现自动合规监管

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [83] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: 该论文提出了一种仅使用程序生成数据训练表征模型的方法，通过视觉记忆（参考图像嵌入数据库）在视觉相似性、分类和语义分割任务上实现零样本性能，性能接近甚至超越基于真实数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要大量真实图像数据训练的问题，探索仅使用程序生成数据训练视觉表征模型的可能性，实现与真实世界图像的完全隔离。

Method: 使用程序生成数据训练表征模型，构建视觉记忆数据库存储参考图像嵌入，在推理时通过查询记忆数据库来完成各种视觉任务，无需额外训练。

Result: 在NIGHTS视觉相似性任务上性能差距仅1%，在CUB200和Flowers102细粒度分类上分别超越8%和15%，在ImageNet-1K分类上差距10%，在COCO零样本分割任务上R²差距10%。

Conclusion: 程序数据模型在多个视觉任务上展现出接近真实数据模型的性能，但物体内部部分表征不相似导致记忆搜索错误，这是剩余性能差距的主要原因。

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [84] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 本研究首次系统评估了单一和融合的眼科基础模型，提出了FusionFM评估框架和两种融合方法。结果显示DINORET和RetiZero在眼科和系统性疾病预测中表现最佳，门控融合策略在特定疾病预测中有一定提升。


<details>
  <summary>Details</summary>
Motivation: 眼科领域已出现多个基础模型，但缺乏系统性的性能评估和比较，不清楚哪个模型表现最好、是否在不同任务中表现一致，以及融合多个模型是否能带来性能提升。

Method: 提出FusionFM评估框架，包含两种融合方法（门控融合等），在标准化多国数据集上评估4个SOTA基础模型（RETFound、VisionFM、RetiZero、DINORET），使用AUC和F1指标，涵盖眼科疾病（青光眼、糖尿病视网膜病变、AMD）和系统性疾病（糖尿病、高血压）预测。

Result: DINORET和RetiZero在眼科和系统性疾病任务中表现最优，RetiZero在外部队据集上泛化能力更强。门控融合策略在青光眼、AMD和高血压预测中有适度提升，但系统性疾病（特别是高血压）在外部队列中的预测仍具挑战性。

Conclusion: 研究提供了眼科基础模型的循证评估，证明了模型融合的益处，并为提升临床适用性指明了方向，但系统性疾病预测仍需进一步改进。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [85] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF是一个统一的多模态深度学习框架，能够通过点云和多视角图像的融合编码来重建多种牙颌面硬组织，解决了现有单模态方法的局限性，在几何精度、结构完整性和空间准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 牙颌面硬组织缺损严重影响患者的生理功能、面部美观和心理健康，当前深度学习模型仅限于单组织场景和特定模态成像输入，导致泛化性差，需要在解剖保真度、计算效率和跨组织适应性之间进行权衡。

Method: UniDCF通过点云和多视角图像的多模态融合编码，利用每种模态的互补优势，并引入基于分数的去噪模块来细化表面平滑度。研究构建了最大的多模态数据集，包含6,609名患者的口内扫描、CBCT和CT数据，共54,555个标注实例。

Result: 评估显示UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进方法。临床模拟表明UniDCF将重建设计时间减少了99%，临床医生接受率超过94%。

Conclusion: UniDCF实现了快速、自动化、高保真度的重建，支持个性化和精确的修复治疗，简化了临床工作流程，改善了患者治疗效果。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [86] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5是Ovis2的升级版，专注于原生分辨率视觉感知和强大多模态推理。它采用原生分辨率视觉变换器处理图像，避免固定分辨率分块带来的质量下降，并通过反思机制增强推理能力。模型经过五阶段课程训练，发布9B和2B两个版本，在OpenCompass多模态排行榜上取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 为了解决固定分辨率图像处理导致的细节丢失和全局布局破坏问题，特别是在视觉密集内容（如复杂图表）中，同时需要增强多模态推理能力，超越传统的线性思维链方法。

Method: 1. 集成原生分辨率视觉变换器，处理可变原生分辨率图像
2. 引入反思机制（自检查和修订）作为可选"思考模式"
3. 采用五阶段课程训练：基础视觉和多模态预训练、大规模指令调优、DPO和GRPO对齐与推理增强
4. 使用多模态数据打包和混合并行技术实现高效扩展

Result: Ovis2.5-9B在OpenCompass多模态排行榜平均得分78.3，显著超越前代Ovis2-8B，在40B参数以下开源MLLM中达到SOTA；Ovis2.5-2B得分73.9，在其规模级别建立SOTA。在STEM基准测试、接地任务、视频任务和复杂图表分析方面均取得领先成绩。

Conclusion: Ovis2.5通过原生分辨率视觉处理和反思推理机制，在多模态理解和推理能力方面实现了显著提升，为不同规模的应用场景提供了高性能解决方案，特别是在资源受限的设备端部署方面表现出色。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [87] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE是首个公开的视频到文本电商属性值提取数据集，涵盖14个领域172个属性，包含22.4万训练数据和2.5万评估数据，并建立了全面的基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决现有AVE数据集仅限于文本到文本或图像到文本设置，缺乏对产品视频支持、多样化属性覆盖和公开可用性的问题。

Method: 提出基于CLIP的混合专家过滤系统(CLIP-MoE)来去除不匹配的视频-产品对，确保数据质量；建立基准测试评估最先进的视频视觉语言模型。

Result: 视频到文本AVE仍然是一个具有挑战性的问题，特别是在开放设置中，现有模型在利用有效时间信息方面还有改进空间。

Conclusion: VideoAVE填补了视频AVE数据集的空白，为开发更先进的视觉语言模型提供了重要资源，展示了视频属性值提取任务的挑战性和发展潜力。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [88] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 研究证明仅使用二阶几何特征（平面曲率大小、曲率符号和梯度方向）就能让MLP在手写字符识别中达到97%的准确率，为CNN提供了可解释的替代方案


<details>
  <summary>Details</summary>
Motivation: 探索是否可以不依赖卷积神经网络，仅使用可解释的手工几何特征就能实现高效的手写字符识别，提供深度学习的替代方案

Method: 使用三个手工特征图（曲率大小、曲率符号和梯度方向）作为输入，构建多层感知机分类器进行手写字符识别

Result: 在MNIST数字数据集上达到97%准确率，在EMNIST字母数据集上达到89%准确率

Conclusion: 曲率特征在手写字符识别中具有很强的判别能力，深度学习优势可以通过可解释的手工特征实现

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [89] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 本文提出双管齐下的方法来改进多模态仇恨内容检测：通过提示优化框架提升模型性能，以及通过多模态数据增强管道生成反事实中性样本减少虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 现代网络充斥着多模态内容，仇恨表情包往往通过文本和图像的微妙互动以幽默或讽刺为幌子传递有害意图。现有的视觉语言模型缺乏细粒度监督支持，容易受到隐式仇恨言论的影响。

Method: 1) 提示优化框架：系统变化提示结构、监督粒度和训练模态；2) 多模态数据增强管道：使用多智能体LLM-VLM设置生成2,479个反事实中性表情包，通过隔离和重写仇恨模态来减少虚假相关性。

Result: 结构化提示即使在小型模型中也能提高鲁棒性，InternVL2在二分类和分级设置中均获得最佳F1分数。数据增强管道成功改善了分类器的泛化能力。

Conclusion: 提示结构和数据组成与模型规模同等重要，针对性数据增强可以支持更可信和上下文敏感的仇恨检测，为构建合成数据训练鲁棒公平的视觉语言模型提供了新方向。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [90] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 该论文研究了高斯曲率在3D表面建模中的作用，发现它提供了稀疏紧凑的表面描述，可作为几何先验改进3D重建，并可能作为立体视觉方法的无监督度量指标


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的计算机视觉方法缺乏可分析的显式3D几何模型，无法跨模态迁移或进行受控实验，因此需要研究高斯曲率这样的几何不变量

Method: 使用Middlebury立体数据集进行研究，分析高斯曲率在3D表面建模中的特性，并与最先进的单目和立体方法进行比较

Result: 发现高斯曲率提供了稀疏紧凑的3D表面描述；现有方法似乎隐式考虑了高斯曲率但无法提取显式模块；高斯曲率可作为几何先验改进3D重建；可能作为立体方法的无监督度量

Conclusion: 高斯曲率作为几何不变量在3D表面建模中具有重要价值，能够提供可分析的几何表示，有望改进现有深度学习方法并开发新的无监督评估指标

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [91] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: 本研究系统评估了多种图像到图转换方法对GNN图级异常检测的影响，发现在皮肤镜图像上，颜色特征贡献最佳性能，结合形状和纹理特征能进一步提升效果，最佳无监督配置达到0.805 AUC-ROC


<details>
  <summary>Details</summary>
Motivation: 虽然GNN已被用于图像衍生的图表示任务，但缺乏对多种图像到图转换方法在GNN图级异常检测中有效性的系统比较

Method: 系统评估多种分割方案、边构建策略和节点特征集（颜色、纹理、形状描述符），使用最先进的GLAD模型在皮肤镜图像上进行广泛实验，涵盖无监督、弱监督和全监督三种模式

Result: 颜色描述符单独性能最佳，结合形状和纹理特征能持续提升检测效果；最佳无监督配置达到0.805 AUC-ROC，弱监督提升至0.872，全监督达到0.914 AUC-ROC

Conclusion: 图像到图转换方法的选择对GNN图级异常检测性能有重要影响，颜色特征是最重要的单一特征，多特征融合能获得最佳性能，且无需依赖预训练骨干网络

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [92] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: 这篇综述论文系统性地分类和评估了Transformer架构在无人机系统中的应用，包括注意力机制、CNN-Transformer混合模型、强化学习Transformer和大型语言模型，提供了统一的分类法、比较分析和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的快速发展，无人机系统的感知、决策和自主性能力得到了显著提升。本文旨在系统梳理和评估这些最新进展，为研究者和从业者提供全面的技术指导。

Method: 采用系统性综述方法，对Transformer在无人机领域的应用进行分类和评估，包括构建统一的分类法、进行对比分析、整理关键数据集和评估指标。

Result: 提出了Transformer在无人机应用中的统一分类法，识别了精准农业和自主导航等新兴应用领域，通过结构化表格和性能基准提供了详细的比较分析。

Conclusion: 论文总结了Transformer驱动无人机技术的关键挑战（如计算效率和实时部署），并指出了未来研究方向，为该领域的进一步发展提供了重要指导。

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [93] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat是一种针对3D高斯泼溅技术的黑盒攻击方法，通过利用标准着色方法创建视角特定的伪装，在特定视角下嵌入对抗性内容，攻击下游物体检测器。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅技术在安全关键任务中的快速应用，需要研究攻击者如何通过篡改图像来造成危害，暴露这种技术在新颖视角合成中的安全风险。

Method: 利用标准3DGS着色方法创建视角特定的伪装（颜色和纹理随视角变化），在场景对象中嵌入仅在特定视角可见的对抗性内容，无需访问模型架构或权重。

Result: 实验表明ComplicitSplat能够成功攻击多种流行的检测器（单阶段、多阶段和基于transformer的模型），在真实世界物理对象捕获和合成场景中均有效。

Conclusion: 这是首个针对下游物体检测器的3DGS黑盒攻击，揭示了在自动驾驶导航和其他关键机器人系统应用中的新型安全风险。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [94] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文评估了医学影像基础模型ProFound在前列腺多参数MRI中的标签效率，发现图像质量分布及其在微调与测试集之间的不匹配显著影响模型性能。当质量比例一致时，微调所需标注数据远少于从头训练，但标签效率取决于图像质量分布。


<details>
  <summary>Details</summary>
Motivation: 研究医学影像基础模型在标签效率方面的表现，特别关注图像质量变化如何影响微调后的模型泛化能力，以及质量分布在微调集和测试集之间的匹配重要性。

Method: 使用在前列腺MRI大数据集上预训练的领域特定视觉基础模型ProFound，系统性地变化微调集和评估集中高/低质量图像的比例，测量微调模型的泛化性能。

Result: 图像质量分布及其微调-测试不匹配显著影响模型性能。微调集中足够高质量图像对维持强性能至关重要，而匹配的微调-测试分布重要性因下游任务（如自动放射学报告和前列腺癌检测）而异。

Conclusion: 需要评估和对齐微调与部署之间的质量分布，为特定下游任务制定微调数据的质量标准，量化图像质量对于充分实现基础模型的数据和计算效率优势至关重要。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [95] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: AdaRing是一个基于跨层张量环分解的视觉语言微调框架，通过整合多样化适配器实现超轻量参数高效适配，在减少90%训练参数的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法存在两个主要限制：1) 由于忽略跨层冗余导致压缩率有限；2) 同质适配器的表示能力有限。需要解决跨层冗余问题并提升适配器的表示能力。

Method: 采用跨层张量环分解(TRD)技术，将适配器表示为层共享的张量核心和层特定切片，利用张量级低秩性消除跨层冗余。通过泛化感知微调指导，让不同秩驱动的多样化适配器协同处理需要不同表示的任务。

Result: 实验表明，AdaRing在减少平均训练参数90%的情况下，仍然达到了最先进的性能表现。

Conclusion: 提出的AdaRing框架通过张量环分解和多样化适配器协作，成功实现了超轻量级的参数高效视觉语言模型适配，在保持高性能的同时显著降低了参数需求。

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [96] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: EVTP-IV是一种新颖的视觉token剪枝方法，通过选择空间代表性强的token子集，在保持精度的同时实现5倍视频任务加速和3.5倍图像任务加速


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在指令视觉分割任务中推理成本过高，特别是视频任务，需要降低计算开销

Method: 基于k-center算法整合空间信息来选择紧凑但空间代表性的token子集，提供信息论分析支持设计

Result: 在标准IVS基准测试中，仅使用20%的token就能保持可比精度，视频任务加速5倍，图像任务加速3.5倍，优于现有剪枝方法

Conclusion: 该方法通过有效的token剪枝策略显著加速推理过程，为多模态大语言模型的高效部署提供了实用解决方案

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [97] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: 提出了基于纯CNN的大核调制网络(LKMN)，通过增强部分大核块和交叉门前馈网络，在轻量级图像超分辨率任务中实现了性能与效率的平衡，超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限场景下图像超分辨率的轻量化需求，CNN延迟低但缺乏非局部特征捕获能力，Transformer非局部建模能力强但推理速度慢，需要平衡性能与延迟的解决方案。

Method: LKMN包含两个核心组件：增强部分大核块(EPLKB)使用通道混洗增强通道交互、通道注意力聚焦关键信息、部分通道大核条带卷积进行非局部特征提取；交叉门前馈网络(CGFN)通过可学习缩放因子动态调整特征差异，采用交叉门策略调制融合特征。

Result: 在Manga109数据集×4超分辨率任务上，LKMN-L比DAT-light提升0.23dB PSNR，推理速度快约4.8倍，在质量和效率方面均优于现有轻量级SR模型。

Conclusion: LKMN证明了纯CNN架构在轻量级超分辨率任务中的有效性，通过创新的模块设计成功平衡了非局部特征捕获能力和推理效率，为资源受限场景提供了实用解决方案。

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [98] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用Sobel算子提取的一阶边缘图作为输入，训练全连接MLP网络进行手写字符识别，在MNIST和EMNIST数据集上达到接近CNN的性能，但模型更小更透明


<details>
  <summary>Details</summary>
Motivation: 重新审视经典Sobel算子，探索仅使用一阶边缘图是否足以驱动全连接MLP进行手写字符识别，作为CNN的替代方案

Method: 仅使用水平和垂直Sobel导数作为输入，在MNIST和EMNIST Letters数据集上训练多层感知机(MLP)

Result: 在MNIST数字上达到98%准确率，在EMNIST字母上达到92%准确率，接近CNN性能但内存占用更小、特征更透明

Conclusion: 手写字符图像中的类别判别信息大部分已被一阶梯度捕获，基于边缘感知的MLP是HCR的一个有吸引力的选择

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [99] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了在线视频定位新任务OVG-HQ，支持文本、图像、视频片段等多模态查询，解决了传统视频定位在流媒体和视觉查询场景的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统视频定位方法在处理流媒体视频和使用视觉线索查询时存在不足，需要开发支持多模态查询的在线定位解决方案。

Method: 提出OVG-HQ-Unify统一框架，包含参数记忆块(PMB)保持历史知识和跨模态蒸馏策略平衡不同模态学习，构建了QVHighlights-Unify多模态数据集。

Result: 实验表明OVG-HQ-Unify优于现有模型，提供了在线混合模态视频定位的鲁棒解决方案，并提出了新的在线评估指标。

Conclusion: 该工作填补了在线多模态视频定位的空白，通过统一框架有效解决了模态不平衡和上下文有限的问题，为实际应用提供了可行方案。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [100] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl是一个轻量级非侵入式插件，通过检测-抑制范式来提升文本到图像模型的安全性，在保持生成质量的同时有效防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型安全方法存在安全性与保真度的权衡问题，基于概念替换的定位方法可能导致语义不连贯，需要更灵活的解决方案。

Method: 采用检测-抑制范式，首先精确定位不安全内容，然后抑制有害语义而非硬性替换，使用DPO训练策略利用图像级偏好数据学习抑制行为，无需像素级标注。

Result: 在安全效果和保真度保持方面显著优于现有最先进方法，证明了分离式抑制控制的有效性和可扩展性。

Conclusion: 基于抑制的解耦控制是构建更负责任生成模型的高效且可扩展方向，SafeCtrl为文本到图像模型安全提供了新的解决方案。

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [101] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP是一个轻量级框架，通过利用单个像素的时空和光谱信息进行土地利用分类，减少对大型空间瓦片和文本监督的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在遥感应用中面临两个关键挑战：依赖大型空间瓦片增加计算成本，以及依赖文本监督但文本数据往往不易获得。

Method: 利用Sentinel-2影像的光谱和时间信息，通过跨视角学习与地理标记的地面照片进行语义对齐，最小化基于标题的训练需求。

Result: 实验证明单个像素输入结合时空和光谱线索足以进行专题制图，为大规模遥感应用提供了可扩展且高效的替代方案。

Conclusion: 该方法在LULC、作物类型和生态系统类型分类任务中表现良好，展示了单像素时空光谱信息在遥感分类中的有效性。

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [102] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 本研究探讨了使用GAN生成的合成MRI数据增强脑肿瘤分割训练的效果，发现40%真实+60%合成数据的混合数据集能改善整体肿瘤边界分割，但肿瘤核心区域的分割精度仍有待提升。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤手动分割面临肿瘤异质性、标注数据稀缺和类别不平衡等挑战，合成数据有望通过增加数据集多样性来解决这些问题。

Method: 使用预训练GAN模型生成合成MRI数据，结合BraTS 2020真实数据，构建不同比例的真实-合成混合数据集，训练U-Net分割网络进行实验。

Result: 定量性能指标（Dice系数、IoU等）在纯真实数据和混合数据训练模型间相当，但定性分析显示40%真实+60%合成数据的混合集改善了整体肿瘤边界分割，肿瘤核心和增强肿瘤区域的准确性仍较低。

Conclusion: 合成数据作为脑肿瘤分割的数据增强策略是可行的，但需要更大规模实验、保持体积数据一致性，并解决类别不平衡问题。

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [103] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 这是一篇关于深度学习基于点云去噪的综述性论文，系统总结了该领域的发展进展、分类法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实际环境中的点云数据存在各种噪声，需要先进行去噪处理才能提升下游任务性能。深度学习方法已经超越传统方法，但缺乏系统的综述性研究来总结该领域的发展。

Method: 将点云去噪模型化为两步过程：离群点移除和表面噪声恢复。系统总结现有方法的主要贡献，提出了专门为去噪任务设计的分类法，并进行方法比较分析。

Result: 该综述性研究填补了深度学习基于点云去噪领域的研究空白，为识别关键挑战、总结方法贡献和提供分类框架做出了贡献。

Conclusion: 论文不仅系统总结了现有研究进展，还讨论了研究限制和未来方向，为点云去噪领域的进一步发展提供了见解和指导。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [104] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose是一个无需重新训练的6D姿态跟踪框架，专门针对快速移动的相机和物体场景，通过视觉惯性里程计、深度信息2D跟踪器和VIO引导的卡尔曼滤波器实现鲁棒跟踪


<details>
  <summary>Details</summary>
Motivation: 现有方法主要适用于静态或准静态场景，当相机和物体都快速移动时性能显著下降，需要解决快速运动场景下的6D姿态跟踪问题

Method: 提出三个协同组件：1）视觉惯性里程计补偿相机运动引起的ROI偏移；2）深度信息2D跟踪器校正大物体平移引起的ROI偏差；3）VIO引导的卡尔曼滤波器预测物体旋转并生成候选姿态进行分层细化

Result: 仿真和真实世界实验证明该方法有效，能够实现快速移动相机和物体的实时鲁棒6D姿态跟踪

Conclusion: 通过闭环系统确保准确的姿态初始化和精确跟踪，解决了快速运动场景下的6D姿态跟踪挑战

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [105] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: 该论文提出了一种基于知识蒸馏的点云多尺度特征近似方法，通过单邻域搜索实现多尺度特征学习，并设计可迁移特征嵌入机制来补偿多样性损失，同时引入中心加权IoU来缓解定位偏差，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征对点云目标检测至关重要，但传统方法需要多次邻域搜索和尺度感知层，计算成本高且不利于轻量化模型开发，特别是在计算资源受限的研究环境中。

Method: 1) 基于知识蒸馏从单邻域近似多尺度特征；2) 设计可迁移特征嵌入机制，使用类别感知统计作为低计算成本的可迁移特征；3) 引入中心加权交并比(CW-IoU)来缓解优化过程中的中心偏移问题。

Result: 在公开数据集上的大量实验证明了该方法的有效性，能够显著节省计算成本的同时保持良好的检测性能。

Conclusion: 该方法成功实现了在降低计算复杂度的前提下保持点云目标检测性能，为资源受限环境下的3D目标检测提供了有效的解决方案。

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [106] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG是首个统一理解和生成3D模态的框架，使用LLM处理文本和3D表示，核心采用潜在扩散模型生成高质量3D内容，支持基于参考图像和视角变换的3D场景生成与空间VQA任务。


<details>
  <summary>Details</summary>
Motivation: 尽管近期统一架构在图像理解和生成方面取得显著进展，但3D任务的整合仍然具有挑战性且探索不足，需要开发能够同时处理3D理解和生成的统一框架。

Method: 提出UniUGG框架，使用LLM理解和解码句子与3D表示；核心采用空间解码器（基于潜在扩散模型）生成高质量3D表示；提出几何语义学习策略预训练视觉编码器，联合捕获输入的语义和几何线索。

Result: 大量实验结果表明，该方法在视觉表示、空间理解和3D生成方面表现出优越性能。

Conclusion: UniUGG成功实现了3D模态的统一理解和生成，通过几何语义学习策略有效提升了空间理解和生成能力，为3D多模态任务提供了有效的解决方案。

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [107] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH是一个针对视频对象分割的moment-aware框架，通过时间对齐的文本-片段对和选择性监督来解决语义错位问题，在MeViS基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象分割方法存在语义错位问题，主要原因是训练时对所有可见对象进行无差别帧采样和监督，而不考虑它们与文本查询的实际相关性。

Method: 提出了SAMDWICH框架，包含：1）新标注数据集MeViS-M，手动标注对象被表达式引用的时间时刻；2）Moment-guided Dual-path Propagation（MDP）传播策略；3）Object-level Selective Supervision（OSS）对象级过滤策略。

Result: 在具有挑战性的MeViS基准测试中实现了最先进的性能，特别是在涉及多样化表达式的复杂场景中表现出色。

Conclusion: 通过时间对齐的监督和选择性训练策略，SAMDWICH有效增强了视频-文本对齐，显著提升了参考视频对象分割的性能。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [108] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++是一个协作学习框架，通过在异构架构、不同训练时刻和多重参数采样中获取交叉信息，实现高精度低计算成本的边缘检测。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习边缘检测模型计算成本高、模型大，难以在资源受限设备上部署的问题，寻求精度与计算效率的平衡。

Method: 提出PEdger++协作学习框架，利用异构架构、多样化训练时刻和多重参数采样的交叉信息，从集成学习角度增强特征学习能力。

Result: 在BSDS500、NYUD和Multicue数据集上实验证明，该方法在定量和定性评估中都优于现有方法，并提供多种计算需求的模型版本。

Conclusion: PEdger++通过协作学习有效降低了计算成本和模型大小，同时提高了边缘检测精度，具有良好的资源适应性。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [109] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 本文提出了一个新的多分辨率多模态微表情数据集，使用同步RGB和事件相机在可变光照条件下录制，展示了事件相机在微表情识别和帧重建方面的优势。


<details>
  <summary>Details</summary>
Motivation: 微表情分析在人机交互和驾驶员监控等领域有重要应用，但传统RGB相机在捕捉细微快速面部动作时存在时间分辨率限制和运动模糊问题。事件相机具有微秒级精度、高动态范围和低延迟的优势，但目前缺乏公开的事件相机微表情数据集。

Method: 创建了一个新颖的多分辨率多模态微表情数据集，使用同步RGB和事件相机录制。评估了两个基线任务：使用脉冲神经网络进行动作单元分类，以及使用条件变分自编码器进行帧重建。

Result: 动作单元分类任务中，事件数据达到51.23%准确率，显著优于RGB数据的23.12%。帧重建任务中，高分辨率事件输入实现了SSIM=0.8513和PSNR=26.89dB的优秀指标。

Conclusion: 事件相机数据在微表情识别和帧重建方面表现出色，证明了事件相机在微表情分析领域的应用潜力，为未来研究提供了有价值的多模态数据集基础。

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [110] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON是首个基于生成式多模态大语言模型的产品表示学习方法，通过引导式混合专家模块、核心语义区域检测和负采样策略，解决了多模态产品理解中的关键挑战，并在多个下游任务中展现出强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有判别式双流架构难以建模产品多图像与文本之间的多对一对应关系，而生成式多模态大语言模型在提升产品表示学习方面具有巨大潜力，但面临缺乏多模态建模模块、产品图像背景噪声干扰以及标准评估基准缺失等挑战。

Method: 提出MOON模型：(1)使用引导式混合专家模块进行多模态和特定方面的目标建模；(2)有效检测产品图像中的核心语义区域以减少背景噪声干扰；(3)引入专门的负采样策略增加负样本的难度和多样性。

Result: 模型在新建的大规模多模态基准MBE和公共数据集上均表现出竞争力的零样本性能，在跨模态检索、产品分类和属性预测等各种下游任务中展现出强大的泛化能力。

Conclusion: MOON通过创新的多模态建模方法有效解决了产品表示学习的关键挑战，案例研究和可视化结果证明了其在产品理解方面的有效性，为生成式MLLM在产品领域的应用提供了新思路。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [111] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive是一个面向动态驾驶场景的实例感知3D高斯泼溅框架，通过SAM生成的掩码作为伪真值指导2D特征学习，在3D层面引入正则化隐式编码实例身份，实现了无需数据预处理的3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法将背景元素统一为单一表示，阻碍了实例级理解和灵活场景编辑；现有2D分割提升到3D的方法依赖预处理实例ID或复杂流程，且主要针对室内场景，不适用于户外驾驶场景。

Method: 使用SAM生成的掩码作为伪真值，通过对比损失和伪监督目标指导2D特征学习；在3D层面引入正则化隐式编码实例身份，通过体素损失确保一致性；使用轻量级静态码本桥接连续特征和离散身份。

Result: 定量和定性实验证明了InstDrive的有效性，据我们所知，这是首个在动态开放世界驾驶场景中实现3D实例分割的框架。

Conclusion: InstDrive成功解决了动态驾驶场景中的实例感知重建问题，无需数据预处理或复杂优化，为自动驾驶和场景理解提供了有效的解决方案。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [112] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: WiseLVAM是一个全自动的左心室线性测量框架，通过结合B模式图像的结构感知和AMM模式的运动感知，自动放置虚拟扫描线并执行测量，提高临床应用的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化方法直接从B模式图像估计地标进行左心室测量，但即使预测点沿左心室壁有微小偏移也会导致显著的测量误差，降低了临床可靠性。需要开发更准确的全自动测量方法。

Method: 提出轮廓感知的扫描线放置方法，首先使用弱监督B模式地标检测器估计左心室轮廓，然后推断左心室长轴和基底水平来放置扫描线。在此基础上构建WiseLVAM框架，在AMM模式下自动执行左心室线性测量。

Result: WiseLVAM框架结合了B模式图像的结构感知和AMM模式的运动感知，增强了测量的鲁棒性和准确性。

Conclusion: WiseLVAM有潜力为常规临床应用提供实用的解决方案，实现了全自动化但可手动调整的左心室测量。

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [113] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU是一个结合频域表示和量子检索增强生成的医疗视觉问答模型，在VQA-RAD数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI中需要同时理解图像和文本的复杂临床问题，现有方法在处理噪声和复杂推理方面存在不足

Method: 使用快速傅里叶变换将图像和文本特征转换到频域过滤噪声，结合量子检索系统从外部源获取医学知识，并将检索结果与频域特征融合

Result: 在VQA-RAD数据集上超越了先前模型，特别是在需要图像-文本推理的复杂病例上表现突出

Conclusion: 频域和量子信息的结合提高了模型性能和可解释性，为构建智能、清晰、有用的医疗AI工具提供了有前景的方法

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [114] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG是一个基于视频检索增强的运动生成框架，通过从大规模视频数据库中检索相关2D人体运动信号来解决运动大语言模型的数据不足问题，显著提升了仅文本输入条件下的3D运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 运动大语言模型由于标注数据有限，面临严重的领域外/词汇外问题，需要利用大规模野外视频数据库来增强3D运动生成能力。

Method: 设计了Gemini Motion Video Retriever机制和Motion-centric Dual-alignment DPO Trainer，分别解决视频检索的有效性和错误传播问题。

Result: 实验结果表明，VimoRAG显著提升了仅文本输入条件下运动大语言模型的性能。

Conclusion: 视频检索增强方法有效解决了运动生成中的数据瓶颈问题，为运动大语言模型提供了新的增强途径。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [115] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 提出了一个自动化目标检测模型评估框架AutoEval，通过预测一致性和可靠性(PCR)指标，无需人工标注即可准确估计检测性能


<details>
  <summary>Details</summary>
Motivation: 当前目标检测器评估依赖昂贵的人工标注，需要开发无需真实标签的自动化评估方法

Method: 提出PCR指标，利用NMS前后的候选框空间一致性和置信度可靠性来评估检测性能，并构建包含不同严重程度图像损坏的元数据集

Result: PCR比现有AutoEval方法提供更准确的性能估计，构建的元数据集覆盖更广的性能范围

Conclusion: 该框架为自动化目标检测评估提供了有效解决方案，减少了人工标注成本

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [116] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD是一个基于扩散模型的通用事件边界检测方法，通过生成式方法处理事件边界的主观多样性问题，在Kinetics-GEBD和TAPOS基准上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法只做确定性预测，忽略了事件边界检测中存在的多种合理解决方案的主观多样性问题。

Method: 提出扩散模型DiffGEBD，通过时间自相似性编码相邻帧的相关变化，然后以编码特征为条件迭代地将随机噪声解码为合理的事件边界，使用分类器自由引导控制多样性。

Result: 在Kinetics-GEBD和TAPOS两个标准基准上实现了强劲性能，能够生成多样且合理的事件边界。

Conclusion: 扩散模型为处理事件边界检测的主观多样性问题提供了有效解决方案，新提出的评估指标能够同时考虑多样性和保真度。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [117] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: 提出基于多阶段卷积神经网络的激光扫描仪3D点精度不确定性降低方法，通过高精度扫描仪作为参考来校正低精度设备的系统误差，在粗糙室内环境中显著提升测量精度。


<details>
  <summary>Details</summary>
Motivation: 由于设备限制和环境因素，高端和低端激光扫描仪存在位置误差，需要开发方法来提高低端设备的测量精度，为高精度几何模型创建和改造提供更准确的空间测量。

Method: 使用高精度扫描仪作为参考，在相同环境中配对测量低精度扫描仪数据，建立测量差异与空间分布的统计关系，结合传统几何处理和神经网络细化，将系统误差量化转化为监督学习问题。

Result: 在粗糙室内房间数据集上，均方误差降低超过70%，峰值信噪比提升约6分贝，使低端设备无需硬件改造即可接近高端设备的测量不确定性水平。

Conclusion: 该方法成功将系统误差校正转化为监督学习问题，显著提升了低端激光扫描仪的测量精度，为低成本高精度空间测量提供了有效解决方案。

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [118] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 这篇论文研究了源于模型的迭代去噪过程中的量化错误传播问题，并提出了一种时间步感知的累积错误补偿方案，以提升低精度源于模型的性能。


<details>
  <summary>Details</summary>
Motivation: 源于模型在图像合成领域形成了新的质量标杆，但其迭代去噪过程计算开销较大，后训练量化(PTQ)可以加速采样过程，但迭代性质导致步进式量化错误累积，影响输出保真度。

Method: 建立理论框架数学地推导源于模型中的错误传播方程，求解累积错误的闭形解，并基于此提出一种时间步感知的累积错误补偿方案。

Result: 在多个图像数据集上的实验结果显示，该补偿策略有效减轻了错误传播，显著提升了现有PTQ方法的性能，在低精度源于模型上达到了最先进水平。

Conclusion: 通过理论分析错误传播机制并提出相应的补偿方案，可以有效解决源于模型量化中的累积错误问题，为源于模型的大规模部署提供了有效的加速解决方案。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [119] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med是一个针对医学3D CT扫描和放射报告的视觉语言预训练框架，通过创新的预训练目标和架构设计，在有限数据（仅38,875对扫描-报告）下实现优异的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域特别是3D CT等体积模态数据的配对数据收集困难且耗时，限制了视觉语言模型在下游任务的表现，需要开发针对有限数据的有效预训练方法。

Method: 1）将单模态自监督学习整合到VLP框架中；2）提出TriBERT语言编码器学习多层次文本语义；3）设计分层对比学习捕获多层次视觉-语言对应关系。

Result: 所学习的编码器展现出强大的迁移能力，在3D分割、跨模态检索、视觉问答和报告生成等多种下游任务中达到最先进性能。

Conclusion: VELVET-Med证明了通过精心设计的预训练目标和架构，即使在有限数据条件下也能有效挖掘医学体积图像和临床叙述中的丰富空间和语义关系，提升编码器的泛化能力。

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [120] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3是一个端到端的多模态大语言模型框架，通过动态视觉工具交互和监督微调，在交错视觉语言推理任务上表现出色，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉语言任务上表现良好，但在多模态场景下的长链思维推理能力尚未充分探索，需要开发更强大的交错视觉语言推理能力。

Method: 提出了Simple o3框架，集成了动态工具交互（裁剪、缩放、重用），通过监督微调实现交错视觉语言推理。开发了可扩展的数据合成流程，通过"观察-推理-行动"循环生成高质量推理链，并创建了TWI-Tools-146K数据集。

Result: 实验结果显示Simple o3在多个基准测试中表现优异，超越了现有方法。通过引入额外的视觉token进行交错推理，重用和放大原始图像显著提升了模型的视觉推理和细粒度感知能力，基于精确视觉定位的图像裁剪使模型能有效关注关键实体或区域。

Conclusion: Simple o3建立了一个强大且计算成本可控的多模态推理范式，首次深入分析了不同交错推理策略对模型性能的影响，为推进多模态推理提供了重要见解。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [121] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit是一种混合虚拟试穿方法，通过两阶段流程解决现有方法无法保留服装细节的问题，第一阶段通过流场对齐服装，第二阶段通过保真试穿模块合成最终结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的免扭曲虚拟试穿方法虽然改善了感知质量，但无法保留服装的精细细节（如logo和印刷文字），这些细节对品牌完整性和客户信任至关重要。

Method: 两阶段方法：第一阶段使用学习到的流场将目标服装与人像对齐；第二阶段通过保真试穿模块合成最终输出，引入保留区域输入和修复掩码来指导过程。

Result: 广泛的定性结果显示DualFit实现了视觉无缝的试穿效果，同时忠实地保持了高频服装细节，在重建准确性和感知真实性之间取得了有效平衡。

Conclusion: DualFil通过混合方法成功解决了虚拟试穿中服装细节保留的问题，为在线时尚零售提供了更可靠的解决方案。

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [122] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef是一个三层次量化感知防御框架，通过特征不对齐惩罚和梯度感知失调惩罚来破坏补丁式对抗攻击在量化神经网络中的可迁移性，在未见过的量化和补丁组合上攻击成功率降低40%以上。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络(QNNs)虽然对传统像素级攻击具有鲁棒性，但对局部高显著性扰动的补丁式对抗攻击仍然存在跨比特宽度的可迁移性漏洞，现有防御方法要么过拟合固定量化设置，要么无法解决这种跨比特泛化漏洞。

Method: TriQDef包含三个核心组件：1)特征不对齐惩罚(FDP)通过惩罚中间表示的感知相似性来强制语义不一致；2)梯度感知失调惩罚(GPDP)通过边缘IoU和HOG余弦度量最小化结构性和方向性一致性来显式错位不同比特宽度的输入梯度；3)联合量化感知训练协议，在多个量化级别上通过共享权重训练方案统一这些惩罚。

Result: 在CIFAR-10和ImageNet上的广泛实验表明，TriQDef在未见过的补丁和量化组合上将攻击成功率(ASR)降低了40%以上，同时保持了高清洁准确率。

Conclusion: 研究结果表明，破坏语义和感知梯度对齐对于缓解QNNs中补丁可迁移性至关重要，TriQDef框架有效解决了跨比特泛化漏洞问题。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [123] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: 提出一种针对视觉语言模型的微调方法，在保持预训练模型多模态能力的同时，有效提升细粒度开放集视觉检索性能


<details>
  <summary>Details</summary>
Motivation: 大规模对比预训练的视觉语言模型在细粒度开放集检索任务中表现不佳，需要领域特定微调，但传统微调会导致灾难性遗忘，丧失模型的通用视觉和跨模态能力

Method: 受持续学习启发，系统分析标准正则化技术并提出高效组合策略，同时关注验证集设计和超参数调优以确保可复现性和泛化能力

Result: 在细粒度和粗粒度图像-图像、图像-文本检索基准测试中取得优异结果，无需使用文本数据或原始文本编码器即可保持视觉-文本对齐

Conclusion: 该方法成功实现了细粒度领域适应与保持预训练VLM广泛多模态知识之间的最佳平衡，为视觉语言模型的领域适应提供了有效解决方案

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [124] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: KP-INR是一种用于心脏电影MRI重建的双分支隐式神经表示方法，通过在k空间同时处理坐标位置嵌入和局部多尺度特征表示，实现了更好的重建性能


<details>
  <summary>Details</summary>
Motivation: 现有INR方法主要关注基于坐标的位置嵌入，但忽略了目标点及其邻域上下文特征表示的重要性，导致重建质量受限

Method: 提出KP-INR双分支网络：一个分支处理k空间坐标的位置嵌入，另一个分支学习局部多尺度k空间特征表示，通过跨分支交互和近似目标k空间值

Result: 在CMRxRecon2024数据集上的实验证实，KP-INR相比基线模型具有更好的性能表现

Conclusion: KP-INR方法在心脏电影MRI重建领域展现出良好潜力，通过结合坐标位置信息和局部特征表示实现了更高质量的重建效果

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [125] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 提出了FB-Mem方法，通过分割技术量化扩散模型中图像局部区域的记忆化现象，发现记忆化比现有认知更普遍，现有缓解方法效果有限


<details>
  <summary>Details</summary>
Motivation: 当前检测方法只能识别完全相同的记忆化，无法量化小图像区域的局部记忆化，也无法捕捉超越特定提示-图像对的记忆化模式

Method: 提出前景背景记忆化(FB-Mem)度量方法，基于分割技术对生成图像中的记忆化区域进行分类和量化分析

Result: 发现记忆化现象比之前理解的更普遍：(1)单个提示的生成可能与多个相似训练图像簇相关；(2)现有缓解方法无法消除局部记忆化，特别是在前景区域

Conclusion: 建立了有效的扩散模型记忆化测量框架，证明了当前缓解方法的不足，并提出基于聚类方法的更强缓解方案

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [126] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk是一个新颖的情感说话头生成框架，通过VAE生成3D面部标志点，结合情感标签嵌入和NeRF技术，实现了高情感准确性、增强的情感可控性和鲁棒的身份保持。


<details>
  <summary>Details</summary>
Motivation: 当前方法在唇同步和图像质量方面表现出色，但在生成准确可控的情感表情同时保持主体身份方面存在不足，需要解决情感表达与身份保持的平衡问题。

Method: 使用变分自编码器(VAE)从驱动音频生成3D面部标志点，通过ResNet-based标志点变形模型(LDM)将情感标签嵌入与标志点拼接，生成情感标志点。结合面部混合形状系数，通过新颖的三平面注意力神经辐射场(NeRF)合成高真实感的情感说话头。

Result: 大量实验表明，RealTalk在情感准确性、可控性和身份保持方面优于现有方法，推动了社交智能AI系统的发展。

Conclusion: RealTalk框架成功解决了情感说话头生成中的关键挑战，为构建更自然、情感丰富的虚拟人物交互提供了有效解决方案。

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [127] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse是一个基于提示的RF信号仿真框架，通过语言引导的4D世界生成器和相位相干射线追踪模拟器，从生成的室内场景和人体运动中模拟真实的射频信号。


<details>
  <summary>Details</summary>
Motivation: 解决在动态多样的室内环境中收集高质量RF数据的挑战，提供隐私保护的感知替代方案。

Method: 使用语言引导的4D世界生成器（包含状态感知因果变换器用于人体运动生成）和相位相干射线追踪模拟器来模拟准确的RF信号。

Result: 实验证明了在条件人体运动生成方面的有效性，相位相干性成功应用于波束成形和呼吸监测，在ML高分辨率成像和人体活动识别中实现性能提升。

Conclusion: WaveVerse首次实现了RF成像数据生成，在数据有限和数据充足场景下都能获得一致的性能增益，为RF感知提供了有效的仿真解决方案。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [128] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: 本文提出了一种统一的、与核和特征无关的特征提升方法，通过稀疏线性逆问题求解，为3D表示提供高质量的特征描述符，在开放词汇3D分割任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 特征提升在3D场景理解中至关重要，但多视角图像的不一致性给3D基元分配丰富通用属性带来了挑战。需要一种能够有效解决不一致性问题并提供高质量提升特征的方法。

Method: 将特征提升问题统一表述为稀疏线性逆问题，可通过闭式解高效求解。引入两种正则化策略：Tikhonov指导通过软对角优势确保数值稳定性，后提升聚合通过特征聚类过滤噪声输入。

Result: 在开放词汇3D分割基准测试中达到最先进性能，超越了基于训练、分组和启发式前向的基线方法，且能在几分钟内生成提升特征。

Conclusion: 该方法提供了一种理论保证的全局最优误差上界，能够有效处理多视角观测中的不一致性和噪声，为3D场景理解提供了高质量的特征提升解决方案。

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [129] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: 基于深度学习的YOLOv11优化模型，用于棉花病害检测，通过C2PSA模块、动态类别加权和改进数据增强技术，显著提升了小目标检测精度和田间性能。


<details>
  <summary>Details</summary>
Motivation: 解决棉花病害检测中的三个关键挑战：早期斑点检测精度低（5mm²以下斑点漏检率35%）、田间条件下性能下降（准确率下降25%）、多病害场景错误率高（34.7%）。

Method: 采用C2PSA模块增强小目标特征提取，动态类别加权处理样本不平衡，Mosaic-MixUp缩放改进数据增强，基于YOLOv11框架进行优化。

Result: 在4,078张图像数据集上测试：mAP50达到0.820（提升8.0%），mAP50-95达到0.705（提升10.5%），推理速度158 FPS，移动端部署实现实时监测。

Conclusion: 提出的优化方法有效解决了棉花病害检测的关键问题，实现了高精度、高效率的实时监测系统，为农业精准防治提供了可行方案。

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [130] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 提出了一种结合生成网络和物理信息神经模拟的生成式神经物理框架，用于快速、高保真度的3D超声计算机断层扫描，解决了传统射线重建方法在强散射条件下对肌肉骨骼成像的限制。


<details>
  <summary>Details</summary>
Motivation: 传统基于射线的超声计算机断层扫描重建方法在肌肉骨骼成像中存在局限性，无法有效处理强散射问题，限制了该技术在临床中的应用。

Method: 开发了一个生成式神经物理框架，通过从少量跨模态图像中学习超声波传播的紧凑代理模型，将波动建模的准确性与深度学习的效率和稳定性相结合。

Result: 在合成和体内数据（乳腺、手臂、腿部）上，能够在10分钟内重建组织参数的3D映射图，对肌肉和骨骼的生物力学特性具有敏感性，分辨率与MRI相当。

Conclusion: 该方法克服了强散射状态下的计算瓶颈，将超声计算机断层扫描推向肌肉骨骼疾病的常规临床评估应用。

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [131] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: 本文提出了一个包含14,945张RGB图像的新颖天气扩展显著目标检测数据集WXSOD，并设计了Weather-aware Feature Aggregation Network (WFANet) 基准方法，在复杂天气噪声环境下显著提升了显著目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测方法在自然场景中表现良好，但缺乏针对天气噪声影响的研究，主要原因是缺少带有像素级标注的天气噪声数据集。

Method: 提出完全监督的双分支架构WFANet：天气预测分支挖掘天气相关深度特征，显著检测分支将主干网络提取的语义特征与天气特征融合进行显著目标检测。

Result: 在WXSOD数据集上与17种SOD方法进行综合比较，WFANet取得了优越的性能表现。

Conclusion: WXSOD数据集填补了天气噪声显著目标检测研究的数据空白，WFANet方法为复杂天气环境下的显著目标检测提供了有效的基准解决方案。

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [132] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: SCTR框架通过超像素分割和连续低秩张量表示，解决了传统LRTR方法对整体低秩假设和离散网格数据的限制，在多个数据集上实现了3-5 dB的PSNR提升。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量表示方法存在两个关键局限：1）假设整体数据低秩，但现实数据往往存在显著空间变化；2）仅限于离散网格数据，灵活性不足。

Method: 提出超像素引导的连续低秩张量表示框架：1）使用超像素作为基本建模单元，利用语义一致区域更强的低秩特性；2）提出非对称低秩张量分解，通过共享神经网络参数化超像素特定因子矩阵，分离全局模式学习和局部适应。

Result: 在多个基准数据集（多光谱图像、视频、彩色图像）上的实验表明，SCTR相比现有LRTR方法实现了3-5 dB的PSNR提升。

Conclusion: SCTR框架通过超像素分割和连续建模，有效克服了传统方法的局限性，在保持模型效率的同时提高了表达能力和适应性。

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [133] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 该论文提出了区域级上下文感知多模态理解(RCMU)任务，通过区域级上下文感知视觉指令调优(RCVIT)方法增强MLLMs能力，构建了RCMU数据集和RC&P-Bench基准，开发了RC-Qwen2-VL模型并在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs研究主要关注通用视觉理解，忽视了整合对象相关文本上下文的能力，需要开发区域级上下文感知的多模态理解能力。

Method: 提出RCVIT方法，将对象信息整合到模型输入中，使用边界框坐标关联视觉内容和文本信息；构建RCMU大规模数据集和RC&P-Bench评估基准；开发RC-Qwen2-VL模型。

Result: RC-Qwen2-VL模型在多个RCMU任务上表现优异，在多模态RAG和个性化对话中成功应用，证明了方法的有效性。

Conclusion: 该研究填补了MLLMs在区域级上下文感知理解方面的空白，提出的方法、数据集和基准为多模态理解领域提供了重要贡献，具有实际应用价值。

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [134] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 提出SNNSIR，一种完全脉冲驱动的脉冲神经网络，用于立体图像恢复，通过脉冲残差基本块、立体卷积调制和交叉注意力模块，在保持低功耗的同时实现竞争性恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合SNN-ANN模型仍依赖浮点运算，与SNN的二进制事件驱动特性不兼容。需要开发完全脉冲驱动的架构来实现低功耗和硬件友好的立体图像恢复。

Method: 1. 轻量级脉冲残差基本块(SRBB)增强信息流；2. 脉冲立体卷积调制(SSCM)模块通过元素乘法引入简化非线性；3. 脉冲立体交叉注意力(SSCA)模块实现双向特征交互

Result: 在雨纹去除、雨滴去除、低光增强和超分辨率等多个立体图像恢复任务上取得竞争性性能，同时显著降低计算开销

Conclusion: SNNSIR展示了在实时低功耗立体视觉应用中的潜力，为完全脉冲驱动的立体图像恢复提供了有效解决方案

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [135] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对自动驾驶平台的语义分割网络动态自适应方法，通过三层控制机制和贝叶斯优化来实现硬件约束下的性能优化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶平台面临多样化的驾驶场景和硬件资源限制，需要在嵌入式设备上考虑计算成本，根据计算能力和特定场景定制语义分割网络。

Method: 采用三层控制机制（宽度乘数、分类器深度、分类器核）实现细粒度模型控制，结合贝叶斯优化和代理模型在有限计算预算下高效探索超参数空间。

Result: 实现了任务特定学习适应（TSLA），能够根据不同的自动驾驶任务生成定制化配置，最大化计算能力和模型精度。

Conclusion: 该方法能够有效优化硬件利用率，为自动驾驶平台提供场景特定和任务特定的语义分割解决方案。

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [136] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出CLAIR方法，针对大型基础模型生成噪声伪标签的弱监督零样本跨域图像检索问题，通过置信度评分、对比学习和跨域映射函数提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型能够轻松为大量未标注数据生成伪标签，使得无监督零样本跨域图像检索变得不那么相关，因此转向研究带有噪声伪标签的弱监督零样本跨域图像检索问题。

Method: 提出CLAIR方法：1）使用CLIP文本和图像特征相似度计算置信度来精炼噪声伪标签；2）设计实例间和簇间对比损失编码类感知潜在空间；3）使用域间对比损失缓解域差异；4）学习闭式跨域映射函数；5）引入可学习提示增强零样本泛化能力。

Result: 在TUBerlin、Sketchy、Quickdraw和DomainNet等零样本数据集上的大量实验表明，CLAIR方法相比现有最先进方法始终表现出优越性能。

Conclusion: CLAIR方法通过多层次的对比学习和跨域映射，有效解决了弱监督零样本跨域图像检索中的噪声标签和域差异问题，显著提升了检索性能。

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [137] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 本文对3D高斯泼溅的致密化策略进行全面改进，通过边缘感知评分、长轴分割和抗过拟合技术，在减少高斯点数量的同时提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅的致密化策略往往导致重建质量不理想，需要从何时致密化、如何致密化以及如何缓解过拟合三个角度进行系统性改进。

Method: 提出边缘感知评分选择候选高斯点，采用长轴分割策略减少几何失真，并设计恢复感知剪枝、多步更新和生长控制等抗过拟合技术。

Result: 方法在不增加训练或推理开销的情况下提升了渲染保真度，用更少的高斯点实现了最先进的性能。

Conclusion: 该工作通过系统性的致密化流程改进，显著提升了3D高斯泼溅的重建质量和渲染效率。

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [138] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 提出基于神经细胞自动机(NCA)的弱监督分割方法NCA-WSS，利用NCA分类过程中的特征图生成分割掩码，无需分割标签重新训练，在三个白细胞显微镜数据集上显著优于现有弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 白细胞检测和分割是医学诊断的关键步骤，但训练稳健准确的模型需要大量标注数据，获取成本高且耗时。

Method: 使用神经细胞自动机(NCA)进行弱监督分割，通过NCA分类过程中生成的特征图提取分割掩码，无需额外的分割标签训练。

Result: 在三个白细胞显微镜数据集上的评估显示，NCA-WSS方法显著优于现有的弱监督分割方法。

Conclusion: NCA在弱监督框架下同时具备分类和分割潜力，为医学图像分析提供了可扩展且高效的解决方案。

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [139] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 该论文提出将注意力池化机制与神经细胞自动机(NCA)结合，用于显微镜图像分类，在8个数据集上显著提升了性能，同时保持了参数效率和可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 神经细胞自动机(NCA)在图像分类中具有鲁棒性和可解释性，但在性能上与大型复杂架构存在差距，需要提升其在显微镜图像分析中的分类准确性。

Method: 集成注意力池化机制到NCA中，通过注意力机制聚焦信息最丰富的区域，增强特征提取能力，提高分类精度。

Result: 在8个不同的显微镜图像数据集上评估，该方法显著优于现有NCA方法，与传统轻量级CNN和Vision Transformer架构相比，在保持参数数量显著更低的同时实现了更好的性能。

Conclusion: 基于NCA的模型具有作为可解释图像分类替代方案的潜力，注意力池化的集成有效提升了NCA的性能表现。

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [140] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive是一种基于多普勒效应的雷达点云时域聚合方法，通过径向位移补偿和动态聚合时长来增强点云密度，减少散射，提升目标检测性能


<details>
  <summary>Details</summary>
Motivation: 雷达在自动驾驶中具有长距离检测优势，但远距离点云稀疏性影响检测精度。现有时域聚合方法会因动态物体引入散射噪声，降低检测性能

Method: 提出DoppDrive方法：1）根据多普勒动态分量对历史帧点云进行径向位移补偿消除径向散射；2）基于多普勒和角度为每个点分配独特聚合时长来减少切向散射

Result: DoppDrive作为检测前预处理步骤，与各种检测器兼容，在多个数据集上显著提升了目标检测性能

Conclusion: 该方法有效解决了雷达点云稀疏性问题，通过多普勒驱动的时域聚合在增强点云密度的同时最小化散射，为雷达目标检测提供了通用有效的解决方案

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [141] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 提出了一种基于几何感知学习的框架，用于从单视角RGB视频中联合移除头戴显示器遮挡并重建完整3D面部几何


<details>
  <summary>Details</summary>
Motivation: 头戴显示器会遮挡用户上半部分面部，影响外部视频录制和社交XR应用中的面部表情和眼神交流体验

Method: 集成GAN-based视频修复网络（通过密集面部标志点和无遮挡参考帧指导）和SynergyNet-based模块回归3DMM参数，结合密集标志点优化

Result: 能够成功从RGB面部视频中移除HMD，保持面部身份和真实感，生成逼真的3D面部几何输出，在不同标志点密度下保持鲁棒性

Conclusion: 该框架有效解决了HMD遮挡问题，为社交XR应用提供了高质量的面部重建解决方案

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [142] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 提出SDD检测器，通过重建学习在细粒度视觉层面对齐伪造空间和语义概念空间，利用预训练视觉语言模型的概念知识，在标准图像伪造数据集上取得优异效果


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，需要强大的伪造检测来确保数字媒体的可信度。现有方法中伪造空间与语义概念空间的不对齐问题阻碍了检测性能

Method: 提出语义差异感知检测器(SDD)：1）语义标记采样模块缓解空间偏移；2）基于视觉重建的概念级伪造差异学习模块；3）低级伪造特征增强器整合学习到的差异

Result: 在两个标准图像伪造数据集上的实验证明SDD的有效性，相比现有方法取得了更优越的结果

Conclusion: SDD通过重建学习和语义概念对齐，有效解决了伪造检测中的空间不对齐问题，为图像伪造检测提供了新的解决方案

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [143] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat是一个即插即用的任务驱动特征增强模块，专门针对水下环境中的目标检测问题，通过多尺度特征增强网络提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 水下环境的严重图像退化损害了目标检测模型的性能，传统的图像增强方法通常没有针对此类下游任务进行优化。

Method: 提出AquaFeat模块，集成多尺度特征增强网络，与检测器的损失函数进行端到端训练，确保增强过程明确指导以优化与检测任务最相关的特征。

Result: 在YOLOv8m上集成AquaFeat后，在挑战性水下数据集上达到最先进的精确度(0.877)和召回率(0.624)，以及具有竞争力的mAP分数(mAP@0.5为0.677，mAP@[0.5:0.95]为0.421)，处理速度为46.5 FPS。

Conclusion: 该模型在保持实用处理速度的同时提供准确性提升，为海洋生态系统监测和基础设施检查等实际应用提供了有效且计算效率高的解决方案。

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [144] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: 提出了MBMamba网络，通过内存缓冲机制和Ising启发的正则化损失来改进Mamba架构在图像去模糊中的性能，解决了局部像素遗忘和通道冗余问题。


<details>
  <summary>Details</summary>
Motivation: Mamba架构在图像去模糊中存在局部像素遗忘和通道冗余问题，现有方法通过修改扫描策略会增加计算复杂度，影响实时性能。

Method: 设计内存缓冲机制保存历史信息用于后续融合，引入Ising启发的正则化损失模拟像素间"相互吸引"的能量最小化，保持图像结构和连贯性。

Result: 在广泛使用的基准测试中优于最先进的方法。

Conclusion: MBMamba网络在不改变原始Mamba架构的情况下，通过内存缓冲和正则化损失有效提升了图像去模糊性能。

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [145] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了EgoLoc方法，用于在自我中心视频中零样本定位手-物体接触和分离的关键时刻，解决了现有方法依赖类别标注和物体掩码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互行为的建模（如何交互），但对手与目标物体接触和分离的关键时刻定位（何时交互）这一更精细的问题研究不足，这对混合现实和机器人运动规划至关重要。

Method: 提出EgoLoc方法，采用手部动力学引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行细化，无需物体掩码和动词-名词分类法。

Result: 在公共数据集和新基准测试上的综合实验表明，EgoLoc能够实现可信的时间交互定位，并能有效促进自我中心视觉和机器人操作任务中的多个下游应用。

Conclusion: EgoLoc提供了一种零样本实现的时间交互定位方法，消除了对物体掩码和类别标注的依赖，具有更好的泛化能力。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [146] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 提出一种通过数据增强和扩散模型生成合成数据的方法，提升视觉离线强化学习在未见环境中的泛化能力


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在视觉数据上面临泛化挑战，由于训练数据多样性不足导致过拟合和性能下降

Method: 两阶段方法：先对原始离线数据进行增强增加多样性，再用扩散模型在潜在空间生成额外合成数据

Result: 在连续动作空间(Visual D4RL)和离散动作空间(Procgen)上都显著改善了泛化性能，减少了泛化差距

Conclusion: 该方法能有效提升视觉离线RL的泛化能力，且无需修改现有模型架构，为合成数据训练通用智能体提供了新方向

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [147] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: IPGPhormer是一种可解释的病理图-Transformer框架，用于癌症生存分析，在预测准确性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡长程空间关系建模与局部上下文依赖，且缺乏内在可解释性，限制了临床实用性。

Method: 提出Interpretable Pathology Graph-Transformer (IPGPhormer)，捕获肿瘤微环境特征并建模其空间依赖性，无需后处理人工标注即可在组织和细胞层面提供可解释性。

Result: 在四个公共基准数据集上的综合评估表明，IPGPhormer在预测准确性和可解释性方面均优于最先进方法。

Conclusion: IPGPhormer为癌症预后评估提供了一个有前景的工具，为病理学中更可靠和可解释的决策支持系统铺平了道路。

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [148] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: 提出ViT-EnsembleAttack方法，通过对ViT模型进行对抗性增强来提升集成攻击的迁移性，在ViT上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有集成攻击研究主要关注集成权重优化或集成路径，忽略了通过增强集成模型本身来提升对抗迁移性，且ViT集成攻击研究较少

Method: 对每个ViT代理模型使用三种策略进行对抗性增强：多头丢弃、注意力分数缩放和MLP特征混合，参数通过贝叶斯优化，并引入自动重加权和步长放大模块

Result: 在ViT上的对抗迁移性显著提升，大幅超越现有方法

Conclusion: 模型对抗性增强能有效提升集成攻击的迁移性，ViT-EnsembleAttack为ViT集成攻击提供了有效解决方案

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [149] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT是一个通过大语言模型分解复杂文本指令来提升文本到图像生成模型性能的新框架，在LongBench-T2I基准测试中显著改善了图像生成质量


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂长文本指令时表现不佳，经常无法准确渲染细节、空间关系和特定约束，需要更好的方法来理解复杂指令

Method: DeCoT采用两阶段框架：1）复杂指令分解和语义增强，使用LLM将原始指令分解为结构化语义单元；2）多阶段提示集成和自适应生成，将语义单元转换为适合T2I模型的层次化或优化提示

Result: 在LongBench-T2I数据集上，DeCoT显著提升了主流T2I模型的性能，特别是在"文本"和"构图"等挑战性维度。与Infinity-8B集成时平均得分从3.44提升到3.52

Conclusion: DeCoT有效弥合了用户意图与T2I模型需求之间的差距，实现了更忠实和准确的图像生成，消融研究证实了各组件的重要性

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [150] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP是一个联邦学习框架，利用CLIP模型的多尺度视觉特征和客户端特定风格统计，生成上下文感知的提示词，提升跨模态模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅使用最终层特征，忽略了客户端数据中的多尺度视觉线索和领域特定风格变化，限制了联邦学习中提示学习的效果。

Method: 提取CLIP视觉编码器的低、中、高层特征，结合客户端批量统计的风格指标，融合视觉细节与文本上下文生成鲁棒的提示词token，在联邦学习框架下进行本地训练和全局聚合。

Result: 在多个图像分类数据集上的实验表明，FedCSAP在准确率和整体泛化能力上优于现有的联邦提示学习方法。

Conclusion: 该方法有效处理非独立同分布类别分布和多样化领域风格，通过多尺度特征融合和风格感知机制显著提升了联邦提示学习的性能。

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [151] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: MPCAR是一种无需微调LVLM参数的推理时策略，通过多角度生成描述、智能整合上下文和深度推理三阶段，显著提升大视觉语言模型在复杂视觉推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在复杂视觉推理任务中存在局限性，单次图像编码和提示无法充分捕捉细微视觉信息，需要多角度上下文增强来提升性能。

Method: 三阶段方法：1）LVLM从多角度生成N个互补描述；2）智能整合描述与原问题构建上下文增强提示；3）使用增强提示进行深度推理生成最终答案。

Result: 在GQA、VQA-CP v2、ScienceQA等挑战性VQA数据集上 consistently超越基线方法，准确率显著提升，特别是在需要强上下文理解的任务上。人类评估也确认了答案连贯性和完整性的改善。

Conclusion: MPCAR通过利用LVLM固有生成能力丰富输入上下文，有效释放了其在复杂多模态任务中的潜在推理能力，无需模型参数微调即可实现性能提升。

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [152] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD是一个针对自动驾驶的新型视觉语言框架，通过引入初步场景交互和专家适配器，显著提升了现有视觉语言模型在驾驶推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在车载多视角图像和场景推理文本上微调视觉语言模型，但缺乏自动驾驶所需的整体细致场景识别和强大空间感知能力，特别是在复杂情况下。

Method: 提出LMAD框架，模拟现代端到端驾驶范式，结合全面场景理解和任务专用结构。引入初步场景交互和专用专家适配器，在相同驾驶任务结构中更好地对齐视觉语言模型与自动驾驶场景。

Result: 在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有视觉语言模型在驾驶推理任务上的性能。

Conclusion: LMAD为可解释自动驾驶设立了新标准，完全兼容现有视觉语言模型，并能无缝集成到规划导向的驾驶系统中。

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [153] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: S5是首个面向遥感半监督语义分割的可扩展框架，通过数据选择策略构建RS4P-1M数据集，预训练不同规模的遥感基础模型，并采用MoE多数据集微调方法，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督语义分割研究依赖小规模数据集和模型，限制了实际应用。遥感领域存在大量未标注的地球观测数据未被充分利用，需要开发可扩展框架来挖掘这些数据的潜力。

Method: 提出S5框架：1）集成基于熵的过滤和多样性扩展的数据选择策略，构建RS4P-1M数据集；2）预训练不同规模的遥感基础模型；3）采用基于混合专家(MoE)的多数据集微调方法，实现高效适应多个遥感基准测试。

Result: 构建的RS4P-1M数据集和预训练的遥感基础模型在土地覆盖分割和目标检测任务上性能显著提升。最终模型在所有基准测试中都达到了最先进的性能水平。

Conclusion: S5框架证明了扩展半监督学习在遥感应用中的可行性，通过有效利用大规模未标注数据，显著提升了遥感基础模型的泛化能力和多功能性，为遥感分析提供了可扩展的解决方案。

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [154] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: SRMA-Mamba是一个基于Mamba架构的新网络，用于肝脏MRI体积数据的3D病理分割，通过整合空间解剖信息和多平面信息，在肝硬化组织分割方面超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 肝硬化早期检测对降低死亡率至关重要，但现有方法未能充分利用MRI体积数据中的空间解剖细节，限制了临床效果和可解释性。

Method: 提出SRMA-Mamba网络，包含SABMamba模块在肝硬化组织内进行选择性Mamba扫描，整合矢状面、冠状面和轴状面的解剖信息构建全局空间上下文表示，并使用SRMA模块逐步细化分割图中的肝硬化细节。

Result: 大量实验表明SRMA-Mamba在3D病理肝脏分割方面超越了最先进方法，表现出卓越性能。

Conclusion: 该方法通过有效建模MRI体积数据中的空间解剖关系，显著提高了肝硬化组织分割的准确性和临床实用性。

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [155] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN是一个文本到动态全景场景生成的先进框架，通过结合全景视频生成和动态场景重建技术，能够创建几何一致、运动丰富的360度沉浸式4D场景。


<details>
  <summary>Details</summary>
Motivation: 随着VR/AR技术的快速发展，对高质量沉浸式动态场景的需求日益增长，但现有方法主要关注静态场景或窄视角动态场景，无法提供真正的360度全视角沉浸体验。

Method: 采用双分支生成模型（全景分支和透视分支）进行视频生成，通过双向交叉注意力机制实现信息交换；基于3D高斯泼溅的几何对齐重建模型，利用度量深度图对齐时空点云并初始化场景相机位姿。

Result: 大量实验证明了所提出设计的有效性，TiP4GEN在生成视觉吸引人且运动连贯的动态全景场景方面表现出优越性。

Conclusion: TiP4GEN框架成功解决了动态全景场景生成的挑战，为创建真正沉浸式的360度虚拟环境提供了有效的解决方案。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [156] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: 本文通过比较生物和人工视觉系统对视觉错觉的反应，揭示了AI视觉系统与人类感知的关键差异，包括AI特有的像素级敏感性和幻觉现象，为开发更稳健、可解释的AI视觉系统提供见解


<details>
  <summary>Details</summary>
Motivation: 随着人工智能视觉系统越来越多地执行类人任务，需要了解AI是否也会经历视觉错觉，以及是否存在AI特有的错觉现象，这对于开发更可靠、人类对齐的AI系统至关重要

Method: 通过系统比较人类和AI对经典视觉错觉（涉及颜色、大小、形状和运动）的反应，分析AI模型中的错觉样效应是如何通过针对性训练或模式识别的副产品出现的

Result: 研究发现AI会出现一些类似错觉的效应，同时也发现了AI特有的错觉现象（如像素级敏感性和幻觉），这些在人类感知中没有对应物，揭示了人类感知与AI感知之间的对齐差距和AI特有的感知脆弱性

Conclusion: 这些发现为未来视觉系统的研究提供了重要见解，有助于开发既保留对人类有益的感知偏见，又避免损害信任和安全性的扭曲的AI视觉系统

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [157] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 该研究发现现有VQA-NLE系统存在解释不一致和缺乏真正理解的问题，提出了图像扰动攻击策略和基于外部知识的缓解方法，揭示了当前系统的安全性和可靠性隐患。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答自然语言解释系统存在黑盒模型透明度不足的问题，系统可能产生不一致的解释并在不理解上下文的情况下得出结论，暴露了推理流程或解释生成机制的弱点。

Method: 利用现有的对抗性问题扰动策略，并提出新颖的图像最小化扰动策略来诱导矛盾或虚假输出，同时引入基于外部知识的缓解方法来增强模型鲁棒性。

Result: 在两个标准基准测试和两个广泛使用的VQA-NLE模型上的广泛评估证明了攻击方法的有效性，以及基于知识的防御方法的潜力。

Conclusion: 研究揭示了当前VQA-NLE系统存在的紧迫安全和可靠性问题，知识驱动的防御方法显示出缓解这些不一致性的潜力，但系统整体仍需改进以提高鲁棒性和透明度。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [158] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT是一个基于视觉语言大模型的新型框架，通过模拟放射科医生的思维链过程，实现胸部X光片的智能诊断和可解释报告生成，在保持竞争性诊断准确率的同时提供透明化的推理过程。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗影像诊断中的黑盒问题，提高临床可接受度，通过模拟人类放射科医生的推理过程来提供可解释的诊断报告。

Method: 采用多模态特征和视觉概念提取，结合基于LLM的组件和结构化思维链提示策略，进行推理并生成详细自然语言诊断报告。

Result: 在CORDA数据集上达到80.52%的平衡准确率和78.65%的F1分数，略优于现有黑盒模型，并能生成高质量的可解释报告。

Conclusion: 该研究代表了医疗影像AI系统向可信赖和临床可操作方向迈出的重要一步，多模态融合和思维链推理对于构建稳健透明的医疗AI至关重要。

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [159] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA提出了一种新的多模态学习方法，无需对齐预训练，通过将文本嵌入映射到视觉表示空间而非传统视觉到文本的映射，在transformer中间层进行融合，显著降低了计算需求并提升了推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 挑战传统多模态学习需要昂贵对齐预训练的基本假设，探索更高效的多模态融合方式，避免大规模图像-文本对齐数据集的需求。

Method: 提出逆向映射方法：将文本嵌入映射到连续视觉表示空间，在transformer中间层通过选择性加性注意力组件进行动态融合，无需对齐预训练。

Result: 在9个多模态基准测试中显示差异化性能：推理和认知任务显著提升（MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, 认知推理: +27.2%），感知任务性能下降（名人识别: -49.5%, OCR: -21.3%），计算需求降低45%。

Conclusion: 首次实证证明对齐预训练对有效多模态学习并非必要，特别是复杂推理任务；建立了一种减少计算需求、挑战传统模态融合观念的新范式，为保留模态特定特征的高效多模态架构开辟了新方向。

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [160] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 开发了一个结合微调视觉语言模型(VLM)联盟和推理大语言模型(LLM)的决策支持系统，用于自动化H反射肌电图波形分析和诊断，提高神经肌肉评估的准确性和标准化。


<details>
  <summary>Details</summary>
Motivation: 传统H反射肌电图波形分析存在变异性和解释偏差问题，限制了可靠性和标准化，需要自动化解决方案来提高神经肌肉反射评估的准确性。

Method: 使用多个经过精心标注的H反射EMG波形图像数据集微调的VLM模型，提取电生理特征并预测神经肌肉状态，然后通过基于共识的方法聚合诊断输出，并由专门的推理LLM进行精炼。

Result: 实验结果表明该混合系统能够提供高度准确、一致且可解释的H反射评估，显著推进了神经肌肉诊断的自动化和标准化。

Conclusion: 这是首个将微调VLM联盟与推理LLM集成用于基于图像的H反射分析的工作，为下一代AI辅助神经肌肉评估和运动员监测平台奠定了基础。

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [161] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: 该研究提出了一种结合CNN-Transformer混合架构和卷积Kolmogorov-Arnold网络(CKAN)的皮肤癌分类方法，在多个数据集上取得了优异的分类性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类在医学图像分析中至关重要，需要精确区分恶性和非恶性病变以实现早期诊断和治疗。传统方法在特征提取和融合方面存在局限。

Method: 采用顺序和并行混合CNN-Transformer模型，结合卷积Kolmogorov-Arnold网络(CKAN)进行非线性特征融合。CNN提取局部空间特征，Transformer建模全局依赖关系，CKAN通过可学习激活函数增强特征融合能力。使用迁移学习和大量数据增强技术。

Result: 在三个基准数据集上取得优异性能：HAM10000数据集准确率92.81%、F1分数92.47%；PAD-UFES数据集准确率97.83%、F1分数97.83%；BCN20000数据集准确率91.17%、F1分数91.79%。

Conclusion: 混合CNN-Transformer架构能有效捕捉空间和上下文特征，CKAN的集成通过可学习激活函数增强了特征融合能力，证明了该方法在不同数据集上的有效性和泛化能力，强调了特征表示和模型设计在医学图像分类中的重要性。

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [162] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR是一个负责任的人工智能系统，用于糖尿病视网膜病变筛查，在准确性和公平性方面显著优于FDA批准的EyeArt系统


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是工作年龄人群视力丧失的主要原因，早期检测可降低95%的视力丧失风险，但眼科医生短缺和及时检查困难阻碍了检测。AI模型虽有潜力，但数据质量差和偏见问题限制了临床应用

Method: 开发RAIS-DR系统，在整个AI生命周期中整合伦理原则，包括高效的卷积模型进行预处理、质量评估和三个专门的DR分类模型

Result: 在1,046名患者的本地数据集上评估，RAIS-DR相比EyeArt系统F1分数提高5-12%，准确率提高6-19%，特异性提高10-20%。公平性指标显示在不同人口统计亚组中表现公平

Conclusion: RAIS-DR是一个稳健且符合伦理的DR筛查解决方案，有潜力减少医疗保健差距，代码和权重已开源

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [163] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: LangVision-LoRA-NAS是一个结合神经架构搜索(NAS)和LoRA的新框架，通过动态搜索最优LoRA秩配置来优化视觉语言模型，在提升性能的同时降低微调成本。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA实现使用固定秩，限制了在不同多模态任务中的灵活性和效率，需要一种能够动态适应不同任务需求的方法。

Method: 将神经架构搜索(NAS)与LoRA结合，动态搜索针对特定多模态任务的最优LoRA秩配置，平衡性能与计算效率。

Result: 在LLaMA-3.2-11B模型上的大量实验显示，该方法显著提升了模型性能，同时降低了微调成本。

Conclusion: LangVision-LoRA-NAS框架为视觉语言模型提供了更灵活高效的微调方案，通过动态秩配置优化实现了性能与效率的平衡。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [164] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: 使用交叉视图变换器(CVT)将相机图像映射到鸟瞰图(BEV)的三个通道：道路、车道标记和规划轨迹，在自动驾驶感知中表现出良好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 鸟瞰图(BEV)为自动驾驶感知提供了结构化的俯视抽象表示，但如何从相机图像有效映射到BEV地图是一个关键挑战

Method: 使用交叉视图变换器(CVT)架构，通过城市驾驶模拟器生成训练数据，比较不同相机布局和损失函数（focal loss和L1 loss）的效果

Result: 在仅使用一个城镇数据训练的情况下，采用L1损失的四相机CVT模型在新城镇测试中表现出最鲁棒的性能，能够生成相当准确的BEV地图

Conclusion: 交叉视图变换器在将相机输入映射到BEV地图方面具有很大潜力，L1损失函数和四相机布局的组合效果最佳

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [165] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo是一种基于协同训练的多模态个性化表情识别方法，通过选择相关源主体并利用多模态互补信息进行主体特异性适应，在BioVid和StressID数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MSDA方法往往忽视多模态信息或将多个源混合为单一域，限制了主体多样性，无法显式捕捉独特的主体特异性特征。

Method: 基于协同训练的多模态主体特异性选择和适应方法，选择与目标相关的源主体，使用主导模态生成伪标签进行类感知学习，并结合类无关损失从低置信度目标样本中学习。

Result: 在BioVid和StressID这两个具有挑战性的多模态表情识别数据集上，MuSACo能够优于UDA（混合）和最先进的MSDA方法。

Conclusion: MuSACo通过有效利用多模态信息和主体特异性适应，为数字健康中的情感计算应用提供了更准确和鲁棒的个性化表情识别解决方案。

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [166] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL框架利用视觉语言模型进行图像伪造检测，通过整体场景评估和区域异常检测两种方法，在多个领域数据集上展现优异性能


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展使得视觉伪造检测和解释变得更具挑战性，现有方法在跨领域泛化方面存在困难，需要既能检测伪造又能提供推理和定位的鲁棒框架

Method: 将伪造检测构建为提示驱动的视觉推理任务，利用大型视觉语言模型的语义对齐能力。提出REVEAL框架，包含两种方法：(1)整体场景级评估-基于物理、语义、透视和真实性的整体分析；(2)区域异常检测-将图像分割成多个区域分别分析

Result: 在多个领域数据集（Photoshop、DeepFake和AIGC编辑）上进行实验，与竞争基线比较，并分析模型提供的推理能力

Conclusion: 基于视觉语言模型的提示驱动方法在跨领域图像伪造检测方面表现出良好的泛化能力和推理解释性

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [167] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: SFAC是一种基于CNN的老照片着色算法，仅需两张图像进行训练，通过特征分布对齐和结构保持机制解决领域差异问题


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在旧照片着色任务中面临缺乏真实标签和自然灰度图像与旧照片之间存在领域差异的挑战

Method: 提出SFAC算法，使用特征分布对齐损失建立语义对应关系，并通过特征级感知约束和像素级冻结更新金字塔结构保持机制来避免结构失真

Result: 大量实验证明该方法在老照片着色方面的有效性，定性和定量指标均得到验证

Conclusion: SFAC算法成功解决了老照片着色的领域差异问题，无需大数据依赖，仅需两张图像即可实现高质量的着色效果

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [168] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: 提出了USDRL框架，首个骨架基础模型，通过多粒度特征解相关和多视角一致性训练，在9个骨架动作理解任务的25个基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作理解方法缺乏可扩展性和泛化能力，没有能够适应广泛动作理解任务的骨架基础模型

Method: USDRL框架包含：1）基于Transformer的密集时空编码器（DSTE）学习时空特征；2）多粒度特征解相关（MG-FD）减少维度冗余；3）多视角一致性训练（MPCT）进行自监督学习

Result: 在25个基准测试的9个骨架动作理解任务中显著超越当前最先进方法，涵盖粗粒度预测、密集预测和迁移预测

Conclusion: 该工作拓宽了骨架动作理解的研究范围，鼓励更多关注密集预测任务，为骨架基础模型的发展奠定了基础

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [169] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 提出MCOUT方法，在联合潜在空间中进行连续向量推理，替代传统的语言链式思维，显著提升多模态推理性能


<details>
  <summary>Details</summary>
Motivation: 传统语言模型推理方法（如CoT）在多模态场景中难以动态对齐音频、视觉和文本信息，需要探索新的推理范式

Method: 开发两种变体：MCOUT-Base重用语言模型隐藏状态进行迭代推理；MCOUT-Multi集成多模态潜在注意力增强跨模态对齐

Result: 在MMMU、ScienceQA和MMStar基准测试中准确率提升最高8.23%，BLEU分数提升最高8.27%

Conclusion: 潜在连续推理是超越语言绑定CoT的有前景方向，为类人反射式多模态推理提供了可扩展框架

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [170] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一个基于扩散模型的新型端到端自动驾驶框架，通过并行生成驾驶决策序列显著降低延迟，支持双向推理，在nuScenes数据集上表现优于现有自回归VLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的自回归架构存在推理延迟高、无法进行双向推理的问题，不适合动态的安全关键环境。

Method: 采用掩码扩散模型实现并行生成整个驾驶决策序列，支持双向推理和渐进式简单优先生成策略。

Result: 在nuScenes数据集上，ViLaD在规划准确性和推理速度上都优于最先进的自回归VLM基线，接近零失败率，并在真实自动驾驶车辆上验证了实用性。

Conclusion: ViLaD代表了端到端自动驾驶的范式转变，通过扩散模型架构解决了自回归方法的局限性，为实际应用提供了可行的解决方案。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [171] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 该研究构建了首个大规模UGC图像视觉失真评估指令调优数据集ViDA-UGC，包含11K图像和细粒度质量标注，通过CoT框架提升多模态大语言模型的图像质量分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前可解释图像质量评估方法存在两个主要问题：对用户生成内容(UGC)和AI生成内容(AIGC)使用相同的失真标准进行评估，以及缺乏详细的图像质量分析和恢复指导。

Method: 通过失真导向的流水线构建ViDA-UGC数据集，包含人工标注和思维链(CoT)评估框架，指导GPT-4o生成质量描述。同时创建ViDA-UGC-Bench基准测试集，包含476张图像和6,149个问答对。

Result: 实验结果表明，ViDA-UGC和CoT框架能持续增强多种基础MLLM在图像质量分析方面的能力，在ViDA-UGC-Bench和Q-Bench基准测试中甚至超越了GPT-4o。

Conclusion: 该研究为UGC图像质量评估提供了首个大规模数据集和评估基准，通过CoT框架有效提升了MLLM的图像质量分析能力，为图像质量监控和恢复指导提供了有力工具。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [172] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 提出了OpenMoCap模型和CMU-Occlu数据集，解决了光学动作捕捉中大规模标记遮挡问题，通过射线追踪模拟真实遮挡模式，采用标记-关节链推理机制，在遮挡环境下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 光学动作捕捉在虚拟现实和电影制作中很重要，但现实应用中大规模标记遮挡严重降低系统性能。现有模型缺乏反映真实遮挡模式的训练数据和捕获标记间长距离依赖的训练策略。

Method: 1) 创建CMU-Occlu数据集，使用射线追踪技术模拟真实标记遮挡模式；2) 提出OpenMoCap模型，采用标记-关节链推理机制，同时优化和构建标记与关节间的深度约束。

Result: 大量对比实验表明，OpenMoCap在不同场景下始终优于竞争方法，CMU-Occlu数据集为鲁棒动作求解的未来研究打开了大门。

Conclusion: OpenMoCap成功解决了光学动作捕捉中的遮挡问题，模型已集成到MoSen动作捕捉系统中实际部署，代码已开源。

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [173] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES是一种基于小波的通用视觉基元表示方法，通过小波的空间-频率局部化优势有效捕捉低频和高频信息，并提供快速渲染。


<details>
  <summary>Details</summary>
Motivation: 现有视觉表示方法依赖频率引导或复杂神经网络解码，导致频谱损失或渲染速度慢，需要一种既能保持高质量又能快速渲染的表示方法。

Method: 基于小波的空间-频率局部化优势构建视觉基元，开发基于小波的可微分光栅化器实现快速视觉渲染。

Result: 在2D图像表示、5D静态和6D动态新视角合成等视觉任务中，WIPES相比基于INR的方法具有更高渲染质量和更快推理速度，在渲染质量上优于基于高斯的方法。

Conclusion: WIPES作为一种视觉基元表示方法，在保持高质量渲染的同时实现了快速推理，为多维度视觉信号表示提供了有效的解决方案。

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [174] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: 本文提出了首个可解释的创意评估与选择范式，基于多模态大语言模型(MLLMs)构建Creative4U系统，通过推理到选择训练方法实现精准的创意图像评估和选择。


<details>
  <summary>Details</summary>
Motivation: 电商平台创意图像对用户体验和广告收入至关重要，但现有方法主要关注创意排名，无法满足可解释创意选择的需求。AIGC技术让广告主能低成本生产大量创意图像，但缺乏质量评估能力。

Method: 构建CreativePair数据集(8k标注图像对)，开发Creative4U创意选择器，采用Reason-to-Select RFT训练方法(包括CoT-SFT监督微调和GRPO强化学习)，整合用户兴趣进行多模态创意评估。

Result: 离线和在线实验证明了方法的有效性，系统能够准确评估和选择创意图像。

Conclusion: 该研究为创意评估和选择提供了首个可解释范式，代码和数据集将公开以推动研究和工业应用。

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [175] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: 提出了SpotVLM框架，通过上下文转移范式利用延迟但准确的大模型输出指导小模型实时推理，解决了云边协作中云延迟波动问题


<details>
  <summary>Details</summary>
Motivation: 现有云边协作方法无法适应云延迟波动，且未能充分利用延迟但准确的大模型响应，需要新的协作范式来提升实时视觉语言应用的性能

Method: 提出上下文转移范式，将大模型的延迟输出作为历史上下文指导小模型推理；设计SpotVLM框架，包含上下文替换和视觉聚焦模块来优化文本输入和增强视觉一致性

Result: 在三个实时视觉任务和四个数据集上的广泛实验证明了该框架的有效性

Conclusion: 该新范式为未来VLM系统中更有效和延迟感知的协作策略奠定了基础

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [176] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: 提出两阶段PMRF管道，从非对比MRI合成对比增强脑MRI，避免使用钆对比剂


<details>
  <summary>Details</summary>
Motivation: 传统对比增强MRI需要钆基对比剂，增加成本、扫描时间、环境问题，并可能对患者造成风险

Method: 两阶段方法：1) 基于3D U-Net预测后验均值；2) 时间条件3D整流流细化，加入真实纹理同时保持结构保真度

Result: 在360个测试样本上，最佳输出达到轴向FID 12.46和KID 0.007（比后验均值降低68.7%），体积MSE为0.057（比后验均值高27%）

Conclusion: 该方法能真实恢复病灶边缘和血管细节，有效平衡感知-失真权衡，适合临床部署

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [177] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: 该论文提出了BEE方法，通过多级一致性正则化和互补锚点回放机制，解决了持续测试时自适应中探索与利用的平衡问题，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的CTTA方法存在两个主要挑战：1）深层预测调整对浅层特征域偏移效率低下，导致探索缓慢；2）单一模型在探索过程中会遗忘先前域的知识，无法利用历史知识处理相似未来域。

Method: 提出基于均值教师框架的BEE方法：1）使用多级一致性正则化(MCR)损失对齐师生模型的中间特征，加速当前域适应；2）采用互补锚点回放(CAR)机制重用历史检查点，恢复不同域的互补知识。

Result: 实验表明，该方法在多个基准测试中显著优于最先进的方法，证明了其在CTTA任务中的有效性。

Conclusion: BEE框架通过MCR和CAR机制有效解决了CTTA中探索与利用的平衡问题，为持续变化的测试域适应提供了有效解决方案。

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [178] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd是首个从大场景视频中重建数百人3D姿态、位置和形状的时空一致性框架，采用粗到细的群体引导运动优化策略，结合VAE运动先验和异步运动一致性损失，有效解决遮挡和时序不一致问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法从静态图像重建3D人群缺乏时序一致性，无法有效处理遮挡问题。大场景动态人群重建在城市场景监控和人群分析中日益重要。

Method: 提出粗到细的群体引导运动优化策略，结合VAE运动先验和分段群体引导优化。利用异步运动一致性损失(AMC)，让高质量无遮挡运动段指导遮挡段的运动恢复。

Result: 实验结果表明该方法在大场景动态人群重建任务中达到最先进性能。贡献了虚拟基准数据集VirtualCrowd用于评估。

Conclusion: DyCrowd框架能够鲁棒地重建大场景中数百人的3D运动，有效处理遮挡和时序不一致问题，为动态人群重建提供了新解决方案。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [179] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 该论文提出了一种两阶段的人体去遮挡方法，首先生成完整的遮挡掩码，然后基于掩码指导进行RGB外观重建，结合人体先验知识和文本特征，显著提升了遮挡人体区域的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 人类能够利用先验知识和可见线索推断被遮挡物体的缺失部分，但让深度学习模型准确预测被遮挡区域仍然具有挑战性。现有基于扩散的去遮挡方法在潜在空间转换时会导致可见区域的像素级退化。

Method: 两阶段方法：1) 掩码补全阶段使用基于扩散的人体先验和遮挡关节点热图提供身体结构表示；2) RGB补全阶段使用重建的掩码作为条件输入，结合VQA模型提取的人体特定文本特征和CLIP编码，使用Stable Diffusion进行RGB生成，并通过解码器微调缓解像素退化问题。

Result: 该方法即使在严重遮挡情况下也能有效重建人体外观，在掩码和RGB补全方面均优于现有方法。生成的去遮挡图像能够提升下游人体中心任务的性能，如2D姿态估计和3D人体重建。

Conclusion: 提出的两阶段人体去遮挡方法通过结合结构先验和外观生成，解决了现有方法的局限性，为人体遮挡恢复提供了有效解决方案，并有助于提升相关计算机视觉任务的性能。

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [180] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 构建了涵盖8个维度46个数据源的视觉推理数据集，提出基于影响函数的数据选择和难度过滤策略，通过多轮RL训练Vision-G1模型，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的训练主要集中在数学和逻辑推理等有限任务上，缺乏跨领域的泛化能力，且多领域数据整合困难

Method: 使用影响函数进行数据选择和基于难度的过滤策略，构建多领域视觉推理数据集，采用多轮强化学习配合数据课程进行模型训练

Result: Vision-G1模型在各种视觉推理基准测试中表现优异，超越同类规模VLMs甚至GPT-4o和Gemini-1.5 Flash等专有模型

Conclusion: 通过构建全面的多领域数据集和创新的训练策略，成功提升了视觉语言模型的跨领域推理能力，为通用视觉推理系统的发展提供了重要贡献

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [181] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 该研究探索使用CLIP视觉语言模型来预测Wölfflin的五项艺术风格原则，发现预训练CLIP无法直接捕捉这些细微风格特征，通过微调后提出的WP-CLIP模型能够有效预测艺术作品的风格原则。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法有效预测Wölfflin的五项艺术风格原则，而视觉语言模型在评估抽象图像属性方面显示出潜力，需要开发能够自动分析艺术风格的度量标准。

Method: 在标注的真实艺术图像数据集上微调预训练的CLIP模型，使其能够预测每项Wölfflin原则的评分，提出WP-CLIP模型。

Result: WP-CLIP在GAN生成的绘画和Pandora-18K艺术数据集上表现出良好的泛化能力，能够准确预测不同艺术风格的Wölfflin原则。

Conclusion: 视觉语言模型在自动化艺术分析方面具有巨大潜力，微调后的CLIP模型能够有效理解和预测复杂的艺术风格特征。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [182] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV是一个用于多无人机协同3D检测的创新框架，通过自适应实例感知的BEV表示学习，在保持低分辨率BEV输入的同时实现了优越的精度-计算权衡。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同3D检测虽然能通过融合多视角观测提供准确鲁棒的感知，但在资源受限的无人机平台上计算面临挑战。现有方法平等对待所有BEV网格，效率不高。

Method: 提出refine-and-contrast范式：1) Box-Guided Refinement Module (BG-RM) - 仅使用2D监督和空间细分精炼前景实例相关的BEV网格；2) Instance-Background Contrastive Learning (IBCL) - 在BEV空间通过对比学习增强前景和背景特征的分离

Result: 在Air-Co-Pred数据集上的广泛实验表明，AdaBEV在不同模型规模下都实现了优越的精度-计算权衡，在低分辨率下优于其他最先进方法，接近上限性能

Conclusion: AdaBEV通过自适应实例感知的BEV表示学习，有效解决了多无人机3D检测中的计算效率问题，在保持低计算开销的同时实现了高性能

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [183] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME方法通过源域数据增强、域判别器和专门域检测器来处理动态域偏移，特别是在驾驶场景中的天气变化，通过多检测器集成和NMS提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应(TTA)在动态变化目标域中的挑战，特别是在真实驾驶场景中频繁发生的天气域偏移问题。

Method: 利用源域数据增强到目标域，引入域判别器和专门域检测器来缓解剧烈域偏移，训练多个检测器并通过非极大值抑制(NMS)整合预测结果。

Result: 在SHIFT基准测试上显示出显著性能提升，验证了方法的有效性。

Conclusion: 提出的TTA-DAME方法能够有效处理动态域偏移问题，特别是在驾驶场景中的天气变化，通过多检测器集成策略实现了更好的适应性。

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [184] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了多级知识蒸馏和动态自监督损失两个组件，用于解决重复类增量学习问题，在CVPR CLVISION挑战赛中获得了第二名。


<details>
  <summary>Details</summary>
Motivation: 传统的类增量学习假设每个任务都包含未见过的类别，而重复类增量学习(CIR)更现实地考虑了先前训练过的类别会在未来任务中重复出现。CIR场景下可以轻松从互联网等外部来源获取大量未标记数据。

Method: 1. 多级知识蒸馏(MLKD)：从多个先前模型的多角度（包括特征和logits）提取知识，使模型能够保持各种先前的知识
2. 动态自监督损失(SSL)：利用未标记数据加速新类别的学习，同时通过动态权重保持训练对主要任务的关注

Result: 所提出的两个组件显著提高了CIR设置下的性能，在CVPR第五届CLVISION挑战赛中获得了第二名

Conclusion: 该方法通过有效利用未标记数据，在重复类增量学习场景中实现了高稳定性和可塑性，为解决现实世界中的增量学习问题提供了有效方案

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [185] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: 本文研究了自动驾驶车辆不同相机传感器配置导致的跨传感器域差距问题，提出了CamShift数据集和基于神经渲染的数据驱动传感器适配方法，显著提升了3D目标检测器在不同传感器配置下的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆因车型限制而具有不同的相机传感器配置，导致在一个传感器配置上训练的感知模型在其他配置上性能下降，即跨传感器域差距问题。

Method: 创建CamShift数据集模拟紧凑型车和SUV之间的传感器差异；提出基于神经渲染的数据驱动传感器适配管道，可将整个数据集转换以匹配不同相机传感器配置；比较不同3D目标检测架构的鲁棒性。

Result: 发现基于密集BEV表示和后向投影的模型架构（如BEVFormer）对传感器变化最鲁棒；提出的传感器适配方法显著提升了所有测试3D目标检测器的性能，大幅减少了跨传感器域差距。

Conclusion: 通过数据驱动的传感器适配方法可以有效缓解跨传感器域差距问题，提高数据在不同传感器配置车辆间的重用性，减少新数据收集需求。

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [186] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI驱动的新闻多样性导致多级漂移，显著降低现有LVLM多模态虚假信息检测系统的鲁棒性，平均F1分数下降14.8%


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具带来的新闻内容高度多样化对多模态虚假信息检测构成新挑战，需要系统研究其对检测系统的影响

Method: 构建DriftBench大规模基准测试集（16,000个新闻实例，6类多样化），设计三个评估任务：真实性验证鲁棒性、对抗性证据污染敏感性、推理一致性分析

Result: 六个最先进LVLM检测器性能显著下降，推理轨迹不稳定，在对抗性证据注入下表现更差

Conclusion: 现有MMD系统存在根本性漏洞，在GenAI时代迫切需要更具弹性的方法

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [187] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: 基于CNN深度学习的手语实时翻译系统，通过摄像头捕捉手势并转换为文本和语音输出，帮助听障人士改善沟通


<details>
  <summary>Details</summary>
Motivation: 解决听障人士在日常环境中的沟通障碍问题，通过技术手段促进他们更好地融入社会生活

Method: 使用卷积神经网络(CNN)在Sign Language MNIST数据集上训练，实时通过摄像头捕捉手势并进行分类识别，结合文本转语音技术

Result: 实验显示系统具有高准确度和实时性能（存在一定延迟），证明了其作为辅助工具的实用性和可靠性

Conclusion: 该系统为手语使用者提供了一个易用、可靠的无障碍沟通工具，有效提升了他们的自主性和社会融入度

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [188] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: Dual Contrastive Denoising Score框架利用文本到图像扩散模型的生成先验，通过双对比损失实现真实图像编辑，既能灵活修改内容又能保持结构一致性


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成模型在真实图像编辑中的两个挑战：用户难以准确描述图像细节的文本提示，以及编辑时对不需要区域产生意外改变的问题

Method: 在潜在扩散模型的自注意力层中间表示中引入简单的双对比损失，利用丰富的空间信息，无需依赖辅助网络

Result: 实验表明该方法在真实图像编辑方面优于现有方法，能够实现零样本图像到图像转换，同时保持直接使用预训练模型的能力

Conclusion: 提出的框架成功解决了真实图像编辑中的关键问题，实现了内容修改灵活性和结构保持的平衡，无需额外训练即可利用现有扩散模型

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [189] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5在多模态空间智能方面取得显著进展但仍未达到人类水平，研究通过8个基准测试和超过10亿token的成本评估发现，尽管GPT-5表现最强，但在复杂空间推理任务中仍存在明显短板，且专有模型在最难题上并无决定性优势。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在空间理解和推理方面仍存在明显局限，这是实现通用人工智能的关键能力。随着GPT-5的发布，需要系统评估当前最先进模型在空间智能方面的真实水平。

Method: 提出统一的空间任务分类法，在8个关键基准测试上评估最先进的专有和开源模型，总成本超过10亿token，并进行定性评估分析。

Result: GPT-5展现出前所未有的空间智能强度，但仍未达到人类在各种任务上的表现；识别出多模态模型更具挑战性的空间智能问题；专有模型在最难题上并无明显优势。

Conclusion: 尽管GPT-5在空间智能方面取得重大进步，但多模态模型在空间理解和推理方面仍存在根本性局限，需要进一步突破才能达到人类水平的空间智能。

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [190] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 3D高斯泼溅在稀疏视角下存在外观伪影问题，研究发现这是由于高斯分布过度纠缠导致的共适应效应。提出了两种轻量级策略（随机高斯丢弃和透明度乘性噪声注入）来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在密集视角下表现优异，但在稀疏视角场景中，尽管训练视角渲染效果真实，新视角会出现外观伪影。本文旨在探究这些伪影的根源并找到解决方案。

Method: 提出了共适应评分(CA)指标来量化高斯分布间的纠缠程度。基于分析提出了两种策略：1)随机高斯丢弃；2)透明度乘性噪声注入。这两种方法都是即插即用的轻量级方案。

Result: 分析发现高斯分布的共适应程度会随着训练视角数量的增加而自然缓解。提出的两种策略在各种方法和基准测试中都验证了有效性。

Conclusion: 揭示了稀疏视角3D高斯泼溅中高斯分布共适应效应的核心问题，提出的轻量级解决方案能有效缓解外观伪影，为社区提供了对稀疏视角3DGS更全面的理解。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [191] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 提出FDIKP网络，通过频率域表示增强核估计的结构可识别性，采用双分支逆核预测策略和位置自适应卷积，在单图像散焦去模糊任务中取得优异性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖空间特征进行核估计，但在严重模糊区域由于局部高频细节缺失导致性能下降，需要利用频率域对模糊建模的优越判别能力

Method: 频率驱动的逆核预测网络(FDIKP)，包含双分支逆核预测策略(DIKP)、位置自适应卷积(PAC)和双域尺度循环模块(DSRM)，融合频率域信息提升核估计精度和去模糊质量

Result: 大量实验表明该方法优于现有方法，在单图像散焦去模糊任务中表现出色

Conclusion: 通过引入频率域表示和创新的网络架构设计，有效解决了严重模糊区域的核估计问题，提升了去模糊性能

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [192] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DCSCR的少样本图像集分类方法，结合传统方法和深度学习，同时学习帧级和概念级特征表示，通过类特定协作表示度量学习提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像集分类方法中特征学习不足和距离度量不灵活的问题，特别是在少样本场景下传统方法忽略特征学习，深度方法无法自适应调整特征的问题。

Method: 提出DCSCR网络，包含全卷积深度特征提取模块、全局特征学习模块和类特定协作表示度量学习模块，使用新的CSCR对比损失函数来学习概念级特征表示和集合间距离相似性。

Result: 在多个知名少样本图像集分类数据集上的大量实验表明，该方法相比现有最先进算法具有更好的性能。

Conclusion: DCSCR方法有效解决了图像集分类中的特征学习和距离度量问题，在少样本场景下表现出优越性能，为图像集分类提供了新的解决方案。

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [193] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: 提出基于Mamba的双尺度融合和双路径扫描网络，通过选择性传播上下文信息来有效去除阴影，在阴影去除基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 阴影去除需要利用非阴影区域的丰富信息进行指导，但阴影区域与光照良好区域的变换差异很大，需要有效整合非局部上下文线索和自适应建模区域特定变换

Method: 提出Dual-Scale Fusion Mamba Block (DFMB)融合原始特征和低分辨率特征增强多尺度表示，Dual-Path Mamba Group (DPMG)通过水平扫描捕获全局特征并采用掩码感知自适应扫描策略

Result: 在阴影去除基准测试中显著优于现有最先进方法

Conclusion: 该方法通过选择性上下文传播和自适应区域建模，有效解决了阴影去除中变换差异大的挑战

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [194] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA是一个基于深度学习的框架，用于在急性缺血性卒中机械取栓术中分类最小强度投影图像的关键属性，提升下游图像质量控制和分割性能。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉模型在机械取栓术中辅助应用时，图像质量差会严重影响性能表现，需要自动化工具来识别和分类图像属性以支持质量控制。

Method: 使用预训练的ResNet骨干网络进行微调，训练独立的分类器来预测9个图像属性（如对比度存在、投影角度、运动伪影严重程度等），基于1758个标注的荧光透视MinIPs数据集。

Result: 模型在所有标签上表现优异，ROC-AUC达到0.91-0.98，精确度0.70-1.00。在分割任务中，过滤低质量图像后分割成功率从42%提升至69%（p<0.001）。

Conclusion: CLAIRE-DSA作为自动化工具在急性缺血性卒中DSA序列中准确分类图像属性方面展现出强大潜力，支持临床和研究应用中的图像标注和质量控制。

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [195] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 本文针对CdZnTe半导体图像标注中低对比度缺陷边界和多视图共享同一真实标签的"多对一"关系问题，提出了基于组内一致性增强框架(ICAF)的半监督语义分割方法，通过视图增强和校正模块显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: CdZnTe半导体图像标注面临低对比度缺陷边界挑战，需要标注者交叉参考多个视图。这些视图共享同一真实标签，形成"多对一"关系，导致现有半监督语义分割方法(基于"一对一"关系)在低对比度区域出现误差积累和确认偏差问题。

Method: 提出组内一致性增强框架(ICAF)：1)通过组内视图采样(IVS)建立组导向基线；2)设计伪标签校正网络(PCN)，包含视图增强模块(VAM)动态合成边界感知视图，以及视图校正模块(VCM)进行信息交互以突出显著区域并减少噪声。

Result: 在CdZnTe数据集上，使用仅2%的组标注数据(千分之五)，以DeepLabV3+和ResNet-101为分割模型，达到了70.6%的mIoU，证明了方法的有效性。

Conclusion: ICAF框架成功解决了CdZnTe材料图像分割中的"多对一"关系挑战，通过组内一致性约束和视图合成校正机制，显著提升了低对比度区域的分割精度，为类似多视图共享标签的场景提供了有效解决方案。

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [196] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack是一个针对无人机视角下多目标跟踪的创新框架，通过多尺度特征增强、速度自适应卡尔曼滤波、群体运动补偿和时空记忆预测等技术，显著提升了复杂城市交通环境中小目标的跟踪精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机视角下的多目标跟踪在智能交通系统分析中具有重要价值，但复杂场景中的小目标尺度变化、遮挡、非线性交叉运动和运动模糊等问题严重影响了跟踪稳定性。

Method: 提出SocialTrack框架，包含：1）多尺度特征增强的小目标检测器；2）速度自适应容积卡尔曼滤波(VACKF)用于轨迹预测；3）群体运动补偿策略(GMCS)建模社会群体运动先验；4）时空记忆预测(STMP)利用历史轨迹信息预测未来状态。

Result: 在UAVDT和MOT17数据集上的实验表明，SocialTrack在多个关键指标上优于现有最先进方法，特别是在MOTA和IDF1等核心性能指标上有显著提升。

Conclusion: SocialTrack框架具有优异的鲁棒性和适应性，且高度模块化和兼容，可与现有跟踪器无缝集成以进一步提升性能。

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [197] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出了一种基于多风格图像的潜在扩散模型风格迁移方法，通过图像提示适配器和去噪过程中的统计特征对齐，解决了现有方法风格匹配不准确、风格图像数量受限和内容-风格纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型在图像风格迁移中存在三个主要问题：风格匹配不准确、可使用的风格图像数量有限、内容和风格不期望的纠缠。需要一种能够更好表征风格特征并防止风格图像内容泄露的方法。

Method: 利用多风格图像来更好表征风格特征，设计结合图像提示适配器和去噪过程中特征统计对齐的方法。在去噪UNet的交叉注意力和自注意力层进行干预，通过聚类从大量风格样本注意力值中提取小规模代表性注意力特征集进行统计对齐。

Result: 实验证明该方法在风格化任务中达到了最先进的性能表现。

Conclusion: 提出的多风格图像方法通过注意力特征聚类和统计对齐，有效解决了风格迁移中的关键问题，实现了state-of-the-art的风格化效果。

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [198] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: 使用深度学习分析谷歌街景图像来估计全球185个城市的自行车和摩托车使用水平，建立了预测模型并应用于60个缺乏传统数据的城市


<details>
  <summary>Details</summary>
Motivation: 交通方式影响健康，但全球范围内自行车和摩托车行为的比较数据稀缺，需要高效的数据收集方法

Method: 使用YOLOv4模型在谷歌街景图像中检测自行车和摩托车，建立beta回归模型预测城市级别的交通方式份额

Result: 摩托车检测与模式份额相关性0.78，自行车相关性0.51；预测模型R²分别为0.614和0.612，中位绝对误差1.3-1.4%

Conclusion: 计算机视觉结合街景图像能有效捕捉交通模式，为传统数据源提供补充，特别适用于缺乏调查数据的地区

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [199] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: 使用计算机视觉方法对食双星光变曲线进行分类，通过预训练的ResNet50和ViT模型在合成数据集上微调，采用极坐标+六边形可视化新图像表示方法提高泛化能力。二分类准确率>96%，但在自动黑子检测方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 开发自动化方法来处理大规模巡天数据中的食双星分类问题，特别是针对形态学分类和黑子特征检测，以应对传统手动分类方法效率低下的挑战。

Method: 使用预训练的卷积神经网络(ResNet50)和视觉变换器(ViT)模型，在合成数据集上微调。创新性地将相位折叠光变曲线转换为极坐标并结合六边形可视化来创建图像表示。采用分层方法：第一阶段分类分离和过接系统，第二阶段检测黑子存在与否。

Result: 在多个波段(Gaia G、I和TESS)的验证数据上获得高准确率(>96%)，在OGLE、DEBCat和WUMaCat观测数据测试中表现强劲(>94%，TESS达100%)。但自动黑子检测任务表现较差，显示模型在识别细微光度特征方面的局限性。

Conclusion: 计算机视觉在大规模巡天食双星形态分类中具有巨大潜力，但自动黑子检测需要进一步研究，突显了开发更鲁棒的自动化特征检测方法的必要性。

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [200] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出了一种新的图像生成方法NVG，通过将图像分解为结构化序列，从全局布局到细节逐步细化生成，在ImageNet上实现了优于VAR系列的FID分数。


<details>
  <summary>Details</summary>
Motivation: 传统图像生成方法缺乏对生成过程的多粒度控制，需要一种能够从粗到细、分层级生成图像的结构化框架。

Method: 将图像分解为结构化序列，每个序列元素具有相同空间分辨率但不同视觉粒度。采用Next Visual Granularity (NVG)生成框架，从空图像开始逐步细化生成视觉粒度序列。

Result: 在ImageNet数据集上训练的NVG模型显示出明显的扩展性，FID分数优于VAR系列（3.30->3.03, 2.57->2.44, 2.09->2.06）。

Conclusion: NVG框架提供了分层级的表示方法，实现了对生成过程的多粒度精细控制，展现出强大的能力和潜力。

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [201] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: 这篇论文介绍了CVPR 2025事件视觉研讨会中的时空实例分割挑战赛，包括任务定义、数据集、挑战细节和结果分析，并详细描述了前5名团队的方法。


<details>
  <summary>Details</summary>
Motivation: 随着事件相机技术的发展，需要开发能够处理时空对齐的事件相机和灰度相机数据的实例分割方法，以推动事件视觉领域的发展。

Method: 组织了一个挑战赛，要求参赛团队使用时空对齐的事件相机和灰度相机数据预测精确的像素级分割掩码，并收集了前5名团队的技术方案进行分析。

Result: 挑战赛成功举办，收集了多个团队的高质量解决方案，为事件视觉领域的实例分割技术发展提供了重要参考和基准。

Conclusion: 该挑战赛为事件相机数据的实例分割任务建立了标准基准，展示了当前最先进的技术水平，并为未来研究提供了宝贵资源和方法参考。

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [202] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA是一种基于深度学习的海底图像恢复模型，通过双频增强自注意力机制在空间和频率域中自适应优化特征表示，有效解决水下图像模糊和颜色失真问题。


<details>
  <summary>Details</summary>
Motivation: 水下环境的光线散射、吸收和浑浊度导致图像清晰度下降和颜色信息失真，严重影响海洋生物多样性监测、生态评估和自主探索的准确性。

Method: 提出DEEP-SEA模型，采用双频增强自注意力空间和频率调制器，同时处理低频和高频信息，保持空间结构完整性。

Result: 在EUVP和LSUI数据集上的实验表明，该方法在恢复精细图像细节和结构一致性方面优于现有最先进方法。

Conclusion: DEEP-SEA能有效缓解水下视觉退化问题，有望提高水下监测平台的可靠性，实现更准确的生态观测、物种识别和自主导航。

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [203] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: 本文提出了MMPDA框架，通过渐进式域适应方法解决多模态欺骗检测中的域转移问题，在MMDD挑战赛中取得第二名，在准确率和F1分数上均超越其他团队。


<details>
  <summary>Details</summary>
Motivation: 针对多模态欺骗检测中源域和目标域之间的域转移问题，需要开发有效的域适应方法来提升模型在目标域上的性能。

Method: 提出了多源多模态渐进式域适应（MMPDA）框架，通过在特征层和决策层逐步对齐源域和目标域，实现跨多模态数据集的域转移桥接。

Result: 在竞赛第二阶段达到60.43%的准确率和56.99%的F1分数，F1分数比第一名团队高5.59%，准确率比第三名团队高6.75%。

Conclusion: MMPDA框架有效解决了多模态欺骗检测中的域适应问题，在MMDD挑战赛中表现出色，证明了该方法在处理跨域多模态数据方面的有效性。

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [204] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出CoMuCo方法，通过多视图协作优化和一致性约束，提升视觉语言模型在跨域少样本任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自然图像上表现优异，但在跨域任务（如医学影像、遥感图像等）中效果有限，需要专门的方法来应对领域差异

Method: CoMuCo策略使用两个功能互补的专家模块提取多视图特征，结合先验知识一致性约束和信息几何共识机制，增强特征学习的鲁棒性

Result: 在现有和新提出的跨域少样本基准测试中，CoMuCo方法持续优于当前最优方法

Conclusion: 该方法有效解决了视觉语言模型在跨域少样本学习中的局限性，建立了新的跨域基准测试，为相关研究提供了重要参考

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [205] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: MPS-Tuning是一种新的视觉语言模型微调方法，通过保持语义流形的几何结构并增强类别可分性来提高少样本图像分类性能


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型微调方法往往忽略数据分布的几何结构，可能导致整体语义表示的扭曲，需要一种能够保持流形结构的方法

Method: 将特征空间中的数据分布视为语义流形，通过对齐微调前后的Gram矩阵来保持流形的宏观和微观拓扑结构，同时优化图像和文本模态的特征对相似性来增强类别可分性

Result: 大量实验表明MPS-Tuning显著提高了模型性能，同时有效保持了语义流形的结构

Conclusion: MPS-Tuning通过几何结构保持和流形雕刻的方法，为视觉语言模型的微调提供了有效的解决方案，在保持语义完整性的同时提升了分类性能

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [206] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S²-Guidance是一种新的扩散模型引导方法，通过随机块丢弃构建子网络来优化预测，解决了CFG方法中的次优预测问题，在文本到图像和文本到视频生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究发现Classifier-free Guidance (CFG)方法在扩散模型中会产生次优预测，导致语义不连贯和低质量输出，需要改进现有引导策略。

Method: 提出S²-Guidance方法，利用前向过程中的随机块丢弃构建随机子网络，引导模型远离低质量预测，朝向高质量输出。

Result: 在文本到图像和文本到视频生成任务上的大量实验表明，S²-Guidance性能优越， consistently超越CFG和其他先进引导策略。

Conclusion: S²-Guidance通过利用模型自身的子网络有效优化预测，解决了CFG的局限性，为扩散模型提供了更高质量的引导方法。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [207] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG是一种基于非负矩阵分解的一次性剪枝方法，通过梯度掩码机制在训练过程中严格保持目标稀疏度，在CIFAR数据集上实现了与现有方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络尺寸庞大导致部署困难，现有剪枝方法存在迭代过程复杂、需要专门标准或在训练中难以有效保持稀疏度的问题。

Method: 提出ONG方法：1）使用非负矩阵分解(NMF)一次性识别重要权重结构进行初始剪枝；2）采用精确的梯度掩码机制确保只有未剪枝权重被更新，严格保持目标稀疏度。

Result: 在CIFAR-10和CIFAR-100数据集上使用ResNet56、ResNet34和ResNet18进行测试，ONG在不同稀疏度水平下都能达到与现有稳定稀疏化方法相当或更好的性能，同时保持剪枝后的结构完整性。

Conclusion: ONG提供了一种有效的一次性剪枝解决方案，能够精确控制目标稀疏度，在保持性能的同时简化了剪枝过程。

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [208] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow是一个基于临床报告生成完整3D CT体积的0.5B潜在流匹配变换器模型，在时间一致性、图像多样性和文本-图像对齐方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 通过生成式建模实现数据增强、隐私保护合成和减少患者数据监管限制，同时保持诊断信号

Method: 使用A-VAE定义潜在空间，CT-Clip文本编码器编码临床报告，采用自定义自回归方法生成一致的整个CT体积

Result: 在FID、FVD、IS评分和CLIP评分方面优于最先进的生成CT模型

Conclusion: CTFlow展示了在临床报告条件下生成高质量3D CT体积的能力，为医学影像研究提供了新的可能性

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [209] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: CMF-IOU是一个多阶段跨模态融合的3D检测框架，通过伪点生成、双边跨视图增强主干网络和迭代体素点感知细化模块，有效融合相机和LiDAR信息，在多个数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 现有的多模态3D检测方法多采用单阶段或部分阶段融合，导致特征提取不足和性能不佳，需要解决3D空间信息与2D语义信息的对齐挑战

Method: 1) 通过深度补全网络将像素信息投影到3D空间生成伪点；2) 设计双边跨视图增强3D主干网络(S2D分支和ResVC分支)；3) 迭代体素点感知细化模块；4) IoU联合预测分支与新颖的候选框生成技术

Result: 在KITTI、nuScenes和Waymo数据集上的大量实验表明该方法具有优越性能

Conclusion: CMF-IOU框架通过多阶段跨模态融合有效解决了3D检测中的信息对齐问题，实现了优异的检测性能

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [210] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench是首个同时评估语义和空间对齐的布局引导文本到图像生成基准测试，包含7个挑战性场景，评估对象生成、颜色保真度、属性识别、对象间关系和空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估文本对齐，但忽略了布局对齐，无法全面评估模型的空间保真度，这在合成数据生成等应用中至关重要。

Method: 构建包含7个挑战性场景的文本-布局对数据集，提出结合布局对齐分数的评估协议来评估空间准确性。

Result: 使用7Bench评估了多个最先进的扩散模型，揭示了它们在不同对齐任务中的优势和局限性。

Conclusion: 7Bench填补了布局引导文本到图像生成评估的空白，为模型的空间保真度评估提供了重要工具，有助于提升合成数据质量。

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [211] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD是一个针对高分辨率图像异常检测的通用框架，通过双分支架构和多分辨率特征融合策略，在有限计算资源下有效检测不同大小的异常区域。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测方法主要针对低分辨率场景，高分辨率图像的下采样会导致细粒度信息丢失，现有方法在检测精度和效率上难以满足工业实际需求。

Method: 采用双分支架构整合多尺度异常线索，结合多分辨率特征融合策略处理细粒度纹理变化，使用检测器池和自适应分配策略平衡性能与计算成本。

Result: 在专门构建的高分辨率异常检测基准测试（MVTec-HD、VisA-HD、RealIAD-HD）上表现出优越性能。

Conclusion: HiAD框架有效解决了高分辨率图像异常检测的挑战，在保持检测性能的同时控制了计算成本，适用于工业实际应用。

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [212] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG是一个两阶段训练框架，通过提升编码器和解码器的泛化能力来解决增量学习中的灾难性遗忘问题，特别在小内存场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法通常只关注编码器或解码器中的一个组件，限制了缓解灾难性遗忘的效果，特别是在小内存场景下表现更差

Method: 采用两阶段训练：第一阶段通过特征增强训练集成编码器学习泛化表示，提升解码器泛化能力；第二阶段使用知识蒸馏策略压缩集成编码器，开发新的泛化编码器

Result: 在三个基准数据集上的广泛实验显示SEDEG具有优越性能，消融研究确认了各组件有效性

Conclusion: SEDEG通过顺序提升编码器和解码器的泛化能力，有效缓解了增量学习中的灾难性遗忘问题，特别在小内存场景下表现突出

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [213] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: 提出基于U-Net的自动化框架，用于猕猴示踪数据中的纤维束分割，通过大补丁尺寸、前景感知采样和半监督预训练，显著提升稀疏束检测能力并降低误报率


<details>
  <summary>Details</summary>
Motivation: 解决解剖示踪研究中手动标注纤维束的劳动密集型问题，现有自动化方法常遗漏稀疏束或需要复杂后处理，限制了大规模数据分析

Method: 采用U-Net架构，结合大补丁尺寸、前景感知采样策略和半监督预训练方法，实现单切片独立分析

Result: 相比现有最佳方法，稀疏束检测提升20%以上，假发现率降低40%，能准确区分终端和纤维束

Conclusion: 该框架将促进解剖示踪数据的大规模自动化分析，为验证和优化扩散MRI纤维束成像方法提供更多真实数据

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [214] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen是一个端到端的视频重光照框架，基于大规模视频生成模型，通过文本描述控制光照和背景，在保持前景一致性的同时实现和谐的视频重光照效果。


<details>
  <summary>Details</summary>
Motivation: 视频重光照是一个具有挑战性但有价值的任务，需要在替换视频背景的同时相应调整前景光照并保持和谐融合。现有方法缺乏高质量配对视频数据，且难以保持时间帧间的一致性和前景属性（如反照率）的保留。

Method: 构建混合真实和合成视频的大规模数据集；使用3D渲染引擎生成合成视频对，采用HDR光照模拟补充真实视频；设计联合训练课程，注入域感知适配器解耦重光照和域外观分布的学习。

Result: 实验结果表明，Lumen能够有效地将输入编辑为具有一致光照和严格前景保留的电影级重光照视频，在综合基准测试中优于现有方法。

Conclusion: Lumen框架通过创新的数据集构建和训练策略，成功解决了视频重光照中的一致性和前景保留问题，为视频编辑提供了高质量的解决方案。

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [215] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: MaskSem是一种基于语义引导掩码的自监督骨架动作识别方法，通过Grad-CAM指导关节掩码选择，并利用混合高阶运动作为重建目标，在多个数据集上取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 现有自监督骨架动作识别方法主要关注有限关节集和低阶运动模式，限制了模型对复杂运动模式的理解能力

Method: 提出语义引导掩码方法MaskSem，使用基于相对运动的Grad-CAM指导关节掩码选择；采用混合高阶运动（速度和加速度）作为重建目标

Result: 在NTU60、NTU120和PKU-MMD数据集上的实验表明，MaskSem结合普通transformer提升了骨架动作识别性能

Conclusion: 该方法能更好地理解运动模式，更适合人机交互应用

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [216] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed是一个针对开放式医学视觉问答的强化学习框架，通过结合文本正确性和自适应语义奖励来提升医学推理质量，在多个基准测试中显著提升了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域的强化学习应用不足，现有方法主要针对封闭式视觉问答，而开放式医学VQA更能反映临床实践但研究有限。基于模型的语义奖励存在奖励崩溃问题，即语义差异显著的响应获得相似分数。

Method: ARMed框架首先通过监督微调整合领域知识，然后应用强化学习结合文本正确性和自适应语义奖励来增强推理质量。

Result: 在六个医学VQA基准测试中，ARMed在域内任务上实现了32.64%的改进，在域外基准上获得了11.65%的提升，显著提高了准确性和泛化能力。

Conclusion: 研究强调了奖励可区分性在医学强化学习中的关键作用，以及语义引导奖励在实现稳健且具有临床意义的多模态推理方面的潜力。

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [217] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: 本文介绍了DLaBella29团队在MICCAI 2025 ToothFairy3挑战赛中使用的基于深度学习的牙齿分割方法，采用3D SegResNet架构和5折交叉验证，在验证集上达到平均Dice系数0.87。


<details>
  <summary>Details</summary>
Motivation: CBCT在牙科诊断和治疗规划中具有重要价值，自动化牙齿结构分割能够有效辅助病理识别（如牙髓或根尖周病变）和头颈癌患者的放射治疗规划。

Method: 使用MONAI Auto3DSeg框架和3D SegResNet架构，进行图像重采样至0.6mm各向同性分辨率和强度裁剪预处理，采用5折交叉验证训练，并通过Multi-Label STAPLE集成融合进行两阶段分割。

Result: 在ToothFairy3挑战赛的样本外验证集上取得了平均Dice系数0.87的优异表现。

Conclusion: 自动化牙齿分割方法在放射肿瘤学中具有重要的临床应用价值，能够显著改善患者护理质量。

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [218] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR：一种新颖的端到端架构，使用两个解耦的解码器分别处理头部定位和视线预测任务，在多个数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端视线检测模型使用单一解码器同时定位头部和预测视线，导致表示纠缠。需要解耦这两个任务以获得更好的性能

Method: 提出GazeDETR架构，包含两个独立的解码器：一个专门用于头部定位（利用局部信息），另一个用于视线预测（结合局部和全局信息），使用相干注意力场

Result: 在GazeFollow、VideoAttentionTarget和ChildPlay数据集上取得state-of-the-art结果，显著优于现有端到端模型

Conclusion: 解耦头部定位和视线预测任务的双解码器架构能够学习独特的表示，有效提升视线检测性能

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [219] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 提出Compact Attention框架，通过自适应分块、时变窗口和自动配置搜索，实现视频扩散transformer中注意力计算1.6-2.5倍加速，保持视觉质量的同时解决长视频生成的计算瓶颈


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长视频生成中计算需求巨大，现有稀疏注意力方法无法充分利用视频数据的时空冗余特性，限制了transformer在视频生成中的应用

Method: 提出硬件感知的Compact Attention框架：1）自适应分块策略通过动态瓦片分组近似不同空间交互模式；2）时变窗口根据帧距离调整稀疏度；3）自动配置搜索算法优化稀疏模式同时保留关键注意力路径

Result: 在单GPU设置上实现注意力计算1.6~2.5倍加速，同时保持与全注意力基线相当的视觉质量

Conclusion: 通过结构化稀疏性利用，为高效长视频生成提供了原则性方法，成功解决了transformer在视频生成中的计算瓶颈问题

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [220] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出一种无需标注数据的零样本神经架构搜索代理，通过奇异值分解和外在曲率来评估网络收敛性、泛化性和表达能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有零成本代理通常依赖标注数据且只关注收敛性/泛化性或表达能力中的一方面，无法在真实无标注场景中有效工作。

Method: 利用神经网络层特征的奇异值分解(SVD)和网络输出的外在曲率，设计简化的调和平均数作为代理指标，仅需单个无标注数据样本。

Result: 在NAS-Bench-101、NAS-Bench-201、TransNAS-Bench-101-micro等多个基准测试以及DARTS和AutoFormer搜索空间中表现出优越性能，且计算高效。

Conclusion: 该方法成功解决了零样本NAS中对标注数据的依赖问题，同时综合考虑了网络的收敛性、泛化性和表达能力，为实际应用提供了有效的架构评估方案。

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [221] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文对多模态视觉目标跟踪(MMVOT)进行了全面综述，涵盖6个MMVOT任务和338篇参考文献，分析了多模态数据收集、对齐、标注、模型设计和评估等关键方面。


<details>
  <summary>Details</summary>
Motivation: 随着智慧城市发展产生大量多模态数据，需要从多模态分析角度研究视觉目标跟踪这一关键任务，探讨多模态跟踪是否总是优于单模态跟踪以及在何种情况下更有益。

Method: 基于处理可见光(RGB)和辅助模态(X)的不同方式对现有MMVOT方法进行分类，X包括热红外、深度、事件、近红外、语言或声纳等模态。

Result: 首次分析了现有MMVOT数据集中目标类别的分布，揭示了其明显的长尾性质和与RGB数据集相比动物类别的显著缺乏。

Conclusion: 多模态视觉目标跟踪在智慧城市监测中具有重要作用，但需要根据具体应用场景评估多模态融合的实际效益，数据集的类别分布不均衡是需要关注的问题。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [222] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: 本文通过实证研究表明，增强特征多样性可以改善开放集识别性能，并促进持续学习中的知识保留和新数据整合


<details>
  <summary>Details</summary>
Motivation: 开放集识别和持续学习是机器学习中的两个关键挑战，虽然许多方法通过启发式方式促进特征多样性来解决这些问题，但很少有研究直接探讨特征多样性在这些任务中的作用

Method: 通过实证研究分析特征多样性对开放集识别和持续学习性能的影响

Result: 增强特征多样性可以改善开放集样本的识别，同时也有利于持续学习中旧知识的保留和新数据的整合

Conclusion: 研究结果可为这两个领域的实践方法和理论理解提供启发，推动进一步的研究

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [223] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm是一个通信高效的协作感知框架，利用4D雷达多普勒信息和查询驱动稀疏方案，在保持精度的同时减少90%带宽使用


<details>
  <summary>Details</summary>
Motivation: 解决协作感知中密集BEV特征图传输对车辆间通信带宽的挑战，克服遮挡和传感器范围限制

Method: 构建运动中心动态地图区分动静物体，生成参考查询和探索查询，通过多尺度门控可变形注意力交换和融合查询特定BEV特征

Result: 在OPV2V-R和Adver-City-R数据集上验证，带宽比全图共享降低90%，在不同交通密度和遮挡情况下性能匹配或超越现有基线

Conclusion: SlimComm成功实现了通信效率与感知精度的平衡，为实际部署提供了可行解决方案

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [224] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0是一个实时交互式世界模型，通过少步自回归扩散生成高质量长视频，速度达25FPS


<details>
  <summary>Details</summary>
Motivation: 现有交互式世界模型依赖双向注意力和冗长推理步骤，严重限制实时性能，难以模拟需要即时更新的真实世界动态

Method: 包含三个关键组件：1)虚幻引擎和GTA5的大规模数据生产流水线；2)帧级鼠标键盘输入的动作注入模块；3)基于因果架构的少步蒸馏实现实时流式视频生成

Result: 能够生成高质量分钟级视频，在多样化场景中以超快速度25FPS运行

Conclusion: 该框架为交互式世界建模研究提供了重要进展，并开源模型权重和代码库

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [225] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了EgoTwin框架，解决第一人称视角视频与人体运动联合生成任务，通过头中心运动表示和网络交互机制实现视角对齐和因果交互。


<details>
  <summary>Details</summary>
Motivation: 虽然外中心视角视频合成已取得很大进展，但第一人称视角视频生成仍未被充分探索，需要同时建模第一人称视角内容和穿戴者身体运动引起的相机运动模式。

Method: 提出EgoTwin框架，基于扩散transformer架构，引入头中心运动表示将人体运动锚定到头部关节，并采用网络交互机制在注意力操作中显式捕捉视频与运动之间的因果交互。

Result: 构建了大规模真实世界同步文本-视频-运动三元组数据集，设计了新颖的指标评估视频-运动一致性，大量实验证明了EgoTwin框架的有效性。

Conclusion: EgoTwin框架成功解决了第一人称视频与人体运动联合生成的两个关键挑战：视角对齐和因果交互，为egocentric视频生成领域提供了有效解决方案。

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [226] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR是一个用于心脏MRI重建的分层特征适应框架，通过参数高效适配器解决多中心部署时的域偏移问题，在CMRxRecon2025数据集上表现出优异的跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习心脏MRI重建在多临床中心部署时面临显著的域偏移挑战，不同扫描仪配置和成像协议导致性能下降。

Method: 采用分层特征适应框架：协议级适配器处理序列特定特征，中心级适配器处理扫描仪相关变化，基于变分展开骨干网络。通用适配器通过随机训练学习中心不变适应，实现完全未见中心的泛化。使用多尺度SSIM损失和频域增强进行优化。

Result: 在CMRxRecon2025数据集（5+中心、10+扫描仪、9种模态）上的综合评估显示，该方法在保持重建质量的同时实现了优越的跨中心泛化性能。

Conclusion: HierAdaptMR通过分层适配器有效解决了多中心心脏MRI重建的域适应问题，为临床部署提供了实用的解决方案。

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [227] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 提出了一种新颖的多尺度扫描视觉引导技术，通过语义分割和视觉语言模型识别重要物体，生成球形代理来指导用户采集图像，以改善3D高斯溅射等视图合成算法的输入质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视图合成算法（如3D高斯溅射）在渲染质量和速度方面已取得显著进展，但缺乏对用户采集输入图像的指导。高质量视图合成需要均匀密集的视角采样，而人类操作者往往因匆忙、不耐烦或缺乏场景理解而难以满足这一要求。

Method: 利用语义分割和类别识别（通过视觉语言模型排序）识别需要扩展图像覆盖的重要物体，为高排名物体生成球形代理来指导用户扫描过程。

Result: 在真实场景中相比传统视角采样策略展现出优越性能。

Conclusion: 该方法通过多尺度扫描引导技术有效解决了视图合成中图像采集的指导问题，显著提升了输入图像质量。

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [228] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 本文提出了第一个大规模人体形状编辑数据集和基于扩散模型的Odo方法，能够通过语义属性引导实现真实的人体形状编辑，在保持身份、服装和背景一致性的同时实现精确的形状变换。


<details>
  <summary>Details</summary>
Motivation: 人体形状编辑相比姿态编辑发展较慢，现有方法依赖3D可变形模型或图像变形，常导致不真实的身体比例、纹理扭曲和背景不一致问题，且缺乏大规模公开数据集。

Method: 提出Odo端到端扩散模型，结合冻结UNet保持外观细节和ControlNet使用目标SMPL深度图引导形状变换，基于新构建的18,573张图像数据集进行训练。

Result: 方法显著优于现有方法，顶点重建误差低至7.5mm（基线为13.6mm），能产生真实且准确匹配目标形状的结果。

Conclusion: 该工作填补了人体形状编辑领域的数据集空白，提出的Odo方法实现了高质量、可控的人体形状编辑，为后续研究提供了重要基础。

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [229] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 提出两阶段多模态框架，结合放射科医生眼动追踪数据提升胸部X光疾病分类和区域感知报告生成性能


<details>
  <summary>Details</summary>
Motivation: 利用放射科医生的眼动追踪数据（注视点）来改进医学影像分析和报告生成，提高疾病分类准确性和生成报告的可解释性

Method: 第一阶段：使用注视引导对比学习架构进行疾病分类，整合视觉特征、临床标签、边界框和眼动信号，采用多术语注视注意力损失函数；第二阶段：模块化报告生成流程，提取置信度加权的诊断关键词，通过结构化提示生成区域对齐的句子

Result: 疾病分类F1分数从0.597提升到0.631（+5.70%），AUC从0.821提升到0.849（+3.41%）；报告生成在临床关键词召回率和ROUGE重叠指标上均有改善

Conclusion: 整合眼动数据能显著提升分类性能和生成医学报告的可解释性，证明了注视信息在医学影像分析中的价值

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [230] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 本文提出使用Stable Diffusion生成ID卡真实样本的合成图像，以解决演示攻击检测(PAD)系统中真实样本数量不足的问题，提高检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前ID卡演示攻击检测系统面临两个主要挑战：缺乏足够的真实样本图像来训练鲁棒的PAD系统，以及攻击手段的多样性不断增加。大多数现有算法只关注生成攻击样本，而忽略了真实样本数量有限的问题。

Method: 采用Stable Diffusion技术生成真实ID卡图像的合成版本，通过数据增强方式来扩充训练数据集。生成的合成图像在两个系统中进行评估：从头训练的系统和一个商业解决方案。

Result: 实验结果显示，PAD系统能够将生成的合成图像识别为真实样本，这对检测性能和数据限制产生了积极影响，表明合成图像有助于提高检测器的泛化能力。

Conclusion: 使用Stable Diffusion生成合成真实样本是解决PAD系统训练数据不足的有效方法，能够显著提升检测性能并缓解数据限制问题，为ID卡安全检测提供了新的技术路径。

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [231] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: 提出了新的RSVQA数据集Chessboard和可解释模型Checkmate，通过细粒度视觉推理解决遥感视觉问答中的可解释性和偏见问题


<details>
  <summary>Details</summary>
Motivation: 当前遥感视觉问答模型缺乏可解释性，存在数据集偏见导致捷径学习问题，需要提高模型决策的透明度和可信度

Method: 创建包含3,123,253个问题的Chessboard数据集，答案分布均衡且与图像单元格关联；开发Checkmate模型，能够识别与决策最相关的图像区域

Result: 通过多模型架构实验证明，该方法提高了RSVQA系统的透明度和可信决策能力

Conclusion: Chessboard数据集和Checkmate模型有效解决了遥感视觉问答中的可解释性和偏见问题，为更可信的决策提供了支持

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [232] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 提出DMS方法，利用扩散模型生成新视角图像来解决自监督立体匹配和深度估计中的遮挡问题，无需额外标注数据即可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 自监督立体匹配和深度估计方法在遮挡区域存在对应像素缺失的问题，导致光度重建模糊，需要更好的解决方案

Method: 基于Stable Diffusion模型微调，沿极线方向生成三个新视角图像（左-左、右-右、中间视图），补充遮挡像素以实现显式光度重建

Result: 在多个基准数据集上达到最先进性能，异常值减少高达35%

Conclusion: DMS是一种模型无关的即插即用方法，仅需无标注立体图像对即可有效提升自监督立体匹配和单目深度估计性能

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [233] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: 本研究比较了RT-DETR-L和RT-DETR-X两种模型在沙滩垃圾检测中的性能，发现RT-DETR-X精度略高但计算成本显著，RT-DETR-L在速度和精度间取得更好平衡，更适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 海岸污染是全球性环境问题，需要可扩展的自动化监测解决方案。研究探索先进的目标检测模型在沙滩垃圾自动检测和计数中的应用效果。

Method: 使用公开的海岸垃圾数据集，对RT-DETR-Large和RT-DETR-Extra-Large两种变体进行严格的比较分析，评估其检测精度和推理时间。

Result: RT-DETR-X模型精度略高（mAP@50: 0.816, mAP@50-95: 0.612），但推理时间较长（34.5ms）；RT-DETR-L模型精度稍低（mAP@50: 0.810, mAP@50-95: 0.606），但推理速度快（20.1ms）。

Conclusion: RT-DETR-L模型在处理速度和检测精度之间提供了更好的平衡，是更实用和高效的实时现场部署解决方案，为基于Transformer的检测器在环境保护中的应用提供了重要见解。

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [234] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 提出视觉动作提示（VAP），一种统一的动作表示方法，通过将动作渲染为视觉骨架来平衡动作精度和跨域动态可迁移性，解决复杂高自由度交互动作的视频生成问题


<details>
  <summary>Details</summary>
Motivation: 现有动作驱动视频生成方法面临精度与泛化性的权衡：文本、原始动作或粗糙掩码方法泛化性好但缺乏精度，而智能体中心动作信号精度高但跨域迁移性差

Method: 将动作渲染为精确的视觉提示（选择视觉骨架作为通用表示），构建从人-物交互和灵巧机器人操作数据中提取骨架的流程，通过轻量级微调将视觉骨架集成到预训练视频生成模型中

Result: 在EgoVid、RT-1和DROID数据集上的实验证明了该方法的有效性，能够精确控制复杂交互动作同时保持跨域动态学习能力

Conclusion: 视觉动作提示提供了一种平衡动作精度和动态可迁移性的统一表示方法，为复杂高自由度交互动作的视频生成提供了有效解决方案

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [235] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion是一个无需训练的新框架，通过稀疏骨骼对应关系实现不同拓扑结构角色间的运动迁移，只需目标骨骼的少量示例动作即可工作。


<details>
  <summary>Details</summary>
Motivation: 解决不同骨骼拓扑结构角色间运动迁移的挑战，现有技术难以建立一对一的骨骼对应关系，且缺乏大规模配对运动数据集限制了数据驱动方法的发展。

Method: 提出训练免费的Motion2Motion框架，仅需目标骨骼的一个或几个示例动作，通过访问源骨骼和目标骨骼之间的稀疏骨骼对应关系来实现运动迁移。

Result: 通过全面定性和定量评估，证明Motion2Motion在相似骨骼和跨物种骨骼迁移场景中都能实现高效可靠的性能，并成功集成到下游应用和用户界面中。

Conclusion: 该方法具有工业应用潜力，代码和数据已公开，为解决跨拓扑运动迁移问题提供了实用有效的解决方案。

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [236] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse是一个新颖的3D场景重建框架，通过融合多视角扫描数据来解决物体遮挡问题，实现高保真渲染和物体级场景操作


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建方法存在多阶段流程复杂、需要密集扫描、容易出错且难以扩展的问题，特别是在处理物体遮挡和传感器覆盖有限的情况下

Method: 构建分割感知的高斯场，通过双向光度和语义一致性约束，引入伪中间场景状态进行统一对齐，采用协作共剪枝策略优化几何结构

Result: 实验验证了该框架对新场景配置的强泛化能力，在真实世界3D重建和真实到仿真转换方面表现出色

Conclusion: IGFuse无需密集观测或复杂流程即可实现高保真渲染和物体级场景操作，为3D场景重建提供了有效的解决方案

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [237] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX是首个从单张图像生成4D（动态3D）场景表示的端到端前馈框架，通过微调预训练视频扩散模型实现高效图像到4D生成


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖计算密集型优化或需要多帧视频输入的问题，为生成式4D世界模型提供可扩展解决方案

Method: 1)构建大规模4D数据集4DNeX-10M；2)引入统一6D视频表示联合建模RGB和XYZ序列；3)提出适配策略将预训练视频扩散模型重新用于4D建模

Result: 生成高质量动态点云，支持新颖视角视频合成，在效率和泛化性方面优于现有4D生成方法

Conclusion: 4DNeX为图像到4D建模提供了高效可扩展的解决方案，为模拟动态场景演化的生成式4D世界模型奠定了基础

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [238] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: 这篇论文研究了使用转换器模型（CodeBERT和CodeLlama）检测多编程语言代码中的安全漏洞，通过动态微调和集成学习方法，达到了超过97%的准确率，显示了AI在漏洞检测领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的静态分析工具在检测上下文依赖性错误时效果不佳且产生高假正率，需要更有效的方法来处理多编程语言代码中的复杂安全漏洞检测问题。

Method: 采用转换器模型（CodeBERT和CodeLlama）进行动态微调，包括数据集收集、语言标准化、模型微调，以及集成学习和可解释性AI技术的结合。

Result: 实验结果显示经过良好训练的CodeBERT模型准确率超过97%，在某些情况下甚至超过现有静态分析器。AI基于方法能够渐近完美的可能性，但精确度可能会降低，通过混合模型和验证流程可以减少假正率。

Conclusion: 这些AI基于的解决方案能够模式化到不同编程语言和漏洞类型，显示了AI在提升机器学习基于漏洞检测器的可用性和可扩展性方面的潜力。但在稳健性、可解释性和部署准备度方面仍需进一步发展。

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [239] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: CodeTracer是一个基于强化学习的自适应代码水印框架，通过在代码生成过程中智能偏置token选择来嵌入水印，同时保持代码功能性和可检测性。


<details>
  <summary>Details</summary>
Motivation: 检测LLM生成的代码需要水印系统能在高度结构化、语法受限的代码环境中有效工作，现有方法难以同时保证水印可检测性和代码功能性。

Method: 采用强化学习训练范式，使用参数化模型在next-token预测时智能偏置token选择；设计综合奖励系统整合执行反馈和水印嵌入信号；使用Gumbel Top-k重参数化实现离散水印决策的梯度优化。

Result: 在广泛比较评估中，CodeTracer在水印可检测性和生成代码功能保持方面显著优于最先进的基线方法。

Conclusion: CodeTracer提供了一个有效的代码水印解决方案，能够在保持代码功能性的同时嵌入统计可检测的水印，为LLM生成代码的检测提供了可靠技术。

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [240] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: Intent-FT是一种简单轻量的微调方法，通过训练LLM先推断指令的潜在意图再回应，显著提升模型对越狱攻击的鲁棒性，同时保持通用能力


<details>
  <summary>Details</summary>
Motivation: 尽管经过广泛的安全调优，大语言模型仍然容易受到通过对抗性指令的越狱攻击，这反映了安全性和任务性能之间的持续权衡

Method: 通过在针对性对抗指令集上进行微调，训练LLM推断指令的底层意图，使模型能够将意图推断泛化到未见过的攻击

Result: Intent-FT持续缓解所有评估的攻击类别，没有单一攻击成功率超过50%，同时保持模型的通用能力，减少对良性指令的过度拒绝

Conclusion: 该方法有效识别对抗攻击中的隐藏有害意图，学习到的意图可以有效地转移到增强普通模型防御中

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [241] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: 本文首次分析了扩散大语言模型(dLLMs)的安全性能，发现中间token对安全性更关键，提出了一种针对中间token的安全对齐方法MOSA，在安全性和实用性方面都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)作为一种新兴的非自回归范式，目前缺乏对其安全性的系统研究，需要针对其独特生成特性开发专门的安全对齐方法。

Method: 提出Middle-tOken Safety Alignment (MOSA)方法，利用强化学习直接对齐模型的中间生成过程与安全拒绝响应，基于发现的中问token对安全性更关键的特性。

Result: 在8种攻击方法和2个基准测试上，MOSA表现出优越的安全性能；在编程、数学和通用推理任务上保持了良好的实用性。

Conclusion: MOSA方法有效利用了dLLMs生成过程中的不对称性，为扩散大语言模型的安全对齐提供了新的有效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


### [242] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier Muñoz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: 本文提出了一种隐私保护的假身份证检测方法FakeIDet2，并发布了包含90万+真实/假身份证图像块的新数据库FakeIDet2-db，解决了假身份证检测研究中真实数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，攻击者能够创建极其逼真的假身份证，而研究人员在开发假身份证检测方法时面临真实数据稀缺的挑战，因为身份证具有敏感性，通常被用户和机构保密。

Method: 提出基于图像块的隐私保护方法，构建包含90万+图像块的新数据库，考虑打印、屏幕和合成三种物理攻击方式，并开发了隐私感知的假身份证检测方法FakeIDet2。

Result: 创建了FakeIDet2-db数据库，包含从2000张身份证图像中提取的图像块，涵盖了不同智能手机传感器、光照和高度条件，并提供了标准可复现的基准测试。

Conclusion: 该研究为解决假身份证检测中的数据隐私和稀缺问题提供了有效方案，通过隐私保护方法和公开数据库推动了该领域的研究发展。

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [243] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: HOMI是一个超低延迟的端到端边缘AI平台，结合事件相机和FPGA芯片，通过硬件优化的预处理管道实现了94%的手势识别准确率和1000fps的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有事件处理解决方案存在局限性，缺乏完整的端到端实现、延迟高且未能充分利用事件数据的稀疏性，需要开发更高效的事件相机处理平台。

Method: 使用Prophesee IMX636事件传感器芯片与Xilinx Zynq UltraScale+ MPSoC FPGA芯片，部署自主研发的AI加速器，开发硬件优化的预处理管道，支持恒定时间和恒定事件模式的直方图累积、线性和指数时间表面。

Result: 在DVS Gesture数据集上达到94%的准确率，低延迟配置下提供1000fps的吞吐量，仅使用33%的FPGA LUT资源，内存占用紧凑。

Conclusion: HOMI平台为边缘机器人应用提供了高效的端到端事件处理解决方案，具有优异的性能和资源利用率，为更复杂的架构集成和多任务部署留出了充足空间。

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


### [244] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: XR-NPE是一个面向XR感知工作负载的高吞吐量混合精度SIMD神经处理引擎，支持多种低精度格式，显著降低内存带宽需求，在28nm工艺下实现1.72GHz频率和42%面积减少。


<details>
  <summary>Details</summary>
Motivation: 为扩展现实(XR)设备如视觉惯性里程计、物体分类和眼球追踪等感知工作负载设计高效能、低功耗的神经处理引擎，解决资源受限环境下的计算需求。

Method: 采用层自适应混合算法实现支持FP4、Posit(4,1)、Posit(8,0)、Posit(16,1)等超低比特精度格式，结合量化感知训练减少精度损失，使用可重构尾数乘法和指数处理电路(RMMEC)减少暗硅，配合选择性电源门控降低能耗。

Result: 在28nm CMOS工艺下实现1.72GHz最大工作频率，面积0.016mm²，算术强度14pJ，相比现有最佳MAC方法减少42%面积和38%功耗。基于AXI的矩阵乘法协处理器在VCU129上减少1.4倍LUTs和1.77倍FFs，能效提升1.2倍。

Conclusion: XR-NPE证明是一个可扩展、精度自适应的计算引擎，适用于未来资源受限的XR设备，代码已开源供研究社区使用。

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [245] [iTrace: Click-Based Gaze Visualization on the Apple Vision Pro](https://arxiv.org/abs/2508.12268)
*Esra Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: iTrace是一个在Apple Vision Pro上通过点击方式提取用户注视数据的系统，使用手动和自动点击方法生成动态热力图，实现了91%的注视精度，可用于教育、设计、营销和临床评估等多种应用场景。


<details>
  <summary>Details</summary>
Motivation: Apple Vision Pro具有精确的眼动追踪能力，但隐私限制阻止直接访问连续用户注视数据。研究旨在克服这一限制，开发能够提取和分析注视数据的系统。

Method: 开发了iTrace系统，采用客户端-服务器架构，通过手动（捏合手势）和自动（停留控制、游戏控制器）点击方法提取注视坐标，并转换为动态热力图进行视频和空间眼动追踪。

Result: 8BitDo控制器实现了14.22次点击/秒的数据收集率，显著高于停留控制的0.45次点击/秒，生成的热力图显示了不同的注意力模式（讲座视频中的集中关注和问题解决任务中的广泛扫描），达到91%的注视精度。

Conclusion: iTrace在保持高精度的同时实现了动态注意力可视化，在教育内容参与、环境设计评估、营销分析和临床认知评估等方面具有广泛应用潜力，但建议仅在研究环境中使用。

Abstract: The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet
the privacy restrictions on the device prevent direct access to continuous user
gaze data. This study introduces iTrace, a novel application that overcomes
these limitations through click-based gaze extraction techniques, including
manual methods like a pinch gesture, and automatic approaches utilizing dwell
control or a gaming controller. We developed a system with a client-server
architecture that captures the gaze coordinates and transforms them into
dynamic heatmaps for video and spatial eye tracking. The system can generate
individual and averaged heatmaps, enabling analysis of personal and collective
attention patterns.
  To demonstrate its effectiveness and evaluate the usability and performance,
a study was conducted with two groups of 10 participants, each testing
different clicking methods. The 8BitDo controller achieved higher average data
collection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell
control, enabling significantly denser heatmap visualizations. The resulting
heatmaps reveal distinct attention patterns, including concentrated focus in
lecture videos and broader scanning during problem-solving tasks. By allowing
dynamic attention visualization while maintaining a high gaze precision of 91
%, iTrace demonstrates strong potential for a wide range of applications in
educational content engagement, environmental design evaluation, marketing
analysis, and clinical cognitive assessment. Despite the current gaze data
restrictions on the Apple Vision Pro, we encourage developers to use iTrace
only in research settings.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [246] [DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model](https://arxiv.org/abs/2508.12190)
*Jingkai Xu,De Cheng,Xiangqian Zhao,Jungang Yang,Zilong Wang,Xinyang Jiang,Xufang Luo,Lili Chen,Xiaoli Ning,Chengxu Li,Xinzhu Zhou,Xuejiao Song,Ang Li,Qingyue Xia,Zhou Zhuang,Hongfei Ouyang,Ke Xue,Yujun Sheng,Rusong Meng,Feng Xu,Xi Yang,Weimin Ma,Yusheng Lee,Dongsheng Li,Xinbo Gao,Jianming Liang,Lili Qiu,Nannan Wang,Xianbo Zuo,Cui Yong*

Main category: eess.IV

TL;DR: DermNIO是一个用于皮肤病学的多功能基础模型，通过混合预训练框架在432,776张图像上训练，在20个数据集上超越现有模型，诊断准确率达95.79%，比医生高22.13%


<details>
  <summary>Details</summary>
Motivation: 皮肤病影响全球70%人口，诊断过程复杂且皮肤科医生短缺。现有AI模型依赖大量人工标注数据且功能单一，在真实场景中效果有限

Method: 使用来自三个来源的432,776张图像训练DermNIO模型，采用新颖的混合预训练框架，结合自监督学习、半监督学习和知识引导的原型初始化

Result: 在20个数据集上持续超越最先进模型，在恶性肿瘤分类、疾病严重程度分级、多类别诊断等任务中表现优异，诊断准确率95.79%，AI辅助使医生性能提升17.21%

Conclusion: DermNIO展示了强大的泛化能力和鲁棒性，在隐私保护联邦学习和不同肤色性别场景中表现稳定，为皮肤病诊断提供了有效的AI解决方案

Abstract: Skin diseases impose a substantial burden on global healthcare systems,
driven by their high prevalence (affecting up to 70% of the population),
complex diagnostic processes, and a critical shortage of dermatologists in
resource-limited areas. While artificial intelligence(AI) tools have
demonstrated promise in dermatological image analysis, current models face
limitations-they often rely on large, manually labeled datasets and are built
for narrow, specific tasks, making them less effective in real-world settings.
To tackle these limitations, we present DermNIO, a versatile foundation model
for dermatology. Trained on a curated dataset of 432,776 images from three
sources (public repositories, web-sourced images, and proprietary collections),
DermNIO incorporates a novel hybrid pretraining framework that augments the
self-supervised learning paradigm through semi-supervised learning and
knowledge-guided prototype initialization. This integrated method not only
deepens the understanding of complex dermatological conditions, but also
substantially enhances the generalization capability across various clinical
tasks. Evaluated across 20 datasets, DermNIO consistently outperforms
state-of-the-art models across a wide range of tasks. It excels in high-level
clinical applications including malignancy classification, disease severity
grading, multi-category diagnosis, and dermatological image caption, while also
achieving state-of-the-art performance in low-level tasks such as skin lesion
segmentation. Furthermore, DermNIO demonstrates strong robustness in
privacy-preserving federated learning scenarios and across diverse skin types
and sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved
95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistance
improved clinician performance by 17.21%.

</details>


### [247] [FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration](https://arxiv.org/abs/2508.12445)
*Shayan Kebriti,Shahabedin Nabavi,Ali Gooya*

Main category: eess.IV

TL;DR: FractMorph是一种基于3D双并行Transformer的新型可变形图像配准架构，通过多域分数傅里叶变换分支增强跨图像特征匹配，在心脏MRI数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在统一框架中同时捕捉细粒度局部变形和大尺度全局变形，需要一种能够同时处理多尺度变形特征的解决方案。

Method: 提出双并行Transformer架构，使用分数傅里叶变换在0°、45°、90°角度和log-magnitude分支并行提取局部、半全局和全局特征，通过交叉注意力融合特征，最后用轻量级U-Net预测变形场。

Result: 在ACDC心脏MRI数据集上达到86.45%的Dice相似系数、75.15%的平均结构DSC和1.54mm的HD95距离，同时开发了仅29.6M参数的轻量版本。

Conclusion: 多域频谱-空间注意力机制能够在单一端到端网络中鲁棒且高效地建模医学图像中的复杂非刚性变形，无需场景特定调优或多尺度网络。

Abstract: Deformable image registration (DIR) is a crucial and challenging technique
for aligning anatomical structures in medical images and is widely applied in
diverse clinical applications. However, existing approaches often struggle to
capture fine-grained local deformations and large-scale global deformations
simultaneously within a unified framework. We present FractMorph, a novel 3D
dual-parallel transformer-based architecture that enhances cross-image feature
matching through multi-domain fractional Fourier transform (FrFT) branches.
Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at
fractional angles of 0{\deg}, 45{\deg}, 90{\deg}, along with a log-magnitude
branch, to effectively extract local, semi-global, and global features at the
same time. These features are fused via cross-attention between the fixed and
moving image streams. A lightweight U-Net style network then predicts a dense
deformation field from the transformer-enriched features. On the ACDC cardiac
MRI dataset, FractMorph achieves state-of-the-art performance with an overall
Dice Similarity Coefficient (DSC) of 86.45%, an average per-structure DSC of
75.15%, and a 95th-percentile Hausdorff distance (HD95) of 1.54 mm on our data
split. We also introduce FractMorph-Light, a lightweight variant of our model
with only 29.6M parameters, which maintains the superior accuracy of the main
model while using approximately half the memory. Our results demonstrate that
multi-domain spectral-spatial attention in transformers can robustly and
efficiently model complex non-rigid deformations in medical images using a
single end-to-end network, without the need for scenario-specific tuning or
hierarchical multi-scale networks. The source code of our implementation is
available at https://github.com/shayankebriti/FractMorph.

</details>


### [248] [Segmenting Thalamic Nuclei: T1 Maps Provide a Reliable and Efficient Solution](https://arxiv.org/abs/2508.12508)
*Anqi Feng,Zhangxing Bian,Samuel W. Remedios,Savannah P. Hays,Blake E. Dewey,Jiachen Zhuo,Dan Benjamini,Jerry L. Prince*

Main category: eess.IV

TL;DR: 本研究系统评估了多种MRI对比度对丘脑核团分割的影响，发现T1映射单独使用即可获得最佳性能，而PD映射无额外价值。


<details>
  <summary>Details</summary>
Motivation: 丘脑核团精确分割对理解神经系统疾病和脑功能至关重要，但目前最佳MRI输入序列尚不明确。

Method: 使用3D U-Net模型，评估MPRAGE、FGATIR序列、定量PD和T1映射以及多反转时间T1加权图像等多种MRI对比度，并对多TI图像采用基于梯度的显著性分析和蒙特卡洛dropout方法选择最优图像。

Result: T1映射单独使用即能获得强大的定量性能和优越的定性结果，PD映射没有提供额外价值。

Conclusion: T1映射是评估选项中可靠且高效的输入选择，为优化丘脑结构相关临床或研究的成像方案提供了重要指导。

Abstract: Accurate thalamic nuclei segmentation is crucial for understanding
neurological diseases, brain functions, and guiding clinical interventions.
However, the optimal inputs for segmentation remain unclear. This study
systematically evaluates multiple MRI contrasts, including MPRAGE and FGATIR
sequences, quantitative PD and T1 maps, and multiple T1-weighted images at
different inversion times (multi-TI), to determine the most effective inputs.
For multi-TI images, we employ a gradient-based saliency analysis with Monte
Carlo dropout and propose an Overall Importance Score to select the images
contributing most to segmentation. A 3D U-Net is trained on each of these
configurations. Results show that T1 maps alone achieve strong quantitative
performance and superior qualitative outcomes, while PD maps offer no added
value. These findings underscore the value of T1 maps as a reliable and
efficient input among the evaluated options, providing valuable guidance for
optimizing imaging protocols when thalamic structures are of clinical or
research interest.

</details>


### [249] [Anatomic Feature Fusion Model for Diagnosing Calcified Pulmonary Nodules on Chest X-Ray](https://arxiv.org/abs/2508.12562)
*Hyeonjin Choi,Yang-gon Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: 本研究提出了一种基于原始图像和结构抑制变体融合特征的钙化分类模型，用于胸部X光片中肺结节钙化的准确识别，在准确率和AUC指标上均优于仅使用原始图像的模型。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片中肺结节钙化的准确识别对早期治疗至关重要，但当前主要依赖医生视觉评估，存在解读差异大和解剖结构重叠干扰的问题。

Method: 使用融合特征方法，结合原始图像和结构抑制变体的特征来减少结构干扰，数据集包含2,517张无病变图像和656张结节图像（151个钙化结节和550个非钙化结节）。

Result: 提出的模型在钙化诊断中达到86.52%的准确率和0.8889的AUC，分别比仅使用原始图像的模型提高了3.54%和0.0385。

Conclusion: 融合特征方法能有效减少结构干扰，显著提升肺结节钙化分类的诊断性能，为临床提供更可靠的辅助诊断工具。

Abstract: Accurate and timely identification of pulmonary nodules on chest X-rays can
differentiate between life-saving early treatment and avoidable invasive
procedures. Calcification is a definitive indicator of benign nodules and is
the primary foundation for diagnosis. In actual practice, diagnosing pulmonary
nodule calcification on chest X-rays predominantly depends on the physician's
visual assessment, resulting in significant diversity in interpretation.
Furthermore, overlapping anatomical elements, such as ribs and spine,
complicate the precise identification of calcification patterns. This study
presents a calcification classification model that attains strong diagnostic
performance by utilizing fused features derived from raw images and their
structure-suppressed variants to reduce structural interference. We used 2,517
lesion-free images and 656 nodule images (151 calcified nodules and 550
non-calcified nodules), all obtained from Ajou University Hospital. The
suggested model attained an accuracy of 86.52% and an AUC of 0.8889 in
calcification diagnosis, surpassing the model trained on raw images by 3.54%
and 0.0385, respectively.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [250] [Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark](https://arxiv.org/abs/2508.12438)
*Yaron Aloni,Rotem Shalev-Arkushin,Yonatan Shafir,Guy Tevet,Ohad Fried,Amit Haim Bermano*

Main category: cs.GR

TL;DR: 提出了Express4D数据集，使用消费级设备和LLM生成的文本指令，为动态面部表情生成提供细粒度标注，并训练了两个基线模型进行文本到表情运动生成。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型面临数据集要么是语音驱动，要么仅限于粗粒度情感标签的问题，缺乏细粒度控制的表达性描述，且数据采集设备昂贵复杂。

Method: 使用消费级设备采集面部运动序列，结合LLM生成的自然语言指令进行语义标注，采用ARKit blendshape格式，训练了两个基线模型进行文本到表情运动生成。

Result: 训练模型能够学习有意义的文本到表情运动生成，捕捉两种模态之间的多对多映射关系。

Conclusion: Express4D数据集为细粒度面部表情生成提供了高质量标注数据，支持未来基准测试，数据集、代码和视频示例已公开。

Abstract: Dynamic facial expression generation from natural language is a crucial task
in Computer Graphics, with applications in Animation, Virtual Avatars, and
Human-Computer Interaction. However, current generative models suffer from
datasets that are either speech-driven or limited to coarse emotion labels,
lacking the nuanced, expressive descriptions needed for fine-grained control,
and were captured using elaborate and expensive equipment. We hence present a
new dataset of facial motion sequences featuring nuanced performances and
semantic annotation. The data is easily collected using commodity equipment and
LLM-generated natural language instructions, in the popular ARKit blendshape
format. This provides riggable motion, rich with expressive performances and
labels. We accordingly train two baseline models, and evaluate their
performance for future benchmarking. Using our Express4D dataset, the trained
models can learn meaningful text-to-expression motion generation and capture
the many-to-many mapping of the two modalities. The dataset, code, and video
examples are available on our webpage: https://jaron1990.github.io/Express4D/

</details>


### [251] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: MixCache是一个无需训练的高效视频DiT推理框架，通过多粒度缓存策略和自适应决策机制，在保持生成质量的同时显著提升推理速度（1.94-1.97倍加速）。


<details>
  <summary>Details</summary>
Motivation: 现有视频DiT模型的迭代去噪过程计算成本高、推理延迟大，而现有的单粒度缓存策略无法灵活平衡生成质量和推理速度。

Method: 提出MixCache框架，包括：1）区分不同缓存策略的干扰和边界；2）上下文感知缓存触发策略；3）自适应混合缓存决策策略动态选择最优缓存粒度。

Result: 在多种模型上的实验表明，MixCache能显著加速视频生成（Wan 14B模型1.94倍加速，HunyuanVideo模型1.97倍加速），同时提供优于基线方法的生成质量和推理效率。

Conclusion: MixCache通过创新的多粒度缓存策略有效解决了视频DiT模型推理效率问题，在保持高质量生成的同时实现了显著的性能提升。

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT
models have emerged as a dominant approach for high-quality video generation.
However, their multi-step iterative denoising process incurs high computational
cost and inference latency. Caching, a widely adopted optimization method in
DiT models, leverages the redundancy in the diffusion process to skip
computations in different granularities (e.g., step, cfg, block). Nevertheless,
existing caching methods are limited to single-granularity strategies,
struggling to balance generation quality and inference speed in a flexible
manner. In this work, we propose MixCache, a training-free caching-based
framework for efficient video DiT inference. It first distinguishes the
interference and boundary between different caching strategies, and then
introduces a context-aware cache triggering strategy to determine when caching
should be enabled, along with an adaptive hybrid cache decision strategy for
dynamically selecting the optimal caching granularity. Extensive experiments on
diverse models demonstrate that, MixCache can significantly accelerate video
generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on
HunyuanVideo) while delivering both superior generation quality and inference
efficiency compared to baseline methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [252] [Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network](https://arxiv.org/abs/2508.12574)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.SI

TL;DR: 本文提出Insight Rumors模型，使用双向Mamba2网络和注意力机制来定位和标记谣言内容，而不仅仅是分类。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测模型只能分类文本是否为谣言，缺乏定位和标记具体谣言内容的能力。

Method: 提出Att_BiMamba2网络（双向Mamba2模型+点积注意力）+谣言定位标记模块（跳连网络+条件随机场约束）

Result: 模型不仅能准确检测谣言，还能精确定位和标记上下文中的谣言内容，优于现有只能粗略判别的方法

Conclusion: 该方案有效解决了谣言内容定位问题，在综合实验中表现出色，超越了最先进的方案

Abstract: With the development of social media networks, rumor detection models have
attracted more and more attention. Whereas, these models primarily focus on
classifying contexts as rumors or not, lacking the capability to locate and
mark specific rumor content. To address this limitation, this paper proposes a
novel rumor detection model named Insight Rumors to locate and mark rumor
content within textual data. Specifically, we propose the Bidirectional Mamba2
Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a
bidirectional Mamba2 model and applies dot-product attention to weight and
combine the outputs from both directions, thereby enhancing the representation
of high-dimensional rumor features. Simultaneously, a Rumor Locating and
Marking module is designed to locate and mark rumors. The module constructs a
skip-connection network to project high-dimensional rumor features onto
low-dimensional label features. Moreover, Conditional Random Fields (CRF) is
employed to impose strong constraints on the output label features, ensuring
accurate rumor content location. Additionally, a labeled dataset for rumor
locating and marking is constructed, with the effectiveness of the proposed
model is evaluated through comprehensive experiments. Extensive experiments
indicate that the proposed scheme not only detects rumors accurately but also
locates and marks them in context precisely, outperforming state-of-the-art
schemes that can only discriminate rumors roughly.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [253] [Statistical analysis of multivariate planar curves and applications to X-ray classification](https://arxiv.org/abs/2508.11780)
*Moindjié Issam-Ali,Descary Marie-Hélène,Beaulac Cédric*

Main category: stat.ME

TL;DR: 提出了一种基于多变量平面曲线形状分析的新方法，利用分割图像的轮廓信息进行医学图像分类，特别是在X射线心脏肥大检测中表现出色


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中分割图像（如放射影像）在诊断中很重要，但现有方法未充分利用分割图像中物体形状信息作为预测因子

Method: 开发多变量平面曲线分析框架，解决统计形状分析中的对齐问题，通过切线投影将获得的形状变量用于功能分类方法

Result: 在分割X射线的心脏肥大检测和合成数据实验中证明了方法的有效性和鲁棒性

Conclusion: 该方法为利用图像中物体形状信息进行监督分类提供了新的有效途径，在医学图像分析领域具有应用价值

Abstract: Recent developments in computer vision have enabled the availability of
segmented images across various domains, such as medicine, where segmented
radiography images play an important role in diagnosis-making. As prediction
problems are common in medical image analysis, this work explores the use of
segmented images (through the associated contours they highlight) as predictors
in a supervised classification context. Consequently, we develop a new approach
for image analysis that takes into account the shape of objects within images.
For this aim, we introduce a new formalism that extends the study of single
random planar curves to the joint analysis of multiple planar curves-referred
to here as multivariate planar curves. In this framework, we propose a solution
to the alignment issue in statistical shape analysis. The obtained multivariate
shape variables are then used in functional classification methods through
tangent projections. Detection of cardiomegaly in segmented X-rays and
numerical experiments on synthetic data demonstrate the appeal and robustness
of the proposed method.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [254] [Point upsampling networks for single-photon sensing](https://arxiv.org/abs/2508.12986)
*Jinyi Liu,Guoyang Zhao,Lijun Liu,Yiguang Hong,Weiping Zhang,Shuming Cheng*

Main category: physics.optics

TL;DR: 本文提出了一种基于状态空间模型的点云上采样网络，用于提升单光子传感点云的密度并减少空间失真，通过多路径扫描机制、双向Mamba主干网络和自适应上采样偏移模块实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 单光子传感技术虽然具有长距离和超灵敏成像的优势，但产生的点云通常稀疏且存在空间偏差，限制了其实际应用价值。

Method: 采用状态空间模型构建网络，包含多路径扫描机制丰富空间上下文，双向Mamba主干网络捕捉全局几何和局部细节，自适应上采样偏移模块校正偏移引起的失真。

Result: 在常用数据集上的实验验证了方法的高重建精度和对失真噪声的强鲁棒性，真实数据实验表明能生成视觉一致、细节保留且噪声抑制的点云。

Conclusion: 这是首个为单光子传感建立的上采样框架，为单光子传感及其在下游任务中的实际应用开辟了新途径。

Abstract: Single-photon sensing has generated great interest as a prominent technique
of long-distance and ultra-sensitive imaging, however, it tends to yield sparse
and spatially biased point clouds, thus limiting its practical utility. In this
work, we propose using point upsampling networks to increase point density and
reduce spatial distortion in single-photon point cloud. Particularly, our
network is built on the state space model which integrates a multi-path
scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to
capture global geometry and local details, and an adaptive upsample shift
module to correct offset-induced distortions. Extensive experiments are
implemented on commonly-used datasets to confirm its high reconstruction
accuracy and strong robustness to the distortion noise, and also on real-world
data to demonstrate that our model is able to generate visually consistent,
detail-preserving, and noise suppressed point clouds. Our work is the first to
establish the upsampling framework for single-photon sensing, and hence opens a
new avenue for single-photon sensing and its practical applications in the
downstreaming tasks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [255] [BeeNet: Reconstructing Flower Shapes from Electric Fields using Deep Learning](https://arxiv.org/abs/2508.11724)
*Jake Turley,Ryan A. Palmer,Isaac V. Chenchiah,Daniel Robert*

Main category: q-bio.QM

TL;DR: 本文开发了一种深度学习算法，能够从蜜蜂与花朵相互作用产生的电场中重建花朵形状，揭示了节肢动物通过电感受获得丰富空间信息的能力。


<details>
  <summary>Details</summary>
Motivation: 研究节肢动物（包括传粉者）如何响应环境电场，探索电场信息如何被解码以重建环境特征，特别是花朵形状的重建。

Method: 开发算法模拟蜜蜂与不同花瓣几何形状花朵相互作用的电场，使用模拟数据训练深度学习UNet模型来重建花瓣形状。

Result: 模型能够准确重建多样化的花朵形状，包括训练中未包含的复杂形状；重建性能在特定蜜蜂-花朵距离处达到峰值，表明形状信息存在距离依赖性编码。

Conclusion: 电感受能够提供丰富的空间细节信息，这为了解节肢动物的环境感知机制提供了重要见解，表明电场信息在环境特征识别中具有重要作用。

Abstract: Arthropods, including pollinators, respond to environmental electrical
fields. Here, we show that electric field information can be decoded to
reconstruct environmental features. We develop an algorithm capable of
inferring the shapes of polarisable flowers from the electric field generated
by a nearby charged bee. We simulated electric fields arising from bee flower
interactions for flowers with varying petal geometries. These simulated data
were used to train a deep learning UNet model to recreate petal shapes. The
model accurately reconstructed diverse flower shapes including more complex
flower shapes not included in training. Reconstruction performance peaked at an
optimal bee flower distance, indicating distance-dependent encoding of shape
information. These findings show that electroreception can impart rich spatial
detail, offering insights into arthropod environmental perception.

</details>


### [256] [On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes](https://arxiv.org/abs/2508.12742)
*Theodoros Bermperidis,Joe Vero,Elizabeth B Torres*

Main category: q-bio.QM

TL;DR: 开发了一种情感计算平台，通过结合时间序列数据中的微峰值分析和AI面部网格估计方法，能够在5秒短视频中保持个性化统计功效，无需传统的大数据平均技术。


<details>
  <summary>Details</summary>
Motivation: 解决传统统计方法在大数据采集与保持统计功效之间的权衡问题，传统方法假设正态分布和线性平稳过程，会丢失重要信息。

Method: 结合新型数据类型（时间序列数据中的微峰值）和AI面部网格估计方法，采用几何和非线性动力系统方法分析运动学特征，特别是速度数据，捕捉所有面部微峰值和情感微表情的细微差别。

Result: 能够从5秒的面部视频中提取个性化统计信息，提供了区分自闭症个体与神经典型发育个体动态和几何模式的新方法。

Conclusion: 该方法实现了在简短数据采样中保持统计功效，为情感计算和神经发育差异研究提供了新的分析途径。

Abstract: There is a tradeoff between attaining statistical power with large, difficult
to gather data sets, and producing highly scalable assays that register brief
data samples. Often, as grand-averaging techniques a priori assume
normally-distributed parameters and linear, stationary processes in
biorhythmic, time series data, important information is lost, averaged out as
gross data. We developed an affective computing platform that enables taking
brief data samples while maintaining personalized statistical power. This is
achieved by combining a new data type derived from the micropeaks present in
time series data registered from brief (5-second-long) face videos with recent
advances in AI-driven face-grid estimation methods. By adopting geometric and
nonlinear dynamical systems approaches to analyze the kinematics, especially
the speed data, the new methods capture all facial micropeaks. These include as
well the nuances of different affective micro expressions. We offer new ways to
differentiate dynamical and geometric patterns present in autistic individuals
from those found more commonly in neurotypical development.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [257] [Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation](https://arxiv.org/abs/2508.11738)
*Kiruthika Balakrishnan,Durgadevi Velusamy,Hana E. Hinkle,Zhi Li,Karthikeyan Ramasamy,Hikmat Khan,Srini Ramaswamy,Pir Masoom Shah*

Main category: cs.CY

TL;DR: 这篇系统综述研究了AI在改善农村医疗保健中的潜力，分析了109项研究，发现AI技术如预测分析、远程医疗平台和自动诊断工具能显著提升农村医疗的可及性、质量和效率，但面临基础设施、数据质量和伦理等挑战。


<details>
  <summary>Details</summary>
Motivation: 农村医疗面临基础设施不足、人力资源短缺和社会经济差异等长期挑战，阻碍了基本医疗服务的获取。研究旨在探索人工智能如何变革性地解决这些农村医疗服务不足的问题。

Method: 系统回顾了2019-2024年间PubMed、Embase、Web of Science、IEEE Xplore和Scopus数据库中的109项研究，使用PRISMA指南和Covidence软件进行筛选，并进行主题分析以识别AI在农村医疗实施中的关键模式和见解。

Result: 研究发现AI应用（如预测分析、远程医疗平台、自动诊断工具）在改善医疗可及性、质量和效率方面具有显著潜力。多模态基础模型和大型语言模型特别具有变革性潜力，能整合多种数据源支持决策，促进临床文档、患者分诊和虚拟协助。

Conclusion: AI技术能够通过增强人力能力、减少诊断延迟和普及专业知识来革命化农村医疗，但仍需解决基础设施限制、数据质量问题和伦理考虑。需要跨学科合作、数字基础设施投资和监管框架开发来确保AI在农村医疗系统中的公平和可持续整合。

Abstract: Rural healthcare faces persistent challenges, including inadequate
infrastructure, workforce shortages, and socioeconomic disparities that hinder
access to essential services. This study investigates the transformative
potential of artificial intelligence (AI) in addressing these issues in
underserved rural areas. We systematically reviewed 109 studies published
between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and
Scopus. Articles were screened using PRISMA guidelines and Covidence software.
A thematic analysis was conducted to identify key patterns and insights
regarding AI implementation in rural healthcare delivery. The findings reveal
significant promise for AI applications, such as predictive analytics,
telemedicine platforms, and automated diagnostic tools, in improving healthcare
accessibility, quality, and efficiency. Among these, advanced AI systems,
including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),
offer particularly transformative potential. MFMs integrate diverse data
sources, such as imaging, clinical records, and bio signals, to support
comprehensive decision-making, while LLMs facilitate clinical documentation,
patient triage, translation, and virtual assistance. Together, these
technologies can revolutionize rural healthcare by augmenting human capacity,
reducing diagnostic delays, and democratizing access to expertise. However,
barriers remain, including infrastructural limitations, data quality concerns,
and ethical considerations. Addressing these challenges requires
interdisciplinary collaboration, investment in digital infrastructure, and the
development of regulatory frameworks. This review offers actionable
recommendations and highlights areas for future research to ensure equitable
and sustainable integration of AI in rural healthcare systems.

</details>


### [258] [Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health](https://arxiv.org/abs/2508.12998)
*Sanja Šćepanović,Sagar Joglekar,Stephen Law,Daniele Quercia,Ke Zhou,Alice Battiston,Rossano Schifanella*

Main category: cs.CY

TL;DR: 研究发现日常可见的道路绿化比传统官方绿化指标更能改善健康，高血压处方在道路绿化好的区域下降3.68%，每年可节省315万英镑处方费用


<details>
  <summary>Details</summary>
Motivation: 传统绿化指标只测量绿化数量和距离，但忽略了人们在日常生活中实际看到和使用绿化的频率，导致研究结果不一致

Method: 结合航拍影像、OpenStreetMap绿化数据、10万+谷歌街景图像量化绿化，分析160,000条道路的可达性，关联74.5亿份NHS处方数据

Result: 道路绿化比四种常用官方指标更能预测健康改善，高血压处方在绿化好的区域下降3.68%，年节省费用可达315万英镑

Conclusion: 日常生活中可见的绿化比公共但隐蔽的绿化更重要，文献中常用的官方绿化指标存在重要局限性

Abstract: Urban greenery is often linked to better health, yet findings from past
research have been inconsistent. One reason is that official greenery metrics
measure the amount or nearness of greenery but ignore how often people actually
may potentially see or use it in daily life. To address this gap, we introduced
a new classification that separates on-road greenery, which people see while
walking through streets, from off-road greenery, which requires planned visits.
We did so by combining aerial imagery of Greater London and greenery data from
OpenStreetMap with quantified greenery from over 100,000 Google Street View
images and accessibility estimates based on 160,000 road segments. We linked
these measures to 7.45 billion medical prescriptions issued by the National
Health Service and processed through our methodology. These prescriptions cover
five conditions: diabetes, hypertension, asthma, depression, and anxiety, as
well as opioid use. As hypothesized, we found that green on-road was more
strongly linked to better health than four widely used official measures. For
example, hypertension prescriptions dropped by 3.68% in wards with on-road
greenery above the median citywide level compared to those below it. If all
below-median wards reached the citywide median in on-road greenery,
prescription costs could fall by up to {\pounds}3.15 million each year. These
results suggest that greenery seen in daily life may be more relevant than
public yet secluded greenery, and that official metrics commonly used in the
literature have important limitations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [259] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV是首个针对多上下文KV缓存进行注意力稀疏化的方法，通过考虑其他上下文的互补信息进行稀疏化并局部重计算，在RAG场景中可将序列长度压缩至15%且保持准确率，显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存重用方法在单上下文场景有效，但在RAG多上下文场景中由于缺乏跨上下文注意力而失效。现有方法需要保留全部KV缓存，无法减少内存开销。

Method: SamKV在多上下文KV缓存稀疏化时考虑其他上下文的互补信息，然后对稀疏化信息进行局部重计算，实现有效的注意力稀疏化。

Result: 实验表明该方法能将序列长度压缩至15%，与完全重计算基线相比无准确率损失，在多上下文RAG场景中显著提升吞吐量。

Conclusion: SamKV成功解决了多上下文KV缓存的高效稀疏化问题，为RAG场景中的长序列推理提供了有效的内存优化方案。

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [260] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 提出Representation Stability (RS)框架，通过掩码重要词汇并测量嵌入表示变化来检测对抗文本攻击，无需模型重训练，在多个数据集和攻击类型上达到88%以上检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗文本防御方法通常针对特定攻击或需要昂贵的模型重训练，需要一种模型无关的通用检测框架来应对持续存在的对抗文本威胁。

Method: RS框架使用重要性启发式对词汇排序，测量对前k个关键词汇掩码的嵌入敏感性，通过BiLSTM检测器处理结果模式。使用NDCG评估扰动识别质量。

Result: 在三个数据集、三种攻击类型和两个受害模型上，RS达到超过88%的检测准确率，计算成本较低。基于梯度的排序方法优于注意力和随机选择方法。

Conclusion: RS框架具有良好的泛化能力，无需重训练即可适应未见过的数据集、攻击和模型，为对抗文本检测提供了实用解决方案。

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [261] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是基于160亿次医疗事件训练的大型医疗基础模型，通过自回归生成模拟患者健康时间线，在78个医疗任务中无需微调即可超越或匹配专用监督模型。


<details>
  <summary>Details</summary>
Motivation: 实现规模化个性化医疗需要从纵向患者旅程中提取洞察，基础模型预训练在大规模医疗事件数据上代表了扩展真实世界证据生成和泛化到多样化下游任务的有希望方向。

Method: 使用Epic Cosmos数据集（163亿次就诊、3亿患者记录），训练解码器Transformer模型CoMET，进行最大规模的医疗数据缩放定律研究，预训练计算最优模型（最高10亿参数），通过自回归生成下一个医疗事件来模拟患者健康时间线。

Result: CoMET在78个真实世界任务（包括诊断预测、疾病预后和医疗运营）中，无需任务特定微调或少量样本，普遍优于或匹配任务特定的监督模型，预测能力随模型规模和预训练规模持续提升。

Conclusion: CoMET作为生成式医疗事件基础模型，能有效捕捉复杂临床动态，为支持临床决策、简化医疗运营和改善患者结局提供了可扩展和可泛化的框架。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [262] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT是一种动态自动化指令调优数据集混合优化方法，使用多臂老虎机框架和先验缩放玻尔兹曼探索，在Tulu-v2-mixture上实现2.2%的性能提升


<details>
  <summary>Details</summary>
Motivation: 随着指令调优数据集不断涌现，如何在训练后阶段动态平衡和优化这些数据集的混合比例成为一个关键挑战

Method: 将问题建模为多臂老虎机，提出先验缩放玻尔兹曼探索方法，使用轻量级1步前瞻奖励更新采样概率，保持原始数据集比例的软锚定

Result: 在包含16个指令调优数据集的Tulu-v2-mixture集合上，DynamixSFT在10个基准测试中实现了最高2.2%的性能提升

Conclusion: 该方法能有效优化数据集混合比例，同时保持数据集的多样性和覆盖范围，为指令调优提供了动态自适应解决方案

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [263] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge是一个统一的统计框架，通过建模人类偏好和LLM评分之间的系统差异，提高LLM作为评估者与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为评估者(LLM-as-a-judge)被广泛使用，但其评估结果往往与人类判断存在系统性差异，需要建立框架来弥合这种差距。

Method: 提出Bridge统计框架，假设每个提示-响应对存在潜在人类偏好分数，将LLM偏差建模为捕捉差异来源的协变量的线性变换，并提供高效的拟合算法和统计推断保证。

Result: 在6个LLM评估者和两个基准测试(BigGen Bench和Chatbot Arena)上，Bridge在准确性、校准和KL散度方面与人类评分达成更高一致性，并揭示了系统性的人-LLM差距。

Conclusion: Bridge提供了一个简单而原则性的框架，用于改进LLM评分并表征人类与LLM之间的系统性差异，为LLM作为评估者的应用提供了统计基础。

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [264] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore提出了一种新的MoE路由范式，通过最小成本最大流问题和SoftTopk算子解决了传统MoE网络中的token丢弃和硬件效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE网络存在专家容量约束导致的token丢弃问题，以及由于填充导致的硬件效率低下。移除容量约束又会损害负载平衡和计算效率。

Method: 将路由建模为最小成本最大流问题，集成SoftTopk算子，避免了迭代重路由和最优传输公式的根本限制。

Result: 在相同FLOPs下实现了更低的训练损失和更高的评估分数，优于有约束和无约束的基线方法。

Conclusion: MaxScore成功解决了MoE路由中的关键问题，为稀疏激活的专家混合网络提供了更高效的路由解决方案。

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [265] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: L2S提出了一种针对多模态大语言模型(MLLMs)的输入特定学习引导方法，通过训练辅助模块预测输入相关的引导向量，相比静态引导方法能更好地减少幻觉并增强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有引导技术如均值引导依赖单一引导向量，无法处理输入依赖的期望行为（如安全回答需要根据具体问题调整），在多模态场景下尤其未充分探索。

Method: 使用对比输入特定提示计算输入特定的线性偏移，训练小型辅助模块来预测这些测试时未知的输入特定引导向量。

Result: L2S方法在减少幻觉和增强安全性方面优于其他静态基线方法。

Conclusion: 输入特定的学习引导方法比静态引导更有效，能够根据具体输入内容动态调整模型行为，在多模态大语言模型中实现更精细的控制。

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [266] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ是一种面向TinyML流式应用的轻量级不确定性监测方法，通过时序一致性和流式共形校准实现单次前向传播的无标签不确定性评估


<details>
  <summary>Details</summary>
Motivation: 解决TinyML设备上传统不确定性方法（如早停和深度集成）资源消耗大、延迟高的问题，提供资源高效的在线不确定性监测方案

Method: 利用短时域时序一致性，通过轻量级信号捕捉后验和特征的时序变化，使用O(W)环形缓冲区和O(1)每步更新的流式共形层将一致性得分转换为校准的风险评分

Result: 在微控制器上比早停和深度集成方法减少50-60%内存占用和30-45%延迟，在分布内数据流损坏情况下AUPRC提升3-7点，最高达到0.86 AUPRC，故障检测AUROC最高达0.92

Conclusion: 时序一致性结合流式共形校准为TinyML设备监控提供了实用且资源高效的基础方案

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [267] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ是一种面向TinyML的单次前向传播、无需标签的不确定性量化方法，通过深度激活预测来估计风险，在资源受限的微控制器上仅增加几十KB开销，相比现有方法显著减少存储和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性量化方法在TinyML设备上存在内存占用大、延迟高、需要多次前向传播等问题，无法满足资源受限的嵌入式设备需求。

Method: 使用int8量化的小型预测头来预测下一层的统计信息，通过轻量级单调映射器将预测差异转换为可操作的置信度分数，无需时间缓冲区、辅助出口或重复前向传播。

Result: 在视觉和音频任务中，相比早期退出和深度集成方法，SNAP-UQ减少40-60%存储占用和25-35%延迟，在数据损坏流中AUPRC提升数个点，AUROC达到约0.9。

Conclusion: 基于层间动态的不确定性量化方法为TinyML设备上的实时监控提供了实用且资源高效的解决方案。

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [268] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: 提出MSLoRA-CR方法解决多模态生物医学图像增量学习问题，通过模态特定LoRA模块和对比正则化实现知识保留与共享，在计算效率下性能提升1.88%


<details>
  <summary>Details</summary>
Motivation: 生物医学领域需要处理多种模态和任务，为每个模态单独训练模型推理成本过高，需要统一的增量学习模型

Method: 基于大型视觉语言模型，冻结预训练参数，为每个模态增量适配LoRA模块，加入对比正则化促进模态内知识共享和模态间知识区分

Result: 在生物医学图像增量学习实验中，MSLoRA-CR优于为每个模态单独训练模型的方法和通用增量学习方法，整体性能提升1.88%

Conclusion: MSLoRA-CR有效解决了多模态生物医学图像增量学习的两个核心挑战，在保持计算效率的同时显著提升性能

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [269] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 利用Google DeepMind的AlphaEarth Foundations (AEF) 全球地理空间表示，通过随机森林和逻辑回归等基础模型，将美国LANDFIRE植被类型数据集扩展到加拿大地区


<details>
  <summary>Details</summary>
Motivation: 高质量的地理空间标注数据集通常只覆盖特定地理区域，无法全球覆盖。需要一种方法来扩展这些数据集的地理覆盖范围

Method: 利用AEF作为信息密集的全球地理空间表示，使用随机森林和逻辑回归等基础分类模型，将美国现有的植被类型标注扩展到加拿大

Result: 在EvtPhys（13类）上，模型在美国和加拿大的验证集上分别达到81%和73%的分类准确率，预测结果与地面实况定性一致

Conclusion: 即使使用基础模型，结合AEF全球表示也能有效扩展地理空间标注数据集的地理覆盖范围，为全球地理空间分析提供了可行方法

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [270] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 论文提出ENA架构，结合线性循环和高维滑动窗口注意力，为超长高维数据建模提供高效解决方案


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列高维数据时效率不足，需要更高效的架构

Method: 研究扫描策略和注意力混合架构，重点评估注意力类型，发现平铺高维滑动窗口注意力(SWA)在理论和实践中都高效，结合线性循环形成ENA架构

Result: 扫描策略收益有限，注意力混合模型效果显著，ENA架构在超长高维数据建模中表现优异

Conclusion: 线性循环压缩全局信息，SWA强制局部建模，两者结合形成简单有效的框架，为高维数据建模提供实用解决方案

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [271] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出了一种新颖的学习型二阶优化器，通过可训练预处理单元增强经典SR1算法，在单目人体网格恢复任务中表现优于现有学习优化方法


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习依赖大数据集且泛化性差，经典优化方法计算高效但收敛慢，学习型二阶优化方法研究不足

Method: 引入可训练预处理单元生成数据驱动向量，构建满足割线约束的正半定秩一矩阵，通过学习投影进行对齐

Result: 在分析实验和单目人体网格恢复任务中优于现有学习优化方法，具有轻量级模型、无需标注数据或微调

Conclusion: 该方法具有良好的泛化能力，适合集成到更广泛的基于优化的框架中，为学习型二阶优化提供了新思路

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [272] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 提出Latent Reconstruction(LR)损失函数来解决VAE后验坍塌问题，无需特定网络架构限制，在多个数据集上验证有效性


<details>
  <summary>Details</summary>
Motivation: 变分自编码器(VAE)存在后验坍塌问题，导致生成样本多样性降低。现有方法需要在重建和正则化之间权衡，或者要求网络架构的结构约束

Method: 定义局部后验坍塌概念，提出基于单射和复合函数数学特性的潜在重建(LR)损失函数，无需特定架构限制

Result: 在MNIST、fashionMNIST、Omniglot、CelebA和FFHQ等多个数据集上实验验证了该方法能有效控制后验坍塌

Conclusion: LR损失函数提供了一种无需架构约束的后验坍塌控制方法，提高了VAE生成样本的多样性

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [273] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 本研究提出了一个去中心化的联邦学习框架，专门用于车辆网络中的交通标志检测，通过不共享原始数据的方式实现协作模型训练，在模拟环境中验证了多种配置的有效性。


<details>
  <summary>Details</summary>
Motivation: 联网和自动驾驶车辆每天产生大量传感器数据，集中式机器学习方法在感知任务中面临严重的隐私和通信挑战，需要寻找隐私保护的分布式解决方案。

Method: 框架将交通标志类别分配到不同车辆进行专门的本地训练，使用轻量级目标检测器，通过Flower框架在模拟环境中聚合模型参数（包括FedProx、FedAdam和FedAVG算法），评估了不同服务器轮次、本地周期、客户端参与比例和数据分布等多种配置。

Result: 实验表明：服务器轮次从2增加到20时准确率从低于0.1提升到超过0.8；适中的本地周期（8-10）提供约0.67的最佳效率；更高的客户端参与比例将泛化能力提升至0.83；FedProx在处理异构性方面优于其他聚合器；非IID数据分布相比IID降低了性能；训练时长主要与轮次数量而非聚合策略相关。

Conclusion: 这种联邦学习方法可为现实世界车辆部署提供可扩展的隐私保护解决方案，可能指导未来集成鲁棒聚合和通信优化以推进智能交通系统的发展。

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [274] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文重新审视因果建模在AI泛化中的作用，挑战了现有领域泛化基准的结论，提出了更细致的因果理论框架


<details>
  <summary>Details</summary>
Motivation: 近期领域泛化(DG)基准研究对因果建模能够实现鲁棒AI泛化的承诺提出了挑战，需要调和因果性与DG文献中的明显矛盾

Method: 重新审视因果性和DG文献的主张，调和表面矛盾，提出更细致的因果理论框架，并提供交互式演示

Result: 建立了对因果性在泛化中作用的更细致理解，调和了不同研究结论之间的矛盾

Conclusion: 因果建模在AI泛化中仍具有重要作用，但需要更细致的理论框架来理解其机制和局限性

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [275] [Data-driven RF Tomography via Cross-modal Sensing and Continual Learning](https://arxiv.org/abs/2508.11654)
*Yang Zhao,Tao Wang,Said Elhadi*

Main category: eess.SP

TL;DR: 提出了一个数据驱动的射频层析成像框架DRIFT，通过跨模态学习和持续学习来提升地下目标检测的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 射频信号具有穿透土壤的特性，但现有方法在动态环境中难以实现准确和鲁棒的检测性能

Method: 设计RF和视觉传感器的跨模态传感系统，采用跨模态学习方法训练RF层析成像DNN模型，并应用持续学习在检测到环境变化时自动更新模型

Result: 平均等效直径误差为2.29厘米，比最先进方法提升了23.2%的性能

Conclusion: DRIFT框架能够有效重建地下根茎作物的横截面图像，即使在RF信号发生显著变化的情况下也能保持良好性能

Abstract: Data-driven radio frequency (RF) tomography has demonstrated significant
potential for underground target detection, due to the penetrative nature of RF
signals through soil. However, it is still challenging to achieve accurate and
robust performance in dynamic environments. In this work, we propose a
data-driven radio frequency tomography (DRIFT) framework with the following key
components to reconstruct cross section images of underground root tubers, even
with significant changes in RF signals. First, we design a cross-modal sensing
system with RF and visual sensors, and propose to train an RF tomography deep
neural network (DNN) model following the cross-modal learning approach. Then we
propose to apply continual learning to automatically update the DNN model, once
environment changes are detected in a dynamic environment. Experimental results
show that our approach achieves an average equivalent diameter error of 2.29
cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and
dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [276] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: 本文探讨如何利用大型语言模型（LLMs）提升机器人的自然语言理解能力，使其能够更好地与人类协作完成物理世界任务。


<details>
  <summary>Details</summary>
Motivation: 传统交互式任务学习系统语言理解能力有限，而LLMs的出现为机器人提供了大幅提升语言理解能力的机会，但将LLM的语言能力与物理世界操作的机器人整合是一个挑战。

Method: 提出基于认知代理的AI系统架构，该架构以物理机器人为核心，同时与人类和LLM交互，并通过经验积累情境知识。通过三个概念验证实验使用ChatGPT展示机器人理解自然语言的能力。

Result: 通过简单的概念验证实验证明了LLM在帮助机器人理解自然语言方面的潜力，为构建集成化的机器人助手奠定了基础。

Conclusion: 需要进一步研究将简单的实验转化为可操作的系统，使LLM辅助的语言理解成为集成机器人助手的一部分，实现真正的人机语言协作。

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [277] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: 本文研究了自动驾驶目标检测中的数据偏移问题，提出了基于CycleGAN数据增强和YOLOv5框架的优化方法，在BDD100K数据集上取得了优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中机器学习模型严重依赖训练和测试数据满足独立同分布假设，但现实应用中季节变化、天气波动等因素导致数据分布动态变化，引发数据偏移问题，影响模型性能。

Method: 系统分析数据偏移问题的复杂性和表现形式，综述数据偏移检测方法，使用偏移检测技术进行数据集分类和平衡，构建目标检测模型，并集成CycleGAN数据增强技术与YOLOv5框架进行优化。

Result: 在BDD100K数据集上的实验结果表明，该方法相比基线模型取得了更优越的性能。

Conclusion: 通过系统分析数据偏移问题并采用适当的数据增强技术，可以有效提升自动驾驶目标检测模型在真实环境中的鲁棒性和性能表现。

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [278] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: 开发了一个基于三台步进电机、微控制器和YOLO检测模型的魔方自动求解系统，通过Kociemba算法求解，平均求解时间约2.2分钟


<details>
  <summary>Details</summary>
Motivation: 构建一个完整的魔方自动求解系统，实现从状态检测到物理求解的全自动化流程，并通过用户友好的GUI界面展示求解过程

Method: 使用三个步进电机进行物理操作，微控制器控制硬件，YOLOv8模型实时检测魔方状态（精度0.98443，召回率0.98419），Unity开发GUI界面，Kociemba算法生成求解方案

Result: 系统成功实现了魔方的自动检测和求解，YOLOv8模型表现出色，平均求解时间约为2.2分钟

Conclusion: 该系统证明了结合计算机视觉、算法求解和机械控制的可行性，为自动化魔方求解提供了一个完整的技术方案

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [279] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD是一种通过触觉交互重建可变形物体形状和力学特性的新方法，使用弹性静力学SDF模型，结合力控表面探测来估计软材料的静态和动态响应。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖纯几何或视觉数据，无法有效估计可变形物体的力学特性。PROD通过整合触觉交互测量，旨在同时恢复物体的形状和材料属性。

Method: 将物体变形建模为弹性静力学过程，推导出控制泊松方程，从稀疏的姿态和力测量中估计SDF。结合稳态弹性动力学假设，从变形观测中恢复未变形SDF，并通过分析位移响应估计材料刚度。

Result: PROD在处理姿态误差、非法向力施加和曲率误差方面表现出鲁棒性，在模拟软体交互中验证了其有效性。

Conclusion: PROD为机器人操作、医学成像和触觉反馈系统等应用中的可变形物体重建提供了强大工具，能够同时恢复几何形状和力学特性。

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [280] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 这篇论文提出了一种专门为事件相机多传感器系统设计的无标定物模基时间和旋转检定框架，通过角速度估计和两步优化实现了高精度检定。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级延迟优势，但多传感器融合中的外参检定研究较少，传统方法依赖于事件-帧转换和专门标定物，存在限制。

Method: 方法利用法向流观测估角速度，采用两步检定流程：首先通过典型相关分析(CCA)初始化时间偏移和旋转外参，然后使用SO(3)连续时间参数化进行非线性优化精细检定。

Result: 在公开和自收数据集上的广泛评估显示，该方法达到了与基于标定物方法相当的检定精度，同时具有更好的稳定性、精确性、稳健性和灵活性。

Conclusion: 该研究提供了一种无需专门检定标定物的高精度外参检定方案，为事件相机多传感器系统的实际应用提供了重要技术支撑，并将开源代码以促进未来研究。

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [281] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA框架通过将动作预测直接基于相机观测空间，解决了VLA模型中观测与动作空间不一致的问题，提高了模型对相机视角变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在泛化到真实环境时面临观测空间和动作空间不一致的挑战，特别是在不同相机视角下预测末端执行器位姿时会出现空间不一致问题。

Method: 利用相机外参标定矩阵，将末端执行器位姿从机器人基坐标系转换到相机坐标系，实现异质视角下预测目标的统一，这是一个轻量级即插即用策略。

Result: 在仿真和真实机器人操作任务上的综合评估表明，OC-VLA加速了收敛速度，提高了任务成功率，并改善了跨视角泛化能力。

Conclusion: OC-VLA框架有效解决了VLA模型中的空间不一致问题，无需对现有架构进行重大修改，显著提升了模型对相机视角变化的适应性和鲁棒性。

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [282] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC是首个基于LLM的约束逆合成规划代理框架，通过工具化推理实现约束评估，在48个约束任务上达到72.9%的成功率，接近人类专家水平但耗时更少。


<details>
  <summary>Details</summary>
Motivation: 约束逆合成规划是化学中的重要但具有挑战性的过程，需要从商业可用起始材料到目标分子的合成路线识别，同时满足实际约束条件。现有方法难以有效处理这些约束。

Method: LARC框架采用代理作为评判员（Agent-as-a-Judge），将代理约束评估直接集成到逆合成规划过程中，使用基于工具推理的代理反馈来指导和约束路线生成。

Result: 在精心策划的48个约束逆合成规划任务上，LARC达到72.9%的成功率，显著优于LLM基线方法，在更短时间内接近人类专家水平。

Conclusion: LARC框架具有可扩展性，是朝着为人类专家提供有效代理工具或合作科学家的第一步，可用于约束逆合成规划。

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [283] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 提出了CHBench评估框架，基于认知层次模型来评估大语言模型的策略推理能力，发现聊天机制会降低策略推理，而记忆机制能提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖效用性能指标评估LLMs的策略推理能力，但这些指标不够稳健，受对手行为和游戏结构变化影响较大。需要更可靠的评估框架。

Method: 提出CHBench评估框架，基于行为经济学的认知层次模型，假设智能体具有有限理性。通过三阶段系统框架，在15个精选的正规形式游戏中评估6个最先进LLMs的行为数据。

Result: 实验显示LLMs在不同对手间展现一致的策略推理水平，证实了框架的稳健性和泛化能力。聊天机制显著降低策略推理，而记忆机制能提升推理性能。

Conclusion: CHBench是评估LLM能力的有前景工具，具有重要的未来研究和实际应用潜力，为理解LLMs的策略推理能力提供了新的视角和方法。

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [284] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided Chain-of-Thought (CoT) 通过将轻量级符号表示整合到少样本提示中，改进了标准CoT方法，在LLM的逻辑推理中提高了透明度、可解释性和可分析性，在多个复杂推理基准测试中显著优于传统CoT方法。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型在逻辑推理任务中的表现，特别是增强推理过程的透明度、可解释性和可分析性，同时保持标准提示技术的泛化能力。

Method: 在少样本提示中整合轻量级符号表示，使用一致的策略构建推理步骤，使推理模式在非迭代推理过程中更加明确。

Result: 在ProofWriter、FOLIO、ProntoQA和LogicalDeduction四个逻辑推理基准测试中，Symbolic-Aided CoT方法显著提升了LLM的推理能力，特别是在需要处理多重约束或规则的复杂推理任务中表现优异，在三个数据集上明显优于传统CoT方法。

Conclusion: Symbolic-Aided CoT是一种有效的改进方法，能够显著提升LLM在逻辑推理任务中的性能，同时增强推理过程的透明度和可解释性，适用于不同规模的模型。

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [285] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: 提出结合大语言模型(LLM)和答案集编程(ASP)的联合实体关系抽取工作流，无需大量标注数据，利用LLM的自然语言理解能力和ASP的知识表示推理能力，在少量训练数据下超越现有最佳系统


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量标注数据且难以融入领域知识，构建模型费时费力。为解决这些问题，利用LLM和ASP的优势来开发通用、高效的联合实体关系抽取方法

Method: 设计通用的LLM+ASP工作流：LLM处理未标注文本进行自然语言理解，ASP负责知识表示和推理，通过类型规范形式融入领域知识而无需修改核心程序

Result: 在三个知名基准测试上，仅使用10%训练数据就超越现有最佳系统。在难度最高的SciERC语料库上，关系抽取任务性能提升2.5倍（从15%提升到35%）

Conclusion: LLM+ASP工作流为联合实体关系抽取提供了有效解决方案，具有领域通用性、低数据需求和强扩展性，在资源受限场景下表现优异

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [286] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文提出了基于评分标准的RLVR方法，将可验证奖励学习扩展到开放式任务，通过构建包含10,000+评分标准的系统，在少量样本下显著提升模型性能并实现细粒度风格控制。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法局限于可自动验证结果的领域，无法应用于开放式主观任务。为了突破这一限制，需要开发能够处理主观输出的奖励机制。

Method: 采用基于评分标准的奖励系统，通过人工、LLM或人机协作构建结构化评分标准，作为模型可解释的自动评分标准，并提出了清晰的实施框架。

Result: 仅用5K+样本就在开放式基准测试上提升5.2%，在人文领域表现突出，超越671B参数的DeepSeek-V3模型2.4%，同时保持通用和推理能力，还能实现细粒度风格控制。

Conclusion: 评分标准RL方法成功扩展了RLVR的应用范围，为开放式任务提供了有效的训练范式，在提升性能的同时实现了更好的风格控制，具有重要实践价值。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [287] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG是一个基于多模态大语言模型的显式情感驱动共情响应生成系统，通过分解任务为多模态共情理解、共情记忆检索和多模态响应生成三部分，无需额外训练即可生成自然、情感丰富且身份一致的多模态响应。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型改进了基于文本的共情响应生成，但在处理多模态情感内容和保持身份一致性方面仍存在挑战，需要构建更有效的情感智能人机交互系统。

Method: 将多模态共情响应生成任务分解为三个部分：多模态共情理解、共情记忆检索和多模态响应生成，并集成先进的表达性语音和视频生成模型。

Result: 在零样本和少样本设置下验证了系统的优越性，在ACM MM 25的Avatar-based Multimodal Empathy Challenge中获得Top-1排名。

Conclusion: E3RG系统能够有效解决多模态情感内容处理和身份一致性问题，为构建情感智能人机交互提供了有效解决方案。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [288] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler是一种新的掩码扩散模型解码策略，通过位置感知权重和置信度校准来解决现有方法在全局轨迹控制和早期解码偏差方面的问题，在多个基准测试中平均提升10%以上性能。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性采样器存在两个关键限制：缺乏全局轨迹控制和早期解码阶段对平凡token的明显偏差，这限制了掩码扩散模型的全部潜力。

Method: 提出了位置感知置信度校准采样（PC-Sampler），结合了全局轨迹规划和内容感知信息最大化，包含位置感知权重机制来调节解码路径和校准置信度分数来抑制平凡token的过早选择。

Result: 在三个先进MDM模型和七个具有挑战性的基准测试（包括逻辑推理和规划任务）上的广泛实验表明，PC-Sampler平均比现有MDM解码策略性能提升超过10%，显著缩小了与最先进自回归模型的性能差距。

Conclusion: PC-Sampler是一种有效的解码策略，能够显著提升掩码扩散模型的生成质量，在多个任务上表现优异。

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [289] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: Bongard-RWR+是一个包含5400个实例的抽象视觉推理数据集，使用VLM生成真实世界图像来代表原始Bongard问题的抽象概念，评估发现VLM在细粒度概念识别上存在困难


<details>
  <summary>Details</summary>
Motivation: 早期Bongard问题数据集使用合成黑白图像，不能完全捕捉真实世界复杂性；后续数据集使用真实图像但概念可从高层特征识别，降低了任务难度；Bongard-RWR数据集虽使用细粒度真实图像但规模太小（仅60个实例），限制了评估稳健性

Method: 基于Bongard-RWR，使用Pixtral-12B描述手动策划的图像并生成与底层概念对齐的新描述，使用Flux.1-dev从这些描述合成图像，并手动验证生成图像是否忠实反映预期概念

Result: 评估了最先进的VLM在不同Bongard问题表述上的表现，包括二元和多类分类以及文本答案生成。发现VLM能够识别粗粒度视觉概念，但在辨别细粒度概念方面持续困难

Conclusion: VLM在抽象视觉推理任务中表现出局限性，特别是在细粒度概念识别方面，突显了其推理能力的不足

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [290] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion是首个评估多模态大语言模型在自我中心视频中幻觉问题的基准，包含1400个视频和8000个人工标注问题，测试显示包括GPT-4o和Gemini在内的顶级模型准确率仅59%


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在第三人称和自我中心视频中表现出色，但容易产生连贯但不准确的幻觉回答，需要专门的基准来评估和解决这个问题

Method: 构建EgoIllusion基准数据集，包含1400个自我中心视频和8000个人工标注的开放性和封闭性问题，专门设计来触发视觉和听觉线索的幻觉

Result: 在10个多模态大语言模型上的评估显示显著挑战，最强大的模型如GPT-4o和Gemini准确率仅为59%，表明现有模型在自我中心视频理解方面存在严重幻觉问题

Conclusion: EgoIllusion为评估多模态大语言模型有效性奠定了基础，将推动开发幻觉率更低的自我中心多模态大语言模型，该基准将开源以确保可复现性

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>
